[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.12240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v3",
                "updated": "2025-05-06T15:23:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    23,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v1",
                "updated": "2025-05-05T18:01:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02533v1",
                "updated": "2025-05-05T10:16:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:16:16Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Partitioning for Low-Latency Inference at the Edge"
                },
                "summary": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches."
                },
                "authors": [
                    {
                        "name": "Dimitrios Kafetzis"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Iordanis Koutsopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Iordanis Koutsopoulos"
                },
                "author": "Iordanis Koutsopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02346v1",
                "updated": "2025-05-05T04:01:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T04:01:56Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "title": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code"
                },
                "summary": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility."
                },
                "authors": [
                    {
                        "name": "Vincenzo Stoico"
                    },
                    {
                        "name": "Andrei Calin Dragomir"
                    },
                    {
                        "name": "Patricia Lago"
                    }
                ],
                "author_detail": {
                    "name": "Patricia Lago"
                },
                "author": "Patricia Lago",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02082v1",
                "updated": "2025-05-04T12:21:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    12,
                    21,
                    16,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T12:21:16Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    12,
                    21,
                    16,
                    6,
                    124,
                    0
                ],
                "title": "Performance Characterization of Containers in Edge Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization of Containers in Edge Computing"
                },
                "summary": "This paper presents an empirical evaluation of container-based virtualization\non embedded operating systems commonly used in Internet of Things (IoT)\ndeployments. Focusing on platforms like the Raspberry Pi, we investigate the\nfeasibility and performance implications of deploying Docker containers in\nresource-constrained edge environments. Our study employs both microbenchmarks\n(CPU, memory, and network profiling) and macrobenchmarks (AI-driven inference,\nsensor IO workloads) to capture a comprehensive view of system behavior. The\nanalysis is conducted on a custom-built physical testbed comprising Raspberry\nPi devices equipped with environmental sensors and camera modules, enabling\nreal-time deployment and measurement of representative IoT workloads. Through\nquantitative analysis across a diverse suite of IoT tasks and real-time\napplication services, we identify key overheads introduced by containerization\nand characterize challenges specific to embedded IoT contexts, including\nlimited hardware resources, cold-start delays, and suboptimal IO handling.\nPerformance metrics include CPU utilization, memory faults, cache misses,\nnetwork throughput, and latency. Our findings highlight trade-offs between\nisolation and efficiency and offer insights for optimizing container\nconfigurations to meet the real-time and reliability requirements of edge\ncomputing applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an empirical evaluation of container-based virtualization\non embedded operating systems commonly used in Internet of Things (IoT)\ndeployments. Focusing on platforms like the Raspberry Pi, we investigate the\nfeasibility and performance implications of deploying Docker containers in\nresource-constrained edge environments. Our study employs both microbenchmarks\n(CPU, memory, and network profiling) and macrobenchmarks (AI-driven inference,\nsensor IO workloads) to capture a comprehensive view of system behavior. The\nanalysis is conducted on a custom-built physical testbed comprising Raspberry\nPi devices equipped with environmental sensors and camera modules, enabling\nreal-time deployment and measurement of representative IoT workloads. Through\nquantitative analysis across a diverse suite of IoT tasks and real-time\napplication services, we identify key overheads introduced by containerization\nand characterize challenges specific to embedded IoT contexts, including\nlimited hardware resources, cold-start delays, and suboptimal IO handling.\nPerformance metrics include CPU utilization, memory faults, cache misses,\nnetwork throughput, and latency. Our findings highlight trade-offs between\nisolation and efficiency and offer insights for optimizing container\nconfigurations to meet the real-time and reliability requirements of edge\ncomputing applications."
                },
                "authors": [
                    {
                        "name": "Ragini Gupta"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10375v2",
                "updated": "2025-05-04T09:49:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    9,
                    49,
                    42,
                    6,
                    124,
                    0
                ],
                "published": "2024-12-16T07:59:21Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    59,
                    21,
                    0,
                    351,
                    0
                ],
                "title": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Yujie Zhang"
                    },
                    {
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "name": "Tulika Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tulika Mitra"
                },
                "author": "Tulika Mitra",
                "arxiv_comment": "7 pages, 10 figures, Accepted by DATE Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02027v1",
                "updated": "2025-05-04T08:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T08:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "title": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning"
                },
                "summary": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter."
                },
                "authors": [
                    {
                        "name": "Rui Lv"
                    },
                    {
                        "name": "Zaixi Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Weibo Gao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiaxia Yan"
                    },
                    {
                        "name": "Linan Yue"
                    },
                    {
                        "name": "Fangzhou Yao"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhou Yao"
                },
                "author": "Fangzhou Yao",
                "arxiv_comment": "14 pages. IEEE International Conference on Data Engineering\n  (ICDE'2025), accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v3",
                "updated": "2025-05-03T04:07:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    4,
                    7,
                    7,
                    5,
                    123,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01658v1",
                "updated": "2025-05-03T02:47:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "published": "2025-05-03T02:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency"
                },
                "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"
                },
                "authors": [
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Sungryeol Jeon"
                    },
                    {
                        "name": "Chaelyn Lee"
                    },
                    {
                        "name": "Seokhun Jeon"
                    },
                    {
                        "name": "Byung-Soo Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jemin Lee"
                },
                "author": "Jemin Lee",
                "arxiv_comment": "Under review; 65 pages; 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20335v2",
                "updated": "2025-05-03T01:10:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    1,
                    10,
                    30,
                    5,
                    123,
                    0
                ],
                "published": "2025-04-29T00:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits"
                },
                "summary": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    },
                    {
                        "name": "Duo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Duo Wang"
                },
                "author": "Duo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v3",
                "updated": "2025-05-02T13:55:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    55,
                    21,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v4",
                "updated": "2025-05-02T11:29:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    11,
                    29,
                    31,
                    4,
                    122,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01164v1",
                "updated": "2025-05-02T10:13:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:13:12Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "title": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems"
                },
                "summary": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Kyuli Park"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v1",
                "updated": "2025-05-02T04:57:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazán"
                    },
                    {
                        "name": "V. Álvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodríguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Cárcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cossío"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Díaz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. García-Barrena"
                    },
                    {
                        "name": "J. J. Gómez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gómez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervés Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. López-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martín-Albo"
                    },
                    {
                        "name": "G. Martínez-Lema"
                    },
                    {
                        "name": "M. Martínez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuñez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Pérez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simón"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnelé"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usón"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00962v1",
                "updated": "2025-05-02T02:36:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T02:36:23Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "title": "The Open-Source BlackParrot-BedRock Cache Coherence System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open-Source BlackParrot-BedRock Cache Coherence System"
                },
                "summary": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines."
                },
                "authors": [
                    {
                        "name": "Mark Unruh Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Mark Unruh Wyse"
                },
                "author": "Mark Unruh Wyse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v1",
                "updated": "2025-05-01T22:32:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00817v1",
                "updated": "2025-05-01T19:18:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:18:56Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models"
                },
                "summary": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains."
                },
                "authors": [
                    {
                        "name": "Andrew Adiletta"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v1",
                "updated": "2025-05-01T18:00:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "19 pages, 9 figures. Supplement 29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00570v1",
                "updated": "2025-05-01T14:53:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T14:53:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension"
                },
                "summary": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Boyi Zeng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00315v1",
                "updated": "2025-05-01T05:22:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T05:22:11Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing"
                },
                "summary": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines."
                },
                "authors": [
                    {
                        "name": "Piotr Piękos"
                    },
                    {
                        "name": "Róbert Csordás"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04532v3",
                "updated": "2025-05-01T02:14:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    2,
                    14,
                    5,
                    3,
                    121,
                    0
                ],
                "published": "2024-05-07T17:59:30Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    17,
                    59,
                    30,
                    1,
                    128,
                    0
                ],
                "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving"
                },
                "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Chuang Gan"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first three authors contribute equally to this project and are\n  listed in the alphabetical order. Yujun Lin leads the quantization algorithm,\n  Haotian Tang and Shang Yang lead the GPU kernels and the serving system. Code\n  is available at https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19602v2",
                "updated": "2025-05-01T00:13:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    0,
                    13,
                    6,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation"
                },
                "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v2",
                "updated": "2025-04-30T19:48:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    19,
                    48,
                    41,
                    2,
                    120,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00074v1",
                "updated": "2025-04-30T18:00:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T18:00:02Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "title": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O"
                },
                "summary": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research."
                },
                "authors": [
                    {
                        "name": "Xu Yan"
                    },
                    {
                        "name": "Ziyin Song"
                    },
                    {
                        "name": "Juntao Song"
                    },
                    {
                        "name": "Zhong Fang"
                    },
                    {
                        "name": "Hongming Weng"
                    },
                    {
                        "name": "Quansheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Quansheng Wu"
                },
                "author": "Quansheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21594v1",
                "updated": "2025-04-30T12:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:51:59Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "title": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations"
                },
                "summary": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency."
                },
                "authors": [
                    {
                        "name": "Y. Xiang"
                    },
                    {
                        "name": "L. Wu"
                    },
                    {
                        "name": "K. Velitsikakis"
                    },
                    {
                        "name": "A. L. J. Janssen"
                    }
                ],
                "author_detail": {
                    "name": "A. L. J. Janssen"
                },
                "author": "A. L. J. Janssen",
                "arxiv_comment": "11 pages, 17 figures, CIGRE conference 2016",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00745v1",
                "updated": "2025-04-30T08:08:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:08:15Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "title": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations"
                },
                "summary": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively."
                },
                "authors": [
                    {
                        "name": "Maozhe Zhao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Sensys 2025 final version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21230v1",
                "updated": "2025-04-29T23:43:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:43:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "Kimina Lean Server: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimina Lean Server: Technical Report"
                },
                "summary": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub."
                },
                "authors": [
                    {
                        "name": "Marco Dos Santos"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Hugues de Saxcé"
                    },
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Mantas Baksys"
                    },
                    {
                        "name": "Mert Unsal"
                    },
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21228v1",
                "updated": "2025-04-29T23:42:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:42:21Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks"
                },
                "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12322v2",
                "updated": "2025-04-29T17:54:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    54,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-01-21T17:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    41,
                    54,
                    1,
                    21,
                    0
                ],
                "title": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel"
                },
                "summary": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC."
                },
                "authors": [
                    {
                        "name": "Yinbin Ma"
                    },
                    {
                        "name": "Daniela Tuninetti"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Tuninetti"
                },
                "author": "Daniela Tuninetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v2",
                "updated": "2025-04-29T14:25:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    25,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20246v1",
                "updated": "2025-04-28T20:30:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T20:30:59Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "title": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks"
                },
                "summary": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility."
                },
                "authors": [
                    {
                        "name": "Yu Mi"
                    },
                    {
                        "name": "Randeep Bhatia"
                    },
                    {
                        "name": "Fang Hao"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Steve Benno"
                    },
                    {
                        "name": "Tv Lakshman"
                    }
                ],
                "author_detail": {
                    "name": "Tv Lakshman"
                },
                "author": "Tv Lakshman",
                "arxiv_comment": "Accepted by IEEE INFOCOM 2025-IEEE Conference on Computer\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v3",
                "updated": "2025-04-28T17:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    17,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "IEEE Internet of Things Journal (Accepted for publication). The\n  Hierarchical coded caching scheme in this updated version unifies the scheme\n  in the previous version and the schemes in arxiv:2402.07188. This version\n  includes a more comprehensive performance analysis. To reflect these the\n  title has been updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19984v1",
                "updated": "2025-04-28T16:59:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:59:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation"
                },
                "summary": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications."
                },
                "authors": [
                    {
                        "name": "Rodrigo Cataldo"
                    },
                    {
                        "name": "Cesar Marcon"
                    },
                    {
                        "name": "Debora Matos"
                    }
                ],
                "author_detail": {
                    "name": "Debora Matos"
                },
                "author": "Debora Matos",
                "arxiv_comment": "Progress Seminar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19874v1",
                "updated": "2025-04-28T15:05:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:05:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate"
                },
                "summary": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Majid Hadian"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19867v1",
                "updated": "2025-04-28T15:00:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage"
                },
                "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."
                },
                "authors": [
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Lufang Chen"
                    },
                    {
                        "name": "Zhong Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Qiuli Mao"
                    },
                    {
                        "name": "Jianping Ma"
                    },
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Guanyu Wu"
                    },
                    {
                        "name": "Buhe Han"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19601v1",
                "updated": "2025-04-28T09:03:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:03:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate"
                },
                "summary": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small."
                },
                "authors": [
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Kang"
                },
                "author": "Wei Kang",
                "arxiv_comment": "Submitted to IEEE Transactions on Information Theory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19561v1",
                "updated": "2025-04-28T08:12:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:12:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Quantifying Memory Utilization with Effective State-Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memory Utilization with Effective State-Size"
                },
                "summary": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models."
                },
                "authors": [
                    {
                        "name": "Rom N. Parnichkun"
                    },
                    {
                        "name": "Neehal Tumma"
                    },
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Alessandro Moro"
                    },
                    {
                        "name": "Qi An"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Atsushi Yamashita"
                    },
                    {
                        "name": "Michael Poli"
                    },
                    {
                        "name": "Stefano Massaroli"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Massaroli"
                },
                "author": "Stefano Massaroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v1",
                "updated": "2025-04-28T04:31:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v2",
                "updated": "2025-04-28T04:02:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    2,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v3",
                "updated": "2025-04-28T02:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    58,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v1",
                "updated": "2025-04-27T22:05:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19266v1",
                "updated": "2025-04-27T14:46:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T14:46:43Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System"
                },
                "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Jin"
                    },
                    {
                        "name": "Matteo Frosi"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19191v1",
                "updated": "2025-04-27T10:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T10:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "title": "WuNeng: Hybrid State with Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WuNeng: Hybrid State with Attention"
                },
                "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v2",
                "updated": "2025-04-26T12:07:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    26,
                    12,
                    7,
                    35,
                    5,
                    116,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "arxiv_comment": "Accepted to IEEE S&P 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v3",
                "updated": "2025-04-25T19:40:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    19,
                    40,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18434v1",
                "updated": "2025-04-25T15:45:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:45:36Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "title": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs"
                },
                "summary": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings."
                },
                "authors": [
                    {
                        "name": "Javad Maheri"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18432v1",
                "updated": "2025-04-25T15:44:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:44:38Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "title": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack"
                },
                "summary": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer."
                },
                "authors": [
                    {
                        "name": "Xuzheng Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Baolin Zhu"
                    },
                    {
                        "name": "Xueying Zhu"
                    },
                    {
                        "name": "Zhongqing Chen"
                    },
                    {
                        "name": "Shu Ma"
                    },
                    {
                        "name": "Lingjun Zhu"
                    },
                    {
                        "name": "Chao Shi"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18242v1",
                "updated": "2025-04-25T10:43:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:43:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Demand Private Coded Caching: Small Cache Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Private Coded Caching: Small Cache Size"
                },
                "summary": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users."
                },
                "authors": [
                    {
                        "name": "Qinyi Lu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18082v1",
                "updated": "2025-04-25T05:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching"
                },
                "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches."
                },
                "authors": [
                    {
                        "name": "Vignesh Balaji"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v2",
                "updated": "2025-04-25T05:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    8,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v2",
                "updated": "2025-04-25T05:05:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    5,
                    49,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_journal_ref": "Science Bulletin 70, 1211-1214 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v1",
                "updated": "2025-04-25T00:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17866v1",
                "updated": "2025-04-24T18:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T18:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "Updated parameters of the LArQL model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Updated parameters of the LArQL model"
                },
                "summary": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented."
                },
                "authors": [
                    {
                        "name": "L. Paulucci"
                    },
                    {
                        "name": "F. Cavanna"
                    },
                    {
                        "name": "V. Vale"
                    },
                    {
                        "name": "F. Marinho"
                    }
                ],
                "author_detail": {
                    "name": "F. Marinho"
                },
                "author": "F. Marinho",
                "arxiv_comment": "Part of the proceedings of LIDINE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17554v1",
                "updated": "2025-04-24T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Rethinking PM Crash Consistency in the CXL Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking PM Crash Consistency in the CXL Era"
                },
                "summary": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools."
                },
                "authors": [
                    {
                        "name": "João Oliveira"
                    },
                    {
                        "name": "João Gonçalves"
                    },
                    {
                        "name": "Miguel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Matos"
                },
                "author": "Miguel Matos",
                "arxiv_comment": "5 pages (2 extra pages for references), 1 figure, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v3",
                "updated": "2025-04-24T08:39:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    39,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v2",
                "updated": "2025-04-24T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    36,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v2",
                "updated": "2025-04-23T18:02:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    18,
                    2,
                    55,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we demonstrate that distinctive keys during LLM inference tend\nto have high attention scores. We explore this phenomenon and propose KeyDiff,\na training-free KV cache eviction method based on key similarity. This method\nfacilitates the deployment of LLM-based application requiring long input\nprompts in resource-constrained environments with limited memory and compute\nbudgets. Unlike other KV cache eviction methods, KeyDiff can process\narbitrarily long prompts within strict resource constraints and efficiently\ngenerate responses. We demonstrate that KeyDiff computes the optimal solution\nto a KV cache selection problem that maximizes key diversity, providing a\ntheoretical understanding of KeyDiff. Notably,KeyDiff does not rely on\nattention scores, allowing the use of optimized attention mechanisms like\nFlashAttention. We demonstrate the effectiveness of KeyDiff across diverse\ntasks and models, illustrating a performance gap of less than 0.04\\% with 8K\ncache budget ($\\sim$ 23\\% KV cache reduction) from the non-evicting baseline on\nthe LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matt J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "8 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15437v2",
                "updated": "2025-04-23T15:02:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    15,
                    2,
                    16,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-21T21:01:57Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    21,
                    1,
                    57,
                    0,
                    111,
                    0
                ],
                "title": "Iris: A Next Generation Digital Pathology Rendering Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: A Next Generation Digital Pathology Rendering Engine"
                },
                "summary": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital pathology is a tool of rapidly evolving importance within the\ndiscipline of pathology. Whole slide imaging promises numerous advantages;\nhowever, adoption is limited by challenges in ease of use and speed of\nhigh-quality image rendering relative to the simplicity and visual quality of\nglass slides. We introduce Iris, a new high-performance digital pathology\nrendering system. Specifically, we outline and detail the performance metrics\nof Iris Core, the core rendering engine technology. Iris Core comprises machine\ncode modules written from the ground up in C++ and using Vulkan, a low-level\nand low-overhead cross-platform graphical processing unit application program\ninterface, and our novel rapid tile buffering algorithms. We provide a detailed\nexplanation of Iris Core's system architecture, including the stateless\nisolation of core processes, interprocess communication paradigms, and explicit\nsynchronization paradigms that provide powerful control over the graphical\nprocessing unit. Iris Core achieves slide rendering at the sustained maximum\nframe rate on all tested platforms and buffers an entire new slide field of,\nview without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is\nable to buffer and compute high-fidelity reduction-enhancements for viewing\nlow-power cytology with increased visual quality at a rate of 100-160 us per\nslide tile, and with a cumulative median buffering rate of 1.36 GB of\ndecompressed image data per second. This buffering rate allows for an entirely\nnew field of view to be fully buffered and rendered in less than a single\nmonitor refresh on a standard display, and high detail features within 2-3\nmonitor refresh frames. These metrics far exceed previously published\nspecifications, beyond an order of magnitude in some contexts. The system shows\nno slowing with high use loads, but rather increases performance due to cache\nmechanisms."
                },
                "authors": [
                    {
                        "name": "Ryan Erik Landvater"
                    },
                    {
                        "name": "Ulysses Balis"
                    }
                ],
                "author_detail": {
                    "name": "Ulysses Balis"
                },
                "author": "Ulysses Balis",
                "arxiv_doi": "10.1016/j.jpi.2024.100414",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jpi.2024.100414",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.15437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_journal_ref": "Journal of Pathology Informatics, 16, 100414 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v2",
                "updated": "2025-04-23T10:48:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    10,
                    48,
                    52,
                    2,
                    113,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a cloud server\noccupying a unique position. Not only is network performance vital to efficient\noperation of the machine, but unlike compute accelerators like GPUs, the\nnetwork subsystem must react to unpredictable events like the arrival of a\nnetwork packet and communicate with the appropriate application end point with\nminimal latency.\n  Current approaches to server stacks navigate a trade-off between flexibility,\nefficiency, and performance: the fastest kernel-bypass approaches dedicate\ncores to applications, busy-wait on receive queues, etc. while more flexible\napproaches appropriate to more dynamic workload mixes incur much greater\nsoftware overhead on the data path.\n  However, we reject this trade-off, which we ascribe to an arbitrary (and\nsub-optimal) split in system state between the OS and the NIC. Instead, by\nexploiting the properties of cache-coherent interconnects and integrating the\nNIC closely with the OS kernel, we can achieve something surprising:\nperformance for RPC workloads better than the fastest kernelbypass approaches\nwithout sacrificing the robustness and dynamic adaptation of kernel-based\nnetwork subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3713082.3730388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.10138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera ready for HotOS'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v2",
                "updated": "2025-04-23T05:04:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    5,
                    4,
                    58,
                    2,
                    113,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Caching through Attention Output Error based Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v2",
                "updated": "2025-04-23T04:21:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    23,
                    4,
                    21,
                    49,
                    2,
                    113,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e they do not adapt to changing cache access\npatterns. Newer developments such as the High-Luminosity - Large Hadron\nCollider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move\ntoward streaming readout based Data Acquisition systems (DAQs) will increase\nthe data production exponentially and hence burden the storage, compute &\nnetwork infrastructures. Moreover, existing caching frameworks are optimized to\nreduce latency, but not optimized for storage. This, in combination with\nlimited cache capacities relative to total data, makes it difficult to achieve\ndata locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, we first present a Long Short-Term Memory-based (LSTM) hourly and\nmulti-step cache usage prediction. Second, we present an hourly file-level\naccess prediction model based on CatboostRegressor. To date, most ML-based\ncache prediction strategies in HEP have focused on daily cache usage and\nlimited works tackled hourly cache usage and even fewer strategies addressed\nhourly file-level access prediction. File-level access prediction allows for\nthe design of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending the\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16324v1",
                "updated": "2025-04-22T23:52:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T23:52:13Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    23,
                    52,
                    13,
                    1,
                    112,
                    0
                ],
                "title": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dawn of Disaggregation and the Coherence Conundrum: A Call for\n  Federated Coherence"
                },
                "summary": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory is an upcoming data center technology that will allow\nnodes (servers) to share data efficiently. Sharing data creates a debate on the\nlevel of cache coherence the system should provide. While current proposals aim\nto provide coherence for all or parts of the disaggregated memory, we argue\nthat this approach is problematic, because of scalability limitations and\nhardware complexity. Instead, we propose and formally define federated\ncoherence, a model that provides coherence only within nodes, not across nodes.\nFederated coherence can use current intra-node coherence provided by processors\nwithout requiring expensive mechanisms for inter-node coherence. Developers can\nuse federated coherence with a few simple programming paradigms and a\nsynchronization library. We sketch some potential applications."
                },
                "authors": [
                    {
                        "name": "Jaewan Hong"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Emmanuel Amaro"
                    },
                    {
                        "name": "Vincent Liu"
                    },
                    {
                        "name": "Aurojit Panda"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v3",
                "updated": "2025-04-22T17:34:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    34,
                    34,
                    1,
                    112,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Presented at IEEE Custom Integrated Circuits Conference (CICC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v2",
                "updated": "2025-04-22T17:23:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    23,
                    28,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, there is a need for\nhigher-density on-chip memory for domain-specific accelerators that goes beyond\nwhat current SRAM technology can provide. We motivate that algorithms and\napplication behavior should guide the composition of heterogeneous on-chip\nmemories. However, there has been little work in factoring dynamic application\nprofiles into such design decisions. We present GainSight, a profiling\nframework that analyzes fine-grained memory access patterns and computes data\nlifetimes in domain-specific accelerators. By combining instrumentation and\nsimulation across retargetable hardware backends, GainSight aligns\nheterogeneous memory designs with workload-specific traffic and lifetime\nmetrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA\nH100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2\nGPU cache accesses, and 79% of systolic array scratchpad accesses across\nprofiled workloads are short-lived and suitable for silicon-based gain cell RAM\n(Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3)\nUp to 90% of GPU cache fetches are never reused, highlighting inefficiencies in\nterms of cache pollution. These insights that GainSight provides can be used to\nbetter understand the design spaces of both emerging on-chip memories and\nsoftware algorithmic optimizations for the next generation of AI accelerators."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "15 pages, 10 figures. Updated references and author name presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14489v2",
                "updated": "2025-04-22T15:19:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    15,
                    19,
                    48,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-20T04:46:34Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    4,
                    46,
                    34,
                    6,
                    110,
                    0
                ],
                "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing"
                },
                "summary": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM services demand high throughput and stringent SLO guarantees\nacross two distinct inference phases-prefill and decode-and complex multi-turn\nworkflows. However, current systems face a fundamental tradeoff: out-of-place\ncompute partition enables per-phase SLO attainment, while in-place memory\nsharing maximizes throughput via KV cache reuse. Moreover, existing in-place\ncompute partition also encounters low utilization and high overhead due to\nphase-coupling design. We present Drift, a new LLM serving framework that\nresolves this tension via PD multiplexing, enabling in-place and\nphase-decoupled compute partition. Drift leverages low-level GPU partitioning\ntechniques to multiplex prefill and decode phases spatially and adaptively on\nshared GPUs, while preserving in-place memory sharing. To fully leverage the\nmultiplexing capability, Drift introduces an adaptive gang scheduling\nmechanism, a contention-free modeling method, and a SLO-aware dispatching\npolicy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput\nimprovement (up to $17.5\\times$) over state-of-the-art baselines, while\nconsistently meeting SLO targets under complex LLM workloads."
                },
                "authors": [
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15720v1",
                "updated": "2025-04-22T09:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "published": "2025-04-22T09:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    9,
                    8,
                    46,
                    1,
                    112,
                    0
                ],
                "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large\n  Language Model Inference"
                },
                "summary": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
                },
                "authors": [
                    {
                        "name": "Yihao Zhao"
                    },
                    {
                        "name": "Jiadun Chen"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v3",
                "updated": "2025-04-21T22:13:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    22,
                    13,
                    7,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v2",
                "updated": "2025-04-21T20:10:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    20,
                    10,
                    11,
                    0,
                    111,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "Accepted by MLSys 2025, code available at\n  http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15260v1",
                "updated": "2025-04-21T17:39:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:39:59Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    39,
                    59,
                    0,
                    111,
                    0
                ],
                "title": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Knowledge and Power Management for Secure Semantic Communication\n  Networks"
                },
                "summary": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, semantic communication (SemCom) has shown its great superiorities\nin resource savings and information exchanges. However, while its unique\nbackground knowledge guarantees accurate semantic reasoning and recovery,\nsemantic information security-related concerns are introduced at the same time.\nSince the potential eavesdroppers may have the same background knowledge to\naccurately decrypt the private semantic information transmitted between legal\nSemCom users, this makes the knowledge management in SemCom networks rather\nchallenging in joint consideration with the power control. To this end, this\npaper focuses on jointly addressing three core issues of power allocation,\nknowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in\nsecure SemCom networks. We first develop a novel performance metric, namely\nsemantic secrecy throughput (SST), to quantify the information security level\nthat can be achieved at each pair of D2D SemCom users. Next, an SST\nmaximization problem is formulated subject to secure SemCom-related delay and\nreliability constraints. Afterward, we propose a security-aware resource\nmanagement solution using the Lagrange primal-dual method and a two-stage\nmethod. Simulation results demonstrate our proposed solution nearly doubles the\nSST performance and realizes less than half of the queuing delay performance\ncompared to different benchmarks."
                },
                "authors": [
                    {
                        "name": "Xuesong Liu"
                    },
                    {
                        "name": "Yansong Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Fangzhou Zhao"
                    },
                    {
                        "name": "Le Xia"
                    },
                    {
                        "name": "Yao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yao Sun"
                },
                "author": "Yao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15247v1",
                "updated": "2025-04-21T17:22:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T17:22:18Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    17,
                    22,
                    18,
                    0,
                    111,
                    0
                ],
                "title": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lance: Efficient Random Access in Columnar Storage through Adaptive\n  Structural Encodings"
                },
                "summary": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing interest in artificial intelligence has created workloads that\nrequire both sequential and random access. At the same time, NVMe-backed\nstorage solutions have emerged, providing caching capability for large columnar\ndatasets in cloud storage. Current columnar storage libraries fall short of\neffectively utilizing an NVMe device's capabilities, especially when it comes\nto random access. Historically, this has been assumed an implicit weakness in\ncolumnar storage formats, but this has not been sufficiently explored. In this\npaper, we examine the effectiveness of popular columnar formats such as Apache\nArrow, Apache Parquet, and Lance in both random access and full scan tasks\nagainst NVMe storage.\n  We argue that effective encoding of a column's structure, such as the\nrepetition and validity information, is the key to unlocking the disk's\nperformance. We show that Parquet, when configured correctly, can achieve over\n60x better random access performance than default settings. We also show that\nthis high random access performance requires making minor trade-offs in scan\nperformance and RAM utilization. We then describe the Lance structural encoding\nscheme, which alternates between two different structural encodings based on\ndata width, and achieves better random access performance without making\ntrade-offs in scan performance or RAM utilization."
                },
                "authors": [
                    {
                        "name": "Weston Pace"
                    },
                    {
                        "name": "Chang She"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Will Jones"
                    },
                    {
                        "name": "Albert Lockett"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Raunak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Raunak Shah"
                },
                "author": "Raunak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v3",
                "updated": "2025-04-21T15:36:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    36,
                    53,
                    0,
                    111,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14866v2",
                "updated": "2025-04-21T15:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    13,
                    44,
                    0,
                    111,
                    0
                ],
                "published": "2025-02-20T18:59:52Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    59,
                    52,
                    3,
                    51,
                    0
                ],
                "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention"
                },
                "summary": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15021v1",
                "updated": "2025-04-21T11:09:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T11:09:43Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    11,
                    9,
                    43,
                    0,
                    111,
                    0
                ],
                "title": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?"
                },
                "summary": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies."
                },
                "authors": [
                    {
                        "name": "Xinglei Dou"
                    },
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Limin Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Limin Xiao"
                },
                "author": "Limin Xiao",
                "arxiv_comment": "25 pages, 14 figures, to be published in ACM Transactions on Storage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v3",
                "updated": "2025-04-21T03:40:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    3,
                    40,
                    10,
                    0,
                    111,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14775v1",
                "updated": "2025-04-21T00:07:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "published": "2025-04-21T00:07:49Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    0,
                    7,
                    49,
                    0,
                    111,
                    0
                ],
                "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM\n  Serving with Token Throttling"
                },
                "summary": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pipeline parallelism has emerged as a predominant approach for deploying\nlarge language models (LLMs) across distributed nodes, owing to its lower\ncommunication overhead compared to tensor parallelism. While demonstrating high\nthroughput in request serving, pipeline parallelism often suffers from\nperformance limitations caused by pipeline bubbles, which are primarily\nresulted from imbalanced computation delays across batches. Existing methods\nlike Sarathi-Serve attempt to address this through hybrid scheduling of chunked\nprefill and decode tokens using a fixed token budget. However, such methods may\nexperience significant fluctuations due to either insufficient prefill tokens\nor uneven distribution of decode tokens, ultimately leading to computational\nimbalance. To overcome these inefficiencies, we present gLLM, a globally\nbalanced pipeline parallelism system incorporating Token Throttling to\neffectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a\nfine-grained scheduling policy that independently regulates the quantities of\nprefill and decode tokens, thus enabling balanced computation by leveraging\nglobal information from the inference system. Specifically, for decode tokens,\ngLLM maintains near-consistent token count across processing batches. For\nprefill tokens, it dynamically adjusts batch sizes based on both total pending\ntokens and the memory utilization rates of key-value cache (KV cache).\nFurthermore, gLLM runtime adopts an asynchronous execution and message passing\narchitecture specifically optimized for pipeline parallelism characteristics.\nExperimental evaluations with representative LLMs show that gLLM achieves\nsignificant performance improvements, delivering 11% to 398% higher maximum\nthroughput compared to state-of-the-art pipeline or tensor parallelism systems,\nwhile simultaneously maintaining lower latency."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Xianwei Zhang"
                    },
                    {
                        "name": "Jiangsu Du"
                    },
                    {
                        "name": "Zhiguang Chen"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Yutong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yutong Lu"
                },
                "author": "Yutong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v2",
                "updated": "2025-04-20T21:50:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    21,
                    50,
                    3,
                    6,
                    110,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.09775v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.09775v3",
                "updated": "2025-04-20T19:57:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    19,
                    57,
                    16,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-14T00:29:49Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    0,
                    29,
                    49,
                    0,
                    104,
                    0
                ],
                "title": "Understanding and Optimizing Multi-Stage AI Inference Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Multi-Stage AI Inference Pipelines"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Rajeshkumar Bambhaniya"
                    },
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Suvinay Subramanian"
                    },
                    {
                        "name": "Sudarshan Srinivasan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Amir Yazdanbakhsh"
                    },
                    {
                        "name": "Midhilesh Elavazhagan"
                    },
                    {
                        "name": "Madhu Kumar"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "arxiv_comment": "Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.09775v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.09775v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11208v2",
                "updated": "2025-04-20T07:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    7,
                    53,
                    9,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-15T14:11:38Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    14,
                    11,
                    38,
                    1,
                    105,
                    0
                ],
                "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink\n  of an Eye"
                },
                "summary": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
                },
                "authors": [
                    {
                        "name": "Bradley Morgan"
                    },
                    {
                        "name": "Gal Horowitz"
                    },
                    {
                        "name": "Sioli O'Connell"
                    },
                    {
                        "name": "Stephan van Schaik"
                    },
                    {
                        "name": "Chitchanok Chuengsatiansup"
                    },
                    {
                        "name": "Daniel Genkin"
                    },
                    {
                        "name": "Olaf Maennel"
                    },
                    {
                        "name": "Paul Montague"
                    },
                    {
                        "name": "Eyal Ronen"
                    },
                    {
                        "name": "Yuval Yarom"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Yarom"
                },
                "author": "Yuval Yarom",
                "arxiv_comment": "Added reference to the ID3 decision tree induction algorithm by J. R.\n  Quinlan in Section 5.4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14435v1",
                "updated": "2025-04-20T00:49:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "published": "2025-04-20T00:49:27Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    0,
                    49,
                    27,
                    6,
                    110,
                    0
                ],
                "title": "Deuteronomy 2.0: Record Caching and Latch Freedom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deuteronomy 2.0: Record Caching and Latch Freedom"
                },
                "summary": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Deuteronomy transactional key-value store is unique architecturally in\nproviding separation between transaction functionality -- its Transactional\nComponent (TC) and data management -- its Data Component (DC). It is unique in\ntechnology by (1) supporting record caching, a smaller unit than the\ntraditional page; and (2) protecting resources during concurrent execution\nusing a latch-free approach. Both technologies are enabled by delta updating.\nThis paper explains how record caching improves cache cost/performance. It also\nshows how a new latch-free approach makes implementation easier and improves\nperformance."
                },
                "authors": [
                    {
                        "name": "David Lomet"
                    }
                ],
                "author_detail": {
                    "name": "David Lomet"
                },
                "author": "David Lomet",
                "arxiv_comment": "6 pages, 5 figures, potential CIDR submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v1",
                "updated": "2025-04-19T18:25:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lübke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) submitted\n  to \"25th International Conference on Computational Science\" (ICCS25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14196v1",
                "updated": "2025-04-19T06:18:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "published": "2025-04-19T06:18:56Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    6,
                    18,
                    56,
                    5,
                    109,
                    0
                ],
                "title": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Room-temperature high-average-power strong-field terahertz source based\n  on industrial high-repetition-rate femtosecond laser"
                },
                "summary": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free-space strong-field terahertz (THz) pulses, generated via optical\nrectification of femtosecond lasers in nonlinear crystals, are pivotal in\nvarious applications. However, conventional Ti:sapphire lasers struggle to\nproduce high-average-power THz due to their limited output power. While\nkilowatt ytterbium lasers are increasingly adopted, their application in THz\ngeneration faces challenges: low optical-to-THz conversion efficiency\n(attributed to long pulse durations and low energy) and crystal damage under\nhigh pumping power. Here, we report a high-average-power strong-field THz\nsource using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ,\n50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By\nsystematically optimizing TPFP implementations and comparing grating- and\nechelon-type configurations, we achieve a THz source with 64.5 mW average power\nat 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at\n0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in\ncobalt-iron ferromagnetic nanofilms. This high-repetition-rate,\nhigh-average-power THz system, combined with its potential capabilities in high\nsignal-to-noise spectroscopy and imaging, promises transformative impacts in\nquantum matter manipulation, non-destructive testing, and biomedicine."
                },
                "authors": [
                    {
                        "name": "Deyin Kong"
                    },
                    {
                        "name": "Yichen Su"
                    },
                    {
                        "name": "Cheng Song"
                    },
                    {
                        "name": "Xiaojun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wu"
                },
                "author": "Xiaojun Wu",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v6",
                "updated": "2025-05-07T01:29:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    1,
                    29,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v1",
                "updated": "2025-04-18T22:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v1",
                "updated": "2025-04-18T13:46:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13981v1",
                "updated": "2025-04-18T06:34:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T06:34:57Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    6,
                    34,
                    57,
                    4,
                    108,
                    0
                ],
                "title": "CacheFormer: High Attention-Based Segment Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFormer: High Attention-Based Segment Caching"
                },
                "summary": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently handling long contexts in transformer-based language models with\nlow perplexity is an active area of research. Numerous recent approaches like\nLinformer, Longformer, Performer, and Structured state space models (SSMs).,\nhave not fully resolved this problem. All these models strive to reduce the\nquadratic time complexity of the attention mechanism while minimizing the loss\nin quality due to the effective compression of the long context. Inspired by\nthe cache and virtual memory principle in computers, where in case of a cache\nmiss, not only the needed data is retrieved from the memory, but the adjacent\ndata is also obtained, we apply this concept to handling long contexts by\ndividing it into small segments. In our design, we retrieve the nearby segments\nin an uncompressed form when high segment-level attention occurs at the\ncompressed level. Our en-hancements for handling long context include\naggregating four attention mechanisms consisting of short sliding window\nattention, long compressed segmented attention, dynamically retrieving top k\nhigh attention uncompressed segments, and overlapping segments in long segment\nattention to avoid segment fragmentation. These enhancements result in an\narchitecture that outperforms ex-isting SOTA architectures with an average\nperplexity improvement of 8.5% over similar model sizes."
                },
                "authors": [
                    {
                        "name": "Sushant Singh"
                    },
                    {
                        "name": "Ausif Mahmood"
                    }
                ],
                "author_detail": {
                    "name": "Ausif Mahmood"
                },
                "author": "Ausif Mahmood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v2",
                "updated": "2025-04-18T05:13:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    5,
                    13,
                    52,
                    4,
                    108,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "32 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16112v1",
                "updated": "2025-04-18T03:31:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T03:31:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM\n  Inference via GPU Co-processing"
                },
                "summary": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention layer, a core component of Transformer-based LLMs, brings out\ninefficiencies in current GPU systems due to its low operational intensity and\nthe substantial memory requirements of KV caches. We propose a High-bandwidth\nProcessing Unit (HPU), a memoryintensive co-processor that enhances GPU\nresource utilization during large-batched LLM inference. By offloading\nmemory-bound operations, the HPU allows the GPU to focus on compute-intensive\ntasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales\nout to accommodate surging memory demands driven by large batch sizes and\nextended sequence lengths. In this paper, we show the HPU prototype implemented\nwith PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU\nheterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy\nefficiency improvements over a GPUonly system, providing scalability without\nincreasing the number of GPUs."
                },
                "authors": [
                    {
                        "name": "Myunghyun Rhee"
                    },
                    {
                        "name": "Joonseop Sim"
                    },
                    {
                        "name": "Taeyoung Ahn"
                    },
                    {
                        "name": "Seungyong Lee"
                    },
                    {
                        "name": "Daegun Yoon"
                    },
                    {
                        "name": "Euiseok Kim"
                    },
                    {
                        "name": "Kyoung Park"
                    },
                    {
                        "name": "Youngpyo Joo"
                    },
                    {
                        "name": "Hosik Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hosik Kim"
                },
                "author": "Hosik Kim",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13385v1",
                "updated": "2025-04-18T00:21:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "published": "2025-04-18T00:21:00Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    21,
                    0,
                    4,
                    108,
                    0
                ],
                "title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for\n  Enhanced Cache Occupancy Attacks"
                },
                "summary": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache occupancy attacks exploit the shared nature of cache hierarchies to\ninfer a victim's activities by monitoring overall cache usage, unlike\naccess-driven cache attacks that focus on specific cache lines or sets. There\nexists some prior work that target the last-level cache (LLC) of Intel\nprocessors, which is inclusive of higher-level caches, and L2 caches of ARM\nsystems. In this paper, we target the System-Level Cache (SLC) of Apple\nM-series SoCs, which is exclusive to higher-level CPU caches. We address the\nchallenges of the exclusiveness and propose a suite of SLC-cache occupancy\nattacks, the first of its kind, where an adversary can monitor GPU and other\nCPU cluster activities from their own CPU cluster. We first discover the\nstructure of SLC in Apple M1 SOC and various policies pertaining to access and\nsharing through reverse engineering. We propose two attacks against websites.\nOne is a coarse-grained fingerprinting attack, recognizing which website is\naccessed based on their different GPU memory access patterns monitored through\nthe SLC occupancy channel. The other attack is a fine-grained pixel stealing\nattack, which precisely monitors the GPU memory usage for rendering different\npixels, through the SLC occupancy channel. Third, we introduce a novel screen\ncapturing attack which works beyond webpages, with the monitoring granularity\nof 57 rows of pixels (there are 1600 rows for the screen). This significantly\nexpands the attack surface, allowing the adversary to retrieve any screen\ndisplay, posing a substantial new threat to system security. Our findings\nreveal critical vulnerabilities in Apple's M-series SoCs and emphasize the\nurgent need for effective countermeasures against cache occupancy attacks in\nheterogeneous computing environments."
                },
                "authors": [
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "Accepted to ACM ASIA CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v2",
                "updated": "2025-04-17T23:45:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    23,
                    45,
                    51,
                    3,
                    107,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "12 pages, 7 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19325v2",
                "updated": "2025-04-17T15:26:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    26,
                    4,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-25T03:38:06Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    3,
                    38,
                    6,
                    1,
                    84,
                    0
                ],
                "title": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Autoregressive Video Modeling with Next-Frame Prediction"
                },
                "summary": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context autoregressive modeling has significantly advanced language\ngeneration, but video generation still struggles to fully utilize extended\ntemporal contexts. To investigate long-context video modeling, we introduce\nFrame AutoRegressive (FAR), a strong baseline for video autoregressive\nmodeling. Just as language models learn causal dependencies between tokens\n(i.e., Token AR), FAR models temporal causal dependencies between continuous\nframes, achieving better convergence than Token AR and video diffusion\ntransformers. Building on FAR, we observe that long-context video modeling\nfaces challenges due to visual redundancy. Training on long videos is\ncomputationally expensive, as vision tokens grow much faster than language\ntokens. To tackle this issue, we propose balancing locality and long-range\ndependency through long short-term context modeling. A high-resolution\nshort-term context window ensures fine-grained temporal consistency, while an\nunlimited long-term context window encodes long-range information using fewer\ntokens. With this approach, we can train on long video sequences with a\nmanageable token context length, thereby significantly reducing training time\nand memory usage. Furthermore, we propose a multi-level KV cache designed to\nsupport the long short-term context modeling, which accelerating inference on\nlong video sequences. We demonstrate that FAR achieves state-of-the-art\nperformance in both short- and long-video generation, providing a simple yet\neffective baseline for video autoregressive modeling. The code is released at\nhttps://github.com/showlab/FAR."
                },
                "authors": [
                    {
                        "name": "Yuchao Gu"
                    },
                    {
                        "name": "Weijia Mao"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page at https://farlongctx.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v3",
                "updated": "2025-04-17T03:51:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    3,
                    51,
                    6,
                    3,
                    107,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10074v2",
                "updated": "2025-04-17T00:38:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    17,
                    0,
                    38,
                    24,
                    3,
                    107,
                    0
                ],
                "published": "2025-03-13T05:43:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    43,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demoting Security via Exploitation of Cache Demote Operation in Intel's\n  Latest ISA Extension"
                },
                "summary": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISA extensions are increasingly adopted to boost the performance of\nspecialized workloads without requiring an entire architectural redesign.\nHowever, these enhancements can inadvertently expose new attack surfaces in the\nmicroarchitecture. In this paper, we investigate Intel's recently introduced\ncldemote extension, which promotes efficient data sharing by transferring cache\nlines from upper-level caches to the Last Level Cache (LLC). Despite its\nperformance benefits, we uncover critical properties-unprivileged access,\ninter-cache state transition, and fault suppression-that render cldemote\nexploitable for microarchitectural attacks. We propose two new attack\nprimitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote\nconstructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate\nof 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on\nLinux. Furthermore, we show that leveraging cldemote accelerates eviction set\nconstruction in non-inclusive LLC designs by obviating the need for helper\nthreads or extensive cache conflicts, thereby reducing construction time by 36%\nyet retaining comparable success rates. Finally, we examine how ISA extensions\ncontribute to broader microarchitectural attacks, identifying five key\nexploitable characteristics and categorizing four distinct attack types. We\nalso discuss potential countermeasures, highlighting the far-reaching security\nimplications of emerging ISA extensions."
                },
                "authors": [
                    {
                        "name": "Taehun Kim"
                    },
                    {
                        "name": "Hyerean Jang"
                    },
                    {
                        "name": "Youngjoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Shin"
                },
                "author": "Youngjoo Shin",
                "arxiv_comment": "The modified version of this preprint has been submitted to ESORICS\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12526v1",
                "updated": "2025-04-16T23:15:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T23:15:09Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    23,
                    15,
                    9,
                    2,
                    106,
                    0
                ],
                "title": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context\n  Language Models"
                },
                "summary": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models exhibit impressive performance but remain\nchallenging to deploy due to high GPU memory demands during inference. We\npropose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that\npartitions critical layers into smaller \"mini-sequences\" and integrates\nseamlessly with KV cache offloading. Experiments on various Llama, Qwen, and\nMistral models demonstrate that MOM reduces peak memory usage by over 50\\% on\naverage. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k\nto 455k tokens on a single A100 80GB GPU, while keeping outputs identical and\nnot compromising accuracy. MOM also maintains highly competitive throughput due\nto minimal computational overhead and efficient last-layer processing. Compared\nto traditional chunked prefill methods, MOM achieves a 35\\% greater context\nlength extension. More importantly, our method drastically reduces prefill\nmemory consumption, eliminating it as the longstanding dominant memory\nbottleneck during inference. This breakthrough fundamentally changes research\npriorities, redirecting future efforts from prefill-stage optimizations to\nimproving decode-stage residual KV cache efficiency."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Tianyi Zhu"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "arxiv_comment": "Submitted to COLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11816v1",
                "updated": "2025-04-16T07:02:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T07:02:38Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    7,
                    2,
                    38,
                    2,
                    106,
                    0
                ],
                "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache\n  Offloading"
                },
                "summary": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
                },
                "authors": [
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Hyunsun Chung"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "author": "Youngjae Kim",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v3",
                "updated": "2025-04-16T05:57:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    5,
                    57,
                    8,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11765v1",
                "updated": "2025-04-16T04:59:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T04:59:18Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    4,
                    59,
                    18,
                    2,
                    106,
                    0
                ],
                "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference\n  in RAG-Powered LLMs"
                },
                "summary": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Hyungwoo Lee"
                    },
                    {
                        "name": "Kihyun Kim"
                    },
                    {
                        "name": "Jinwoo Kim"
                    },
                    {
                        "name": "Jungmin So"
                    },
                    {
                        "name": "Myung-Hoon Cha"
                    },
                    {
                        "name": "Hong-Yeon Kim"
                    },
                    {
                        "name": "James J. Kim"
                    },
                    {
                        "name": "Youngjae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Kim"
                },
                "arxiv_affiliation": "Dept. of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea",
                "author": "Youngjae Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11729v1",
                "updated": "2025-04-16T03:07:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "published": "2025-04-16T03:07:07Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    3,
                    7,
                    7,
                    2,
                    106,
                    0
                ],
                "title": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G\n  Networks"
                },
                "summary": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As sixth-generation (6G) networks advance, large language models (LLMs) are\nincreasingly integrated into 6G infrastructure to enhance network management\nand intelligence. However, traditional LLMs architecture struggle to meet the\nstringent latency and security requirements of 6G, especially as the increasing\nin sequence length leads to greater task complexity. This paper proposes\nEdge-Prompt, a cloud-edge collaborative framework based on a hierarchical\nattention splicing mechanism. EdgePrompt employs distributed key-value (KV)\npair optimization techniques to accelerate inference and adapt to network\nconditions. Additionally, to reduce the risk of data leakage, EdgePrompt\nincorporates a privacy preserving strategy by isolating sensitive information\nduring processing. Experiments on public dataset show that EdgePrompt\neffectively improves the inference throughput and reduces the latency, which\nprovides a reliable solution for LLMs deployment in 6G environments."
                },
                "authors": [
                    {
                        "name": "Jiahong Ning"
                    },
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Gary Lee"
                    },
                    {
                        "name": "Sumei Sun"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.03739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03739v1",
                "updated": "2025-05-06T17:59:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    59,
                    53,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T17:59:53Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    59,
                    53,
                    1,
                    126,
                    0
                ],
                "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model"
                },
                "summary": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks."
                },
                "authors": [
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Heting Gao"
                    },
                    {
                        "name": "Lijiang Li"
                    },
                    {
                        "name": "Peixian Chen"
                    },
                    {
                        "name": "Mengdan Zhang"
                    },
                    {
                        "name": "Hang Shao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Jinlong Peng"
                    },
                    {
                        "name": "Haoyu Cao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "Training and Inference Codes: https://github.com/VITA-MLLM/VITA-Audio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03737v1",
                "updated": "2025-05-06T17:59:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    59,
                    45,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T17:59:45Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    59,
                    45,
                    1,
                    126,
                    0
                ],
                "title": "Impact of Galactic non-Gaussian foregrounds on CMB lensing measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Galactic non-Gaussian foregrounds on CMB lensing measurements"
                },
                "summary": "Weak gravitational lensing of the CMB has been established as a robust and\npowerful observable for precision cosmology. However, the impact of Galactic\nforegrounds, which has been studied less extensively than many other potential\nsystematics, could in principle pose a problem for CMB lensing measurements.\nThese foregrounds are inherently non-Gaussian and hence might mimic the\ncharacteristic signal that lensing estimators are designed to measure. We\npresent an analysis that quantifies the level of contamination from Galactic\ndust in lensing measurements, focusing particularly on measurements with the\nAtacama Cosmology Telescope and the Simons Observatory. We employ a whole suite\nof foreground models and study the contamination of lensing measurements with\nboth individual frequency channels and multifrequency combinations. We test the\nsensitivity of different estimators to the level of foreground non-Gaussianity,\nand the dependence on sky fraction and multipole range used. We find that\nGalactic foregrounds do not present a problem for the Atacama Cosmology\nTelescope experiment (the bias in the inferred CMB lensing power spectrum\namplitude remains below $0.3\\sigma$). For Simons Observatory, not all\nforeground models remain below this threshold. Although our results are\nconservative upper limits, they suggest that further work on characterizing\ndust biases and determining the impact of mitigation methods is well motivated,\nespecially for the largest sky fractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak gravitational lensing of the CMB has been established as a robust and\npowerful observable for precision cosmology. However, the impact of Galactic\nforegrounds, which has been studied less extensively than many other potential\nsystematics, could in principle pose a problem for CMB lensing measurements.\nThese foregrounds are inherently non-Gaussian and hence might mimic the\ncharacteristic signal that lensing estimators are designed to measure. We\npresent an analysis that quantifies the level of contamination from Galactic\ndust in lensing measurements, focusing particularly on measurements with the\nAtacama Cosmology Telescope and the Simons Observatory. We employ a whole suite\nof foreground models and study the contamination of lensing measurements with\nboth individual frequency channels and multifrequency combinations. We test the\nsensitivity of different estimators to the level of foreground non-Gaussianity,\nand the dependence on sky fraction and multipole range used. We find that\nGalactic foregrounds do not present a problem for the Atacama Cosmology\nTelescope experiment (the bias in the inferred CMB lensing power spectrum\namplitude remains below $0.3\\sigma$). For Simons Observatory, not all\nforeground models remain below this threshold. Although our results are\nconservative upper limits, they suggest that further work on characterizing\ndust biases and determining the impact of mitigation methods is well motivated,\nespecially for the largest sky fractions."
                },
                "authors": [
                    {
                        "name": "Irene Abril-Cabezas"
                    },
                    {
                        "name": "Frank J. Qu"
                    },
                    {
                        "name": "Blake D. Sherwin"
                    },
                    {
                        "name": "Alexander van Engelen"
                    },
                    {
                        "name": "Niall MacCrann"
                    },
                    {
                        "name": "Carlos Hervías-Caimapo"
                    },
                    {
                        "name": "Omar Darwish"
                    },
                    {
                        "name": "J. Colin Hill"
                    },
                    {
                        "name": "Mathew S. Madhavacheril"
                    },
                    {
                        "name": "Neelima Sehgal"
                    }
                ],
                "author_detail": {
                    "name": "Neelima Sehgal"
                },
                "author": "Neelima Sehgal",
                "arxiv_comment": "24 pages, 16 figures, prepared for submission to PRD, comments\n  welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03733v1",
                "updated": "2025-05-06T17:59:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    59,
                    15,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T17:59:15Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    59,
                    15,
                    1,
                    126,
                    0
                ],
                "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch"
                },
                "summary": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model."
                },
                "authors": [
                    {
                        "name": "Zimu Lu"
                    },
                    {
                        "name": "Yunqiao Yang"
                    },
                    {
                        "name": "Houxing Ren"
                    },
                    {
                        "name": "Haotian Hou"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Weikang Shi"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Mingjie Zhan"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01153v3",
                "updated": "2025-05-06T17:40:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    40,
                    4,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-01T19:36:14Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    36,
                    14,
                    1,
                    91,
                    0
                ],
                "title": "Catch Me if You Search: When Contextual Web Search Results Affect the\n  Detection of Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catch Me if You Search: When Contextual Web Search Results Affect the\n  Detection of Hallucinations"
                },
                "summary": "While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or `hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N = 560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or `hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N = 560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts."
                },
                "authors": [
                    {
                        "name": "Mahjabin Nahar"
                    },
                    {
                        "name": "Eun-Ju Lee"
                    },
                    {
                        "name": "Jin Won Park"
                    },
                    {
                        "name": "Dongwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongwon Lee"
                },
                "author": "Dongwon Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20953v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20953v2",
                "updated": "2025-05-06T17:34:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    34,
                    29,
                    1,
                    126,
                    0
                ],
                "published": "2025-03-26T19:36:43Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    19,
                    36,
                    43,
                    2,
                    85,
                    0
                ],
                "title": "Clean & Clear: Feasibility of Safe LLM Clinical Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clean & Clear: Feasibility of Safe LLM Clinical Guidance"
                },
                "summary": "Background:\n  Clinical guidelines are central to safe evidence-based medicine in modern\nhealthcare, providing diagnostic criteria, treatment options and monitoring\nadvice for a wide range of illnesses. LLM-empowered chatbots have shown great\npromise in Healthcare Q&A tasks, offering the potential to provide quick and\naccurate responses to medical inquiries.\n  Our main objective was the development and preliminary assessment of an\nLLM-empowered chatbot software capable of reliably answering clinical guideline\nquestions using University College London Hospital (UCLH) clinical guidelines.\n  Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant\ninformation from the UCLH guidelines to answer questions. Our approach\nhighlights the safety and reliability of referencing information over its\ninterpretation and response generation. Seven doctors from the ward assessed\nthe chatbot's performance by comparing its answers to the gold standard.\n  Results: Our chatbot demonstrates promising performance in terms of\nrelevance, with ~73% of its responses rated as very relevant, showcasing a\nstrong understanding of the clinical context. Importantly, our chatbot achieves\na recall of 1.00 for extracted guideline lines, substantially minimising the\nrisk of missing critical information. Approximately 78% of responses were rated\nsatisfactory in terms of completeness. A small portion (~14.5%) contained minor\nunnecessary information, indicating occasional lapses in precision. The\nchatbot' showed high efficiency, with an average completion time of 10 seconds,\ncompared to 30 seconds for human respondents. Evaluation of clinical reasoning\nshowed that 72% of the chatbot's responses were without flaws. Our chatbot\ndemonstrates significant potential to speed up and improve the process of\naccessing locally relevant clinical information for healthcare professionals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background:\n  Clinical guidelines are central to safe evidence-based medicine in modern\nhealthcare, providing diagnostic criteria, treatment options and monitoring\nadvice for a wide range of illnesses. LLM-empowered chatbots have shown great\npromise in Healthcare Q&A tasks, offering the potential to provide quick and\naccurate responses to medical inquiries.\n  Our main objective was the development and preliminary assessment of an\nLLM-empowered chatbot software capable of reliably answering clinical guideline\nquestions using University College London Hospital (UCLH) clinical guidelines.\n  Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant\ninformation from the UCLH guidelines to answer questions. Our approach\nhighlights the safety and reliability of referencing information over its\ninterpretation and response generation. Seven doctors from the ward assessed\nthe chatbot's performance by comparing its answers to the gold standard.\n  Results: Our chatbot demonstrates promising performance in terms of\nrelevance, with ~73% of its responses rated as very relevant, showcasing a\nstrong understanding of the clinical context. Importantly, our chatbot achieves\na recall of 1.00 for extracted guideline lines, substantially minimising the\nrisk of missing critical information. Approximately 78% of responses were rated\nsatisfactory in terms of completeness. A small portion (~14.5%) contained minor\nunnecessary information, indicating occasional lapses in precision. The\nchatbot' showed high efficiency, with an average completion time of 10 seconds,\ncompared to 30 seconds for human respondents. Evaluation of clinical reasoning\nshowed that 72% of the chatbot's responses were without flaws. Our chatbot\ndemonstrates significant potential to speed up and improve the process of\naccessing locally relevant clinical information for healthcare professionals."
                },
                "authors": [
                    {
                        "name": "Julia Ive"
                    },
                    {
                        "name": "Felix Jozsa"
                    },
                    {
                        "name": "Nick Jackson"
                    },
                    {
                        "name": "Paulina Bondaronek"
                    },
                    {
                        "name": "Ciaran Scott Hill"
                    },
                    {
                        "name": "Richard Dobson"
                    }
                ],
                "author_detail": {
                    "name": "Richard Dobson"
                },
                "author": "Richard Dobson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20953v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20953v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11976v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11976v3",
                "updated": "2025-05-06T17:30:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    30,
                    37,
                    1,
                    126,
                    0
                ],
                "published": "2025-02-17T16:22:09Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    16,
                    22,
                    9,
                    0,
                    48,
                    0
                ],
                "title": "Constraining first-order phase transition inside neutron stars with\n  application of Bayesian techniques on PSR J0437-4715 NICER data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining first-order phase transition inside neutron stars with\n  application of Bayesian techniques on PSR J0437-4715 NICER data"
                },
                "summary": "Understanding the existence of exotic matter phases and phase transitions\nwithin the core of neutron stars is crucial to advancing our knowledge of\ncold-dense matter physics. Recent multi-messenger observations, including\ngravitational waves from neutron star mergers and precise X-ray data from\nNASA's Neutron Star Interior Composition Explorer (NICER) mission, have\nsignificantly constrained the neutron star equation of state (EOS). This study\ninvestigates the effects of phase transitions in neutron stars, focusing on\nNICER's latest observation of PSR J0437--4715. We employ Bayesian inference\ntechniques to evaluate the presence of first-order phase transitions using a\npiecewise polytropic EOS model. Our analysis incorporates data from multiple\nNICER sources, to refine constraints on key phase transition parameters,\nincluding critical density and transition depth. We find that including data\nfrom PSR J0437--4715 improves the evidence of phase transitions and tightens\nthe EOS constraints, especially at higher densities. However, Bayes factor\nanalysis only indicates a slight preference for models without phase\ntransitions and current observational precision is insufficient to draw\ndefinitive conclusions. In particular, this polytropic model identifies the\ncritical phase transition mass of neutron stars as being close to 1.4 solar\nmasses, which coincides with the approximate mass range of PSR J0437--4715.\nThis work emphasizes the importance of precise measurements of PSR J0437--4715\nfor deepening our understanding of neutron star interiors and exploring\npotential new physics at extreme densities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the existence of exotic matter phases and phase transitions\nwithin the core of neutron stars is crucial to advancing our knowledge of\ncold-dense matter physics. Recent multi-messenger observations, including\ngravitational waves from neutron star mergers and precise X-ray data from\nNASA's Neutron Star Interior Composition Explorer (NICER) mission, have\nsignificantly constrained the neutron star equation of state (EOS). This study\ninvestigates the effects of phase transitions in neutron stars, focusing on\nNICER's latest observation of PSR J0437--4715. We employ Bayesian inference\ntechniques to evaluate the presence of first-order phase transitions using a\npiecewise polytropic EOS model. Our analysis incorporates data from multiple\nNICER sources, to refine constraints on key phase transition parameters,\nincluding critical density and transition depth. We find that including data\nfrom PSR J0437--4715 improves the evidence of phase transitions and tightens\nthe EOS constraints, especially at higher densities. However, Bayes factor\nanalysis only indicates a slight preference for models without phase\ntransitions and current observational precision is insufficient to draw\ndefinitive conclusions. In particular, this polytropic model identifies the\ncritical phase transition mass of neutron stars as being close to 1.4 solar\nmasses, which coincides with the approximate mass range of PSR J0437--4715.\nThis work emphasizes the importance of precise measurements of PSR J0437--4715\nfor deepening our understanding of neutron star interiors and exploring\npotential new physics at extreme densities."
                },
                "authors": [
                    {
                        "name": "Chun Huang"
                    },
                    {
                        "name": "Shashwat Sourav"
                    }
                ],
                "author_detail": {
                    "name": "Shashwat Sourav"
                },
                "author": "Shashwat Sourav",
                "arxiv_doi": "10.3847/1538-4357/adbb67 10.3847/1538-4357/adbb67 10.3847/1538-4357/adbb67\n  10.3847/1538-4357/adbb67",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adbb67",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adbb67",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adbb67",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adbb67",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.11976v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11976v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published version on ApJ",
                "arxiv_journal_ref": "2025, The Astrophysical Journal, Volume 983, Number 1",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03708v1",
                "updated": "2025-05-06T17:27:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    27,
                    54,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T17:27:54Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    27,
                    54,
                    1,
                    126,
                    0
                ],
                "title": "CHEX-MATE: exploring the kinematical properties of Planck galaxy\n  clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHEX-MATE: exploring the kinematical properties of Planck galaxy\n  clusters"
                },
                "summary": "We analyse the kinematical properties of the CHEX-MATE (Cluster HEritage\nproject with XMM-Newton - Mass Assembly and Thermodynamics at the Endpoint of\nstructure formation) galaxy cluster sample. [...] We derive cluster mass\nprofiles for 75 clusters using the \\textsc{MG-MAMPOSSt} procedure, which\nrecovers the gravitational potential and the anisotropy profiles from\nline-of-sight velocities and projected positions of galaxy members. The\nstandard NFW and the Burkert models with flatter cores than NFW both adequately\nfit the kinematic data, with only marginal statistical preference for one model\nover the other. An estimation of the mass bias $(1-B_1) =\nM^{SZ}_{500}/M^{M}_{500} $ is performed from the comparison with\nSZ-X-ray-calibrated mass estimates, resulting in a value of $ 0.54 \\pm 0.11$\nwhen four evidently disturbed clusters are removed from the sample. We assess\nthe dynamical state of the clusters by inferring the Anderson-Darling\ncoefficient $(A^2)$ and the fraction of galaxies in substructures\n($f_\\text{sub}$). Except for a few cases, we found relatively low values for\n$A^2$, suggesting that CHEX-MATE clusters are not too far from relaxation.\nMoreover, no significant trends emerge among $A^2,\\,f_\\text{sub}$ and the\ndifference between the log-masses estimated by \\textsc{MG-MAMPOSSt} and by\nSZ-X-ray.\n  We study the concentration-mass relation for the sample; despite the large\nscatter, we observe signs of an increasing trend for large-mass clusters, in\nagreement with recent theoretical expectations.\n  Finally, the analysis of radial anisotropy profiles of member galaxies -\nstacked in five bins of mass and redshift - reveals that orbits tend to be\nisotropic at the center and more radial towards the edge, as already found in\nprevious studies. A slight trend of increasing radial orbits at $r_{200}$ is\nobserved in clusters with larger velocity dispersion",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyse the kinematical properties of the CHEX-MATE (Cluster HEritage\nproject with XMM-Newton - Mass Assembly and Thermodynamics at the Endpoint of\nstructure formation) galaxy cluster sample. [...] We derive cluster mass\nprofiles for 75 clusters using the \\textsc{MG-MAMPOSSt} procedure, which\nrecovers the gravitational potential and the anisotropy profiles from\nline-of-sight velocities and projected positions of galaxy members. The\nstandard NFW and the Burkert models with flatter cores than NFW both adequately\nfit the kinematic data, with only marginal statistical preference for one model\nover the other. An estimation of the mass bias $(1-B_1) =\nM^{SZ}_{500}/M^{M}_{500} $ is performed from the comparison with\nSZ-X-ray-calibrated mass estimates, resulting in a value of $ 0.54 \\pm 0.11$\nwhen four evidently disturbed clusters are removed from the sample. We assess\nthe dynamical state of the clusters by inferring the Anderson-Darling\ncoefficient $(A^2)$ and the fraction of galaxies in substructures\n($f_\\text{sub}$). Except for a few cases, we found relatively low values for\n$A^2$, suggesting that CHEX-MATE clusters are not too far from relaxation.\nMoreover, no significant trends emerge among $A^2,\\,f_\\text{sub}$ and the\ndifference between the log-masses estimated by \\textsc{MG-MAMPOSSt} and by\nSZ-X-ray.\n  We study the concentration-mass relation for the sample; despite the large\nscatter, we observe signs of an increasing trend for large-mass clusters, in\nagreement with recent theoretical expectations.\n  Finally, the analysis of radial anisotropy profiles of member galaxies -\nstacked in five bins of mass and redshift - reveals that orbits tend to be\nisotropic at the center and more radial towards the edge, as already found in\nprevious studies. A slight trend of increasing radial orbits at $r_{200}$ is\nobserved in clusters with larger velocity dispersion"
                },
                "authors": [
                    {
                        "name": "Lorenzo Pizzuti"
                    },
                    {
                        "name": "Rafael Barrena"
                    },
                    {
                        "name": "Mauro Sereno"
                    },
                    {
                        "name": "Alina Streblyanska"
                    },
                    {
                        "name": "Antonio Ferragamo"
                    },
                    {
                        "name": "Sophie Maurogordato"
                    },
                    {
                        "name": "Alberto Cappi"
                    },
                    {
                        "name": "Stefano Ettori"
                    },
                    {
                        "name": "Gabriel W. Pratt"
                    },
                    {
                        "name": "Gianluca Castignani"
                    },
                    {
                        "name": "Megan Donahue"
                    },
                    {
                        "name": "Dominique Eckert"
                    },
                    {
                        "name": "Fabio Gastaldello"
                    },
                    {
                        "name": "Raphael Gavazzi"
                    },
                    {
                        "name": "Christopher P. Haines"
                    },
                    {
                        "name": "Scott T. Kay"
                    },
                    {
                        "name": "Lorenzo Lovisari"
                    },
                    {
                        "name": "Ben J. Maughan"
                    },
                    {
                        "name": "Etienne Pointecouteau"
                    },
                    {
                        "name": "Elena Rasia"
                    },
                    {
                        "name": "Mario Radovich"
                    },
                    {
                        "name": "Jack Sayers"
                    }
                ],
                "author_detail": {
                    "name": "Jack Sayers"
                },
                "author": "Jack Sayers",
                "arxiv_comment": "12 pages (plus appendices), 14 Figures. To be submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10707v2",
                "updated": "2025-05-06T17:04:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    4,
                    18,
                    1,
                    126,
                    0
                ],
                "published": "2025-03-12T18:36:41Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    18,
                    36,
                    41,
                    2,
                    71,
                    0
                ],
                "title": "CALLM: Understanding Cancer Survivors' Emotions and Intervention\n  Opportunities via Mobile Diaries and Context-Aware Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLM: Understanding Cancer Survivors' Emotions and Intervention\n  Opportunities via Mobile Diaries and Context-Aware Language Models"
                },
                "summary": "Cancer survivors face unique emotional challenges that impact their quality\nof life. Mobile diary entries provide a promising method for tracking emotional\nstates, improving self-awareness, and promoting well-being outcome. This paper\naims to, through mobile diaries, understand cancer survivors' emotional states\nand key variables related to just-in-time intervention opportunities, including\nthe desire to regulate emotions and the availability to engage in\ninterventions. Although emotion analysis tools show potential for recognizing\nemotions from text, current methods lack the contextual understanding necessary\nto interpret brief mobile diary narratives. Our analysis of diary entries from\ncancer survivors (N=407) reveals systematic relationships between described\ncontexts and emotional states, with administrative and health-related contexts\nassociated with negative affect and regulation needs, while leisure activities\npromote positive emotions. We propose CALLM, a Context-Aware framework\nleveraging Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG) to analyze these brief entries by integrating retrieved peer experiences\nand personal diary history. CALLM demonstrates strong performance with balanced\naccuracies reaching 72.96% for positive affect, 73.29% for negative affect,\n73.72% for emotion regulation desire, and 60.09% for intervention availability,\noutperforming language model baselines. Post-hoc analysis reveals that model\nconfidence strongly predicts accuracy, with longer diary entries generally\nenhancing performance, and brief personalization periods yielding meaningful\nimprovements. Our findings demonstrate how contextual information in mobile\ndiaries can be effectively leveraged to understand emotional experiences,\npredict key states, and identify optimal intervention moments for personalized\njust-in-time support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer survivors face unique emotional challenges that impact their quality\nof life. Mobile diary entries provide a promising method for tracking emotional\nstates, improving self-awareness, and promoting well-being outcome. This paper\naims to, through mobile diaries, understand cancer survivors' emotional states\nand key variables related to just-in-time intervention opportunities, including\nthe desire to regulate emotions and the availability to engage in\ninterventions. Although emotion analysis tools show potential for recognizing\nemotions from text, current methods lack the contextual understanding necessary\nto interpret brief mobile diary narratives. Our analysis of diary entries from\ncancer survivors (N=407) reveals systematic relationships between described\ncontexts and emotional states, with administrative and health-related contexts\nassociated with negative affect and regulation needs, while leisure activities\npromote positive emotions. We propose CALLM, a Context-Aware framework\nleveraging Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG) to analyze these brief entries by integrating retrieved peer experiences\nand personal diary history. CALLM demonstrates strong performance with balanced\naccuracies reaching 72.96% for positive affect, 73.29% for negative affect,\n73.72% for emotion regulation desire, and 60.09% for intervention availability,\noutperforming language model baselines. Post-hoc analysis reveals that model\nconfidence strongly predicts accuracy, with longer diary entries generally\nenhancing performance, and brief personalization periods yielding meaningful\nimprovements. Our findings demonstrate how contextual information in mobile\ndiaries can be effectively leveraged to understand emotional experiences,\npredict key states, and identify optimal intervention moments for personalized\njust-in-time support."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Katharine E. Daniel"
                    },
                    {
                        "name": "Laura E. Barnes"
                    },
                    {
                        "name": "Philip I. Chow"
                    }
                ],
                "author_detail": {
                    "name": "Philip I. Chow"
                },
                "author": "Philip I. Chow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02445v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02445v3",
                "updated": "2025-05-06T16:32:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    32,
                    17,
                    1,
                    126,
                    0
                ],
                "published": "2025-03-04T09:40:00Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    40,
                    0,
                    1,
                    63,
                    0
                ],
                "title": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via\n  Multi-Agent Iterative Optimization and Diffusion Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via\n  Multi-Agent Iterative Optimization and Diffusion Modelling"
                },
                "summary": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yuhao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Goran Nenadic"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02445v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02445v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03678v1",
                "updated": "2025-05-06T16:23:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    23,
                    42,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T16:23:42Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    23,
                    42,
                    1,
                    126,
                    0
                ],
                "title": "Graph Drawing for LLMs: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Drawing for LLMs: An Empirical Evaluation"
                },
                "summary": "Our work contributes to the fast-growing literature on the use of Large\nLanguage Models (LLMs) to perform graph-related tasks. In particular, we focus\non usage scenarios that rely on the visual modality, feeding the model with a\ndrawing of the graph under analysis. We investigate how the model's performance\nis affected by the chosen layout paradigm, the aesthetics of the drawing, and\nthe prompting technique used for the queries. We formulate three corresponding\nresearch questions and present the results of a thorough experimental analysis.\nOur findings reveal that choosing the right layout paradigm and optimizing the\nreadability of the input drawing from a human perspective can significantly\nimprove the performance of the model on the given task. Moreover, selecting the\nmost effective prompting technique is a challenging yet crucial task for\nachieving optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our work contributes to the fast-growing literature on the use of Large\nLanguage Models (LLMs) to perform graph-related tasks. In particular, we focus\non usage scenarios that rely on the visual modality, feeding the model with a\ndrawing of the graph under analysis. We investigate how the model's performance\nis affected by the chosen layout paradigm, the aesthetics of the drawing, and\nthe prompting technique used for the queries. We formulate three corresponding\nresearch questions and present the results of a thorough experimental analysis.\nOur findings reveal that choosing the right layout paradigm and optimizing the\nreadability of the input drawing from a human perspective can significantly\nimprove the performance of the model on the given task. Moreover, selecting the\nmost effective prompting technique is a challenging yet crucial task for\nachieving optimal performance."
                },
                "authors": [
                    {
                        "name": "Walter Didimo"
                    },
                    {
                        "name": "Fabrizio Montecchiani"
                    },
                    {
                        "name": "Tommaso Piselli"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Piselli"
                },
                "author": "Tommaso Piselli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00832v2",
                "updated": "2025-05-06T16:21:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    21,
                    23,
                    1,
                    126,
                    0
                ],
                "published": "2024-08-01T18:00:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    18,
                    0,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Leveraging Time-Dependent Instrumental Noise for LISA SGWB Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Time-Dependent Instrumental Noise for LISA SGWB Analysis"
                },
                "summary": "Variations in the instrumental noise of the Laser Interferometer Space\nAntenna (LISA) over time are expected as a result of e.g. scheduled satellite\noperations or unscheduled glitches. We demonstrate that these fluctuations can\nbe leveraged to improve the sensitivity to stochastic gravitational wave\nbackgrounds (SGWBs) compared to the stationary noise scenario. This requires\noptimal use of data segments with downward noise fluctuations, and thus a data\nanalysis pipeline capable of analysing and combining shorter time segments of\nmission data. We propose that simulation based inference is well suited for\nthis challenge. In an approximate, but state-of-the-art, modeling setup, we\nshow by comparison with Fisher Information Matrix estimates that the optimal\ninformation gain can be achieved in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations in the instrumental noise of the Laser Interferometer Space\nAntenna (LISA) over time are expected as a result of e.g. scheduled satellite\noperations or unscheduled glitches. We demonstrate that these fluctuations can\nbe leveraged to improve the sensitivity to stochastic gravitational wave\nbackgrounds (SGWBs) compared to the stationary noise scenario. This requires\noptimal use of data segments with downward noise fluctuations, and thus a data\nanalysis pipeline capable of analysing and combining shorter time segments of\nmission data. We propose that simulation based inference is well suited for\nthis challenge. In an approximate, but state-of-the-art, modeling setup, we\nshow by comparison with Fisher Information Matrix estimates that the optimal\ninformation gain can be achieved in practice."
                },
                "authors": [
                    {
                        "name": "James Alvey"
                    },
                    {
                        "name": "Uddipta Bhardwaj"
                    },
                    {
                        "name": "Valerie Domcke"
                    },
                    {
                        "name": "Mauro Pieroni"
                    },
                    {
                        "name": "Christoph Weniger"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Weniger"
                },
                "author": "Christoph Weniger",
                "arxiv_comment": "12 pages, 5 figures. saqqara available at\n  https://github.com/peregrine-gw/saqqara, GW_response available at\n  https://github.com/Mauropieroni/GW_response - v2: Version accepted by PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03674v1",
                "updated": "2025-05-06T16:15:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    15,
                    24,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T16:15:24Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    15,
                    24,
                    1,
                    126,
                    0
                ],
                "title": "Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts\n  Collaboration Perception, Not Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts\n  Collaboration Perception, Not Performance"
                },
                "summary": "In human-agent teams, openly sharing goals is often assumed to enhance\nplanning, collaboration, and effectiveness. However, direct communication of\nthese goals is not always feasible, requiring teammates to infer their\npartner's intentions through actions. Building on this, we investigate whether\nan AI agent's ability to share its inferred understanding of a human teammate's\ngoals can improve task performance and perceived collaboration. Through an\nexperiment comparing three conditions-no recognition (NR), viable goals (VG),\nand viable goals on-demand (VGod) - we find that while goal-sharing information\ndid not yield significant improvements in task performance or overall\nsatisfaction scores, thematic analysis suggests that it supported strategic\nadaptations and subjective perceptions of collaboration. Cognitive load\nassessments revealed no additional burden across conditions, highlighting the\nchallenge of balancing informativeness and simplicity in human-agent\ninteractions. These findings highlight the nuanced trade-off of goal-sharing:\nwhile it fosters trust and enhances perceived collaboration, it can\noccasionally hinder objective performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In human-agent teams, openly sharing goals is often assumed to enhance\nplanning, collaboration, and effectiveness. However, direct communication of\nthese goals is not always feasible, requiring teammates to infer their\npartner's intentions through actions. Building on this, we investigate whether\nan AI agent's ability to share its inferred understanding of a human teammate's\ngoals can improve task performance and perceived collaboration. Through an\nexperiment comparing three conditions-no recognition (NR), viable goals (VG),\nand viable goals on-demand (VGod) - we find that while goal-sharing information\ndid not yield significant improvements in task performance or overall\nsatisfaction scores, thematic analysis suggests that it supported strategic\nadaptations and subjective perceptions of collaboration. Cognitive load\nassessments revealed no additional burden across conditions, highlighting the\nchallenge of balancing informativeness and simplicity in human-agent\ninteractions. These findings highlight the nuanced trade-off of goal-sharing:\nwhile it fosters trust and enhances perceived collaboration, it can\noccasionally hinder objective performance gains."
                },
                "authors": [
                    {
                        "name": "Yotam Amitai"
                    },
                    {
                        "name": "Reuth Mirsky"
                    },
                    {
                        "name": "Ofra Amir"
                    }
                ],
                "author_detail": {
                    "name": "Ofra Amir"
                },
                "author": "Ofra Amir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03673v1",
                "updated": "2025-05-06T16:11:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    11,
                    49,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T16:11:49Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    11,
                    49,
                    1,
                    126,
                    0
                ],
                "title": "RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and\n  Multi-Agent Collaboration"
                },
                "summary": "The dawn of embodied intelligence has ushered in an unprecedented imperative\nfor resilient, cognition-enabled multi-agent collaboration across\nnext-generation ecosystems, revolutionizing paradigms in autonomous\nmanufacturing, adaptive service robotics, and cyber-physical production\narchitectures. However, current robotic systems face significant limitations,\nsuch as limited cross-embodiment adaptability, inefficient task scheduling, and\ninsufficient dynamic error correction. While End-to-end VLA models demonstrate\ninadequate long-horizon planning and task generalization, hierarchical VLA\nmodels suffer from a lack of cross-embodiment and multi-agent coordination\ncapabilities. To address these challenges, we introduce RoboOS, the first\nopen-source embodied system built on a Brain-Cerebellum hierarchical\narchitecture, enabling a paradigm shift from single-agent to multi-agent\nintelligence. Specifically, RoboOS consists of three key components: (1)\nEmbodied Brain Model (RoboBrain), a MLLM designed for global perception and\nhigh-level decision-making; (2) Cerebellum Skill Library, a modular,\nplug-and-play toolkit that facilitates seamless execution of multiple skills;\nand (3) Real-Time Shared Memory, a spatiotemporal synchronization mechanism for\ncoordinating multi-agent states. By integrating hierarchical information flow,\nRoboOS bridges Embodied Brain and Cerebellum Skill Library, facilitating robust\nplanning, scheduling, and error correction for long-horizon tasks, while\nensuring efficient multi-agent collaboration through Real-Time Shared Memory.\nFurthermore, we enhance edge-cloud communication and cloud-based distributed\ninference to facilitate high-frequency interactions and enable scalable\ndeployment. Extensive real-world experiments across various scenarios,\ndemonstrate RoboOS's versatility in supporting heterogeneous embodiments.\nProject website: https://github.com/FlagOpen/RoboOS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dawn of embodied intelligence has ushered in an unprecedented imperative\nfor resilient, cognition-enabled multi-agent collaboration across\nnext-generation ecosystems, revolutionizing paradigms in autonomous\nmanufacturing, adaptive service robotics, and cyber-physical production\narchitectures. However, current robotic systems face significant limitations,\nsuch as limited cross-embodiment adaptability, inefficient task scheduling, and\ninsufficient dynamic error correction. While End-to-end VLA models demonstrate\ninadequate long-horizon planning and task generalization, hierarchical VLA\nmodels suffer from a lack of cross-embodiment and multi-agent coordination\ncapabilities. To address these challenges, we introduce RoboOS, the first\nopen-source embodied system built on a Brain-Cerebellum hierarchical\narchitecture, enabling a paradigm shift from single-agent to multi-agent\nintelligence. Specifically, RoboOS consists of three key components: (1)\nEmbodied Brain Model (RoboBrain), a MLLM designed for global perception and\nhigh-level decision-making; (2) Cerebellum Skill Library, a modular,\nplug-and-play toolkit that facilitates seamless execution of multiple skills;\nand (3) Real-Time Shared Memory, a spatiotemporal synchronization mechanism for\ncoordinating multi-agent states. By integrating hierarchical information flow,\nRoboOS bridges Embodied Brain and Cerebellum Skill Library, facilitating robust\nplanning, scheduling, and error correction for long-horizon tasks, while\nensuring efficient multi-agent collaboration through Real-Time Shared Memory.\nFurthermore, we enhance edge-cloud communication and cloud-based distributed\ninference to facilitate high-frequency interactions and enable scalable\ndeployment. Extensive real-world experiments across various scenarios,\ndemonstrate RoboOS's versatility in supporting heterogeneous embodiments.\nProject website: https://github.com/FlagOpen/RoboOS"
                },
                "authors": [
                    {
                        "name": "Huajie Tan"
                    },
                    {
                        "name": "Xiaoshuai Hao"
                    },
                    {
                        "name": "Minglan Lin"
                    },
                    {
                        "name": "Pengwei Wang"
                    },
                    {
                        "name": "Yaoxu Lyu"
                    },
                    {
                        "name": "Mingyu Cao"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "arxiv_comment": "22 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01525v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01525v3",
                "updated": "2025-05-06T16:11:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    11,
                    21,
                    1,
                    126,
                    0
                ],
                "published": "2024-08-02T18:35:58Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    35,
                    58,
                    4,
                    215,
                    0
                ],
                "title": "Gravitational-Wave and Gravitational-Wave Memory Signatures of\n  Core-Collapse Supernovae",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational-Wave and Gravitational-Wave Memory Signatures of\n  Core-Collapse Supernovae"
                },
                "summary": "In this paper, we calculate the energy, signal-to-noise ratio, detection\nrange, and angular anisotropy of the matter, matter memory, and neutrino memory\ngravitational wave (GW) signatures of 21 three-dimensional initially\nnon-rotating core-collapse supernova (CCSN) models carried to late times. We\nfind that inferred energy, signal-to-noise ratio, and detection range are\nangle-dependent quantities, and that the spread of possible energy,\nsignal-to-noise, and detection ranges across all viewing angles generally\nincreases with progenitor mass. When examining the low-frequency matter memory\nand neutrino memory components of the signal, we find that the neutrino memory\nis the most detectable component of a CCSN GW signal, and that DECIGO is\nbest-equipped to detect both matter memory and neutrino memory. Moreover, we\nfind that the polarization angle between the $h_+$ and $h_{\\times}$ strains\nserves as a unique identifier of matter and neutrino memory. Finally, we\ndevelop a galactic density- and stellar mass-weighted formalism to calculate\nthe rate at which we can expect to detect CCSN GW signals with Advanced LIGO.\nWhen considering only the matter component of the signal, the aLIGO detection\nrate is around 24$\\%$ of the total galactic supernova rate, but increases to\n92$\\%$ when incorporating the neutrino memory component. We find that all\nfuture detectors (ET, CE, DECIGO) will be able to detect CCSN GW signals from\nthe entire galaxy, and for the higher-mass progenitors even into the local\ngroup of galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we calculate the energy, signal-to-noise ratio, detection\nrange, and angular anisotropy of the matter, matter memory, and neutrino memory\ngravitational wave (GW) signatures of 21 three-dimensional initially\nnon-rotating core-collapse supernova (CCSN) models carried to late times. We\nfind that inferred energy, signal-to-noise ratio, and detection range are\nangle-dependent quantities, and that the spread of possible energy,\nsignal-to-noise, and detection ranges across all viewing angles generally\nincreases with progenitor mass. When examining the low-frequency matter memory\nand neutrino memory components of the signal, we find that the neutrino memory\nis the most detectable component of a CCSN GW signal, and that DECIGO is\nbest-equipped to detect both matter memory and neutrino memory. Moreover, we\nfind that the polarization angle between the $h_+$ and $h_{\\times}$ strains\nserves as a unique identifier of matter and neutrino memory. Finally, we\ndevelop a galactic density- and stellar mass-weighted formalism to calculate\nthe rate at which we can expect to detect CCSN GW signals with Advanced LIGO.\nWhen considering only the matter component of the signal, the aLIGO detection\nrate is around 24$\\%$ of the total galactic supernova rate, but increases to\n92$\\%$ when incorporating the neutrino memory component. We find that all\nfuture detectors (ET, CE, DECIGO) will be able to detect CCSN GW signals from\nthe entire galaxy, and for the higher-mass progenitors even into the local\ngroup of galaxies."
                },
                "authors": [
                    {
                        "name": "Lyla Choi"
                    },
                    {
                        "name": "Adam Burrows"
                    },
                    {
                        "name": "David Vartanyan"
                    }
                ],
                "author_detail": {
                    "name": "David Vartanyan"
                },
                "author": "David Vartanyan",
                "arxiv_comment": "37 pages, 18 figures. Published in ApJ. Minor corrections made to the\n  final aLIGO detection rates for both the matter and combined matter and\n  neutrino components of the signal. These changes do not affect any of our\n  figures, tables, or results from other sections of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01525v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01525v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03668v1",
                "updated": "2025-05-06T16:08:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    8,
                    55,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T16:08:55Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    8,
                    55,
                    1,
                    126,
                    0
                ],
                "title": "Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time"
                },
                "summary": "This paper proposes an integration of temporal logical reasoning and\nPartially Observable Markov Decision Processes (POMDPs) to achieve\ninterpretable decision-making under uncertainty with macro-actions. Our method\nleverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus\n(EC) to generate \\emph{persistent} (i.e., constant) macro-actions, which guide\nMonte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,\nsignificantly reducing inference time while ensuring robust performance. Such\nmacro-actions are learnt via Inductive Logic Programming (ILP) from a few\ntraces of execution (belief-action pairs), thus eliminating the need for\nmanually designed heuristics and requiring only the specification of the POMDP\ntransition model. In the Pocman and Rocksample benchmark scenarios, our learned\nmacro-actions demonstrate increased expressiveness and generality when compared\nto time-independent heuristics, indeed offering substantial computational\nefficiency improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an integration of temporal logical reasoning and\nPartially Observable Markov Decision Processes (POMDPs) to achieve\ninterpretable decision-making under uncertainty with macro-actions. Our method\nleverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus\n(EC) to generate \\emph{persistent} (i.e., constant) macro-actions, which guide\nMonte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,\nsignificantly reducing inference time while ensuring robust performance. Such\nmacro-actions are learnt via Inductive Logic Programming (ILP) from a few\ntraces of execution (belief-action pairs), thus eliminating the need for\nmanually designed heuristics and requiring only the specification of the POMDP\ntransition model. In the Pocman and Rocksample benchmark scenarios, our learned\nmacro-actions demonstrate increased expressiveness and generality when compared\nto time-independent heuristics, indeed offering substantial computational\nefficiency improvements."
                },
                "authors": [
                    {
                        "name": "Celeste Veronese"
                    },
                    {
                        "name": "Daniele Meli"
                    },
                    {
                        "name": "Alessandro Farinelli"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Farinelli"
                },
                "author": "Alessandro Farinelli",
                "arxiv_comment": "Accepted at 9th Conference on Neurosymbolic Learning and Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03666v1",
                "updated": "2025-05-06T16:06:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    6,
                    44,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T16:06:44Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    6,
                    44,
                    1,
                    126,
                    0
                ],
                "title": "A Centrality-independent Framework for Revealing Genuine Higher-Order\n  Cumulants in Heavy-Ion Collisions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Centrality-independent Framework for Revealing Genuine Higher-Order\n  Cumulants in Heavy-Ion Collisions"
                },
                "summary": "We propose a novel centrality definition-independent method for analyzing\nhigher-order proton cumulants, specifically addressing the challenge of volume\nfluctuations that dominate in low-energy heavy-ion collisions. This method\nreconstructs particle number distributions using the Edgeworth expansion, with\nparameters optimized via a combination of differential evolution algorithm and\nBayesian inference. Its effectiveness is validated using UrQMD model\nsimulations and benchmarked against traditional approaches, including\ncentrality definitions based on particle multiplicity. Our results show that\nthe proposed framework yields cumulant patterns consistent with those obtained\nusing number of participant nucleon ($N_{\\text{part}}$) based centrality\nobservables, while eliminating the conventional reliance on centrality\ndetermination. This consistency confirms the method's ability to extract\ngenuine physical signals, thereby paving the way for probing the intrinsic\nthermodynamic properties of the produced medium through event-by-event\nfluctuations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel centrality definition-independent method for analyzing\nhigher-order proton cumulants, specifically addressing the challenge of volume\nfluctuations that dominate in low-energy heavy-ion collisions. This method\nreconstructs particle number distributions using the Edgeworth expansion, with\nparameters optimized via a combination of differential evolution algorithm and\nBayesian inference. Its effectiveness is validated using UrQMD model\nsimulations and benchmarked against traditional approaches, including\ncentrality definitions based on particle multiplicity. Our results show that\nthe proposed framework yields cumulant patterns consistent with those obtained\nusing number of participant nucleon ($N_{\\text{part}}$) based centrality\nobservables, while eliminating the conventional reliance on centrality\ndetermination. This consistency confirms the method's ability to extract\ngenuine physical signals, thereby paving the way for probing the intrinsic\nthermodynamic properties of the produced medium through event-by-event\nfluctuations."
                },
                "authors": [
                    {
                        "name": "Zhaohui Wang"
                    },
                    {
                        "name": "Xiaofeng Luo"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Luo"
                },
                "author": "Xiaofeng Luo",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03655v1",
                "updated": "2025-05-06T16:00:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    0,
                    41,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T16:00:41Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    0,
                    41,
                    1,
                    126,
                    0
                ],
                "title": "Counterfactual Inference for Eliminating Sentiment Bias in Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Inference for Eliminating Sentiment Bias in Recommender\n  Systems"
                },
                "summary": "Recommender Systems (RSs) aim to provide personalized recommendations for\nusers. A newly discovered bias, known as sentiment bias, uncovers a common\nphenomenon within Review-based RSs (RRSs): the recommendation accuracy of users\nor items with negative reviews deteriorates compared with users or items with\npositive reviews. Critical users and niche items are disadvantaged by such\nunfair recommendations. We study this problem from the perspective of\ncounterfactual inference with two stages. At the model training stage, we build\na causal graph and model how sentiment influences the final rating score.\nDuring the inference stage, we decouple the direct and indirect effects to\nmitigate the impact of sentiment bias and remove the indirect effect using\ncounterfactual inference. We have conducted extensive experiments, and the\nresults validate that our model can achieve comparable performance on rating\nprediction for better recommendations and effective mitigation of sentiment\nbias. To the best of our knowledge, this is the first work to employ\ncounterfactual inference on sentiment bias mitigation in RSs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender Systems (RSs) aim to provide personalized recommendations for\nusers. A newly discovered bias, known as sentiment bias, uncovers a common\nphenomenon within Review-based RSs (RRSs): the recommendation accuracy of users\nor items with negative reviews deteriorates compared with users or items with\npositive reviews. Critical users and niche items are disadvantaged by such\nunfair recommendations. We study this problem from the perspective of\ncounterfactual inference with two stages. At the model training stage, we build\na causal graph and model how sentiment influences the final rating score.\nDuring the inference stage, we decouple the direct and indirect effects to\nmitigate the impact of sentiment bias and remove the indirect effect using\ncounterfactual inference. We have conducted extensive experiments, and the\nresults validate that our model can achieve comparable performance on rating\nprediction for better recommendations and effective mitigation of sentiment\nbias. To the best of our knowledge, this is the first work to employ\ncounterfactual inference on sentiment bias mitigation in RSs."
                },
                "authors": [
                    {
                        "name": "Le Pan"
                    },
                    {
                        "name": "Yuanjiang Cao"
                    },
                    {
                        "name": "Chengkai Huang"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17761v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17761v3",
                "updated": "2025-05-06T15:58:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    58,
                    40,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-24T17:25:12Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    25,
                    12,
                    3,
                    114,
                    0
                ],
                "title": "Step1X-Edit: A Practical Framework for General Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step1X-Edit: A Practical Framework for General Image Editing"
                },
                "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing."
                },
                "authors": [
                    {
                        "name": "Shiyu Liu"
                    },
                    {
                        "name": "Yucheng Han"
                    },
                    {
                        "name": "Peng Xing"
                    },
                    {
                        "name": "Fukun Yin"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Jiaqi Liao"
                    },
                    {
                        "name": "Yingming Wang"
                    },
                    {
                        "name": "Honghao Fu"
                    },
                    {
                        "name": "Chunrui Han"
                    },
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Yuang Peng"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Jingwei Wu"
                    },
                    {
                        "name": "Yan Cai"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Ranchen Ming"
                    },
                    {
                        "name": "Lei Xia"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "code: https://github.com/stepfun-ai/Step1X-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17761v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17761v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15627v2",
                "updated": "2025-05-06T15:53:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    53,
                    34,
                    1,
                    126,
                    0
                ],
                "published": "2025-01-26T18:25:26Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    18,
                    25,
                    26,
                    6,
                    26,
                    0
                ],
                "title": "HardML: A Benchmark For Evaluating Data Science And Machine Learning\n  knowledge and reasoning in AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HardML: A Benchmark For Evaluating Data Science And Machine Learning\n  knowledge and reasoning in AI"
                },
                "summary": "We present HardML, a benchmark designed to evaluate the knowledge and\nreasoning abilities in the fields of data science and machine learning. HardML\ncomprises a diverse set of 100 challenging multiple-choice questions,\nhandcrafted over a period of 6 months, covering the most popular and modern\nbranches of data science and machine learning. These questions are challenging\neven for a typical Senior Machine Learning Engineer to answer correctly. To\nminimize the risk of data contamination, HardML uses mostly original content\ndevised by the author. Current state of the art AI models achieve a 30% error\nrate on this benchmark, which is about 3 times larger than the one achieved on\nthe equivalent, well known MMLU ML. While HardML is limited in scope and not\naiming to push the frontier, primarily due to its multiple choice nature, it\nserves as a rigorous and modern testbed to quantify and track the progress of\ntop AI. While plenty benchmarks and experimentation in LLM evaluation exist in\nother STEM fields like mathematics, physics and chemistry, the subfields of\ndata science and machine learning remain fairly underexplored.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present HardML, a benchmark designed to evaluate the knowledge and\nreasoning abilities in the fields of data science and machine learning. HardML\ncomprises a diverse set of 100 challenging multiple-choice questions,\nhandcrafted over a period of 6 months, covering the most popular and modern\nbranches of data science and machine learning. These questions are challenging\neven for a typical Senior Machine Learning Engineer to answer correctly. To\nminimize the risk of data contamination, HardML uses mostly original content\ndevised by the author. Current state of the art AI models achieve a 30% error\nrate on this benchmark, which is about 3 times larger than the one achieved on\nthe equivalent, well known MMLU ML. While HardML is limited in scope and not\naiming to push the frontier, primarily due to its multiple choice nature, it\nserves as a rigorous and modern testbed to quantify and track the progress of\ntop AI. While plenty benchmarks and experimentation in LLM evaluation exist in\nother STEM fields like mathematics, physics and chemistry, the subfields of\ndata science and machine learning remain fairly underexplored."
                },
                "authors": [
                    {
                        "name": "Tidor-Vlad Pricope"
                    }
                ],
                "author_detail": {
                    "name": "Tidor-Vlad Pricope"
                },
                "author": "Tidor-Vlad Pricope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03646v1",
                "updated": "2025-05-06T15:52:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    52,
                    14,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T15:52:14Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    52,
                    14,
                    1,
                    126,
                    0
                ],
                "title": "ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders"
                },
                "summary": "Despite the extensive use of deep autoencoders (AEs) in critical\napplications, their adversarial robustness remains relatively underexplored\ncompared to classification models. AE robustness is characterized by the\nLipschitz bounds of its components. Existing robustness evaluation frameworks\nbased on white-box attacks do not fully exploit the vulnerabilities of\nintermediate ill-conditioned layers in AEs. In the context of optimizing\nimperceptible norm-bounded additive perturbations to maximize output damage,\nexisting methods struggle to effectively propagate adversarial loss gradients\nthroughout the network, often converging to less effective perturbations. To\naddress this, we propose a novel layer-conditioning-based adversarial\noptimization objective that effectively guides the adversarial map toward\nregions of local Lipschitz bounds by enhancing loss gradient information\npropagation during attack optimization. We demonstrate through extensive\nexperiments on state-of-the-art AEs that our adversarial objective results in\nstronger attacks, outperforming existing methods in both universal and\nsample-specific scenarios. As a defense method against this attack, we\nintroduce an inference-time adversarially trained defense plugin that mitigates\nthe effects of adversarial examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the extensive use of deep autoencoders (AEs) in critical\napplications, their adversarial robustness remains relatively underexplored\ncompared to classification models. AE robustness is characterized by the\nLipschitz bounds of its components. Existing robustness evaluation frameworks\nbased on white-box attacks do not fully exploit the vulnerabilities of\nintermediate ill-conditioned layers in AEs. In the context of optimizing\nimperceptible norm-bounded additive perturbations to maximize output damage,\nexisting methods struggle to effectively propagate adversarial loss gradients\nthroughout the network, often converging to less effective perturbations. To\naddress this, we propose a novel layer-conditioning-based adversarial\noptimization objective that effectively guides the adversarial map toward\nregions of local Lipschitz bounds by enhancing loss gradient information\npropagation during attack optimization. We demonstrate through extensive\nexperiments on state-of-the-art AEs that our adversarial objective results in\nstronger attacks, outperforming existing methods in both universal and\nsample-specific scenarios. As a defense method against this attack, we\nintroduce an inference-time adversarially trained defense plugin that mitigates\nthe effects of adversarial examples."
                },
                "authors": [
                    {
                        "name": "Chethan Krishnamurthy Ramanaik"
                    },
                    {
                        "name": "Arjun Roy"
                    },
                    {
                        "name": "Eirini Ntoutsi"
                    }
                ],
                "author_detail": {
                    "name": "Eirini Ntoutsi"
                },
                "author": "Eirini Ntoutsi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08827v2",
                "updated": "2025-05-06T15:37:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    37,
                    59,
                    1,
                    126,
                    0
                ],
                "published": "2024-12-11T23:58:28Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    23,
                    58,
                    28,
                    2,
                    346,
                    0
                ],
                "title": "A Debiased Estimator for the Mediation Functional in\n  Ultra-High-Dimensional Setting in the Presence of Interaction Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Debiased Estimator for the Mediation Functional in\n  Ultra-High-Dimensional Setting in the Presence of Interaction Effects"
                },
                "summary": "Mediation analysis is a crucial tool for uncovering the mechanisms through\nwhich a treatment affects the outcome, providing deeper causal insights and\nguiding effective interventions. Despite advances in analyzing the mediation\neffect with fixed/low-dimensional mediators and covariates, our understanding\nof estimation and inference of mediation functional in the presence of\n(ultra)-high-dimensional mediators and covariates is still limited. In this\npaper, we present an estimator for mediation functional in a high-dimensional\nsetting that accommodates the interaction between covariates and treatment in\ngenerating mediators, as well as interactions between both covariates and\ntreatment and mediators and treatment in generating the response. We\ndemonstrate that our estimator is $\\sqrt{n}$-consistent and asymptotically\nnormal, thus enabling reliable inference on direct and indirect treatment\neffects with asymptotically valid confidence intervals. A key technical\ncontribution of our work is to develop a multi-step debiasing technique, which\nmay also be valuable in other statistical settings with similar structural\ncomplexities where accurate estimation depends on debiasing. We evaluate our\nproposed methodology through extensive simulation studies and apply it to the\nTCGA lung cancer dataset to estimate the effect of smoking, mediated by DNA\nmethylation, on the survival time of lung cancer patients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mediation analysis is a crucial tool for uncovering the mechanisms through\nwhich a treatment affects the outcome, providing deeper causal insights and\nguiding effective interventions. Despite advances in analyzing the mediation\neffect with fixed/low-dimensional mediators and covariates, our understanding\nof estimation and inference of mediation functional in the presence of\n(ultra)-high-dimensional mediators and covariates is still limited. In this\npaper, we present an estimator for mediation functional in a high-dimensional\nsetting that accommodates the interaction between covariates and treatment in\ngenerating mediators, as well as interactions between both covariates and\ntreatment and mediators and treatment in generating the response. We\ndemonstrate that our estimator is $\\sqrt{n}$-consistent and asymptotically\nnormal, thus enabling reliable inference on direct and indirect treatment\neffects with asymptotically valid confidence intervals. A key technical\ncontribution of our work is to develop a multi-step debiasing technique, which\nmay also be valuable in other statistical settings with similar structural\ncomplexities where accurate estimation depends on debiasing. We evaluate our\nproposed methodology through extensive simulation studies and apply it to the\nTCGA lung cancer dataset to estimate the effect of smoking, mediated by DNA\nmethylation, on the survival time of lung cancer patients."
                },
                "authors": [
                    {
                        "name": "Shi Bo"
                    },
                    {
                        "name": "AmirEmad Ghassami"
                    },
                    {
                        "name": "Debarghya Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debarghya Mukherjee"
                },
                "author": "Debarghya Mukherjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15037v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15037v5",
                "updated": "2025-05-06T15:36:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    36,
                    35,
                    1,
                    126,
                    0
                ],
                "published": "2025-02-20T20:46:09Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    20,
                    46,
                    9,
                    3,
                    51,
                    0
                ],
                "title": "DEFT: Differentiable Branched Discrete Elastic Rods for Modeling\n  Furcated DLOs in Real-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DEFT: Differentiable Branched Discrete Elastic Rods for Modeling\n  Furcated DLOs in Real-Time"
                },
                "summary": "Autonomous wire harness assembly requires robots to manipulate complex\nbranched cables with high precision and reliability. A key challenge in\nautomating this process is predicting how these flexible and branched\nstructures behave under manipulation. Without accurate predictions, it is\ndifficult for robots to reliably plan or execute assembly operations. While\nexisting research has made progress in modeling single-threaded Deformable\nLinear Objects (DLOs), extending these approaches to Branched Deformable Linear\nObjects (BDLOs) presents fundamental challenges. The junction points in BDLOs\ncreate complex force interactions and strain propagation patterns that cannot\nbe adequately captured by simply connecting multiple single-DLO models. To\naddress these challenges, this paper presents Differentiable discrete branched\nElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework\nthat combines a differentiable physics-based model with a learning framework\nto: 1) accurately model BDLO dynamics, including dynamic propagation at\njunction points and grasping in the middle of a BDLO, 2) achieve efficient\ncomputation for real-time inference, and 3) enable planning to demonstrate\ndexterous BDLO manipulation. A comprehensive series of real-world experiments\ndemonstrates DEFT's efficacy in terms of accuracy, computational speed, and\ngeneralizability compared to state-of-the-art alternatives. Project\npage:https://roahmlab.github.io/DEFT/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous wire harness assembly requires robots to manipulate complex\nbranched cables with high precision and reliability. A key challenge in\nautomating this process is predicting how these flexible and branched\nstructures behave under manipulation. Without accurate predictions, it is\ndifficult for robots to reliably plan or execute assembly operations. While\nexisting research has made progress in modeling single-threaded Deformable\nLinear Objects (DLOs), extending these approaches to Branched Deformable Linear\nObjects (BDLOs) presents fundamental challenges. The junction points in BDLOs\ncreate complex force interactions and strain propagation patterns that cannot\nbe adequately captured by simply connecting multiple single-DLO models. To\naddress these challenges, this paper presents Differentiable discrete branched\nElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework\nthat combines a differentiable physics-based model with a learning framework\nto: 1) accurately model BDLO dynamics, including dynamic propagation at\njunction points and grasping in the middle of a BDLO, 2) achieve efficient\ncomputation for real-time inference, and 3) enable planning to demonstrate\ndexterous BDLO manipulation. A comprehensive series of real-world experiments\ndemonstrates DEFT's efficacy in terms of accuracy, computational speed, and\ngeneralizability compared to state-of-the-art alternatives. Project\npage:https://roahmlab.github.io/DEFT/."
                },
                "authors": [
                    {
                        "name": "Yizhou Chen"
                    },
                    {
                        "name": "Xiaoyue Wu"
                    },
                    {
                        "name": "Yeheng Zong"
                    },
                    {
                        "name": "Yuzhen Chen"
                    },
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Bohao Zhang"
                    },
                    {
                        "name": "Ram Vasudevan"
                    }
                ],
                "author_detail": {
                    "name": "Ram Vasudevan"
                },
                "author": "Ram Vasudevan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15037v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15037v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v3",
                "updated": "2025-05-06T15:23:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    23,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03621v1",
                "updated": "2025-05-06T15:18:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    18,
                    38,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T15:18:38Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    18,
                    38,
                    1,
                    126,
                    0
                ],
                "title": "PhysLLM: Harnessing Large Language Models for Cross-Modal Remote\n  Physiological Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysLLM: Harnessing Large Language Models for Cross-Modal Remote\n  Physiological Sensing"
                },
                "summary": "Remote photoplethysmography (rPPG) enables non-contact physiological\nmeasurement but remains highly susceptible to illumination changes, motion\nartifacts, and limited temporal modeling. Large Language Models (LLMs) excel at\ncapturing long-range dependencies, offering a potential solution but struggle\nwith the continuous, noise-sensitive nature of rPPG signals due to their\ntext-centric design. To bridge this gap, we introduce PhysLLM, a collaborative\noptimization framework that synergizes LLMs with domain-specific rPPG\ncomponents. Specifically, the Text Prototype Guidance (TPG) strategy is\nproposed to establish cross-modal alignment by projecting hemodynamic features\ninto LLM-interpretable semantic space, effectively bridging the\nrepresentational gap between physiological signals and linguistic tokens.\nBesides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for\nresolving signal instability through adaptive time-frequency domain feature\nre-weighting. Finally, rPPG task-specific cues systematically inject\nphysiological priors through physiological statistics, environmental contextual\nanswering, and task description, leveraging cross-modal learning to integrate\nboth visual and textual information, enabling dynamic adaptation to challenging\nscenarios like variable illumination and subject movements. Evaluation on four\nbenchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,\ndemonstrating superior generalization across lighting variations and motion\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote photoplethysmography (rPPG) enables non-contact physiological\nmeasurement but remains highly susceptible to illumination changes, motion\nartifacts, and limited temporal modeling. Large Language Models (LLMs) excel at\ncapturing long-range dependencies, offering a potential solution but struggle\nwith the continuous, noise-sensitive nature of rPPG signals due to their\ntext-centric design. To bridge this gap, we introduce PhysLLM, a collaborative\noptimization framework that synergizes LLMs with domain-specific rPPG\ncomponents. Specifically, the Text Prototype Guidance (TPG) strategy is\nproposed to establish cross-modal alignment by projecting hemodynamic features\ninto LLM-interpretable semantic space, effectively bridging the\nrepresentational gap between physiological signals and linguistic tokens.\nBesides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for\nresolving signal instability through adaptive time-frequency domain feature\nre-weighting. Finally, rPPG task-specific cues systematically inject\nphysiological priors through physiological statistics, environmental contextual\nanswering, and task description, leveraging cross-modal learning to integrate\nboth visual and textual information, enabling dynamic adaptation to challenging\nscenarios like variable illumination and subject movements. Evaluation on four\nbenchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,\ndemonstrating superior generalization across lighting variations and motion\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yiping Xie"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Mingtong Dai"
                    },
                    {
                        "name": "Jian-Ping Zhou"
                    },
                    {
                        "name": "Yue Sun"
                    },
                    {
                        "name": "Tao Tan"
                    },
                    {
                        "name": "Weicheng Xie"
                    },
                    {
                        "name": "Linlin Shen"
                    },
                    {
                        "name": "Zitong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zitong Yu"
                },
                "author": "Zitong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19187v2",
                "updated": "2025-05-06T15:11:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    11,
                    32,
                    1,
                    126,
                    0
                ],
                "published": "2025-02-26T14:50:50Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    50,
                    50,
                    2,
                    57,
                    0
                ],
                "title": "BIG-Bench Extra Hard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIG-Bench Extra Hard"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in everyday\napplications, demanding robust general reasoning capabilities and diverse\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\nwhich has served as a crucial benchmark for evaluating the general reasoning\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\nallowed for a comprehensive assessment of general reasoning across various\nskills within a unified framework. However, recent advances in LLMs have led to\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\nprobes a similar reasoning capability but exhibits significantly increased\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\naccuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best\nreasoning-specialized model, indicating substantial room for improvement and\nhighlighting the ongoing challenge of achieving robust general reasoning in\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in everyday\napplications, demanding robust general reasoning capabilities and diverse\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\nwhich has served as a crucial benchmark for evaluating the general reasoning\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\nallowed for a comprehensive assessment of general reasoning across various\nskills within a unified framework. However, recent advances in LLMs have led to\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\nprobes a similar reasoning capability but exhibits significantly increased\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\naccuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best\nreasoning-specialized model, indicating substantial room for improvement and\nhighlighting the ongoing challenge of achieving robust general reasoning in\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh."
                },
                "authors": [
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Bahare Fatemi"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "John Palowitch"
                    },
                    {
                        "name": "Chrysovalantis Anastasiou"
                    },
                    {
                        "name": "Sanket Vaibhav Mehta"
                    },
                    {
                        "name": "Lalit K. Jain"
                    },
                    {
                        "name": "Virginia Aglietti"
                    },
                    {
                        "name": "Disha Jindal"
                    },
                    {
                        "name": "Peter Chen"
                    },
                    {
                        "name": "Nishanth Dikkala"
                    },
                    {
                        "name": "Gladys Tyen"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Uri Shalit"
                    },
                    {
                        "name": "Silvia Chiappa"
                    },
                    {
                        "name": "Kate Olszewska"
                    },
                    {
                        "name": "Yi Tay"
                    },
                    {
                        "name": "Vinh Q. Tran"
                    },
                    {
                        "name": "Quoc V. Le"
                    },
                    {
                        "name": "Orhan Firat"
                    }
                ],
                "author_detail": {
                    "name": "Orhan Firat"
                },
                "author": "Orhan Firat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03603v2",
                "updated": "2025-05-07T03:47:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    3,
                    47,
                    51,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-06T15:03:58Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    3,
                    58,
                    1,
                    126,
                    0
                ],
                "title": "PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model"
                },
                "summary": "Audio-driven human animation technology is widely used in human-computer\ninteraction, and the emergence of diffusion models has further advanced its\ndevelopment. Currently, most methods rely on multi-stage generation and\nintermediate representations, resulting in long inference time and issues with\ngeneration quality in specific foreground regions and audio-motion consistency.\nThese shortcomings are primarily due to the lack of localized fine-grained\nsupervised guidance. To address above challenges, we propose PAHA, an\nend-to-end audio-driven upper-body human animation framework with diffusion\nmodel. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts\nConsistency Enhancement (PCE). PAR dynamically adjusts regional training loss\nweights based on pose confidence scores, effectively improving visual quality.\nPCE constructs and trains diffusion-based regional audio-visual classifiers to\nimprove the consistency of motion and co-speech audio. Afterwards, we design\ntwo novel inference guidance methods for the foregoing classifiers, Sequential\nGuidance (SG) and Differential Guidance (DG), to balance efficiency and quality\nrespectively. Additionally, we build CNAS, the first public Chinese News Anchor\nSpeech dataset, to advance research and validation in this field. Extensive\nexperimental results and user studies demonstrate that PAHA significantly\noutperforms existing methods in audio-motion alignment and video-related\nevaluations. The codes and CNAS dataset will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-driven human animation technology is widely used in human-computer\ninteraction, and the emergence of diffusion models has further advanced its\ndevelopment. Currently, most methods rely on multi-stage generation and\nintermediate representations, resulting in long inference time and issues with\ngeneration quality in specific foreground regions and audio-motion consistency.\nThese shortcomings are primarily due to the lack of localized fine-grained\nsupervised guidance. To address above challenges, we propose PAHA, an\nend-to-end audio-driven upper-body human animation framework with diffusion\nmodel. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts\nConsistency Enhancement (PCE). PAR dynamically adjusts regional training loss\nweights based on pose confidence scores, effectively improving visual quality.\nPCE constructs and trains diffusion-based regional audio-visual classifiers to\nimprove the consistency of motion and co-speech audio. Afterwards, we design\ntwo novel inference guidance methods for the foregoing classifiers, Sequential\nGuidance (SG) and Differential Guidance (DG), to balance efficiency and quality\nrespectively. Additionally, we build CNAS, the first public Chinese News Anchor\nSpeech dataset, to advance research and validation in this field. Extensive\nexperimental results and user studies demonstrate that PAHA significantly\noutperforms existing methods in audio-motion alignment and video-related\nevaluations. The codes and CNAS dataset will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Y. B. Wang"
                    },
                    {
                        "name": "S. Z. Zhou"
                    },
                    {
                        "name": "J. F. Wu"
                    },
                    {
                        "name": "T. Hu"
                    },
                    {
                        "name": "J. N. Zhang"
                    },
                    {
                        "name": "Y. Liu"
                    }
                ],
                "author_detail": {
                    "name": "Y. Liu"
                },
                "author": "Y. Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13379v2",
                "updated": "2025-05-06T14:56:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    56,
                    21,
                    1,
                    126,
                    0
                ],
                "published": "2025-01-23T04:43:10Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    4,
                    43,
                    10,
                    3,
                    23,
                    0
                ],
                "title": "A Quantitative Evaluation of Approximate Softmax Functions for Deep\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Quantitative Evaluation of Approximate Softmax Functions for Deep\n  Neural Networks"
                },
                "summary": "The softmax function is a widely used activation function in the output\nlayers of neural networks, responsible for converting raw scores into class\nprobabilities while introducing essential non-linearity. Implementing Softmax\nefficiently poses challenges on low-end FPGAs due to limited hardware resources\nand the computational complexity of exponential and division operations. This\nwork evaluates approximate computing techniques for softmax acceleration using\nTaylor series and interpolation methods using Look-Up Tables (LUTs). These\napproximations aim to reduce execution time and resource consumption while\nmaintaining acceptable levels of numerical precision. Our findings show that\nquadratic interpolation with LUTs yields the lowest numerical error. In\ncontrast, Taylor-based approximations offer significantly better performance in\nterms of execution time and resource efficiency due to their computational\nsimplicity. When applied to real-world deep learning models such as LeNet-5 and\nMobileNet v2, the first- and second-order Taylor approximations provided\nsubstantial trade-offs between accuracy and resource savings, achieving up to\n0.2% accuracy degradation and 14% resource reduction compared to exact\nimplementations. These results highlight the effectiveness of approximate\nSoftmax designs on resource-constrained FPGAs and lay the groundwork for their\nintegration into larger models, including large language models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The softmax function is a widely used activation function in the output\nlayers of neural networks, responsible for converting raw scores into class\nprobabilities while introducing essential non-linearity. Implementing Softmax\nefficiently poses challenges on low-end FPGAs due to limited hardware resources\nand the computational complexity of exponential and division operations. This\nwork evaluates approximate computing techniques for softmax acceleration using\nTaylor series and interpolation methods using Look-Up Tables (LUTs). These\napproximations aim to reduce execution time and resource consumption while\nmaintaining acceptable levels of numerical precision. Our findings show that\nquadratic interpolation with LUTs yields the lowest numerical error. In\ncontrast, Taylor-based approximations offer significantly better performance in\nterms of execution time and resource efficiency due to their computational\nsimplicity. When applied to real-world deep learning models such as LeNet-5 and\nMobileNet v2, the first- and second-order Taylor approximations provided\nsubstantial trade-offs between accuracy and resource savings, achieving up to\n0.2% accuracy degradation and 14% resource reduction compared to exact\nimplementations. These results highlight the effectiveness of approximate\nSoftmax designs on resource-constrained FPGAs and lay the groundwork for their\nintegration into larger models, including large language models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Anthony Leiva-Valverde"
                    },
                    {
                        "name": "Fabricio Elizondo-Fernández"
                    },
                    {
                        "name": "Luis G. León-Vega"
                    },
                    {
                        "name": "Cristina Meinhardt"
                    },
                    {
                        "name": "Jorge Castro-Godínez"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Castro-Godínez"
                },
                "author": "Jorge Castro-Godínez",
                "arxiv_comment": "A new author has been added due to his contributions in the FPGA part\n  (Section IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.04306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.04306v2",
                "updated": "2025-05-06T14:51:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    51,
                    7,
                    1,
                    126,
                    0
                ],
                "published": "2023-10-06T15:05:41Z",
                "published_parsed": [
                    2023,
                    10,
                    6,
                    15,
                    5,
                    41,
                    4,
                    279,
                    0
                ],
                "title": "Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware\n  Learning"
                },
                "summary": "Group-level emotion recognition (GER) is an inseparable part of human\nbehavior analysis, aiming to recognize an overall emotion in a multi-person\nscene. However, the existing methods are devoted to combing diverse emotion\ncues while ignoring the inherent uncertainties under unconstrained\nenvironments, such as congestion and occlusion occurring within a group.\nAdditionally, since only group-level labels are available, inconsistent emotion\npredictions among individuals in one group can confuse the network. In this\npaper, we propose an uncertainty-aware learning (UAL) method to extract more\nrobust representations for GER. By explicitly modeling the uncertainty of each\nindividual, we utilize stochastic embedding drawn from a Gaussian distribution\ninstead of deterministic point embedding. This representation captures the\nprobabilities of different emotions and generates diverse predictions through\nthis stochasticity during the inference stage. Furthermore,\nuncertainty-sensitive scores are adaptively assigned as the fusion weights of\nindividuals' face within each group. Moreover, we develop an image enhancement\nmodule to enhance the model's robustness against severe noise. The overall\nthree-branch model, encompassing face, object, and scene component, is guided\nby a proportional-weighted fusion strategy and integrates the proposed\nuncertainty-aware method to produce the final group-level output. Experimental\nresults demonstrate the effectiveness and generalization ability of our method\nacross three widely used databases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group-level emotion recognition (GER) is an inseparable part of human\nbehavior analysis, aiming to recognize an overall emotion in a multi-person\nscene. However, the existing methods are devoted to combing diverse emotion\ncues while ignoring the inherent uncertainties under unconstrained\nenvironments, such as congestion and occlusion occurring within a group.\nAdditionally, since only group-level labels are available, inconsistent emotion\npredictions among individuals in one group can confuse the network. In this\npaper, we propose an uncertainty-aware learning (UAL) method to extract more\nrobust representations for GER. By explicitly modeling the uncertainty of each\nindividual, we utilize stochastic embedding drawn from a Gaussian distribution\ninstead of deterministic point embedding. This representation captures the\nprobabilities of different emotions and generates diverse predictions through\nthis stochasticity during the inference stage. Furthermore,\nuncertainty-sensitive scores are adaptively assigned as the fusion weights of\nindividuals' face within each group. Moreover, we develop an image enhancement\nmodule to enhance the model's robustness against severe noise. The overall\nthree-branch model, encompassing face, object, and scene component, is guided\nby a proportional-weighted fusion strategy and integrates the proposed\nuncertainty-aware method to produce the final group-level output. Experimental\nresults demonstrate the effectiveness and generalization ability of our method\nacross three widely used databases."
                },
                "authors": [
                    {
                        "name": "Qing Zhu"
                    },
                    {
                        "name": "Qirong Mao"
                    },
                    {
                        "name": "Jialin Zhang"
                    },
                    {
                        "name": "Xiaohua Huang"
                    },
                    {
                        "name": "Wenming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Wenming Zheng"
                },
                "author": "Wenming Zheng",
                "arxiv_comment": "11 pages,3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.04306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.04306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03590v1",
                "updated": "2025-05-06T14:50:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    50,
                    14,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:50:14Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    50,
                    14,
                    1,
                    126,
                    0
                ],
                "title": "Physics-Informed Sylvester Normalizing Flows for Bayesian Inference in\n  Magnetic Resonance Spectroscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Sylvester Normalizing Flows for Bayesian Inference in\n  Magnetic Resonance Spectroscopy"
                },
                "summary": "Magnetic resonance spectroscopy (MRS) is a non-invasive technique to measure\nthe metabolic composition of tissues, offering valuable insights into\nneurological disorders, tumor detection, and other metabolic dysfunctions.\nHowever, accurate metabolite quantification is hindered by challenges such as\nspectral overlap, low signal-to-noise ratio, and various artifacts. Traditional\nmethods like linear-combination modeling are susceptible to ambiguities and\ncommonly only provide a theoretical lower bound on estimation accuracy in the\nform of the Cram\\'er-Rao bound. This work introduces a Bayesian inference\nframework using Sylvester normalizing flows (SNFs) to approximate posterior\ndistributions over metabolite concentrations, enhancing quantification\nreliability. A physics-based decoder incorporates prior knowledge of MRS signal\nformation, ensuring realistic distribution representations. We validate the\nmethod on simulated 7T proton MRS data, demonstrating accurate metabolite\nquantification, well-calibrated uncertainties, and insights into parameter\ncorrelations and multi-modal distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic resonance spectroscopy (MRS) is a non-invasive technique to measure\nthe metabolic composition of tissues, offering valuable insights into\nneurological disorders, tumor detection, and other metabolic dysfunctions.\nHowever, accurate metabolite quantification is hindered by challenges such as\nspectral overlap, low signal-to-noise ratio, and various artifacts. Traditional\nmethods like linear-combination modeling are susceptible to ambiguities and\ncommonly only provide a theoretical lower bound on estimation accuracy in the\nform of the Cram\\'er-Rao bound. This work introduces a Bayesian inference\nframework using Sylvester normalizing flows (SNFs) to approximate posterior\ndistributions over metabolite concentrations, enhancing quantification\nreliability. A physics-based decoder incorporates prior knowledge of MRS signal\nformation, ensuring realistic distribution representations. We validate the\nmethod on simulated 7T proton MRS data, demonstrating accurate metabolite\nquantification, well-calibrated uncertainties, and insights into parameter\ncorrelations and multi-modal distributions."
                },
                "authors": [
                    {
                        "name": "Julian P. Merkofer"
                    },
                    {
                        "name": "Dennis M. J. van de Sande"
                    },
                    {
                        "name": "Alex A. Bhogal"
                    },
                    {
                        "name": "Ruud J. G. van Sloun"
                    }
                ],
                "author_detail": {
                    "name": "Ruud J. G. van Sloun"
                },
                "author": "Ruud J. G. van Sloun",
                "arxiv_comment": "Preprint submitted to IEEE MLSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05356v2",
                "updated": "2025-05-06T14:46:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    46,
                    21,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-07T09:26:25Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    26,
                    25,
                    0,
                    97,
                    0
                ],
                "title": "DyTTP: Trajectory Prediction with Normalization-Free Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyTTP: Trajectory Prediction with Normalization-Free Transformers"
                },
                "summary": "Accurate trajectory prediction is a cornerstone for the safe operation of\nautonomous driving systems, where understanding the dynamic behavior of\nsurrounding agents is crucial. Transformer-based architectures have\ndemonstrated significant promise in capturing complex spatio-temporality\ndependencies. However, their reliance on normalization layers can lead to\ncomputation overhead and training instabilities. In this work, we present a\ntwo-fold approach to address these challenges. First, we integrate DynamicTanh\n(DyT), which is the latest method to promote transformers, into the backbone,\nreplacing traditional layer normalization. This modification simplifies the\nnetwork architecture and improves the stability of the inference. We are the\nfirst work to deploy the DyT to the trajectory prediction task. Complementing\nthis, we employ a snapshot ensemble strategy to further boost trajectory\nprediction performance. Using cyclical learning rate scheduling, multiple model\nsnapshots are captured during a single training run. These snapshots are then\naggregated via simple averaging at inference time, allowing the model to\nbenefit from diverse hypotheses without incurring substantial additional\ncomputational cost. Extensive experiments on Argoverse datasets demonstrate\nthat our combined approach significantly improves prediction accuracy,\ninference speed and robustness in diverse driving scenarios. This work\nunderscores the potential of normalization-free transformer designs augmented\nwith lightweight ensemble techniques in advancing trajectory forecasting for\nautonomous vehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate trajectory prediction is a cornerstone for the safe operation of\nautonomous driving systems, where understanding the dynamic behavior of\nsurrounding agents is crucial. Transformer-based architectures have\ndemonstrated significant promise in capturing complex spatio-temporality\ndependencies. However, their reliance on normalization layers can lead to\ncomputation overhead and training instabilities. In this work, we present a\ntwo-fold approach to address these challenges. First, we integrate DynamicTanh\n(DyT), which is the latest method to promote transformers, into the backbone,\nreplacing traditional layer normalization. This modification simplifies the\nnetwork architecture and improves the stability of the inference. We are the\nfirst work to deploy the DyT to the trajectory prediction task. Complementing\nthis, we employ a snapshot ensemble strategy to further boost trajectory\nprediction performance. Using cyclical learning rate scheduling, multiple model\nsnapshots are captured during a single training run. These snapshots are then\naggregated via simple averaging at inference time, allowing the model to\nbenefit from diverse hypotheses without incurring substantial additional\ncomputational cost. Extensive experiments on Argoverse datasets demonstrate\nthat our combined approach significantly improves prediction accuracy,\ninference speed and robustness in diverse driving scenarios. This work\nunderscores the potential of normalization-free transformer designs augmented\nwith lightweight ensemble techniques in advancing trajectory forecasting for\nautonomous vehicles."
                },
                "authors": [
                    {
                        "name": "JianLin Zhu"
                    },
                    {
                        "name": "HongKuo Niu"
                    }
                ],
                "author_detail": {
                    "name": "HongKuo Niu"
                },
                "author": "HongKuo Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03577v1",
                "updated": "2025-05-06T14:36:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    36,
                    7,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:36:07Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    36,
                    7,
                    1,
                    126,
                    0
                ],
                "title": "Information-theoretic reduction of deep neural networks to linear models\n  in the overparametrized proportional regime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-theoretic reduction of deep neural networks to linear models\n  in the overparametrized proportional regime"
                },
                "summary": "We rigorously analyse fully-trained neural networks of arbitrary depth in the\nBayesian optimal setting in the so-called proportional scaling regime where the\nnumber of training samples and width of the input and all inner layers diverge\nproportionally. We prove an information-theoretic equivalence between the\nBayesian deep neural network model trained from data generated by a teacher\nwith matching architecture, and a simpler model of optimal inference in a\ngeneralized linear model. This equivalence enables us to compute the optimal\ngeneralization error for deep neural networks in this regime. We thus prove the\n\"deep Gaussian equivalence principle\" conjectured in Cui et al. (2023)\n(arXiv:2302.00375). Our result highlights that in order to escape this\n\"trivialisation\" of deep neural networks (in the sense of reduction to a linear\nmodel) happening in the strongly overparametrized proportional regime, models\ntrained from much more data have to be considered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We rigorously analyse fully-trained neural networks of arbitrary depth in the\nBayesian optimal setting in the so-called proportional scaling regime where the\nnumber of training samples and width of the input and all inner layers diverge\nproportionally. We prove an information-theoretic equivalence between the\nBayesian deep neural network model trained from data generated by a teacher\nwith matching architecture, and a simpler model of optimal inference in a\ngeneralized linear model. This equivalence enables us to compute the optimal\ngeneralization error for deep neural networks in this regime. We thus prove the\n\"deep Gaussian equivalence principle\" conjectured in Cui et al. (2023)\n(arXiv:2302.00375). Our result highlights that in order to escape this\n\"trivialisation\" of deep neural networks (in the sense of reduction to a linear\nmodel) happening in the strongly overparametrized proportional regime, models\ntrained from much more data have to be considered."
                },
                "authors": [
                    {
                        "name": "Francesco Camilli"
                    },
                    {
                        "name": "Daria Tieplova"
                    },
                    {
                        "name": "Eleonora Bergamin"
                    },
                    {
                        "name": "Jean Barbier"
                    }
                ],
                "author_detail": {
                    "name": "Jean Barbier"
                },
                "author": "Jean Barbier",
                "arxiv_comment": "Accepted to the 38th Annual Conference on Learning Theory (COLT\n  2025), 41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17279v2",
                "updated": "2025-05-06T14:34:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    34,
                    33,
                    1,
                    126,
                    0
                ],
                "published": "2025-03-21T16:27:12Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    27,
                    12,
                    4,
                    80,
                    0
                ],
                "title": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic\n  Textual Similarity Measurement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic\n  Textual Similarity Measurement"
                },
                "summary": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance."
                },
                "authors": [
                    {
                        "name": "Gaifan Zhang"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Danushka Bollegala"
                    }
                ],
                "author_detail": {
                    "name": "Danushka Bollegala"
                },
                "author": "Danushka Bollegala",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03574v1",
                "updated": "2025-05-06T14:34:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    34,
                    21,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:34:21Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    34,
                    21,
                    1,
                    126,
                    0
                ],
                "title": "LlamaFirewall: An open source guardrail system for building secure AI\n  agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaFirewall: An open source guardrail system for building secure AI\n  agents"
                },
                "summary": "Large language models (LLMs) have evolved from simple chatbots into\nautonomous agents capable of performing complex tasks such as editing\nproduction code, orchestrating workflows, and taking higher-stakes actions\nbased on untrusted inputs like webpages and emails. These capabilities\nintroduce new security risks that existing security measures, such as model\nfine-tuning or chatbot-focused guardrails, do not fully address. Given the\nhigher stakes and the absence of deterministic solutions to mitigate these\nrisks, there is a critical need for a real-time guardrail monitor to serve as a\nfinal layer of defense, and support system level, use case specific safety\npolicy definition and enforcement. We introduce LlamaFirewall, an open-source\nsecurity focused guardrail framework designed to serve as a final layer of\ndefense against security risks associated with AI Agents. Our framework\nmitigates risks such as prompt injection, agent misalignment, and insecure code\nrisks through three powerful guardrails: PromptGuard 2, a universal jailbreak\ndetector that demonstrates clear state of the art performance; Agent Alignment\nChecks, a chain-of-thought auditor that inspects agent reasoning for prompt\ninjection and goal misalignment, which, while still experimental, shows\nstronger efficacy at preventing indirect injections in general scenarios than\npreviously proposed approaches; and CodeShield, an online static analysis\nengine that is both fast and extensible, aimed at preventing the generation of\ninsecure or dangerous code by coding agents. Additionally, we include\neasy-to-use customizable scanners that make it possible for any developer who\ncan write a regular expression or an LLM prompt to quickly update an agent's\nsecurity guardrails.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have evolved from simple chatbots into\nautonomous agents capable of performing complex tasks such as editing\nproduction code, orchestrating workflows, and taking higher-stakes actions\nbased on untrusted inputs like webpages and emails. These capabilities\nintroduce new security risks that existing security measures, such as model\nfine-tuning or chatbot-focused guardrails, do not fully address. Given the\nhigher stakes and the absence of deterministic solutions to mitigate these\nrisks, there is a critical need for a real-time guardrail monitor to serve as a\nfinal layer of defense, and support system level, use case specific safety\npolicy definition and enforcement. We introduce LlamaFirewall, an open-source\nsecurity focused guardrail framework designed to serve as a final layer of\ndefense against security risks associated with AI Agents. Our framework\nmitigates risks such as prompt injection, agent misalignment, and insecure code\nrisks through three powerful guardrails: PromptGuard 2, a universal jailbreak\ndetector that demonstrates clear state of the art performance; Agent Alignment\nChecks, a chain-of-thought auditor that inspects agent reasoning for prompt\ninjection and goal misalignment, which, while still experimental, shows\nstronger efficacy at preventing indirect injections in general scenarios than\npreviously proposed approaches; and CodeShield, an online static analysis\nengine that is both fast and extensible, aimed at preventing the generation of\ninsecure or dangerous code by coding agents. Additionally, we include\neasy-to-use customizable scanners that make it possible for any developer who\ncan write a regular expression or an LLM prompt to quickly update an agent's\nsecurity guardrails."
                },
                "authors": [
                    {
                        "name": "Sahana Chennabasappa"
                    },
                    {
                        "name": "Cyrus Nikolaidis"
                    },
                    {
                        "name": "Daniel Song"
                    },
                    {
                        "name": "David Molnar"
                    },
                    {
                        "name": "Stephanie Ding"
                    },
                    {
                        "name": "Shengye Wan"
                    },
                    {
                        "name": "Spencer Whitman"
                    },
                    {
                        "name": "Lauren Deason"
                    },
                    {
                        "name": "Nicholas Doucette"
                    },
                    {
                        "name": "Abraham Montilla"
                    },
                    {
                        "name": "Alekhya Gampa"
                    },
                    {
                        "name": "Beto de Paola"
                    },
                    {
                        "name": "Dominik Gabi"
                    },
                    {
                        "name": "James Crnkovich"
                    },
                    {
                        "name": "Jean-Christophe Testud"
                    },
                    {
                        "name": "Kat He"
                    },
                    {
                        "name": "Rashnil Chaturvedi"
                    },
                    {
                        "name": "Wu Zhou"
                    },
                    {
                        "name": "Joshua Saxe"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Saxe"
                },
                "author": "Joshua Saxe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03568v1",
                "updated": "2025-05-06T14:26:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    26,
                    0,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:26:00Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    26,
                    0,
                    1,
                    126,
                    0
                ],
                "title": "Familiarizing with Music: Discovery Patterns for Different Music\n  Discovery Needs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Familiarizing with Music: Discovery Patterns for Different Music\n  Discovery Needs"
                },
                "summary": "Humans have the tendency to discover and explore. This natural tendency is\nreflected in data from streaming platforms as the amount of previously unknown\ncontent accessed by users. Additionally, in domains such as that of music\nstreaming there is evidence that recommending novel content improves users'\nexperience with the platform. Therefore, understanding users' discovery\npatterns, such as the amount to which and the way users access previously\nunknown content, is a topic of relevance for both the scientific community and\nthe streaming industry, particularly the music one. Previous works studied how\nmusic consumption differs for users of different traits and looked at\ndiversity, novelty, and consistency over time of users' music preferences.\nHowever, very little is known about how users discover and explore previously\nunknown music, and how this behavior differs for users of varying discovery\nneeds. In this paper we bridge this gap by analyzing data from a survey\nanswered by users of the major music streaming platform Deezer in combination\nwith their streaming data. We first address questions regarding whether users\nwho declare a higher interest in unfamiliar music listen to more diverse music,\nhave more stable music preferences over time, and explore more music within a\nsame time window, compared to those who declare a lower interest. We then\ninvestigate which type of music tracks users choose to listen to when they\nexplore unfamiliar music, identifying clear patterns of popularity and genre\nrepresentativeness that vary for users of different discovery needs.\n  Our findings open up possibilities to infer users' interest in unfamiliar\nmusic from streaming data as well as possibilities to develop recommender\nsystems that guide users in exploring music in a more natural way.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans have the tendency to discover and explore. This natural tendency is\nreflected in data from streaming platforms as the amount of previously unknown\ncontent accessed by users. Additionally, in domains such as that of music\nstreaming there is evidence that recommending novel content improves users'\nexperience with the platform. Therefore, understanding users' discovery\npatterns, such as the amount to which and the way users access previously\nunknown content, is a topic of relevance for both the scientific community and\nthe streaming industry, particularly the music one. Previous works studied how\nmusic consumption differs for users of different traits and looked at\ndiversity, novelty, and consistency over time of users' music preferences.\nHowever, very little is known about how users discover and explore previously\nunknown music, and how this behavior differs for users of varying discovery\nneeds. In this paper we bridge this gap by analyzing data from a survey\nanswered by users of the major music streaming platform Deezer in combination\nwith their streaming data. We first address questions regarding whether users\nwho declare a higher interest in unfamiliar music listen to more diverse music,\nhave more stable music preferences over time, and explore more music within a\nsame time window, compared to those who declare a lower interest. We then\ninvestigate which type of music tracks users choose to listen to when they\nexplore unfamiliar music, identifying clear patterns of popularity and genre\nrepresentativeness that vary for users of different discovery needs.\n  Our findings open up possibilities to infer users' interest in unfamiliar\nmusic from streaming data as well as possibilities to develop recommender\nsystems that guide users in exploring music in a more natural way."
                },
                "authors": [
                    {
                        "name": "Marta Moscati"
                    },
                    {
                        "name": "Darius Afchar"
                    },
                    {
                        "name": "Markus Schedl"
                    },
                    {
                        "name": "Bruno Sguerra"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Sguerra"
                },
                "author": "Bruno Sguerra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15279v2",
                "updated": "2025-05-06T14:25:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    25,
                    0,
                    1,
                    126,
                    0
                ],
                "published": "2024-11-22T15:29:12Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    29,
                    12,
                    4,
                    327,
                    0
                ],
                "title": "Don't Mesh with Me: Generating Constructive Solid Geometry Instead of\n  Meshes by Fine-Tuning a Code-Generation LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Mesh with Me: Generating Constructive Solid Geometry Instead of\n  Meshes by Fine-Tuning a Code-Generation LLM"
                },
                "summary": "While recent advancements in machine learning, such as LLMs, are\nrevolutionizing software development and creative industries, they have had\nminimal impact on engineers designing mechanical parts, which remains largely a\nmanual process. Existing approaches to generating 3D geometry most commonly use\nmeshes as a 3D representation. While meshes are suitable for assets in video\ngames or animations, they lack sufficient precision and adaptability for\nmechanical engineering purposes. This paper introduces a novel approach for the\ngeneration of 3D geometry that generates surface-based Constructive Solid\nGeometry (CSG) by leveraging a code-generation LLM. First, we create a dataset\nof 3D mechanical parts represented as code scripts by converting Boundary\nRepresentation geometry (BREP) into CSG-based Python scripts. Second, we create\nannotations in natural language using GPT-4. The resulting dataset is used to\nfine-tune a code-generation LLM. The fine-tuned LLM can complete geometries\nbased on positional input and natural language in a plausible way,\ndemonstrating geometric understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent advancements in machine learning, such as LLMs, are\nrevolutionizing software development and creative industries, they have had\nminimal impact on engineers designing mechanical parts, which remains largely a\nmanual process. Existing approaches to generating 3D geometry most commonly use\nmeshes as a 3D representation. While meshes are suitable for assets in video\ngames or animations, they lack sufficient precision and adaptability for\nmechanical engineering purposes. This paper introduces a novel approach for the\ngeneration of 3D geometry that generates surface-based Constructive Solid\nGeometry (CSG) by leveraging a code-generation LLM. First, we create a dataset\nof 3D mechanical parts represented as code scripts by converting Boundary\nRepresentation geometry (BREP) into CSG-based Python scripts. Second, we create\nannotations in natural language using GPT-4. The resulting dataset is used to\nfine-tune a code-generation LLM. The fine-tuned LLM can complete geometries\nbased on positional input and natural language in a plausible way,\ndemonstrating geometric understanding."
                },
                "authors": [
                    {
                        "name": "Maximilian Mews"
                    },
                    {
                        "name": "Ansar Aynetdinov"
                    },
                    {
                        "name": "Vivian Schiller"
                    },
                    {
                        "name": "Peter Eisert"
                    },
                    {
                        "name": "Alan Akbik"
                    }
                ],
                "author_detail": {
                    "name": "Alan Akbik"
                },
                "author": "Alan Akbik",
                "arxiv_comment": "Accepted to the AI for Content Creation Workshop at CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03563v1",
                "updated": "2025-05-06T14:17:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    17,
                    30,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:17:30Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    17,
                    30,
                    1,
                    126,
                    0
                ],
                "title": "Say It Another Way: A Framework for User-Grounded Paraphrasing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Say It Another Way: A Framework for User-Grounded Paraphrasing"
                },
                "summary": "Small changes in how a prompt is worded can lead to meaningful differences in\nthe behavior of large language models (LLMs), raising concerns about the\nstability and reliability of their evaluations. While prior work has explored\nsimple formatting changes, these rarely capture the kinds of natural variation\nseen in real-world language use. We propose a controlled paraphrasing framework\nbased on a taxonomy of minimal linguistic transformations to systematically\ngenerate natural prompt variations. Using the BBQ dataset, we validate our\nmethod with both human annotations and automated checks, then use it to study\nhow LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our\nanalysis shows that even subtle prompt modifications can lead to substantial\nchanges in model behavior. These results highlight the need for robust,\nparaphrase-aware evaluation protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small changes in how a prompt is worded can lead to meaningful differences in\nthe behavior of large language models (LLMs), raising concerns about the\nstability and reliability of their evaluations. While prior work has explored\nsimple formatting changes, these rarely capture the kinds of natural variation\nseen in real-world language use. We propose a controlled paraphrasing framework\nbased on a taxonomy of minimal linguistic transformations to systematically\ngenerate natural prompt variations. Using the BBQ dataset, we validate our\nmethod with both human annotations and automated checks, then use it to study\nhow LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our\nanalysis shows that even subtle prompt modifications can lead to substantial\nchanges in model behavior. These results highlight the need for robust,\nparaphrase-aware evaluation protocols."
                },
                "authors": [
                    {
                        "name": "Cléa Chataigner"
                    },
                    {
                        "name": "Rebecca Ma"
                    },
                    {
                        "name": "Prakhar Ganesh"
                    },
                    {
                        "name": "Afaf Taïk"
                    },
                    {
                        "name": "Elliot Creager"
                    },
                    {
                        "name": "Golnoosh Farnadi"
                    }
                ],
                "author_detail": {
                    "name": "Golnoosh Farnadi"
                },
                "author": "Golnoosh Farnadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03556v1",
                "updated": "2025-05-06T14:09:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    9,
                    29,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:09:29Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    9,
                    29,
                    1,
                    126,
                    0
                ],
                "title": "A Comprehensive Survey of Large AI Models for Future Communications:\n  Foundations, Applications and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Large AI Models for Future Communications:\n  Foundations, Applications and Challenges"
                },
                "summary": "The 6G wireless communications aim to establish an intelligent world of\nubiquitous connectivity, providing an unprecedented communication experience.\nLarge artificial intelligence models (LAMs) are characterized by significantly\nlarger scales (e.g., billions or trillions of parameters) compared to typical\nartificial intelligence (AI) models. LAMs exhibit outstanding cognitive\nabilities, including strong generalization capabilities for fine-tuning to\ndownstream tasks, and emergent capabilities to handle tasks unseen during\ntraining. Therefore, LAMs efficiently provide AI services for diverse\ncommunication applications, making them crucial tools for addressing complex\nchallenges in future wireless communication systems. This study provides a\ncomprehensive review of the foundations, applications, and challenges of LAMs\nin communication. First, we introduce the current state of AI-based\ncommunication systems, emphasizing the motivation behind integrating LAMs into\ncommunications and summarizing the key contributions. We then present an\noverview of the essential concepts of LAMs in communication. This includes an\nintroduction to the main architectures of LAMs, such as transformer, diffusion\nmodels, and mamba. We also explore the classification of LAMs, including large\nlanguage models (LLMs), large vision models (LVMs), large multimodal models\n(LMMs), and world models, and examine their potential applications in\ncommunication. Additionally, we cover the training methods and evaluation\ntechniques for LAMs in communication systems. Lastly, we introduce optimization\nstrategies such as chain of thought (CoT), retrieval augmented generation\n(RAG), and agentic systems. Following this, we discuss the research\nadvancements of LAMs across various communication scenarios. Finally, we\nanalyze the challenges in the current research and provide insights into\npotential future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 6G wireless communications aim to establish an intelligent world of\nubiquitous connectivity, providing an unprecedented communication experience.\nLarge artificial intelligence models (LAMs) are characterized by significantly\nlarger scales (e.g., billions or trillions of parameters) compared to typical\nartificial intelligence (AI) models. LAMs exhibit outstanding cognitive\nabilities, including strong generalization capabilities for fine-tuning to\ndownstream tasks, and emergent capabilities to handle tasks unseen during\ntraining. Therefore, LAMs efficiently provide AI services for diverse\ncommunication applications, making them crucial tools for addressing complex\nchallenges in future wireless communication systems. This study provides a\ncomprehensive review of the foundations, applications, and challenges of LAMs\nin communication. First, we introduce the current state of AI-based\ncommunication systems, emphasizing the motivation behind integrating LAMs into\ncommunications and summarizing the key contributions. We then present an\noverview of the essential concepts of LAMs in communication. This includes an\nintroduction to the main architectures of LAMs, such as transformer, diffusion\nmodels, and mamba. We also explore the classification of LAMs, including large\nlanguage models (LLMs), large vision models (LVMs), large multimodal models\n(LMMs), and world models, and examine their potential applications in\ncommunication. Additionally, we cover the training methods and evaluation\ntechniques for LAMs in communication systems. Lastly, we introduce optimization\nstrategies such as chain of thought (CoT), retrieval augmented generation\n(RAG), and agentic systems. Following this, we discuss the research\nadvancements of LAMs across various communication scenarios. Finally, we\nanalyze the challenges in the current research and provide insights into\npotential future research directions."
                },
                "authors": [
                    {
                        "name": "Feibo Jiang"
                    },
                    {
                        "name": "Cunhua Pan"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zhu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Han"
                },
                "author": "Zhu Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03553v1",
                "updated": "2025-05-06T14:05:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    5,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:05:12Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    5,
                    12,
                    1,
                    126,
                    0
                ],
                "title": "A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model\n  Reasoning"
                },
                "summary": "Inconsistent outputs and hallucinations from large language models (LLMs) are\nmajor obstacles to reliable AI systems. When different proprietary reasoning\nmodels (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,\nare given the same complex request, they often produce divergent results due to\nvariations in training and inference. This paper proposes a novel consensus\nmechanism, inspired by distributed ledger technology, to validate and converge\nthese outputs, treating each RM as a black-box peer. Building on the Hashgraph\nconsensus algorithm, our approach employs gossip-about-gossip communication and\nvirtual voting to achieve agreement among an ensemble of RMs. We present an\narchitectural design for a prototype system in which RMs iteratively exchange\nand update their answers, using information from each round to improve accuracy\nand confidence in subsequent rounds. This approach goes beyond simple majority\nvoting by incorporating the knowledge and cross-verification content of every\nmodel. We justify the feasibility of this Hashgraph-inspired consensus for AI\nensembles and outline its advantages over traditional ensembling techniques in\nreducing nonfactual outputs. Preliminary considerations for implementation,\nevaluation criteria for convergence and accuracy, and potential challenges are\ndiscussed. The proposed mechanism demonstrates a promising direction for\nmulti-agent AI systems to self-validate and deliver high-fidelity responses in\ncomplex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inconsistent outputs and hallucinations from large language models (LLMs) are\nmajor obstacles to reliable AI systems. When different proprietary reasoning\nmodels (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,\nare given the same complex request, they often produce divergent results due to\nvariations in training and inference. This paper proposes a novel consensus\nmechanism, inspired by distributed ledger technology, to validate and converge\nthese outputs, treating each RM as a black-box peer. Building on the Hashgraph\nconsensus algorithm, our approach employs gossip-about-gossip communication and\nvirtual voting to achieve agreement among an ensemble of RMs. We present an\narchitectural design for a prototype system in which RMs iteratively exchange\nand update their answers, using information from each round to improve accuracy\nand confidence in subsequent rounds. This approach goes beyond simple majority\nvoting by incorporating the knowledge and cross-verification content of every\nmodel. We justify the feasibility of this Hashgraph-inspired consensus for AI\nensembles and outline its advantages over traditional ensembling techniques in\nreducing nonfactual outputs. Preliminary considerations for implementation,\nevaluation criteria for convergence and accuracy, and potential challenges are\ndiscussed. The proposed mechanism demonstrates a promising direction for\nmulti-agent AI systems to self-validate and deliver high-fidelity responses in\ncomplex tasks."
                },
                "authors": [
                    {
                        "name": "Kolawole E. Ogunsina"
                    },
                    {
                        "name": "Morayo A. Ogunsina"
                    }
                ],
                "author_detail": {
                    "name": "Morayo A. Ogunsina"
                },
                "author": "Morayo A. Ogunsina",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03547v1",
                "updated": "2025-05-06T14:00:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    0,
                    41,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:00:41Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    0,
                    41,
                    1,
                    126,
                    0
                ],
                "title": "STORY2GAME: Generating (Almost) Everything in an Interactive Fiction\n  Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STORY2GAME: Generating (Almost) Everything in an Interactive Fiction\n  Game"
                },
                "summary": "We introduce STORY2GAME, a novel approach to using Large Language Models to\ngenerate text-based interactive fiction games that starts by generating a\nstory, populates the world, and builds the code for actions in a game engine\nthat enables the story to play out interactively. Whereas a given set of\nhard-coded actions can artificially constrain story generation, the ability to\ngenerate actions means the story generation process can be more open-ended but\nstill allow for experiences that are grounded in a game state. The key to\nsuccessful action generation is to use LLM-generated preconditions and effects\nof actions in the stories as guides for what aspects of the game state must be\ntracked and changed by the game engine when a player performs an action. We\nalso introduce a technique for dynamically generating new actions to\naccommodate the player's desire to perform actions that they think of that are\nnot part of the story. Dynamic action generation may require on-the-fly updates\nto the game engine's state representation and revision of previously generated\nactions. We evaluate the success rate of action code generation with respect to\nwhether a player can interactively play through the entire generated story.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce STORY2GAME, a novel approach to using Large Language Models to\ngenerate text-based interactive fiction games that starts by generating a\nstory, populates the world, and builds the code for actions in a game engine\nthat enables the story to play out interactively. Whereas a given set of\nhard-coded actions can artificially constrain story generation, the ability to\ngenerate actions means the story generation process can be more open-ended but\nstill allow for experiences that are grounded in a game state. The key to\nsuccessful action generation is to use LLM-generated preconditions and effects\nof actions in the stories as guides for what aspects of the game state must be\ntracked and changed by the game engine when a player performs an action. We\nalso introduce a technique for dynamically generating new actions to\naccommodate the player's desire to perform actions that they think of that are\nnot part of the story. Dynamic action generation may require on-the-fly updates\nto the game engine's state representation and revision of previously generated\nactions. We evaluate the success rate of action code generation with respect to\nwhether a player can interactively play through the entire generated story."
                },
                "authors": [
                    {
                        "name": "Eric Zhou"
                    },
                    {
                        "name": "Shreyas Basavatia"
                    },
                    {
                        "name": "Moontashir Siam"
                    },
                    {
                        "name": "Zexin Chen"
                    },
                    {
                        "name": "Mark O. Riedl"
                    }
                ],
                "author_detail": {
                    "name": "Mark O. Riedl"
                },
                "author": "Mark O. Riedl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18991v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18991v2",
                "updated": "2025-05-06T13:47:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    47,
                    34,
                    1,
                    126,
                    0
                ],
                "published": "2025-03-23T16:40:29Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    16,
                    40,
                    29,
                    6,
                    82,
                    0
                ],
                "title": "HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective\n  Reasoning for LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective\n  Reasoning for LLM Alignment"
                },
                "summary": "The alignment of large language models (LLMs) with human values remains\ncritical yet hindered by four key challenges: (1) scarcity of balanced safety\ndatasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to\nshallow alignment, and (4) inability to dynamically adapt rewards according to\ntask difficulty. To address these limitations, we introduce HAIR\n(Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a\nnovel alignment approach inspired by shadow models in membership inference\nattacks. Our approach consists of two main components: (1) construction of a\nbalanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using\nstructured prompts that leverage the introspective reasoning capabilities of\nLLMs; and (2) training of category-specific reward models with Group Relative\nPolicy Optimization (GRPO), dynamically tuning optimization to task difficulty\nat both the data and model levels. Comprehensive experiments across four\nharmlessness and four usefulness benchmarks demonstrate that HAIR achieves\nstate-of-the-art performance, outperforming all baseline methods in safety\nwhile maintaining high levels of usefulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of large language models (LLMs) with human values remains\ncritical yet hindered by four key challenges: (1) scarcity of balanced safety\ndatasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to\nshallow alignment, and (4) inability to dynamically adapt rewards according to\ntask difficulty. To address these limitations, we introduce HAIR\n(Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a\nnovel alignment approach inspired by shadow models in membership inference\nattacks. Our approach consists of two main components: (1) construction of a\nbalanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using\nstructured prompts that leverage the introspective reasoning capabilities of\nLLMs; and (2) training of category-specific reward models with Group Relative\nPolicy Optimization (GRPO), dynamically tuning optimization to task difficulty\nat both the data and model levels. Comprehensive experiments across four\nharmlessness and four usefulness benchmarks demonstrate that HAIR achieves\nstate-of-the-art performance, outperforming all baseline methods in safety\nwhile maintaining high levels of usefulness."
                },
                "authors": [
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Haoxuan Ma"
                    },
                    {
                        "name": "Weixin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weixin Wang"
                },
                "author": "Weixin Wang",
                "arxiv_comment": "The three authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18991v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18991v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03531v1",
                "updated": "2025-05-06T13:41:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    41,
                    17,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T13:41:17Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    41,
                    17,
                    1,
                    126,
                    0
                ],
                "title": "Faster MoE LLM Inference for Extremely Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster MoE LLM Inference for Extremely Large Models"
                },
                "summary": "Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually\nbecoming the mainstream approach for ultra-large-scale models. Existing\noptimization efforts for MoE models have focused primarily on coarse-grained\nMoE architectures. With the emergence of DeepSeek Models, fine-grained MoE\nmodels are gaining popularity, yet research on them remains limited. Therefore,\nwe want to discuss the efficiency dynamic under different service loads.\nAdditionally, fine-grained models allow deployers to reduce the number of\nrouted experts, both activated counts and total counts, raising the question of\nhow this reduction affects the trade-off between MoE efficiency and\nperformance. Our findings indicate that while deploying MoE models presents\ngreater challenges, it also offers significant optimization opportunities.\nReducing the number of activated experts can lead to substantial efficiency\nimprovements in certain scenarios, with only minor performance degradation.\nReducing the total number of experts provides limited efficiency gains but\nresults in severe performance degradation. Our method can increase throughput\nby at least 10\\% without any performance degradation. Overall, we conclude that\nMoE inference optimization remains an area with substantial potential for\nexploration and improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually\nbecoming the mainstream approach for ultra-large-scale models. Existing\noptimization efforts for MoE models have focused primarily on coarse-grained\nMoE architectures. With the emergence of DeepSeek Models, fine-grained MoE\nmodels are gaining popularity, yet research on them remains limited. Therefore,\nwe want to discuss the efficiency dynamic under different service loads.\nAdditionally, fine-grained models allow deployers to reduce the number of\nrouted experts, both activated counts and total counts, raising the question of\nhow this reduction affects the trade-off between MoE efficiency and\nperformance. Our findings indicate that while deploying MoE models presents\ngreater challenges, it also offers significant optimization opportunities.\nReducing the number of activated experts can lead to substantial efficiency\nimprovements in certain scenarios, with only minor performance degradation.\nReducing the total number of experts provides limited efficiency gains but\nresults in severe performance degradation. Our method can increase throughput\nby at least 10\\% without any performance degradation. Overall, we conclude that\nMoE inference optimization remains an area with substantial potential for\nexploration and improvement."
                },
                "authors": [
                    {
                        "name": "Haoqi Yang"
                    },
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Qiwei Li"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Mengjia Shen"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03513v1",
                "updated": "2025-05-06T13:22:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    22,
                    37,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T13:22:37Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    22,
                    37,
                    1,
                    126,
                    0
                ],
                "title": "Ruled by the Representation Space: On the University's Embrace of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ruled by the Representation Space: On the University's Embrace of Large\n  Language Models"
                },
                "summary": "This paper explores the implications of universities' rapid adoption of large\nlanguage models (LLMs) for studying, teaching, and research by analyzing the\nlogics underpinning their representation space. It argues that by uncritically\nadopting LLMs, the University surrenders its autonomy to a field of heteronomy,\nthat of generative AI, whose norms are not democratically shaped. Unlike\nearlier forms of rule-based AI, which sought to exclude human judgment and\ninterpretation, generative AI's new normative rationality is explicitly based\non the automation of moral judgment, valuation, and interpretation. By\nintegrating LLMs into pedagogical and research contexts before establishing a\ncritical framework for their use, the University subjects itself to being\ngoverned by contingent, ever-evolving, and domain-non-specific norms that\nstructure the model's virtual representation space and thus everything it\ngenerates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the implications of universities' rapid adoption of large\nlanguage models (LLMs) for studying, teaching, and research by analyzing the\nlogics underpinning their representation space. It argues that by uncritically\nadopting LLMs, the University surrenders its autonomy to a field of heteronomy,\nthat of generative AI, whose norms are not democratically shaped. Unlike\nearlier forms of rule-based AI, which sought to exclude human judgment and\ninterpretation, generative AI's new normative rationality is explicitly based\non the automation of moral judgment, valuation, and interpretation. By\nintegrating LLMs into pedagogical and research contexts before establishing a\ncritical framework for their use, the University subjects itself to being\ngoverned by contingent, ever-evolving, and domain-non-specific norms that\nstructure the model's virtual representation space and thus everything it\ngenerates."
                },
                "authors": [
                    {
                        "name": "Katia Schwerzmann"
                    }
                ],
                "author_detail": {
                    "name": "Katia Schwerzmann"
                },
                "author": "Katia Schwerzmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13685v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13685v2",
                "updated": "2025-05-06T13:11:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    11,
                    19,
                    1,
                    126,
                    0
                ],
                "published": "2025-02-19T12:53:55Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    53,
                    55,
                    2,
                    50,
                    0
                ],
                "title": "MoM: Linear Sequence Modeling with Mixture-of-Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoM: Linear Sequence Modeling with Mixture-of-Memories"
                },
                "summary": "Linear sequence modeling methods, such as linear attention, state space\nmodeling, and linear RNNs, offer significant efficiency improvements by\nreducing the complexity of training and inference. However, these methods\ntypically compress the entire input sequence into a single fixed-size memory\nstate, which leads to suboptimal performance on recall-intensive downstream\ntasks. Drawing inspiration from neuroscience, particularly the brain's ability\nto maintain robust long-term memory while mitigating \"memory interference\", we\nintroduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes\nmultiple independent memory states, with a router network directing input\ntokens to specific memory states. This approach greatly enhances the overall\nmemory capacity while minimizing memory interference. As a result, MoM performs\nexceptionally well on recall-intensive tasks, surpassing existing linear\nsequence modeling techniques. Despite incorporating multiple memory states, the\ncomputation of each memory state remains linear in complexity, allowing MoM to\nretain the linear-complexity advantage during training, while\nconstant-complexity during inference. Our experimental results show that MoM\nsignificantly outperforms current linear sequence models on downstream language\ntasks, particularly recall-intensive tasks, and even achieves performance\ncomparable to Transformer models. The code is released at\nhttps://github.com/OpenSparseLLMs/MoM and is also released as a part of\nhttps://github.com/OpenSparseLLMs/Linear-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear sequence modeling methods, such as linear attention, state space\nmodeling, and linear RNNs, offer significant efficiency improvements by\nreducing the complexity of training and inference. However, these methods\ntypically compress the entire input sequence into a single fixed-size memory\nstate, which leads to suboptimal performance on recall-intensive downstream\ntasks. Drawing inspiration from neuroscience, particularly the brain's ability\nto maintain robust long-term memory while mitigating \"memory interference\", we\nintroduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes\nmultiple independent memory states, with a router network directing input\ntokens to specific memory states. This approach greatly enhances the overall\nmemory capacity while minimizing memory interference. As a result, MoM performs\nexceptionally well on recall-intensive tasks, surpassing existing linear\nsequence modeling techniques. Despite incorporating multiple memory states, the\ncomputation of each memory state remains linear in complexity, allowing MoM to\nretain the linear-complexity advantage during training, while\nconstant-complexity during inference. Our experimental results show that MoM\nsignificantly outperforms current linear sequence models on downstream language\ntasks, particularly recall-intensive tasks, and even achieves performance\ncomparable to Transformer models. The code is released at\nhttps://github.com/OpenSparseLLMs/MoM and is also released as a part of\nhttps://github.com/OpenSparseLLMs/Linear-MoE."
                },
                "authors": [
                    {
                        "name": "Jusen Du"
                    },
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Disen Lan"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "Technical report, 16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13685v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13685v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03501v1",
                "updated": "2025-05-06T13:07:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    7,
                    57,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T13:07:57Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    7,
                    57,
                    1,
                    126,
                    0
                ],
                "title": "BadLingual: A Novel Lingual-Backdoor Attack against Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadLingual: A Novel Lingual-Backdoor Attack against Large Language\n  Models"
                },
                "summary": "In this paper, we present a new form of backdoor attack against Large\nLanguage Models (LLMs): lingual-backdoor attacks. The key novelty of\nlingual-backdoor attacks is that the language itself serves as the trigger to\nhijack the infected LLMs to generate inflammatory speech. They enable the\nprecise targeting of a specific language-speaking group, exacerbating racial\ndiscrimination by malicious entities. We first implement a baseline\nlingual-backdoor attack, which is carried out by poisoning a set of training\ndata for specific downstream tasks through translation into the trigger\nlanguage. However, this baseline attack suffers from poor task generalization\nand is impractical in real-world settings. To address this challenge, we design\nBadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any\ndownstream tasks within the chat LLMs, regardless of the specific questions of\nthese tasks. We design a new approach using PPL-constrained Greedy Coordinate\nGradient-based Search (PGCG) based adversarial training to expand the decision\nboundary of lingual-backdoor, thereby enhancing the generalization ability of\nlingual-backdoor across various tasks. We perform extensive experiments to\nvalidate the effectiveness of our proposed attacks. Specifically, the baseline\nattack achieves an ASR of over 90% on the specified tasks. However, its ASR\nreaches only 37.61% across six tasks in the task-agnostic scenario. In\ncontrast, BadLingual brings up to 37.35% improvement over the baseline. Our\nstudy sheds light on a new perspective of vulnerabilities in LLMs with\nmultilingual capabilities and is expected to promote future research on the\npotential defenses to enhance the LLMs' robustness",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a new form of backdoor attack against Large\nLanguage Models (LLMs): lingual-backdoor attacks. The key novelty of\nlingual-backdoor attacks is that the language itself serves as the trigger to\nhijack the infected LLMs to generate inflammatory speech. They enable the\nprecise targeting of a specific language-speaking group, exacerbating racial\ndiscrimination by malicious entities. We first implement a baseline\nlingual-backdoor attack, which is carried out by poisoning a set of training\ndata for specific downstream tasks through translation into the trigger\nlanguage. However, this baseline attack suffers from poor task generalization\nand is impractical in real-world settings. To address this challenge, we design\nBadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any\ndownstream tasks within the chat LLMs, regardless of the specific questions of\nthese tasks. We design a new approach using PPL-constrained Greedy Coordinate\nGradient-based Search (PGCG) based adversarial training to expand the decision\nboundary of lingual-backdoor, thereby enhancing the generalization ability of\nlingual-backdoor across various tasks. We perform extensive experiments to\nvalidate the effectiveness of our proposed attacks. Specifically, the baseline\nattack achieves an ASR of over 90% on the specified tasks. However, its ASR\nreaches only 37.61% across six tasks in the task-agnostic scenario. In\ncontrast, BadLingual brings up to 37.35% improvement over the baseline. Our\nstudy sheds light on a new perspective of vulnerabilities in LLMs with\nmultilingual capabilities and is expected to promote future research on the\npotential defenses to enhance the LLMs' robustness"
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Hongwei Li"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Wenbo Jiang"
                    },
                    {
                        "name": "Kangjie Chen"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Qingchuan Zhao"
                    },
                    {
                        "name": "Guowen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Guowen Xu"
                },
                "author": "Guowen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03500v1",
                "updated": "2025-05-06T13:05:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    5,
                    4,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T13:05:04Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    5,
                    4,
                    1,
                    126,
                    0
                ],
                "title": "Task Reconstruction and Extrapolation for $π_0$ using Text Latent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Reconstruction and Extrapolation for $π_0$ using Text Latent"
                },
                "summary": "Vision-language-action models (VLAs) often achieve high performance on\ndemonstrated tasks but struggle significantly when required to extrapolate,\ncombining skills learned from different tasks in novel ways. For instance, VLAs\nmight successfully put the cream cheese in the bowl and put the bowl on top of\nthe cabinet, yet still fail to put the cream cheese on top of the cabinet. In\nthis work, we demonstrate that behaviors from distinct tasks can be effectively\nrecombined by manipulating the VLA's internal representations at inference\ntime. Concretely, we identify the text latent by averaging the text tokens'\nhidden states across all demonstrated trajectories for a specific base task.\nFor executing an extrapolated task, we can temporally interpolate the text\nlatent of the two base tasks and add it back to the text hidden states, so\nsub-behaviors from the two tasks will be activated sequentially. We evaluate\nthis approach using the newly created libero-ood benchmark, featuring 20 tasks\nextrapolated from standard LIBERO suites. The results on libero-ood show that\nall SOTA VLAs achieve < 15% success rate, while $\\pi0$ with text latent\ninterpolation reaches an 83% success rate. Further qualitative analysis reveals\na tendency for VLAs to exhibit spatial overfitting, mapping object names to\ndemonstrated locations rather than achieving genuine object and goal\nunderstanding. Additionally, we find that decoding the text latent yields\nhuman-unreadable prompts that can nevertheless instruct the VLA to achieve a\n70% success rate on standard LIBERO suites, enabling private instruction or\nbackdoor attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action models (VLAs) often achieve high performance on\ndemonstrated tasks but struggle significantly when required to extrapolate,\ncombining skills learned from different tasks in novel ways. For instance, VLAs\nmight successfully put the cream cheese in the bowl and put the bowl on top of\nthe cabinet, yet still fail to put the cream cheese on top of the cabinet. In\nthis work, we demonstrate that behaviors from distinct tasks can be effectively\nrecombined by manipulating the VLA's internal representations at inference\ntime. Concretely, we identify the text latent by averaging the text tokens'\nhidden states across all demonstrated trajectories for a specific base task.\nFor executing an extrapolated task, we can temporally interpolate the text\nlatent of the two base tasks and add it back to the text hidden states, so\nsub-behaviors from the two tasks will be activated sequentially. We evaluate\nthis approach using the newly created libero-ood benchmark, featuring 20 tasks\nextrapolated from standard LIBERO suites. The results on libero-ood show that\nall SOTA VLAs achieve < 15% success rate, while $\\pi0$ with text latent\ninterpolation reaches an 83% success rate. Further qualitative analysis reveals\na tendency for VLAs to exhibit spatial overfitting, mapping object names to\ndemonstrated locations rather than achieving genuine object and goal\nunderstanding. Additionally, we find that decoding the text latent yields\nhuman-unreadable prompts that can nevertheless instruct the VLA to achieve a\n70% success rate on standard LIBERO suites, enabling private instruction or\nbackdoor attacks."
                },
                "authors": [
                    {
                        "name": "Quanyi Li"
                    }
                ],
                "author_detail": {
                    "name": "Quanyi Li"
                },
                "author": "Quanyi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18376v2",
                "updated": "2025-05-06T13:04:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    4,
                    22,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-25T14:20:57Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    14,
                    20,
                    57,
                    4,
                    115,
                    0
                ],
                "title": "Pushing the boundary on Natural Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the boundary on Natural Language Inference"
                },
                "summary": "Natural Language Inference (NLI) is a central task in natural language\nunderstanding with applications in fact-checking, question answering, and\ninformation retrieval. Despite its importance, current NLI systems heavily rely\non supervised learning with datasets that often contain annotation artifacts\nand biases, limiting generalization and real-world applicability. In this work,\nwe apply a reinforcement learning-based approach using Group Relative Policy\nOptimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the\nneed for labeled rationales and enabling this type of training on more\nchallenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language\nmodels using parameter-efficient techniques (LoRA and QLoRA), demonstrating\nstrong performance across standard and adversarial NLI benchmarks. Our 32B\nAWQ-quantized model surpasses state-of-the-art results on 7 out of 11\nadversarial sets$\\unicode{x2013}$or on all of them considering our\nreplication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust\nreasoning can be retained under aggressive quantization. This work provides a\nscalable and practical framework for building robust NLI systems without\nsacrificing inference quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Inference (NLI) is a central task in natural language\nunderstanding with applications in fact-checking, question answering, and\ninformation retrieval. Despite its importance, current NLI systems heavily rely\non supervised learning with datasets that often contain annotation artifacts\nand biases, limiting generalization and real-world applicability. In this work,\nwe apply a reinforcement learning-based approach using Group Relative Policy\nOptimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the\nneed for labeled rationales and enabling this type of training on more\nchallenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language\nmodels using parameter-efficient techniques (LoRA and QLoRA), demonstrating\nstrong performance across standard and adversarial NLI benchmarks. Our 32B\nAWQ-quantized model surpasses state-of-the-art results on 7 out of 11\nadversarial sets$\\unicode{x2013}$or on all of them considering our\nreplication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust\nreasoning can be retained under aggressive quantization. This work provides a\nscalable and practical framework for building robust NLI systems without\nsacrificing inference quality."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03492v1",
                "updated": "2025-05-06T12:48:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    48,
                    38,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T12:48:38Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    48,
                    38,
                    1,
                    126,
                    0
                ],
                "title": "Augmenting Human Cognition through Everyday AR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Human Cognition through Everyday AR"
                },
                "summary": "As spatial computing and multimodal LLMs mature, AR is tending to become an\nintuitive \"thinking tool,\" embedding semantic and context-aware intelligence\ndirectly into everyday environments. This paper explores how always-on AR can\nseamlessly bridge digital cognition and physical affordances, enabling\nproactive, context-sensitive interactions that enhance human task performance\nand understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As spatial computing and multimodal LLMs mature, AR is tending to become an\nintuitive \"thinking tool,\" embedding semantic and context-aware intelligence\ndirectly into everyday environments. This paper explores how always-on AR can\nseamlessly bridge digital cognition and physical affordances, enabling\nproactive, context-sensitive interactions that enhance human task performance\nand understanding."
                },
                "authors": [
                    {
                        "name": "Xiaoan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoan Liu"
                },
                "author": "Xiaoan Liu",
                "arxiv_comment": "3 pages, 4 figures. Position paper accepted to CHI'25 Workshop\n  'Everyday AR through AI-in-the-Loop'",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03490v1",
                "updated": "2025-05-06T12:47:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    47,
                    24,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T12:47:24Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    47,
                    24,
                    1,
                    126,
                    0
                ],
                "title": "A new membership inference attack that spots memorization in generative\n  and predictive models: Loss-Based with Reference Model algorithm (LBRM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new membership inference attack that spots memorization in generative\n  and predictive models: Loss-Based with Reference Model algorithm (LBRM)"
                },
                "summary": "Generative models can unintentionally memorize training data, posing\nsignificant privacy risks. This paper addresses the memorization phenomenon in\ntime series imputation models, introducing the Loss-Based with Reference Model\n(LBRM) algorithm. The LBRM method leverages a reference model to enhance the\naccuracy of membership inference attacks, distinguishing between training and\ntest data. Our contributions are twofold: first, we propose an innovative\nmethod to effectively extract and identify memorized training data,\nsignificantly improving detection accuracy. On average, without fine-tuning,\nthe AUROC improved by approximately 40\\%. With fine-tuning, the AUROC increased\nby approximately 60\\%. Second, we validate our approach through membership\ninference attacks on two types of architectures designed for time series\nimputation, demonstrating the robustness and versatility of the LBRM approach\nin different contexts. These results highlight the significant enhancement in\ndetection accuracy provided by the LBRM approach, addressing privacy risks in\ntime series imputation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models can unintentionally memorize training data, posing\nsignificant privacy risks. This paper addresses the memorization phenomenon in\ntime series imputation models, introducing the Loss-Based with Reference Model\n(LBRM) algorithm. The LBRM method leverages a reference model to enhance the\naccuracy of membership inference attacks, distinguishing between training and\ntest data. Our contributions are twofold: first, we propose an innovative\nmethod to effectively extract and identify memorized training data,\nsignificantly improving detection accuracy. On average, without fine-tuning,\nthe AUROC improved by approximately 40\\%. With fine-tuning, the AUROC increased\nby approximately 60\\%. Second, we validate our approach through membership\ninference attacks on two types of architectures designed for time series\nimputation, demonstrating the robustness and versatility of the LBRM approach\nin different contexts. These results highlight the significant enhancement in\ndetection accuracy provided by the LBRM approach, addressing privacy risks in\ntime series imputation models."
                },
                "authors": [
                    {
                        "name": "Faiz Taleb"
                    },
                    {
                        "name": "Ivan Gazeau"
                    },
                    {
                        "name": "Maryline Laurent"
                    }
                ],
                "author_detail": {
                    "name": "Maryline Laurent"
                },
                "author": "Maryline Laurent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.04497v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.04497v4",
                "updated": "2025-05-06T12:35:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    35,
                    56,
                    1,
                    126,
                    0
                ],
                "published": "2023-04-10T10:22:21Z",
                "published_parsed": [
                    2023,
                    4,
                    10,
                    10,
                    22,
                    21,
                    0,
                    100,
                    0
                ],
                "title": "A Unified Framework for Exploratory Learning-Aided Community Detection\n  Under Topological Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Exploratory Learning-Aided Community Detection\n  Under Topological Uncertainty"
                },
                "summary": "In social networks, the discovery of community structures has received\nconsiderable attention as a fundamental problem in various network analysis\ntasks. However, due to privacy concerns or access restrictions, the network\nstructure is often uncertain, thereby rendering established community detection\napproaches ineffective without costly network topology acquisition. To tackle\nthis challenge, we present META-CODE, a unified framework for detecting\noverlapping communities via exploratory learning aided by easy-to-collect node\nmetadata when networks are topologically unknown (or only partially known).\nSpecifically, META-CODE consists of three iterative steps in addition to the\ninitial network inference step: 1) node-level community-affiliation embeddings\nbased on graph neural networks (GNNs) trained by our new reconstruction loss,\n2) network exploration via community-affiliation-based node queries, and 3)\nnetwork inference using an edge connectivity-based Siamese neural network model\nfrom the explored network. Through extensive experiments on three real-world\ndatasets including two large networks, we demonstrate: (a) the superiority of\nMETA-CODE over benchmark community detection methods, achieving remarkable\ngains up to 65.55% on the Facebook dataset over the best competitor among our\nselected competitive methods in terms of normalized mutual information (NMI),\n(b) the impact of each module in META-CODE, (c) the effectiveness of node\nqueries in META-CODE based on empirical evaluations and theoretical findings,\nand (d) the convergence of the inferred network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In social networks, the discovery of community structures has received\nconsiderable attention as a fundamental problem in various network analysis\ntasks. However, due to privacy concerns or access restrictions, the network\nstructure is often uncertain, thereby rendering established community detection\napproaches ineffective without costly network topology acquisition. To tackle\nthis challenge, we present META-CODE, a unified framework for detecting\noverlapping communities via exploratory learning aided by easy-to-collect node\nmetadata when networks are topologically unknown (or only partially known).\nSpecifically, META-CODE consists of three iterative steps in addition to the\ninitial network inference step: 1) node-level community-affiliation embeddings\nbased on graph neural networks (GNNs) trained by our new reconstruction loss,\n2) network exploration via community-affiliation-based node queries, and 3)\nnetwork inference using an edge connectivity-based Siamese neural network model\nfrom the explored network. Through extensive experiments on three real-world\ndatasets including two large networks, we demonstrate: (a) the superiority of\nMETA-CODE over benchmark community detection methods, achieving remarkable\ngains up to 65.55% on the Facebook dataset over the best competitor among our\nselected competitive methods in terms of normalized mutual information (NMI),\n(b) the impact of each module in META-CODE, (c) the effectiveness of node\nqueries in META-CODE based on empirical evaluations and theoretical findings,\nand (d) the convergence of the inferred network."
                },
                "authors": [
                    {
                        "name": "Yu Hou"
                    },
                    {
                        "name": "Cong Tran"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Won-Yong Shin"
                    }
                ],
                "author_detail": {
                    "name": "Won-Yong Shin"
                },
                "author": "Won-Yong Shin",
                "arxiv_comment": "17 pages, 9 figures, 8 tables; IEEE Transactions on Network Science\n  and Engineering (to appear) (Please cite our journal version.)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.04497v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.04497v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02138v2",
                "updated": "2025-05-06T12:35:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    35,
                    29,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-04T14:57:42Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    14,
                    57,
                    42,
                    6,
                    124,
                    0
                ],
                "title": "Efficient Multivariate Time Series Forecasting via Calibrated Language\n  Models with Privileged Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Multivariate Time Series Forecasting via Calibrated Language\n  Models with Privileged Knowledge Distillation"
                },
                "summary": "Multivariate time series forecasting (MTSF) endeavors to predict future\nobservations given historical data, playing a crucial role in time series data\nmanagement systems. With advancements in large language models (LLMs), recent\nstudies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF.\nHowever, the deployment of LLMs often suffers from low efficiency during the\ninference phase. To address this problem, we introduce TimeKD, an efficient\nMTSF framework that leverages the calibrated language models and privileged\nknowledge distillation. TimeKD aims to generate high-quality future\nrepresentations from the proposed cross-modality teacher model and cultivate an\neffective student model. The cross-modality teacher model adopts calibrated\nlanguage models (CLMs) with ground truth prompts, motivated by the paradigm of\nLearning Under Privileged Information (LUPI). In addition, we design a\nsubtractive cross attention (SCA) mechanism to refine these representations. To\ncultivate an effective student model, we propose an innovative privileged\nknowledge distillation (PKD) mechanism including correlation and feature\ndistillation. PKD enables the student to replicate the teacher's behavior while\nminimizing their output discrepancy. Extensive experiments on real data offer\ninsight into the effectiveness, efficiency, and scalability of the proposed\nTimeKD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time series forecasting (MTSF) endeavors to predict future\nobservations given historical data, playing a crucial role in time series data\nmanagement systems. With advancements in large language models (LLMs), recent\nstudies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF.\nHowever, the deployment of LLMs often suffers from low efficiency during the\ninference phase. To address this problem, we introduce TimeKD, an efficient\nMTSF framework that leverages the calibrated language models and privileged\nknowledge distillation. TimeKD aims to generate high-quality future\nrepresentations from the proposed cross-modality teacher model and cultivate an\neffective student model. The cross-modality teacher model adopts calibrated\nlanguage models (CLMs) with ground truth prompts, motivated by the paradigm of\nLearning Under Privileged Information (LUPI). In addition, we design a\nsubtractive cross attention (SCA) mechanism to refine these representations. To\ncultivate an effective student model, we propose an innovative privileged\nknowledge distillation (PKD) mechanism including correlation and feature\ndistillation. PKD enables the student to replicate the teacher's behavior while\nminimizing their output discrepancy. Extensive experiments on real data offer\ninsight into the effectiveness, efficiency, and scalability of the proposed\nTimeKD."
                },
                "authors": [
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Hao Miao"
                    },
                    {
                        "name": "Qianxiong Xu"
                    },
                    {
                        "name": "Shaowen Zhou"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Yan Zhao"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Rui Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhao"
                },
                "author": "Rui Zhao",
                "arxiv_comment": "Accepted by ICDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03475v1",
                "updated": "2025-05-06T12:28:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    28,
                    50,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T12:28:50Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    28,
                    50,
                    1,
                    126,
                    0
                ],
                "title": "am-ELO: A Stable Framework for Arena-based LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "am-ELO: A Stable Framework for Arena-based LLM Evaluation"
                },
                "summary": "Arena-based evaluation is a fundamental yet significant evaluation paradigm\nfor modern AI models, especially large language models (LLMs). Existing\nframework based on ELO rating system suffers from the inevitable instability\nproblem due to ranking inconsistency and the lack of attention to the varying\nabilities of annotators. In this paper, we introduce a novel stable arena\nframework to address these issues by enhancing the ELO Rating System.\nSpecifically, we replace the iterative update method with a Maximum Likelihood\nEstimation (MLE) approach, m-ELO, and provide theoretical proof of the\nconsistency and stability of the MLE approach for model ranking. Additionally,\nwe proposed the am-ELO, which modify the Elo Rating's probability function to\nincorporate annotator abilities, enabling the simultaneous estimation of model\nscores and annotator reliability. Experiments demonstrate that this method\nensures stability, proving that this framework offers a more robust, accurate,\nand stable evaluation method for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arena-based evaluation is a fundamental yet significant evaluation paradigm\nfor modern AI models, especially large language models (LLMs). Existing\nframework based on ELO rating system suffers from the inevitable instability\nproblem due to ranking inconsistency and the lack of attention to the varying\nabilities of annotators. In this paper, we introduce a novel stable arena\nframework to address these issues by enhancing the ELO Rating System.\nSpecifically, we replace the iterative update method with a Maximum Likelihood\nEstimation (MLE) approach, m-ELO, and provide theoretical proof of the\nconsistency and stability of the MLE approach for model ranking. Additionally,\nwe proposed the am-ELO, which modify the Elo Rating's probability function to\nincorporate annotator abilities, enabling the simultaneous estimation of model\nscores and annotator reliability. Experiments demonstrate that this method\nensures stability, proving that this framework offers a more robust, accurate,\nand stable evaluation method for LLMs."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Yan Zhuang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Shuanghong Shen"
                    },
                    {
                        "name": "Jie Ouyang"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Shijin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shijin Wang"
                },
                "author": "Shijin Wang",
                "arxiv_comment": "ICML2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03473v1",
                "updated": "2025-05-06T12:25:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    25,
                    15,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T12:25:15Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    25,
                    15,
                    1,
                    126,
                    0
                ],
                "title": "Evaluation of LLMs on Long-tail Entity Linking in Historical Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of LLMs on Long-tail Entity Linking in Historical Documents"
                },
                "summary": "Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)\napplications, enabling the disambiguation of entity mentions by linking them to\ntheir corresponding entries in a reference knowledge base (KB). Thanks to their\ndeep contextual understanding capabilities, LLMs offer a new perspective to\ntackle EL, promising better results than traditional methods. Despite the\nimpressive generalization capabilities of LLMs, linking less popular, long-tail\nentities remains challenging as these entities are often underrepresented in\ntraining data and knowledge bases. Furthermore, the long-tail EL task is an\nunderstudied problem, and limited studies address it with LLMs. In the present\nwork, we assess the performance of two popular LLMs, GPT and LLama3, in a\nlong-tail entity linking scenario. Using MHERCL v0.1, a manually annotated\nbenchmark of sentences from domain-specific historical texts, we quantitatively\ncompare the performance of LLMs in identifying and linking entities to their\ncorresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity\nLinking and Relation Extraction framework. Our preliminary experiments reveal\nthat LLMs perform encouragingly well in long-tail EL, indicating that this\ntechnology can be a valuable adjunct in filling the gap between head and\nlong-tail EL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)\napplications, enabling the disambiguation of entity mentions by linking them to\ntheir corresponding entries in a reference knowledge base (KB). Thanks to their\ndeep contextual understanding capabilities, LLMs offer a new perspective to\ntackle EL, promising better results than traditional methods. Despite the\nimpressive generalization capabilities of LLMs, linking less popular, long-tail\nentities remains challenging as these entities are often underrepresented in\ntraining data and knowledge bases. Furthermore, the long-tail EL task is an\nunderstudied problem, and limited studies address it with LLMs. In the present\nwork, we assess the performance of two popular LLMs, GPT and LLama3, in a\nlong-tail entity linking scenario. Using MHERCL v0.1, a manually annotated\nbenchmark of sentences from domain-specific historical texts, we quantitatively\ncompare the performance of LLMs in identifying and linking entities to their\ncorresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity\nLinking and Relation Extraction framework. Our preliminary experiments reveal\nthat LLMs perform encouragingly well in long-tail EL, indicating that this\ntechnology can be a valuable adjunct in filling the gap between head and\nlong-tail EL."
                },
                "authors": [
                    {
                        "name": "Marta Boscariol"
                    },
                    {
                        "name": "Luana Bulla"
                    },
                    {
                        "name": "Lia Draetta"
                    },
                    {
                        "name": "Beatrice Fiumanò"
                    },
                    {
                        "name": "Emanuele Lenzi"
                    },
                    {
                        "name": "Leonardo Piano"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Piano"
                },
                "author": "Leonardo Piano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18116v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18116v3",
                "updated": "2025-05-06T12:24:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    24,
                    43,
                    1,
                    126,
                    0
                ],
                "published": "2024-12-24T02:54:56Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    54,
                    56,
                    1,
                    359,
                    0
                ],
                "title": "AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation"
                },
                "summary": "Large language models (LLMs) have brought exciting new advances to mobile UI\nagents, a long-standing research field that aims to complete arbitrary natural\nlanguage tasks through mobile UI interactions. However, existing UI agents\nusually demand powerful large language models that are difficult to be deployed\nlocally on end-users' devices, raising huge concerns about user privacy and\ncentralized serving cost. Inspired by the remarkable coding abilities of recent\nsmall language models (SLMs), we propose to convert the UI task automation\nproblem to a code generation problem, which can be effectively solved by an\non-device SLM and efficiently executed with an on-device code interpreter.\nUnlike normal coding tasks that can be extensively pre-trained with public\ndatasets, generating UI automation code is challenging due to the diversity,\ncomplexity, and variability of target apps. Therefore, we adopt a\ndocument-centered approach that automatically builds fine-grained API\ndocumentation for each app and generates diverse task samples based on this\ndocumentation. By guiding the agent with the synthetic documents and task\nsamples, it learns to generate precise and efficient scripts to complete unseen\ntasks. Based on detailed comparisons with state-of-the-art mobile UI agents,\nour approach effectively improves the mobile task automation with significantly\nhigher success rates and lower latency/token consumption. Code is open-sourced\nat https://github.com/MobileLLM/AutoDroid-V2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have brought exciting new advances to mobile UI\nagents, a long-standing research field that aims to complete arbitrary natural\nlanguage tasks through mobile UI interactions. However, existing UI agents\nusually demand powerful large language models that are difficult to be deployed\nlocally on end-users' devices, raising huge concerns about user privacy and\ncentralized serving cost. Inspired by the remarkable coding abilities of recent\nsmall language models (SLMs), we propose to convert the UI task automation\nproblem to a code generation problem, which can be effectively solved by an\non-device SLM and efficiently executed with an on-device code interpreter.\nUnlike normal coding tasks that can be extensively pre-trained with public\ndatasets, generating UI automation code is challenging due to the diversity,\ncomplexity, and variability of target apps. Therefore, we adopt a\ndocument-centered approach that automatically builds fine-grained API\ndocumentation for each app and generates diverse task samples based on this\ndocumentation. By guiding the agent with the synthetic documents and task\nsamples, it learns to generate precise and efficient scripts to complete unseen\ntasks. Based on detailed comparisons with state-of-the-art mobile UI agents,\nour approach effectively improves the mobile task automation with significantly\nhigher success rates and lower latency/token consumption. Code is open-sourced\nat https://github.com/MobileLLM/AutoDroid-V2."
                },
                "authors": [
                    {
                        "name": "Hao Wen"
                    },
                    {
                        "name": "Shizuo Tian"
                    },
                    {
                        "name": "Borislav Pavlov"
                    },
                    {
                        "name": "Wenjie Du"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Ge Chang"
                    },
                    {
                        "name": "Shanhui Zhao"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Yuanchun Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Li"
                },
                "author": "Yuanchun Li",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18116v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18116v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03470v1",
                "updated": "2025-05-06T12:22:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    22,
                    45,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T12:22:45Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    22,
                    45,
                    1,
                    126,
                    0
                ],
                "title": "Blending 3D Geometry and Machine Learning for Multi-View Stereopsis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending 3D Geometry and Machine Learning for Multi-View Stereopsis"
                },
                "summary": "Traditional multi-view stereo (MVS) methods primarily depend on photometric\nand geometric consistency constraints. In contrast, modern learning-based\nalgorithms often rely on the plane sweep algorithm to infer 3D geometry,\napplying explicit geometric consistency (GC) checks only as a post-processing\nstep, with no impact on the learning process itself. In this work, we introduce\nGC MVSNet plus plus, a novel approach that actively enforces geometric\nconsistency of reference view depth maps across multiple source views (multi\nview) and at various scales (multi scale) during the learning phase (see Fig.\n1). This integrated GC check significantly accelerates the learning process by\ndirectly penalizing geometrically inconsistent pixels, effectively halving the\nnumber of training iterations compared to other MVS methods. Furthermore, we\nintroduce a densely connected cost regularization network with two distinct\nblock designs simple and feature dense optimized to harness dense feature\nconnections for enhanced regularization. Extensive experiments demonstrate that\nour approach achieves a new state of the art on the DTU and BlendedMVS datasets\nand secures second place on the Tanks and Temples benchmark. To our knowledge,\nGC MVSNet plus plus is the first method to enforce multi-view, multi-scale\nsupervised geometric consistency during learning. Our code is available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional multi-view stereo (MVS) methods primarily depend on photometric\nand geometric consistency constraints. In contrast, modern learning-based\nalgorithms often rely on the plane sweep algorithm to infer 3D geometry,\napplying explicit geometric consistency (GC) checks only as a post-processing\nstep, with no impact on the learning process itself. In this work, we introduce\nGC MVSNet plus plus, a novel approach that actively enforces geometric\nconsistency of reference view depth maps across multiple source views (multi\nview) and at various scales (multi scale) during the learning phase (see Fig.\n1). This integrated GC check significantly accelerates the learning process by\ndirectly penalizing geometrically inconsistent pixels, effectively halving the\nnumber of training iterations compared to other MVS methods. Furthermore, we\nintroduce a densely connected cost regularization network with two distinct\nblock designs simple and feature dense optimized to harness dense feature\nconnections for enhanced regularization. Extensive experiments demonstrate that\nour approach achieves a new state of the art on the DTU and BlendedMVS datasets\nand secures second place on the Tanks and Temples benchmark. To our knowledge,\nGC MVSNet plus plus is the first method to enforce multi-view, multi-scale\nsupervised geometric consistency during learning. Our code is available."
                },
                "authors": [
                    {
                        "name": "Vibhas Vats"
                    },
                    {
                        "name": "Md. Alimoor Reza"
                    },
                    {
                        "name": "David Crandall"
                    },
                    {
                        "name": "Soon-heung Jung"
                    }
                ],
                "author_detail": {
                    "name": "Soon-heung Jung"
                },
                "author": "Soon-heung Jung",
                "arxiv_comment": "A pre-print -- paper under-review. arXiv admin note: substantial text\n  overlap with arXiv:2310.19583",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04430v3",
                "updated": "2025-05-06T12:19:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    19,
                    55,
                    1,
                    126,
                    0
                ],
                "published": "2024-08-08T12:57:14Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    12,
                    57,
                    14,
                    3,
                    221,
                    0
                ],
                "title": "The Struggles of LLMs in Cross-lingual Code Clone Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Struggles of LLMs in Cross-lingual Code Clone Detection"
                },
                "summary": "With the involvement of multiple programming languages in modern software\ndevelopment, cross-lingual code clone detection has gained traction within the\nsoftware engineering community. Numerous studies have explored this topic,\nproposing various promising approaches. Inspired by the significant advances in\nmachine learning in recent years, particularly Large Language Models (LLMs),\nwhich have demonstrated their ability to tackle various tasks, this paper\nrevisits cross-lingual code clone detection. We evaluate the performance of\nfive (05) LLMs and eight prompts (08) for the identification of cross-lingual\ncode clones. Additionally, we compare these results against two baseline\nmethods. Finally, we evaluate a pre-trained embedding model to assess the\neffectiveness of the generated representations for classifying clone and\nnon-clone pairs. The studies involving LLMs and Embedding models are evaluated\nusing two widely used cross-lingual datasets, XLCoST and CodeNet. Our results\nshow that LLMs can achieve high F1 scores, up to 0.99, for straightforward\nprogramming examples. However, they not only perform less well on programs\nassociated with complex programming challenges but also do not necessarily\nunderstand the meaning of \"code clones\" in a cross-lingual setting. We show\nthat embedding models used to represent code fragments from different\nprogramming languages in the same representation space enable the training of a\nbasic classifier that outperforms all LLMs by ~1 and ~20 percentage points on\nthe XLCoST and CodeNet datasets, respectively. This finding suggests that,\ndespite the apparent capabilities of LLMs, embeddings provided by embedding\nmodels offer suitable representations to achieve state-of-the-art performance\nin cross-lingual code clone detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the involvement of multiple programming languages in modern software\ndevelopment, cross-lingual code clone detection has gained traction within the\nsoftware engineering community. Numerous studies have explored this topic,\nproposing various promising approaches. Inspired by the significant advances in\nmachine learning in recent years, particularly Large Language Models (LLMs),\nwhich have demonstrated their ability to tackle various tasks, this paper\nrevisits cross-lingual code clone detection. We evaluate the performance of\nfive (05) LLMs and eight prompts (08) for the identification of cross-lingual\ncode clones. Additionally, we compare these results against two baseline\nmethods. Finally, we evaluate a pre-trained embedding model to assess the\neffectiveness of the generated representations for classifying clone and\nnon-clone pairs. The studies involving LLMs and Embedding models are evaluated\nusing two widely used cross-lingual datasets, XLCoST and CodeNet. Our results\nshow that LLMs can achieve high F1 scores, up to 0.99, for straightforward\nprogramming examples. However, they not only perform less well on programs\nassociated with complex programming challenges but also do not necessarily\nunderstand the meaning of \"code clones\" in a cross-lingual setting. We show\nthat embedding models used to represent code fragments from different\nprogramming languages in the same representation space enable the training of a\nbasic classifier that outperforms all LLMs by ~1 and ~20 percentage points on\nthe XLCoST and CodeNet datasets, respectively. This finding suggests that,\ndespite the apparent capabilities of LLMs, embeddings provided by embedding\nmodels offer suitable representations to achieve state-of-the-art performance\nin cross-lingual code clone detection."
                },
                "authors": [
                    {
                        "name": "Micheline Bénédicte Moumoula"
                    },
                    {
                        "name": "Abdoul Kader Kabore"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawendé Bissyande"
                    }
                ],
                "author_detail": {
                    "name": "Tegawendé Bissyande"
                },
                "author": "Tegawendé Bissyande",
                "arxiv_doi": "10.1145/3715764",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715764",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.04430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication at the ACM International Conference on the\n  Foundations of Software Engineering (FSE) 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03469v1",
                "updated": "2025-05-06T12:18:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    18,
                    11,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T12:18:11Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    18,
                    11,
                    1,
                    126,
                    0
                ],
                "title": "Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting\n  Efficient Reasoning in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting\n  Efficient Reasoning in Large Language Models"
                },
                "summary": "Recent advances in large language models have demonstrated that Supervised\nFine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from\nlarge reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning\ncapabilities to non-reasoning models. However, models fine-tuned with this\napproach inherit the \"overthinking\" problem from teacher models, producing\nverbose and redundant reasoning chains during inference. To address this\nchallenge, we propose \\textbf{L}ong-\\textbf{S}hort Chain-of-Thought\n\\textbf{Mixture} \\textbf{S}upervised \\textbf{F}ine-\\textbf{T}uning\n(\\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their\nshort counterparts obtained through structure-preserved rewriting. Our\nexperiments demonstrate that models trained using the LS-Mixture SFT method,\ncompared to those trained with direct SFT, achieved an average accuracy\nimprovement of 2.3\\% across various benchmarks while substantially reducing\nmodel response length by approximately 47.61\\%. This work offers an approach to\nendow non-reasoning models with reasoning capabilities through supervised\nfine-tuning while avoiding the inherent overthinking problems inherited from\nteacher models, thereby enabling efficient reasoning in the fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated that Supervised\nFine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from\nlarge reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning\ncapabilities to non-reasoning models. However, models fine-tuned with this\napproach inherit the \"overthinking\" problem from teacher models, producing\nverbose and redundant reasoning chains during inference. To address this\nchallenge, we propose \\textbf{L}ong-\\textbf{S}hort Chain-of-Thought\n\\textbf{Mixture} \\textbf{S}upervised \\textbf{F}ine-\\textbf{T}uning\n(\\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their\nshort counterparts obtained through structure-preserved rewriting. Our\nexperiments demonstrate that models trained using the LS-Mixture SFT method,\ncompared to those trained with direct SFT, achieved an average accuracy\nimprovement of 2.3\\% across various benchmarks while substantially reducing\nmodel response length by approximately 47.61\\%. This work offers an approach to\nendow non-reasoning models with reasoning capabilities through supervised\nfine-tuning while avoiding the inherent overthinking problems inherited from\nteacher models, thereby enabling efficient reasoning in the fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Bin Yu"
                    },
                    {
                        "name": "Hang Yuan"
                    },
                    {
                        "name": "Yuliang Wei"
                    },
                    {
                        "name": "Bailing Wang"
                    },
                    {
                        "name": "Weizhen Qi"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "11 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03467v1",
                "updated": "2025-05-06T12:12:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    12,
                    48,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T12:12:48Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    12,
                    48,
                    1,
                    126,
                    0
                ],
                "title": "Uncertainty-Aware Large Language Models for Explainable Disease\n  Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Large Language Models for Explainable Disease\n  Diagnosis"
                },
                "summary": "Explainable disease diagnosis, which leverages patient information (e.g.,\nsigns and symptoms) and computational models to generate probable diagnoses and\nreasonings, offers clear clinical values. However, when clinical notes\nencompass insufficient evidence for a definite diagnosis, such as the absence\nof definitive symptoms, diagnostic uncertainty usually arises, increasing the\nrisk of misdiagnosis and adverse outcomes. Although explicitly identifying and\nexplaining diagnostic uncertainties is essential for trustworthy diagnostic\nsystems, it remains under-explored. To fill this gap, we introduce ConfiDx, an\nuncertainty-aware large language model (LLM) created by fine-tuning open-source\nLLMs with diagnostic criteria. We formalized the task and assembled richly\nannotated datasets that capture varying degrees of diagnostic ambiguity.\nEvaluating ConfiDx on real-world datasets demonstrated that it excelled in\nidentifying diagnostic uncertainties, achieving superior diagnostic\nperformance, and generating trustworthy explanations for diagnoses and\nuncertainties. To our knowledge, this is the first study to jointly address\ndiagnostic uncertainty recognition and explanation, substantially enhancing the\nreliability of automatic diagnostic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable disease diagnosis, which leverages patient information (e.g.,\nsigns and symptoms) and computational models to generate probable diagnoses and\nreasonings, offers clear clinical values. However, when clinical notes\nencompass insufficient evidence for a definite diagnosis, such as the absence\nof definitive symptoms, diagnostic uncertainty usually arises, increasing the\nrisk of misdiagnosis and adverse outcomes. Although explicitly identifying and\nexplaining diagnostic uncertainties is essential for trustworthy diagnostic\nsystems, it remains under-explored. To fill this gap, we introduce ConfiDx, an\nuncertainty-aware large language model (LLM) created by fine-tuning open-source\nLLMs with diagnostic criteria. We formalized the task and assembled richly\nannotated datasets that capture varying degrees of diagnostic ambiguity.\nEvaluating ConfiDx on real-world datasets demonstrated that it excelled in\nidentifying diagnostic uncertainties, achieving superior diagnostic\nperformance, and generating trustworthy explanations for diagnoses and\nuncertainties. To our knowledge, this is the first study to jointly address\ndiagnostic uncertainty recognition and explanation, substantially enhancing the\nreliability of automatic diagnostic systems."
                },
                "authors": [
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Jiashuo Wang"
                    },
                    {
                        "name": "Zidu Xu"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "David Brauer"
                    },
                    {
                        "name": "Lindsay Welton"
                    },
                    {
                        "name": "Jacob Cogan"
                    },
                    {
                        "name": "Yuen-Hei Chung"
                    },
                    {
                        "name": "Lei Tian"
                    },
                    {
                        "name": "Zaifu Zhan"
                    },
                    {
                        "name": "Yu Hou"
                    },
                    {
                        "name": "Mingquan Lin"
                    },
                    {
                        "name": "Genevieve B. Melton"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "arxiv_comment": "22 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03460v1",
                "updated": "2025-05-06T12:00:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    0,
                    49,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T12:00:49Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    0,
                    49,
                    1,
                    126,
                    0
                ],
                "title": "LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal\n  Delivery Based on Agentic UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal\n  Delivery Based on Agentic UAVs"
                },
                "summary": "The growing demand for intelligent logistics, particularly fine-grained\nterminal delivery, underscores the need for autonomous UAV (Unmanned Aerial\nVehicle)-based delivery systems. However, most existing last-mile delivery\nstudies rely on ground robots, while current UAV-based Vision-Language\nNavigation (VLN) tasks primarily focus on coarse-grained, long-range goals,\nmaking them unsuitable for precise terminal delivery. To bridge this gap, we\npropose LogisticsVLN, a scalable aerial delivery system built on multimodal\nlarge language models (MLLMs) for autonomous terminal delivery. LogisticsVLN\nintegrates lightweight Large Language Models (LLMs) and Visual-Language Models\n(VLMs) in a modular pipeline for request understanding, floor localization,\nobject detection, and action-decision making. To support research and\nevaluation in this new setting, we construct the Vision-Language Delivery (VLD)\ndataset within the CARLA simulator. Experimental results on the VLD dataset\nshowcase the feasibility of the LogisticsVLN system. In addition, we conduct\nsubtask-level evaluations of each module of our system, offering valuable\ninsights for improving the robustness and real-world deployment of foundation\nmodel-based vision-language delivery systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for intelligent logistics, particularly fine-grained\nterminal delivery, underscores the need for autonomous UAV (Unmanned Aerial\nVehicle)-based delivery systems. However, most existing last-mile delivery\nstudies rely on ground robots, while current UAV-based Vision-Language\nNavigation (VLN) tasks primarily focus on coarse-grained, long-range goals,\nmaking them unsuitable for precise terminal delivery. To bridge this gap, we\npropose LogisticsVLN, a scalable aerial delivery system built on multimodal\nlarge language models (MLLMs) for autonomous terminal delivery. LogisticsVLN\nintegrates lightweight Large Language Models (LLMs) and Visual-Language Models\n(VLMs) in a modular pipeline for request understanding, floor localization,\nobject detection, and action-decision making. To support research and\nevaluation in this new setting, we construct the Vision-Language Delivery (VLD)\ndataset within the CARLA simulator. Experimental results on the VLD dataset\nshowcase the feasibility of the LogisticsVLN system. In addition, we conduct\nsubtask-level evaluations of each module of our system, offering valuable\ninsights for improving the robustness and real-world deployment of foundation\nmodel-based vision-language delivery systems."
                },
                "authors": [
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Yonglin Tian"
                    },
                    {
                        "name": "Fei Lin"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Kornélia Sára Szatmáry"
                    },
                    {
                        "name": "Fei-Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei-Yue Wang"
                },
                "author": "Fei-Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02560v2",
                "updated": "2025-05-06T11:44:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    44,
                    32,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T11:02:31Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    2,
                    31,
                    0,
                    125,
                    0
                ],
                "title": "Evaluating Contrastive Feedback for Effective User Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Contrastive Feedback for Effective User Simulations"
                },
                "summary": "The use of Large Language Models (LLMs) for simulating user behavior in the\ndomain of Interactive Information Retrieval has recently gained significant\npopularity. However, their application and capabilities remain highly debated\nand understudied. This study explores whether the underlying principles of\ncontrastive training techniques, which have been effective for fine-tuning\nLLMs, can also be applied beneficially in the area of prompt engineering for\nuser simulations.\n  Previous research has shown that LLMs possess comprehensive world knowledge,\nwhich can be leveraged to provide accurate estimates of relevant documents.\nThis study attempts to simulate a knowledge state by enhancing the model with\nadditional implicit contextual information gained during the simulation. This\napproach enables the model to refine the scope of desired documents further.\nThe primary objective of this study is to analyze how different modalities of\ncontextual information influence the effectiveness of user simulations.\n  Various user configurations were tested, where models are provided with\nsummaries of already judged relevant, irrelevant, or both types of documents in\na contrastive manner. The focus of this study is the assessment of the impact\nof the prompting techniques on the simulated user agent performance. We hereby\nlay the foundations for leveraging LLMs as part of more realistic simulated\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) for simulating user behavior in the\ndomain of Interactive Information Retrieval has recently gained significant\npopularity. However, their application and capabilities remain highly debated\nand understudied. This study explores whether the underlying principles of\ncontrastive training techniques, which have been effective for fine-tuning\nLLMs, can also be applied beneficially in the area of prompt engineering for\nuser simulations.\n  Previous research has shown that LLMs possess comprehensive world knowledge,\nwhich can be leveraged to provide accurate estimates of relevant documents.\nThis study attempts to simulate a knowledge state by enhancing the model with\nadditional implicit contextual information gained during the simulation. This\napproach enables the model to refine the scope of desired documents further.\nThe primary objective of this study is to analyze how different modalities of\ncontextual information influence the effectiveness of user simulations.\n  Various user configurations were tested, where models are provided with\nsummaries of already judged relevant, irrelevant, or both types of documents in\na contrastive manner. The focus of this study is the assessment of the impact\nof the prompting techniques on the simulated user agent performance. We hereby\nlay the foundations for leveraging LLMs as part of more realistic simulated\nusers."
                },
                "authors": [
                    {
                        "name": "Andreas Konstantin Kruff"
                    },
                    {
                        "name": "Timo Breuer"
                    },
                    {
                        "name": "Philipp Schaer"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Schaer"
                },
                "author": "Philipp Schaer",
                "arxiv_doi": "10.1145/3726302.3730189",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730189",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.02560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18135v2",
                "updated": "2025-05-06T11:41:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    41,
                    37,
                    1,
                    126,
                    0
                ],
                "published": "2024-12-24T03:43:15Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    3,
                    43,
                    15,
                    1,
                    359,
                    0
                ],
                "title": "LSAQ: Layer-Specific Adaptive Quantization for Large Language Model\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSAQ: Layer-Specific Adaptive Quantization for Large Language Model\n  Deployment"
                },
                "summary": "As Large Language Models (LLMs) demonstrate exceptional performance across\nvarious domains, deploying LLMs on edge devices has emerged as a new trend.\nQuantization techniques, which reduce the size and memory requirements of LLMs,\nare effective for deploying LLMs on resource-limited edge devices. However,\nexisting one-size-fits-all quantization methods often fail to dynamically\nadjust the memory requirements of LLMs, limiting their applications to\npractical edge devices with various computation resources. To tackle this\nissue, we propose Layer-Specific Adaptive Quantization (LSAQ), a system for\nadaptive quantization and dynamic deployment of LLMs based on layer importance.\nSpecifically, LSAQ evaluates the importance of LLMs' neural layers by\nconstructing top-k token sets from the inputs and outputs of each layer and\ncalculating their Jaccard similarity. Based on layer importance, our system\nadaptively adjusts quantization strategies in real time according to the\ncomputation resource of edge devices, which applies higher quantization\nprecision to layers with higher importance, and vice versa. {Experimental\nresults show that LSAQ consistently outperforms the selected quantization\nbaselines in terms of perplexity and zero-shot tasks. Additionally, it can\ndevise appropriate quantization schemes for different usage scenarios to\nfacilitate the deployment of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) demonstrate exceptional performance across\nvarious domains, deploying LLMs on edge devices has emerged as a new trend.\nQuantization techniques, which reduce the size and memory requirements of LLMs,\nare effective for deploying LLMs on resource-limited edge devices. However,\nexisting one-size-fits-all quantization methods often fail to dynamically\nadjust the memory requirements of LLMs, limiting their applications to\npractical edge devices with various computation resources. To tackle this\nissue, we propose Layer-Specific Adaptive Quantization (LSAQ), a system for\nadaptive quantization and dynamic deployment of LLMs based on layer importance.\nSpecifically, LSAQ evaluates the importance of LLMs' neural layers by\nconstructing top-k token sets from the inputs and outputs of each layer and\ncalculating their Jaccard similarity. Based on layer importance, our system\nadaptively adjusts quantization strategies in real time according to the\ncomputation resource of edge devices, which applies higher quantization\nprecision to layers with higher importance, and vice versa. {Experimental\nresults show that LSAQ consistently outperforms the selected quantization\nbaselines in terms of perplexity and zero-shot tasks. Additionally, it can\ndevise appropriate quantization schemes for different usage scenarios to\nfacilitate the deployment of LLMs."
                },
                "authors": [
                    {
                        "name": "Binrui Zeng"
                    },
                    {
                        "name": "Bin Ji"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Jie Yu"
                    },
                    {
                        "name": "Shasha Li"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Shangwen Wang"
                    },
                    {
                        "name": "Xinran Hong"
                    },
                    {
                        "name": "Yongtao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yongtao Tang"
                },
                "author": "Yongtao Tang",
                "arxiv_comment": "8 pages, 4 figures, accepted to IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13551v3",
                "updated": "2025-05-06T11:38:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    38,
                    24,
                    1,
                    126,
                    0
                ],
                "published": "2025-03-16T15:18:40Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    15,
                    18,
                    40,
                    6,
                    75,
                    0
                ],
                "title": "Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in\n  Large Language Models"
                },
                "summary": "Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate step.\nIn addition, the cost of annotating reasoning processes for reward modeling is\nhigh, making large-scale collection of high-quality data challenging. To\naddress this, we propose a novel reward model approach called the Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps at both fine-grained and coarse-grained levels. HRM excels at assessing\nmulti-step reasoning coherence, especially when flawed steps are later\ncorrected through self-reflection. To further reduce the cost of generating\ntraining data, we introduce a lightweight and effective data augmentation\nstrategy called Hierarchical Node Compression (HNC), which merges two\nconsecutive reasoning steps into one within the tree structure. By applying HNC\nto MCTS-generated reasoning trajectories, we enhance the diversity and\nrobustness of HRM training data while introducing controlled noise with minimal\ncomputational overhead. Empirical results on the PRM800K dataset show that HRM,\ntogether with HNC, provides more stable and reliable evaluations than PRM.\nFurthermore, cross-domain evaluations on the MATH500 and GSM8K datasets\ndemonstrate HRM's strong generalization and robustness across a variety of\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate step.\nIn addition, the cost of annotating reasoning processes for reward modeling is\nhigh, making large-scale collection of high-quality data challenging. To\naddress this, we propose a novel reward model approach called the Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps at both fine-grained and coarse-grained levels. HRM excels at assessing\nmulti-step reasoning coherence, especially when flawed steps are later\ncorrected through self-reflection. To further reduce the cost of generating\ntraining data, we introduce a lightweight and effective data augmentation\nstrategy called Hierarchical Node Compression (HNC), which merges two\nconsecutive reasoning steps into one within the tree structure. By applying HNC\nto MCTS-generated reasoning trajectories, we enhance the diversity and\nrobustness of HRM training data while introducing controlled noise with minimal\ncomputational overhead. Empirical results on the PRM800K dataset show that HRM,\ntogether with HNC, provides more stable and reliable evaluations than PRM.\nFurthermore, cross-domain evaluations on the MATH500 and GSM8K datasets\ndemonstrate HRM's strong generalization and robustness across a variety of\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Zhangyi Jiang"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Shenyang Tong"
                    },
                    {
                        "name": "Wenhan Yang"
                    },
                    {
                        "name": "Yanan Zheng"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Zifan He"
                    },
                    {
                        "name": "Hailei Gong"
                    }
                ],
                "author_detail": {
                    "name": "Hailei Gong"
                },
                "author": "Hailei Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03441v1",
                "updated": "2025-05-06T11:27:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    27,
                    48,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T11:27:48Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    27,
                    48,
                    1,
                    126,
                    0
                ],
                "title": "Simultaneous global and local clustering in multiplex networks with\n  covariate information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous global and local clustering in multiplex networks with\n  covariate information"
                },
                "summary": "Understanding both global and layer-specific group structures is useful for\nuncovering complex patterns in networks with multiple interaction types. In\nthis work, we introduce a new model, the hierarchical multiplex stochastic\nblockmodel (HMPSBM), that simultaneously detects communities within individual\nlayers of a multiplex network while inferring a global node clustering across\nthe layers. A stochastic blockmodel is assumed in each layer, with\nprobabilities of layer-level group memberships determined by a node's global\ngroup assignment. Our model uses a Bayesian framework, employing a probit\nstick-breaking process to construct node-specific mixing proportions over a set\nof shared Griffiths-Engen-McCloseky (GEM) distributions. These proportions\ndetermine layer-level community assignment, allowing for an unknown and varying\nnumber of groups across layers, while incorporating nodal covariate information\nto inform the global clustering. We propose a scalable variational inference\nprocedure with parallelisable updates for application to large networks.\nExtensive simulation studies demonstrate our model's ability to accurately\nrecover both global and layer-level clusters in complicated settings, and\napplications to real data showcase the model's effectiveness in uncovering\ninteresting latent network structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding both global and layer-specific group structures is useful for\nuncovering complex patterns in networks with multiple interaction types. In\nthis work, we introduce a new model, the hierarchical multiplex stochastic\nblockmodel (HMPSBM), that simultaneously detects communities within individual\nlayers of a multiplex network while inferring a global node clustering across\nthe layers. A stochastic blockmodel is assumed in each layer, with\nprobabilities of layer-level group memberships determined by a node's global\ngroup assignment. Our model uses a Bayesian framework, employing a probit\nstick-breaking process to construct node-specific mixing proportions over a set\nof shared Griffiths-Engen-McCloseky (GEM) distributions. These proportions\ndetermine layer-level community assignment, allowing for an unknown and varying\nnumber of groups across layers, while incorporating nodal covariate information\nto inform the global clustering. We propose a scalable variational inference\nprocedure with parallelisable updates for application to large networks.\nExtensive simulation studies demonstrate our model's ability to accurately\nrecover both global and layer-level clusters in complicated settings, and\napplications to real data showcase the model's effectiveness in uncovering\ninteresting latent network structure."
                },
                "authors": [
                    {
                        "name": "Joshua Corneck"
                    },
                    {
                        "name": "Edward A. K. Cohen"
                    },
                    {
                        "name": "James S. Martin"
                    },
                    {
                        "name": "Lekha Patel"
                    },
                    {
                        "name": "Kurtis W. Shuler"
                    },
                    {
                        "name": "Francesco Sanna Passino"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Sanna Passino"
                },
                "author": "Francesco Sanna Passino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03439v1",
                "updated": "2025-05-06T11:25:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    25,
                    52,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T11:25:52Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    25,
                    52,
                    1,
                    126,
                    0
                ],
                "title": "The Steganographic Potentials of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Steganographic Potentials of Language Models"
                },
                "summary": "The potential for large language models (LLMs) to hide messages within plain\ntext (steganography) poses a challenge to detection and thwarting of unaligned\nAI agents, and undermines faithfulness of LLMs reasoning. We explore the\nsteganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)\nto: (1) develop covert encoding schemes, (2) engage in steganography when\nprompted, and (3) utilize steganography in realistic scenarios where hidden\nreasoning is likely, but not prompted. In these scenarios, we detect the\nintention of LLMs to hide their reasoning as well as their steganography\nperformance. Our findings in the fine-tuning experiments as well as in\nbehavioral non fine-tuning evaluations reveal that while current models exhibit\nrudimentary steganographic abilities in terms of security and capacity,\nexplicit algorithmic guidance markedly enhances their capacity for information\nconcealment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The potential for large language models (LLMs) to hide messages within plain\ntext (steganography) poses a challenge to detection and thwarting of unaligned\nAI agents, and undermines faithfulness of LLMs reasoning. We explore the\nsteganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)\nto: (1) develop covert encoding schemes, (2) engage in steganography when\nprompted, and (3) utilize steganography in realistic scenarios where hidden\nreasoning is likely, but not prompted. In these scenarios, we detect the\nintention of LLMs to hide their reasoning as well as their steganography\nperformance. Our findings in the fine-tuning experiments as well as in\nbehavioral non fine-tuning evaluations reveal that while current models exhibit\nrudimentary steganographic abilities in terms of security and capacity,\nexplicit algorithmic guidance markedly enhances their capacity for information\nconcealment."
                },
                "authors": [
                    {
                        "name": "Artem Karpov"
                    },
                    {
                        "name": "Tinuade Adeleke"
                    },
                    {
                        "name": "Seong Hah Cho"
                    },
                    {
                        "name": "Natalia Perez-Campanero"
                    }
                ],
                "author_detail": {
                    "name": "Natalia Perez-Campanero"
                },
                "author": "Natalia Perez-Campanero",
                "arxiv_comment": "Published at Building Trust Workshop at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03434v1",
                "updated": "2025-05-06T11:18:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    18,
                    34,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T11:18:34Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    18,
                    34,
                    1,
                    126,
                    0
                ],
                "title": "Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in\n  LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in\n  LLM-Based Agents"
                },
                "summary": "Large Language Models (LLMs) represent a landmark achievement in Artificial\nIntelligence (AI), demonstrating unprecedented proficiency in procedural tasks\nsuch as text generation, code completion, and conversational coherence. These\ncapabilities stem from their architecture, which mirrors human procedural\nmemory -- the brain's ability to automate repetitive, pattern-driven tasks\nthrough practice. However, as LLMs are increasingly deployed in real-world\napplications, it becomes impossible to ignore their limitations operating in\ncomplex, unpredictable environments. This paper argues that LLMs, while\ntransformative, are fundamentally constrained by their reliance on procedural\nmemory. To create agents capable of navigating ``wicked'' learning environments\n-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must\naugment LLMs with semantic memory and associative learning systems. By adopting\na modular architecture that decouples these cognitive functions, we can bridge\nthe gap between narrow procedural expertise and the adaptive intelligence\nrequired for real-world problem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a landmark achievement in Artificial\nIntelligence (AI), demonstrating unprecedented proficiency in procedural tasks\nsuch as text generation, code completion, and conversational coherence. These\ncapabilities stem from their architecture, which mirrors human procedural\nmemory -- the brain's ability to automate repetitive, pattern-driven tasks\nthrough practice. However, as LLMs are increasingly deployed in real-world\napplications, it becomes impossible to ignore their limitations operating in\ncomplex, unpredictable environments. This paper argues that LLMs, while\ntransformative, are fundamentally constrained by their reliance on procedural\nmemory. To create agents capable of navigating ``wicked'' learning environments\n-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must\naugment LLMs with semantic memory and associative learning systems. By adopting\na modular architecture that decouples these cognitive functions, we can bridge\nthe gap between narrow procedural expertise and the adaptive intelligence\nrequired for real-world problem-solving."
                },
                "authors": [
                    {
                        "name": "Schaun Wheeler"
                    },
                    {
                        "name": "Olivier Jeunen"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Jeunen"
                },
                "author": "Olivier Jeunen",
                "arxiv_doi": "10.1145/3708319.3734172",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708319.3734172",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.03434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to the workshop on Hybrid AI for Human-Centric\n  Personalization (HyPer), co-located with ACM UMAP '25",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03427v1",
                "updated": "2025-05-06T11:07:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    7,
                    26,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T11:07:26Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    7,
                    26,
                    1,
                    126,
                    0
                ],
                "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare."
                },
                "authors": [
                    {
                        "name": "Mouath Abu Daoud"
                    },
                    {
                        "name": "Chaimae Abouzahir"
                    },
                    {
                        "name": "Leen Kharouf"
                    },
                    {
                        "name": "Walid Al-Eisawi"
                    },
                    {
                        "name": "Nizar Habash"
                    },
                    {
                        "name": "Farah E. Shamout"
                    }
                ],
                "author_detail": {
                    "name": "Farah E. Shamout"
                },
                "author": "Farah E. Shamout",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03425v1",
                "updated": "2025-05-06T11:04:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    4,
                    7,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T11:04:07Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    4,
                    7,
                    1,
                    126,
                    0
                ],
                "title": "Directed Greybox Fuzzing via Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed Greybox Fuzzing via Large Language Model"
                },
                "summary": "Directed greybox fuzzing (DGF) focuses on efficiently reaching specific\nprogram locations or triggering particular behaviors, making it essential for\ntasks like vulnerability detection and crash reproduction. However, existing\nmethods often suffer from path explosion and randomness in input mutation,\nleading to inefficiencies in exploring and exploiting target paths. In this\npaper, we propose HGFuzzer, an automatic framework that leverages the large\nlanguage model (LLM) to address these challenges. HGFuzzer transforms path\nconstraint problems into targeted code generation tasks, systematically\ngenerating test harnesses and reachable inputs to reduce unnecessary\nexploration paths significantly. Additionally, we implement custom mutators\ndesigned specifically for target functions, minimizing randomness and improving\nthe precision of directed fuzzing. We evaluated HGFuzzer on 20 real-world\nvulnerabilities, successfully triggering 17, including 11 within the first\nminute, achieving a speedup of at least 24.8x compared to state-of-the-art\ndirected fuzzers. Furthermore, HGFuzzer discovered 9 previously unknown\nvulnerabilities, all of which were assigned CVE IDs, demonstrating the\neffectiveness of our approach in identifying real-world vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed greybox fuzzing (DGF) focuses on efficiently reaching specific\nprogram locations or triggering particular behaviors, making it essential for\ntasks like vulnerability detection and crash reproduction. However, existing\nmethods often suffer from path explosion and randomness in input mutation,\nleading to inefficiencies in exploring and exploiting target paths. In this\npaper, we propose HGFuzzer, an automatic framework that leverages the large\nlanguage model (LLM) to address these challenges. HGFuzzer transforms path\nconstraint problems into targeted code generation tasks, systematically\ngenerating test harnesses and reachable inputs to reduce unnecessary\nexploration paths significantly. Additionally, we implement custom mutators\ndesigned specifically for target functions, minimizing randomness and improving\nthe precision of directed fuzzing. We evaluated HGFuzzer on 20 real-world\nvulnerabilities, successfully triggering 17, including 11 within the first\nminute, achieving a speedup of at least 24.8x compared to state-of-the-art\ndirected fuzzers. Furthermore, HGFuzzer discovered 9 previously unknown\nvulnerabilities, all of which were assigned CVE IDs, demonstrating the\neffectiveness of our approach in identifying real-world vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Hanxiang Xu"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10148v2",
                "updated": "2025-05-06T11:03:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    3,
                    22,
                    1,
                    126,
                    0
                ],
                "published": "2025-02-14T13:23:18Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    23,
                    18,
                    4,
                    45,
                    0
                ],
                "title": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis"
                },
                "summary": "Despite much progress in training distributed artificial intelligence (AI),\nbuilding cooperative multi-agent systems with multi-agent reinforcement\nlearning (MARL) faces challenges in sample efficiency, interpretability, and\ntransferability. Unlike traditional learning-based methods that require\nextensive interaction with the environment, large language models (LLMs)\ndemonstrate remarkable capabilities in zero-shot planning and complex\nreasoning. However, existing LLM-based approaches heavily rely on text-based\nobservations and struggle with the non-Markovian nature of multi-agent\ninteractions under partial observability. We present COMPASS, a novel\nmulti-agent architecture that integrates vision-language models (VLMs) with a\ndynamic skill library and structured communication for decentralized\nclosed-loop decision-making. The skill library, bootstrapped from\ndemonstrations, evolves via planner-guided tasks to enable adaptive strategies.\nCOMPASS propagates entity information through multi-hop communication under\npartial observability. Evaluations on the improved StarCraft Multi-Agent\nChallenge (SMACv2) demonstrate COMPASS's strong performance against\nstate-of-the-art MARL baselines across both symmetric and asymmetric scenarios.\nNotably, in the symmetric Protoss 5v5 task, COMPASS achieved a 57\\% win rate,\nrepresenting a 30 percentage point advantage over QMIX (27\\%). Project page can\nbe found at https://stellar-entremet-1720bb.netlify.app/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite much progress in training distributed artificial intelligence (AI),\nbuilding cooperative multi-agent systems with multi-agent reinforcement\nlearning (MARL) faces challenges in sample efficiency, interpretability, and\ntransferability. Unlike traditional learning-based methods that require\nextensive interaction with the environment, large language models (LLMs)\ndemonstrate remarkable capabilities in zero-shot planning and complex\nreasoning. However, existing LLM-based approaches heavily rely on text-based\nobservations and struggle with the non-Markovian nature of multi-agent\ninteractions under partial observability. We present COMPASS, a novel\nmulti-agent architecture that integrates vision-language models (VLMs) with a\ndynamic skill library and structured communication for decentralized\nclosed-loop decision-making. The skill library, bootstrapped from\ndemonstrations, evolves via planner-guided tasks to enable adaptive strategies.\nCOMPASS propagates entity information through multi-hop communication under\npartial observability. Evaluations on the improved StarCraft Multi-Agent\nChallenge (SMACv2) demonstrate COMPASS's strong performance against\nstate-of-the-art MARL baselines across both symmetric and asymmetric scenarios.\nNotably, in the symmetric Protoss 5v5 task, COMPASS achieved a 57\\% win rate,\nrepresenting a 30 percentage point advantage over QMIX (27\\%). Project page can\nbe found at https://stellar-entremet-1720bb.netlify.app/."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Wenshuai Zhao"
                    },
                    {
                        "name": "Joni Pajarinen"
                    }
                ],
                "author_detail": {
                    "name": "Joni Pajarinen"
                },
                "author": "Joni Pajarinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03420v1",
                "updated": "2025-05-06T10:55:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    55,
                    21,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T10:55:21Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    55,
                    21,
                    1,
                    126,
                    0
                ],
                "title": "Mitigating Image Captioning Hallucinations in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Image Captioning Hallucinations in Vision-Language Models"
                },
                "summary": "Hallucinations in vision-language models (VLMs) hinder reliability and\nreal-world applicability, usually stemming from distribution shifts between\npretraining data and test samples. Existing solutions, such as retraining or\nfine-tuning on additional data, demand significant computational resources and\nlabor-intensive data collection, while ensemble-based methods incur additional\ncosts by introducing auxiliary VLMs. To address these challenges, we propose a\nnovel test-time adaptation framework using reinforcement learning to mitigate\nhallucinations during inference without retraining or any auxiliary VLMs. By\nupdating only the learnable parameters in the layer normalization of the\nlanguage model (approximately 0.003% of the model parameters), our method\nreduces distribution shifts between test samples and pretraining samples. A\nCLIP-based hallucination evaluation model is proposed to provide dual rewards\nto VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in\nhallucination rates on LLaVA and InstructBLIP, respectively. Our approach\noutperforms state-of-the-art baselines with a 68.3% improvement in\nhallucination mitigation, demonstrating its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in vision-language models (VLMs) hinder reliability and\nreal-world applicability, usually stemming from distribution shifts between\npretraining data and test samples. Existing solutions, such as retraining or\nfine-tuning on additional data, demand significant computational resources and\nlabor-intensive data collection, while ensemble-based methods incur additional\ncosts by introducing auxiliary VLMs. To address these challenges, we propose a\nnovel test-time adaptation framework using reinforcement learning to mitigate\nhallucinations during inference without retraining or any auxiliary VLMs. By\nupdating only the learnable parameters in the layer normalization of the\nlanguage model (approximately 0.003% of the model parameters), our method\nreduces distribution shifts between test samples and pretraining samples. A\nCLIP-based hallucination evaluation model is proposed to provide dual rewards\nto VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in\nhallucination rates on LLaVA and InstructBLIP, respectively. Our approach\noutperforms state-of-the-art baselines with a 68.3% improvement in\nhallucination mitigation, demonstrating its effectiveness."
                },
                "authors": [
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Chengcui Zhang"
                    },
                    {
                        "name": "Runlin Zhang"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Xi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xi Li"
                },
                "author": "Xi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03418v1",
                "updated": "2025-05-06T10:53:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    53,
                    58,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T10:53:58Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    53,
                    58,
                    1,
                    126,
                    0
                ],
                "title": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey"
                },
                "summary": "Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification."
                },
                "authors": [
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Yuchen Tian"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.00579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.00579v2",
                "updated": "2025-05-06T10:47:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    47,
                    10,
                    1,
                    126,
                    0
                ],
                "published": "2023-11-01T15:23:04Z",
                "published_parsed": [
                    2023,
                    11,
                    1,
                    15,
                    23,
                    4,
                    2,
                    305,
                    0
                ],
                "title": "Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based\n  Inference Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based\n  Inference Accelerators"
                },
                "summary": "Convolutional Neural Networks (CNNs) are widely used in various domains,\nincluding image recognition, medical diagnosis and autonomous driving. Recent\nadvances in dataflow-based CNN accelerators have enabled CNN inference in\nresource-constrained edge devices. These dataflow accelerators utilize inherent\ndata reuse of convolution layers to process CNN models efficiently. Concealing\nthe architecture of CNN models is critical for privacy and security. This\narticle evaluates memory-based side-channel information to recover CNN\narchitectures from dataflow-based CNN inference accelerators. The proposed\nattack exploits spatial and temporal data reuse of the dataflow mapping on CNN\naccelerators and architectural hints to recover the structure of CNN models.\nExperimental results demonstrate that our proposed side-channel attack can\nrecover the structures of popular CNN models, namely, Lenet, Alexnet, VGGnet16,\nand YOLOv2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Neural Networks (CNNs) are widely used in various domains,\nincluding image recognition, medical diagnosis and autonomous driving. Recent\nadvances in dataflow-based CNN accelerators have enabled CNN inference in\nresource-constrained edge devices. These dataflow accelerators utilize inherent\ndata reuse of convolution layers to process CNN models efficiently. Concealing\nthe architecture of CNN models is critical for privacy and security. This\narticle evaluates memory-based side-channel information to recover CNN\narchitectures from dataflow-based CNN inference accelerators. The proposed\nattack exploits spatial and temporal data reuse of the dataflow mapping on CNN\naccelerators and architectural hints to recover the structure of CNN models.\nExperimental results demonstrate that our proposed side-channel attack can\nrecover the structures of popular CNN models, namely, Lenet, Alexnet, VGGnet16,\nand YOLOv2."
                },
                "authors": [
                    {
                        "name": "Hansika Weerasena"
                    },
                    {
                        "name": "Prabhat Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Prabhat Mishra"
                },
                "author": "Prabhat Mishra",
                "arxiv_doi": "10.1145/3688001",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3688001",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.00579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.00579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 23(6):1-25, 2024",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03406v1",
                "updated": "2025-05-06T10:31:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    31,
                    54,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T10:31:54Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    31,
                    54,
                    1,
                    126,
                    0
                ],
                "title": "Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs\n  and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs\n  and Retrieval-Augmented Generation"
                },
                "summary": "This research paper investigates the application of Large Language Models\n(LLMs) in healthcare, specifically focusing on enhancing medical decision\nsupport through Retrieval-Augmented Generation (RAG) integrated with\nhospital-specific data and fine-tuning using Quantized Low-Rank Adaptation\n(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By\nembedding and retrieving context-relevant healthcare information, the system\nsignificantly improves response accuracy. QLoRA facilitates notable parameter\nefficiency and memory optimization, preserving the integrity of medical\ninformation through specialized quantization techniques. Our research also\nshows that our model performs relatively well on various medical benchmarks,\nindicating that it can be used to make basic medical suggestions. This paper\ndetails the system's technical components, including its architecture,\nquantization methods, and key healthcare applications such as enhanced disease\nprediction from patient symptoms and medical history, treatment suggestions,\nand efficient summarization of complex medical reports. We touch on the ethical\nconsiderations-patient privacy, data security, and the need for rigorous\nclinical validation-as well as the practical challenges of integrating such\nsystems into real-world healthcare workflows. Furthermore, the lightweight\nquantized weights ensure scalability and ease of deployment even in\nlow-resource hospital environments. Finally, the paper concludes with an\nanalysis of the broader impact of LLMs on healthcare and outlines future\ndirections for LLMs in medical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research paper investigates the application of Large Language Models\n(LLMs) in healthcare, specifically focusing on enhancing medical decision\nsupport through Retrieval-Augmented Generation (RAG) integrated with\nhospital-specific data and fine-tuning using Quantized Low-Rank Adaptation\n(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By\nembedding and retrieving context-relevant healthcare information, the system\nsignificantly improves response accuracy. QLoRA facilitates notable parameter\nefficiency and memory optimization, preserving the integrity of medical\ninformation through specialized quantization techniques. Our research also\nshows that our model performs relatively well on various medical benchmarks,\nindicating that it can be used to make basic medical suggestions. This paper\ndetails the system's technical components, including its architecture,\nquantization methods, and key healthcare applications such as enhanced disease\nprediction from patient symptoms and medical history, treatment suggestions,\nand efficient summarization of complex medical reports. We touch on the ethical\nconsiderations-patient privacy, data security, and the need for rigorous\nclinical validation-as well as the practical challenges of integrating such\nsystems into real-world healthcare workflows. Furthermore, the lightweight\nquantized weights ensure scalability and ease of deployment even in\nlow-resource hospital environments. Finally, the paper concludes with an\nanalysis of the broader impact of LLMs on healthcare and outlines future\ndirections for LLMs in medical settings."
                },
                "authors": [
                    {
                        "name": "Mohammad Shoaib Ansari"
                    },
                    {
                        "name": "Mohd Sohail Ali Khan"
                    },
                    {
                        "name": "Shubham Revankar"
                    },
                    {
                        "name": "Aditya Varma"
                    },
                    {
                        "name": "Anil S. Mokhade"
                    }
                ],
                "author_detail": {
                    "name": "Anil S. Mokhade"
                },
                "author": "Anil S. Mokhade",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03400v1",
                "updated": "2025-05-06T10:28:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    28,
                    39,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T10:28:39Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    28,
                    39,
                    1,
                    126,
                    0
                ],
                "title": "Close-Fitting Dressing Assistance Based on State Estimation of Feet and\n  Garments with Semantic-based Visual Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Close-Fitting Dressing Assistance Based on State Estimation of Feet and\n  Garments with Semantic-based Visual Attention"
                },
                "summary": "As the population continues to age, a shortage of caregivers is expected in\nthe future. Dressing assistance, in particular, is crucial for opportunities\nfor social participation. Especially dressing close-fitting garments, such as\nsocks, remains challenging due to the need for fine force adjustments to handle\nthe friction or snagging against the skin, while considering the shape and\nposition of the garment. This study introduces a method uses multi-modal\ninformation including not only robot's camera images, joint angles, joint\ntorques, but also tactile forces for proper force interaction that can adapt to\nindividual differences in humans. Furthermore, by introducing semantic\ninformation based on object concepts, rather than relying solely on RGB data,\nit can be generalized to unseen feet and background. In addition, incorporating\ndepth data helps infer relative spatial relationship between the sock and the\nfoot. To validate its capability for semantic object conceptualization and to\nensure safety, training data were collected using a mannequin, and subsequent\nexperiments were conducted with human subjects. In experiments, the robot\nsuccessfully adapted to previously unseen human feet and was able to put socks\non 10 participants, achieving a higher success rate than Action Chunking with\nTransformer and Diffusion Policy. These results demonstrate that the proposed\nmodel can estimate the state of both the garment and the foot, enabling precise\ndressing assistance for close-fitting garments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the population continues to age, a shortage of caregivers is expected in\nthe future. Dressing assistance, in particular, is crucial for opportunities\nfor social participation. Especially dressing close-fitting garments, such as\nsocks, remains challenging due to the need for fine force adjustments to handle\nthe friction or snagging against the skin, while considering the shape and\nposition of the garment. This study introduces a method uses multi-modal\ninformation including not only robot's camera images, joint angles, joint\ntorques, but also tactile forces for proper force interaction that can adapt to\nindividual differences in humans. Furthermore, by introducing semantic\ninformation based on object concepts, rather than relying solely on RGB data,\nit can be generalized to unseen feet and background. In addition, incorporating\ndepth data helps infer relative spatial relationship between the sock and the\nfoot. To validate its capability for semantic object conceptualization and to\nensure safety, training data were collected using a mannequin, and subsequent\nexperiments were conducted with human subjects. In experiments, the robot\nsuccessfully adapted to previously unseen human feet and was able to put socks\non 10 participants, achieving a higher success rate than Action Chunking with\nTransformer and Diffusion Policy. These results demonstrate that the proposed\nmodel can estimate the state of both the garment and the foot, enabling precise\ndressing assistance for close-fitting garments."
                },
                "authors": [
                    {
                        "name": "Takuma Tsukakoshi"
                    },
                    {
                        "name": "Tamon Miyake"
                    },
                    {
                        "name": "Tetsuya Ogata"
                    },
                    {
                        "name": "Yushi Wang"
                    },
                    {
                        "name": "Takumi Akaishi"
                    },
                    {
                        "name": "Shigeki Sugano"
                    }
                ],
                "author_detail": {
                    "name": "Shigeki Sugano"
                },
                "author": "Shigeki Sugano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03397v2",
                "updated": "2025-05-07T06:55:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    6,
                    55,
                    17,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-06T10:25:50Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    25,
                    50,
                    1,
                    126,
                    0
                ],
                "title": "Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath"
                },
                "summary": "Qubit control protocols have traditionally leveraged a characterisation of\nthe qubit-bath coupling via its power spectral density. Previous work proposed\nthe inference of noise operators that characterise the influence of a classical\nbath using a grey-box approach that combines deep neural networks with\nphysics-encoded layers. This overall structure is complex and poses challenges\nin scaling and real-time operations. Here, we show that no expensive neural\nnetworks are needed and that this noise operator description admits an\nefficient parameterisation. We refer to the resulting parameter space as the\n\\textit{quantum feature space} of the qubit dynamics resulting from the coupled\nbath. We show that the Euclidean distance defined over the quantum feature\nspace provides an effective method for classifying noise processes in the\npresence of a given set of controls. Using the quantum feature space as the\ninput space for a simple machine learning algorithm (random forest, in this\ncase), we demonstrate that it can effectively classify the stationarity and the\nbroad class of noise processes perturbing a qubit. Finally, we explore how\ncontrol pulse parameters map to the quantum feature space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qubit control protocols have traditionally leveraged a characterisation of\nthe qubit-bath coupling via its power spectral density. Previous work proposed\nthe inference of noise operators that characterise the influence of a classical\nbath using a grey-box approach that combines deep neural networks with\nphysics-encoded layers. This overall structure is complex and poses challenges\nin scaling and real-time operations. Here, we show that no expensive neural\nnetworks are needed and that this noise operator description admits an\nefficient parameterisation. We refer to the resulting parameter space as the\n\\textit{quantum feature space} of the qubit dynamics resulting from the coupled\nbath. We show that the Euclidean distance defined over the quantum feature\nspace provides an effective method for classifying noise processes in the\npresence of a given set of controls. Using the quantum feature space as the\ninput space for a simple machine learning algorithm (random forest, in this\ncase), we demonstrate that it can effectively classify the stationarity and the\nbroad class of noise processes perturbing a qubit. Finally, we explore how\ncontrol pulse parameters map to the quantum feature space."
                },
                "authors": [
                    {
                        "name": "Chris Wise"
                    },
                    {
                        "name": "Akram Youssry"
                    },
                    {
                        "name": "Alberto Peruzzo"
                    },
                    {
                        "name": "Jo Plested"
                    },
                    {
                        "name": "Matt Woolley"
                    }
                ],
                "author_detail": {
                    "name": "Matt Woolley"
                },
                "author": "Matt Woolley",
                "arxiv_comment": "19 pages, 3 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02825v2",
                "updated": "2025-05-06T10:17:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    17,
                    58,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T17:51:56Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    51,
                    56,
                    0,
                    125,
                    0
                ],
                "title": "Towards Application-Specific Evaluation of Vision Models: Case Studies\n  in Ecology and Biology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Application-Specific Evaluation of Vision Models: Case Studies\n  in Ecology and Biology"
                },
                "summary": "Computer vision methods have demonstrated considerable potential to\nstreamline ecological and biological workflows, with a growing number of\ndatasets and models becoming available to the research community. However,\nthese resources focus predominantly on evaluation using machine learning\nmetrics, with relatively little emphasis on how their application impacts\ndownstream analysis. We argue that models should be evaluated using\napplication-specific metrics that directly represent model performance in the\ncontext of its final use case. To support this argument, we present two\ndisparate case studies: (1) estimating chimpanzee abundance and density with\ncamera trap distance sampling when using a video-based behaviour classifier and\n(2) estimating head rotation in pigeons using a 3D posture estimator. We show\nthat even models with strong machine learning performance (e.g., 87% mAP) can\nyield data that leads to discrepancies in abundance estimates compared to\nexpert-derived data. Similarly, the highest-performing models for posture\nestimation do not produce the most accurate inferences of gaze direction in\npigeons. Motivated by these findings, we call for researchers to integrate\napplication-specific metrics in ecological/biological datasets, allowing for\nmodels to be benchmarked in the context of their downstream application and to\nfacilitate better integration of models into application workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer vision methods have demonstrated considerable potential to\nstreamline ecological and biological workflows, with a growing number of\ndatasets and models becoming available to the research community. However,\nthese resources focus predominantly on evaluation using machine learning\nmetrics, with relatively little emphasis on how their application impacts\ndownstream analysis. We argue that models should be evaluated using\napplication-specific metrics that directly represent model performance in the\ncontext of its final use case. To support this argument, we present two\ndisparate case studies: (1) estimating chimpanzee abundance and density with\ncamera trap distance sampling when using a video-based behaviour classifier and\n(2) estimating head rotation in pigeons using a 3D posture estimator. We show\nthat even models with strong machine learning performance (e.g., 87% mAP) can\nyield data that leads to discrepancies in abundance estimates compared to\nexpert-derived data. Similarly, the highest-performing models for posture\nestimation do not produce the most accurate inferences of gaze direction in\npigeons. Motivated by these findings, we call for researchers to integrate\napplication-specific metrics in ecological/biological datasets, allowing for\nmodels to be benchmarked in the context of their downstream application and to\nfacilitate better integration of models into application workflows."
                },
                "authors": [
                    {
                        "name": "Alex Hoi Hang Chan"
                    },
                    {
                        "name": "Otto Brookes"
                    },
                    {
                        "name": "Urs Waldmann"
                    },
                    {
                        "name": "Hemal Naik"
                    },
                    {
                        "name": "Iain D. Couzin"
                    },
                    {
                        "name": "Majid Mirmehdi"
                    },
                    {
                        "name": "Noël Adiko Houa"
                    },
                    {
                        "name": "Emmanuelle Normand"
                    },
                    {
                        "name": "Christophe Boesch"
                    },
                    {
                        "name": "Lukas Boesch"
                    },
                    {
                        "name": "Mimi Arandjelovic"
                    },
                    {
                        "name": "Hjalmar Kühl"
                    },
                    {
                        "name": "Tilo Burghardt"
                    },
                    {
                        "name": "Fumihiro Kano"
                    }
                ],
                "author_detail": {
                    "name": "Fumihiro Kano"
                },
                "author": "Fumihiro Kano",
                "arxiv_comment": "Accepted at CVPR Workshop, CV4Animals 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03392v1",
                "updated": "2025-05-06T10:15:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    15,
                    5,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T10:15:05Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    15,
                    5,
                    1,
                    126,
                    0
                ],
                "title": "Automatic Calibration for Membership Inference Attack on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Calibration for Membership Inference Attack on Large Language\n  Models"
                },
                "summary": "Membership Inference Attacks (MIAs) have recently been employed to determine\nwhether a specific text was part of the pre-training data of Large Language\nModels (LLMs). However, existing methods often misinfer non-members as members,\nleading to a high false positive rate, or depend on additional reference models\nfor probability calibration, which limits their practicality. To overcome these\nchallenges, we introduce a novel framework called Automatic Calibration\nMembership Inference Attack (ACMIA), which utilizes a tunable temperature to\ncalibrate output probabilities effectively. This approach is inspired by our\ntheoretical insights into maximum likelihood estimation during the pre-training\nof LLMs. We introduce ACMIA in three configurations designed to accommodate\ndifferent levels of model access and increase the probability gap between\nmembers and non-members, improving the reliability and robustness of membership\ninference. Extensive experiments on various open-source LLMs demonstrate that\nour proposed attack is highly effective, robust, and generalizable, surpassing\nstate-of-the-art baselines across three widely used benchmarks. Our code is\navailable at:\n\\href{https://github.com/Salehzz/ACMIA}{\\textcolor{blue}{Github}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks (MIAs) have recently been employed to determine\nwhether a specific text was part of the pre-training data of Large Language\nModels (LLMs). However, existing methods often misinfer non-members as members,\nleading to a high false positive rate, or depend on additional reference models\nfor probability calibration, which limits their practicality. To overcome these\nchallenges, we introduce a novel framework called Automatic Calibration\nMembership Inference Attack (ACMIA), which utilizes a tunable temperature to\ncalibrate output probabilities effectively. This approach is inspired by our\ntheoretical insights into maximum likelihood estimation during the pre-training\nof LLMs. We introduce ACMIA in three configurations designed to accommodate\ndifferent levels of model access and increase the probability gap between\nmembers and non-members, improving the reliability and robustness of membership\ninference. Extensive experiments on various open-source LLMs demonstrate that\nour proposed attack is highly effective, robust, and generalizable, surpassing\nstate-of-the-art baselines across three widely used benchmarks. Our code is\navailable at:\n\\href{https://github.com/Salehzz/ACMIA}{\\textcolor{blue}{Github}}."
                },
                "authors": [
                    {
                        "name": "Saleh Zare Zade"
                    },
                    {
                        "name": "Yao Qiang"
                    },
                    {
                        "name": "Xiangyu Zhou"
                    },
                    {
                        "name": "Hui Zhu"
                    },
                    {
                        "name": "Mohammad Amin Roshani"
                    },
                    {
                        "name": "Prashant Khanduri"
                    },
                    {
                        "name": "Dongxiao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiao Zhu"
                },
                "author": "Dongxiao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03382v1",
                "updated": "2025-05-06T10:01:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    1,
                    16,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T10:01:16Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    1,
                    16,
                    1,
                    126,
                    0
                ],
                "title": "Physics-informed neural network estimation of active material properties\n  in time-dependent cardiac biomechanical models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-informed neural network estimation of active material properties\n  in time-dependent cardiac biomechanical models"
                },
                "summary": "Active stress models in cardiac biomechanics account for the mechanical\ndeformation caused by muscle activity, thus providing a link between the\nelectrophysiological and mechanical properties of the tissue. The accurate\nassessment of active stress parameters is fundamental for a precise\nunderstanding of myocardial function but remains difficult to achieve in a\nclinical setting, especially when only displacement and strain data from\nmedical imaging modalities are available. This work investigates, through an\nin-silico study, the application of physics-informed neural networks (PINNs)\nfor inferring active contractility parameters in time-dependent cardiac\nbiomechanical models from these types of imaging data. In particular, by\nparametrising the sought state and parameter field with two neural networks,\nrespectively, and formulating an energy minimisation problem to search for the\noptimal network parameters, we are able to reconstruct in various settings\nactive stress fields in the presence of noise and with a high spatial\nresolution. To this end, we also advance the vanilla PINN learning algorithm\nwith the use of adaptive weighting schemes, ad-hoc regularisation strategies,\nFourier features, and suitable network architectures. In addition, we\nthoroughly analyse the influence of the loss weights in the reconstruction of\nactive stress parameters. Finally, we apply the method to the characterisation\nof tissue inhomogeneities and detection of fibrotic scars in myocardial tissue.\nThis approach opens a new pathway to significantly improve the diagnosis,\ntreatment planning, and management of heart conditions associated with cardiac\nfibrosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active stress models in cardiac biomechanics account for the mechanical\ndeformation caused by muscle activity, thus providing a link between the\nelectrophysiological and mechanical properties of the tissue. The accurate\nassessment of active stress parameters is fundamental for a precise\nunderstanding of myocardial function but remains difficult to achieve in a\nclinical setting, especially when only displacement and strain data from\nmedical imaging modalities are available. This work investigates, through an\nin-silico study, the application of physics-informed neural networks (PINNs)\nfor inferring active contractility parameters in time-dependent cardiac\nbiomechanical models from these types of imaging data. In particular, by\nparametrising the sought state and parameter field with two neural networks,\nrespectively, and formulating an energy minimisation problem to search for the\noptimal network parameters, we are able to reconstruct in various settings\nactive stress fields in the presence of noise and with a high spatial\nresolution. To this end, we also advance the vanilla PINN learning algorithm\nwith the use of adaptive weighting schemes, ad-hoc regularisation strategies,\nFourier features, and suitable network architectures. In addition, we\nthoroughly analyse the influence of the loss weights in the reconstruction of\nactive stress parameters. Finally, we apply the method to the characterisation\nof tissue inhomogeneities and detection of fibrotic scars in myocardial tissue.\nThis approach opens a new pathway to significantly improve the diagnosis,\ntreatment planning, and management of heart conditions associated with cardiac\nfibrosis."
                },
                "authors": [
                    {
                        "name": "Matthias Höfler"
                    },
                    {
                        "name": "Francesco Regazzoni"
                    },
                    {
                        "name": "Stefano Pagani"
                    },
                    {
                        "name": "Elias Karabelas"
                    },
                    {
                        "name": "Christoph Augustin"
                    },
                    {
                        "name": "Gundolf Haase"
                    },
                    {
                        "name": "Gernot Plank"
                    },
                    {
                        "name": "Federica Caforio"
                    }
                ],
                "author_detail": {
                    "name": "Federica Caforio"
                },
                "author": "Federica Caforio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.8; I.2.0; I.6.4; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03373v1",
                "updated": "2025-05-06T09:47:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    47,
                    53,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:47:53Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    47,
                    53,
                    1,
                    126,
                    0
                ],
                "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty\n  Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAP: Structured Pruning via Alternating Optimization and Penalty\n  Methods"
                },
                "summary": "The deployment of large language models (LLMs) is often constrained by their\nsubstantial computational and memory demands. While structured pruning presents\na viable approach by eliminating entire network components, existing methods\nsuffer from performance degradation, reliance on heuristic metrics, or\nexpensive finetuning. To address these challenges, we propose SPAP (Structured\nPruning via Alternating Optimization and Penalty Methods), a novel and\nefficient structured pruning framework for LLMs grounded in optimization\ntheory. SPAP formulates the pruning problem through a mixed-integer\noptimization model, employs a penalty method that effectively makes pruning\ndecisions to minimize pruning errors, and introduces an alternating\nminimization algorithm tailored to the splittable problem structure for\nefficient weight updates and performance recovery. Extensive experiments on\nOPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over\nstate-of-the-art methods, delivering linear inference speedups (1.29$\\times$ at\n30% sparsity) and proportional memory reductions. Our work offers a practical,\noptimization-driven solution for pruning LLMs while preserving model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often constrained by their\nsubstantial computational and memory demands. While structured pruning presents\na viable approach by eliminating entire network components, existing methods\nsuffer from performance degradation, reliance on heuristic metrics, or\nexpensive finetuning. To address these challenges, we propose SPAP (Structured\nPruning via Alternating Optimization and Penalty Methods), a novel and\nefficient structured pruning framework for LLMs grounded in optimization\ntheory. SPAP formulates the pruning problem through a mixed-integer\noptimization model, employs a penalty method that effectively makes pruning\ndecisions to minimize pruning errors, and introduces an alternating\nminimization algorithm tailored to the splittable problem structure for\nefficient weight updates and performance recovery. Extensive experiments on\nOPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over\nstate-of-the-art methods, delivering linear inference speedups (1.29$\\times$ at\n30% sparsity) and proportional memory reductions. Our work offers a practical,\noptimization-driven solution for pruning LLMs while preserving model\nperformance."
                },
                "authors": [
                    {
                        "name": "Hanyu Hu"
                    },
                    {
                        "name": "Xiaoming Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Yuan"
                },
                "author": "Xiaoming Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03369v1",
                "updated": "2025-05-06T09:40:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    40,
                    47,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:40:47Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    40,
                    47,
                    1,
                    126,
                    0
                ],
                "title": "Validating the Effectiveness of a Large Language Model-based Approach\n  for Identifying Children's Development across Various Free Play Settings in\n  Kindergarten",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validating the Effectiveness of a Large Language Model-based Approach\n  for Identifying Children's Development across Various Free Play Settings in\n  Kindergarten"
                },
                "summary": "Free play is a fundamental aspect of early childhood education, supporting\nchildren's cognitive, social, emotional, and motor development. However,\nassessing children's development during free play poses significant challenges\ndue to the unstructured and spontaneous nature of the activity. Traditional\nassessment methods often rely on direct observations by teachers, parents, or\nresearchers, which may fail to capture comprehensive insights from free play\nand provide timely feedback to educators. This study proposes an innovative\napproach combining Large Language Models (LLMs) with learning analytics to\nanalyze children's self-narratives of their play experiences. The LLM\nidentifies developmental abilities, while performance scores across different\nplay settings are calculated using learning analytics techniques. We collected\n2,224 play narratives from 29 children in a kindergarten, covering four\ndistinct play areas over one semester. According to the evaluation results from\neight professionals, the LLM-based approach achieved high accuracy in\nidentifying cognitive, motor, and social abilities, with accuracy exceeding 90%\nin most domains. Moreover, significant differences in developmental outcomes\nwere observed across play settings, highlighting each area's unique\ncontributions to specific abilities. These findings confirm that the proposed\napproach is effective in identifying children's development across various free\nplay settings. This study demonstrates the potential of integrating LLMs and\nlearning analytics to provide child-centered insights into developmental\ntrajectories, offering educators valuable data to support personalized learning\nand enhance early childhood education practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free play is a fundamental aspect of early childhood education, supporting\nchildren's cognitive, social, emotional, and motor development. However,\nassessing children's development during free play poses significant challenges\ndue to the unstructured and spontaneous nature of the activity. Traditional\nassessment methods often rely on direct observations by teachers, parents, or\nresearchers, which may fail to capture comprehensive insights from free play\nand provide timely feedback to educators. This study proposes an innovative\napproach combining Large Language Models (LLMs) with learning analytics to\nanalyze children's self-narratives of their play experiences. The LLM\nidentifies developmental abilities, while performance scores across different\nplay settings are calculated using learning analytics techniques. We collected\n2,224 play narratives from 29 children in a kindergarten, covering four\ndistinct play areas over one semester. According to the evaluation results from\neight professionals, the LLM-based approach achieved high accuracy in\nidentifying cognitive, motor, and social abilities, with accuracy exceeding 90%\nin most domains. Moreover, significant differences in developmental outcomes\nwere observed across play settings, highlighting each area's unique\ncontributions to specific abilities. These findings confirm that the proposed\napproach is effective in identifying children's development across various free\nplay settings. This study demonstrates the potential of integrating LLMs and\nlearning analytics to provide child-centered insights into developmental\ntrajectories, offering educators valuable data to support personalized learning\nand enhance early childhood education practices."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Yang"
                    },
                    {
                        "name": "Yuan Shen"
                    },
                    {
                        "name": "Tianchen Sun"
                    },
                    {
                        "name": "Yangbin Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yangbin Xie"
                },
                "author": "Yangbin Xie",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03368v1",
                "updated": "2025-05-06T09:40:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    40,
                    6,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:40:06Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    40,
                    6,
                    1,
                    126,
                    0
                ],
                "title": "Geospatial Mechanistic Interpretability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geospatial Mechanistic Interpretability of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography."
                },
                "authors": [
                    {
                        "name": "Stef De Sabbata"
                    },
                    {
                        "name": "Stefano Mizzaro"
                    },
                    {
                        "name": "Kevin Roitero"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Roitero"
                },
                "author": "Kevin Roitero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.13521v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.13521v4",
                "updated": "2025-05-06T09:38:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    38,
                    9,
                    1,
                    126,
                    0
                ],
                "published": "2022-12-27T15:31:40Z",
                "published_parsed": [
                    2022,
                    12,
                    27,
                    15,
                    31,
                    40,
                    1,
                    361,
                    0
                ],
                "title": "On the asymptotics of extremal lp-blocks cluster inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the asymptotics of extremal lp-blocks cluster inference"
                },
                "summary": "Extremes occur in stationary regularly varying time series as short periods\nwith several large observations, known as extremal blocks. We study cluster\nstatistics summarizing the behavior of functions acting on these extremal\nblocks. Examples of cluster statistics are the extremal index, cluster size\nprobabilities, and other cluster indices. The purpose of our work is twofold.\nFirst, we state the asymptotic normality of block estimators for cluster\ninference based on consecutive observations with large lp-norms, for p < 0. The\ncase p=$\\alpha$, where $\\alpha$ > 0 is the tail index of the time series, has\nspecific nice properties thus we analyze the asymptotic of blocks estimators\nwhen approximating $\\alpha$ using the Hill estimator. Second, we verify the\nconditions we require on classical models such as linear models and solutions\nof stochastic recurrence equations. Regarding linear models, we prove that the\nasymptotic variance of classical index cluster-based estimators is null as\nfirst conjectured in Hsing T. [26]. We illustrate our findings on simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extremes occur in stationary regularly varying time series as short periods\nwith several large observations, known as extremal blocks. We study cluster\nstatistics summarizing the behavior of functions acting on these extremal\nblocks. Examples of cluster statistics are the extremal index, cluster size\nprobabilities, and other cluster indices. The purpose of our work is twofold.\nFirst, we state the asymptotic normality of block estimators for cluster\ninference based on consecutive observations with large lp-norms, for p < 0. The\ncase p=$\\alpha$, where $\\alpha$ > 0 is the tail index of the time series, has\nspecific nice properties thus we analyze the asymptotic of blocks estimators\nwhen approximating $\\alpha$ using the Hill estimator. Second, we verify the\nconditions we require on classical models such as linear models and solutions\nof stochastic recurrence equations. Regarding linear models, we prove that the\nasymptotic variance of classical index cluster-based estimators is null as\nfirst conjectured in Hsing T. [26]. We illustrate our findings on simulations."
                },
                "authors": [
                    {
                        "name": "Gloria Buriticá"
                    },
                    {
                        "name": "Olivier Wintenberger"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Wintenberger"
                },
                "arxiv_affiliation": "LPSM",
                "author": "Olivier Wintenberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.13521v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.13521v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02380v2",
                "updated": "2025-05-06T09:37:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    37,
                    57,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T05:42:14Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    5,
                    42,
                    14,
                    0,
                    125,
                    0
                ],
                "title": "EntroLLM: Entropy Encoded Weight Compression for Efficient Large\n  Language Model Inference on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EntroLLM: Entropy Encoded Weight Compression for Efficient Large\n  Language Model Inference on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional performance across\nvarious tasks, but their large storage and computational requirements constrain\ntheir deployment on edge devices. To address this, we propose EntroLLM, a novel\ncompression framework that integrates mixed quantization with entropy coding to\nreduce storage overhead while maintaining model accuracy. Our method applies a\nlayer-wise mixed quantization scheme - choosing between symmetric and\nasymmetric quantization based on individual layer weight distributions - to\noptimize compressibility. We then employ Huffman encoding for lossless\ncompression of the quantized weights, significantly reducing memory bandwidth\nrequirements. Furthermore, we introduce parallel Huffman decoding, which\nenables efficient retrieval of encoded weights during inference, ensuring\nminimal latency impact. Our experiments on edge-compatible LLMs, including\nsmolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct,\ndemonstrate that EntroLLM achieves up to $30\\%$ storage reduction compared to\nuint8 models and up to $65%$ storage reduction compared to uint4 models, while\npreserving perplexity and accuracy, on language benchmark tasks. We further\nshow that our method enables $31.9\\%$ - $146.6\\%$ faster inference throughput\non memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by\nreducing the required data movement. The proposed approach requires no\nadditional re-training and is fully compatible with existing post-training\nquantization methods, making it a practical solution for edge LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional performance across\nvarious tasks, but their large storage and computational requirements constrain\ntheir deployment on edge devices. To address this, we propose EntroLLM, a novel\ncompression framework that integrates mixed quantization with entropy coding to\nreduce storage overhead while maintaining model accuracy. Our method applies a\nlayer-wise mixed quantization scheme - choosing between symmetric and\nasymmetric quantization based on individual layer weight distributions - to\noptimize compressibility. We then employ Huffman encoding for lossless\ncompression of the quantized weights, significantly reducing memory bandwidth\nrequirements. Furthermore, we introduce parallel Huffman decoding, which\nenables efficient retrieval of encoded weights during inference, ensuring\nminimal latency impact. Our experiments on edge-compatible LLMs, including\nsmolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct,\ndemonstrate that EntroLLM achieves up to $30\\%$ storage reduction compared to\nuint8 models and up to $65%$ storage reduction compared to uint4 models, while\npreserving perplexity and accuracy, on language benchmark tasks. We further\nshow that our method enables $31.9\\%$ - $146.6\\%$ faster inference throughput\non memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by\nreducing the required data movement. The proposed approach requires no\nadditional re-training and is fully compatible with existing post-training\nquantization methods, making it a practical solution for edge LLMs."
                },
                "authors": [
                    {
                        "name": "Arnab Sanyal"
                    },
                    {
                        "name": "Prithwish Mukherjee"
                    },
                    {
                        "name": "Gourav Datta"
                    },
                    {
                        "name": "Sandeep P. Chinchali"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep P. Chinchali"
                },
                "author": "Sandeep P. Chinchali",
                "arxiv_comment": "6 pages, 1 reference page. Under submission and review at ISLPED 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03361v1",
                "updated": "2025-05-06T09:30:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    30,
                    30,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:30:30Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    30,
                    30,
                    1,
                    126,
                    0
                ],
                "title": "Interpretable Zero-shot Learning with Infinite Class Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Zero-shot Learning with Infinite Class Concepts"
                },
                "summary": "Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images\nwith intermediate class semantics, like human-annotated concepts or class\ndefinitions. An emerging alternative leverages Large-scale Language Models\n(LLMs) to automatically generate class documents. However, these methods often\nface challenges with transparency in the classification process and may suffer\nfrom the notorious hallucination problem in LLMs, resulting in non-visual class\nsemantics. This paper redefines class semantics in ZSL with a focus on\ntransferability and discriminability, introducing a novel framework called\nZero-shot Learning with Infinite Class Concepts (InfZSL). Our approach\nleverages the powerful capabilities of LLMs to dynamically generate an\nunlimited array of phrase-level class concepts. To address the hallucination\nchallenge, we introduce an entropy-based scoring process that incorporates a\n``goodness\" concept selection mechanism, ensuring that only the most\ntransferable and discriminative concepts are selected. Our InfZSL framework not\nonly demonstrates significant improvements on three popular benchmark datasets\nbut also generates highly interpretable, image-grounded concepts. Code will be\nreleased upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images\nwith intermediate class semantics, like human-annotated concepts or class\ndefinitions. An emerging alternative leverages Large-scale Language Models\n(LLMs) to automatically generate class documents. However, these methods often\nface challenges with transparency in the classification process and may suffer\nfrom the notorious hallucination problem in LLMs, resulting in non-visual class\nsemantics. This paper redefines class semantics in ZSL with a focus on\ntransferability and discriminability, introducing a novel framework called\nZero-shot Learning with Infinite Class Concepts (InfZSL). Our approach\nleverages the powerful capabilities of LLMs to dynamically generate an\nunlimited array of phrase-level class concepts. To address the hallucination\nchallenge, we introduce an entropy-based scoring process that incorporates a\n``goodness\" concept selection mechanism, ensuring that only the most\ntransferable and discriminative concepts are selected. Our InfZSL framework not\nonly demonstrates significant improvements on three popular benchmark datasets\nbut also generates highly interpretable, image-grounded concepts. Code will be\nreleased upon acceptance."
                },
                "authors": [
                    {
                        "name": "Zihan Ye"
                    },
                    {
                        "name": "Shreyank N Gowda"
                    },
                    {
                        "name": "Shiming Chen"
                    },
                    {
                        "name": "Yaochu Jin"
                    },
                    {
                        "name": "Kaizhu Huang"
                    },
                    {
                        "name": "Xiaobo Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobo Jin"
                },
                "author": "Xiaobo Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03351v1",
                "updated": "2025-05-06T09:19:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    19,
                    16,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:19:16Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    19,
                    16,
                    1,
                    126,
                    0
                ],
                "title": "GUAVA: Generalizable Upper Body 3D Gaussian Avatar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUAVA: Generalizable Upper Body 3D Gaussian Avatar"
                },
                "summary": "Reconstructing a high-quality, animatable 3D human avatar with expressive\nfacial and hand motions from a single image has gained significant attention\ndue to its broad application potential. 3D human avatar reconstruction\ntypically requires multi-view or monocular videos and training on individual\nIDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's\nexpressiveness, these methods often focus on body motion but struggle with\nfacial expressions. To address these challenges, we first introduce an\nexpressive human model (EHM) to enhance facial expression capabilities and\ndevelop an accurate tracking method. Based on this template model, we propose\nGUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar\nreconstruction. We leverage inverse texture mapping and projection sampling\ntechniques to infer Ubody (upper-body) Gaussians from a single image. The\nrendered images are refined through a neural refiner. Experimental results\ndemonstrate that GUAVA significantly outperforms previous methods in rendering\nquality and offers significant speed improvements, with reconstruction times in\nthe sub-second range (0.1s), and supports real-time animation and rendering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing a high-quality, animatable 3D human avatar with expressive\nfacial and hand motions from a single image has gained significant attention\ndue to its broad application potential. 3D human avatar reconstruction\ntypically requires multi-view or monocular videos and training on individual\nIDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's\nexpressiveness, these methods often focus on body motion but struggle with\nfacial expressions. To address these challenges, we first introduce an\nexpressive human model (EHM) to enhance facial expression capabilities and\ndevelop an accurate tracking method. Based on this template model, we propose\nGUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar\nreconstruction. We leverage inverse texture mapping and projection sampling\ntechniques to infer Ubody (upper-body) Gaussians from a single image. The\nrendered images are refined through a neural refiner. Experimental results\ndemonstrate that GUAVA significantly outperforms previous methods in rendering\nquality and offers significant speed improvements, with reconstruction times in\nthe sub-second range (0.1s), and supports real-time animation and rendering."
                },
                "authors": [
                    {
                        "name": "Dongbin Zhang"
                    },
                    {
                        "name": "Yunfei Liu"
                    },
                    {
                        "name": "Lijian Lin"
                    },
                    {
                        "name": "Ye Zhu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Minghan Qin"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Haoqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoqian Wang"
                },
                "author": "Haoqian Wang",
                "arxiv_comment": "Project page: https://eastbeanzhang.github.io/GUAVA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03350v1",
                "updated": "2025-05-06T09:19:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    19,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:19:12Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    19,
                    12,
                    1,
                    126,
                    0
                ],
                "title": "A Vision-Language Model for Focal Liver Lesion Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Vision-Language Model for Focal Liver Lesion Classification"
                },
                "summary": "Accurate classification of focal liver lesions is crucial for diagnosis and\ntreatment in hepatology. However, traditional supervised deep learning models\ndepend on large-scale annotated datasets, which are often limited in medical\nimaging. Recently, Vision-Language models (VLMs) such as Contrastive\nLanguage-Image Pre-training model (CLIP) has been applied to image\nclassifications. Compared to the conventional convolutional neural network\n(CNN), which classifiers image based on visual information only, VLM leverages\nmultimodal learning with text and images, allowing it to learn effectively even\nwith a limited amount of labeled data. Inspired by CLIP, we pro-pose a\nLiver-VLM, a model specifically designed for focal liver lesions (FLLs)\nclassification. First, Liver-VLM incorporates class information into the text\nencoder without introducing additional inference overhead. Second, by\ncalculating the pairwise cosine similarities between image and text embeddings\nand optimizing the model with a cross-entropy loss, Liver-VLM ef-fectively\naligns image features with class-level text features. Experimental results on\nMPCT-FLLs dataset demonstrate that the Liver-VLM model out-performs both the\nstandard CLIP and MedCLIP models in terms of accuracy and Area Under the Curve\n(AUC). Further analysis shows that using a lightweight ResNet18 backbone\nenhances classification performance, particularly under data-constrained\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate classification of focal liver lesions is crucial for diagnosis and\ntreatment in hepatology. However, traditional supervised deep learning models\ndepend on large-scale annotated datasets, which are often limited in medical\nimaging. Recently, Vision-Language models (VLMs) such as Contrastive\nLanguage-Image Pre-training model (CLIP) has been applied to image\nclassifications. Compared to the conventional convolutional neural network\n(CNN), which classifiers image based on visual information only, VLM leverages\nmultimodal learning with text and images, allowing it to learn effectively even\nwith a limited amount of labeled data. Inspired by CLIP, we pro-pose a\nLiver-VLM, a model specifically designed for focal liver lesions (FLLs)\nclassification. First, Liver-VLM incorporates class information into the text\nencoder without introducing additional inference overhead. Second, by\ncalculating the pairwise cosine similarities between image and text embeddings\nand optimizing the model with a cross-entropy loss, Liver-VLM ef-fectively\naligns image features with class-level text features. Experimental results on\nMPCT-FLLs dataset demonstrate that the Liver-VLM model out-performs both the\nstandard CLIP and MedCLIP models in terms of accuracy and Area Under the Curve\n(AUC). Further analysis shows that using a lightweight ResNet18 backbone\nenhances classification performance, particularly under data-constrained\nconditions."
                },
                "authors": [
                    {
                        "name": "Song Jian"
                    },
                    {
                        "name": "Hu Yuchang"
                    },
                    {
                        "name": "Wang Hui"
                    },
                    {
                        "name": "Chen Yen-Wei"
                    }
                ],
                "author_detail": {
                    "name": "Chen Yen-Wei"
                },
                "author": "Chen Yen-Wei",
                "arxiv_comment": "9 pages,4 figures, 4 tables,Innovation in Medicine and Healthcare\n  Proceedings of 13th KES-InMed 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03345v1",
                "updated": "2025-05-06T09:13:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    13,
                    32,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:13:32Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    13,
                    32,
                    1,
                    126,
                    0
                ],
                "title": "Elevating Cyber Threat Intelligence against Disinformation Campaigns\n  with LLM-based Concept Extraction and the FakeCTI Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elevating Cyber Threat Intelligence against Disinformation Campaigns\n  with LLM-based Concept Extraction and the FakeCTI Dataset"
                },
                "summary": "The swift spread of fake news and disinformation campaigns poses a\nsignificant threat to public trust, political stability, and cybersecurity.\nTraditional Cyber Threat Intelligence (CTI) approaches, which rely on low-level\nindicators such as domain names and social media handles, are easily evaded by\nadversaries who frequently modify their online infrastructure. To address these\nlimitations, we introduce a novel CTI framework that focuses on high-level,\nsemantic indicators derived from recurrent narratives and relationships of\ndisinformation campaigns. Our approach extracts structured CTI indicators from\nunstructured disinformation content, capturing key entities and their\ncontextual dependencies within fake news using Large Language Models (LLMs). We\nfurther introduce FakeCTI, the first dataset that systematically links fake\nnews to disinformation campaigns and threat actors. To evaluate the\neffectiveness of our CTI framework, we analyze multiple fake news attribution\ntechniques, spanning from traditional Natural Language Processing (NLP) to\nfine-tuned LLMs. This work shifts the focus from low-level artifacts to\npersistent conceptual structures, establishing a scalable and adaptive approach\nto tracking and countering disinformation campaigns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The swift spread of fake news and disinformation campaigns poses a\nsignificant threat to public trust, political stability, and cybersecurity.\nTraditional Cyber Threat Intelligence (CTI) approaches, which rely on low-level\nindicators such as domain names and social media handles, are easily evaded by\nadversaries who frequently modify their online infrastructure. To address these\nlimitations, we introduce a novel CTI framework that focuses on high-level,\nsemantic indicators derived from recurrent narratives and relationships of\ndisinformation campaigns. Our approach extracts structured CTI indicators from\nunstructured disinformation content, capturing key entities and their\ncontextual dependencies within fake news using Large Language Models (LLMs). We\nfurther introduce FakeCTI, the first dataset that systematically links fake\nnews to disinformation campaigns and threat actors. To evaluate the\neffectiveness of our CTI framework, we analyze multiple fake news attribution\ntechniques, spanning from traditional Natural Language Processing (NLP) to\nfine-tuned LLMs. This work shifts the focus from low-level artifacts to\npersistent conceptual structures, establishing a scalable and adaptive approach\nto tracking and countering disinformation campaigns."
                },
                "authors": [
                    {
                        "name": "Domenico Cotroneo"
                    },
                    {
                        "name": "Roberto Natella"
                    },
                    {
                        "name": "Vittorio Orbinato"
                    }
                ],
                "author_detail": {
                    "name": "Vittorio Orbinato"
                },
                "author": "Vittorio Orbinato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13748v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13748v3",
                "updated": "2025-05-06T09:09:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    9,
                    19,
                    1,
                    126,
                    0
                ],
                "published": "2024-03-20T16:56:08Z",
                "published_parsed": [
                    2024,
                    3,
                    20,
                    16,
                    56,
                    8,
                    2,
                    80,
                    0
                ],
                "title": "Variational Inference for Uncertainty Quantification: an Analysis of\n  Trade-offs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Inference for Uncertainty Quantification: an Analysis of\n  Trade-offs"
                },
                "summary": "Given an intractable distribution $p$, the problem of variational inference\n(VI) is to find the best approximation from some more tractable family $Q$.\nCommonly, one chooses $Q$ to be a family of factorized distributions (i.e., the\nmean-field assumption), even though~$p$ itself does not factorize. We show that\nthis mismatch leads to an impossibility theorem: if $p$ does not factorize,\nthen any factorized approximation $q\\in Q$ can correctly estimate at most one\nof the following three measures of uncertainty: (i) the marginal variances,\n(ii) the marginal precisions, or (iii) the generalized variance (which can be\nrelated to the entropy). In practice, the best variational approximation in $Q$\nis found by minimizing some divergence $D(q,p)$ between distributions, and so\nwe ask: how does the choice of divergence determine which measure of\nuncertainty, if any, is correctly estimated by VI? We consider the classic\nKullback-Leibler divergences, the more general $\\alpha$-divergences, and a\nscore-based divergence which compares $\\nabla \\log p$ and $\\nabla \\log q$. We\nprovide a thorough theoretical analysis in the setting where $p$ is a Gaussian\nand $q$ is a (factorized) Gaussian. We show that all the considered divergences\ncan be \\textit{ordered} based on the estimates of uncertainty they yield as\nobjective functions for~VI. Finally, we empirically evaluate the validity of\nthis ordering when the target distribution $p$ is not Gaussian.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given an intractable distribution $p$, the problem of variational inference\n(VI) is to find the best approximation from some more tractable family $Q$.\nCommonly, one chooses $Q$ to be a family of factorized distributions (i.e., the\nmean-field assumption), even though~$p$ itself does not factorize. We show that\nthis mismatch leads to an impossibility theorem: if $p$ does not factorize,\nthen any factorized approximation $q\\in Q$ can correctly estimate at most one\nof the following three measures of uncertainty: (i) the marginal variances,\n(ii) the marginal precisions, or (iii) the generalized variance (which can be\nrelated to the entropy). In practice, the best variational approximation in $Q$\nis found by minimizing some divergence $D(q,p)$ between distributions, and so\nwe ask: how does the choice of divergence determine which measure of\nuncertainty, if any, is correctly estimated by VI? We consider the classic\nKullback-Leibler divergences, the more general $\\alpha$-divergences, and a\nscore-based divergence which compares $\\nabla \\log p$ and $\\nabla \\log q$. We\nprovide a thorough theoretical analysis in the setting where $p$ is a Gaussian\nand $q$ is a (factorized) Gaussian. We show that all the considered divergences\ncan be \\textit{ordered} based on the estimates of uncertainty they yield as\nobjective functions for~VI. Finally, we empirically evaluate the validity of\nthis ordering when the target distribution $p$ is not Gaussian."
                },
                "authors": [
                    {
                        "name": "Charles C. Margossian"
                    },
                    {
                        "name": "Loucas Pillaud-Vivien"
                    },
                    {
                        "name": "Lawrence K. Saul"
                    }
                ],
                "author_detail": {
                    "name": "Lawrence K. Saul"
                },
                "author": "Lawrence K. Saul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13748v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13748v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03336v1",
                "updated": "2025-05-06T09:08:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    8,
                    36,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:08:36Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    8,
                    36,
                    1,
                    126,
                    0
                ],
                "title": "Avoid Recommending Out-of-Domain Items: Constrained Generative\n  Recommendation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avoid Recommending Out-of-Domain Items: Constrained Generative\n  Recommendation with LLMs"
                },
                "summary": "Large Language Models (LLMs) have shown promise for generative recommender\nsystems due to their transformative capabilities in user interaction. However,\nensuring they do not recommend out-of-domain (OOD) items remains a challenge.\nWe study two distinct methods to address this issue: RecLM-ret, a\nretrieval-based method, and RecLM-cgen, a constrained generation method. Both\nmethods integrate seamlessly with existing LLMs to ensure in-domain\nrecommendations. Comprehensive experiments on three recommendation datasets\ndemonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing\nLLM-based recommender models in accuracy while eliminating OOD recommendations,\nmaking it the preferred method for adoption. Additionally, RecLM-cgen maintains\nstrong generalist capabilities and is a lightweight plug-and-play module for\neasy integration into LLMs, offering valuable practical benefits for the\ncommunity. Source code is available at https://github.com/microsoft/RecAI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promise for generative recommender\nsystems due to their transformative capabilities in user interaction. However,\nensuring they do not recommend out-of-domain (OOD) items remains a challenge.\nWe study two distinct methods to address this issue: RecLM-ret, a\nretrieval-based method, and RecLM-cgen, a constrained generation method. Both\nmethods integrate seamlessly with existing LLMs to ensure in-domain\nrecommendations. Comprehensive experiments on three recommendation datasets\ndemonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing\nLLM-based recommender models in accuracy while eliminating OOD recommendations,\nmaking it the preferred method for adoption. Additionally, RecLM-cgen maintains\nstrong generalist capabilities and is a lightweight plug-and-play module for\neasy integration into LLMs, offering valuable practical benefits for the\ncommunity. Source code is available at https://github.com/microsoft/RecAI"
                },
                "authors": [
                    {
                        "name": "Hao Liao"
                    },
                    {
                        "name": "Wensheng Lu"
                    },
                    {
                        "name": "Jianxun Lian"
                    },
                    {
                        "name": "Mingqi Wu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Yitian Huang"
                    },
                    {
                        "name": "Mingyang Zhou"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03332v1",
                "updated": "2025-05-06T09:06:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    6,
                    18,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:06:18Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    6,
                    18,
                    1,
                    126,
                    0
                ],
                "title": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting,\n  Meta-Prompting, and Meta-Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting,\n  Meta-Prompting, and Meta-Reasoning"
                },
                "summary": "Critical peer review of scientific manuscripts presents a significant\nchallenge for Large Language Models (LLMs), partly due to data limitations and\nthe complexity of expert reasoning. This report introduces Persistent Workflow\nPrompting (PWP), a potentially broadly applicable prompt engineering\nmethodology designed to bridge this gap using standard LLM chat interfaces\n(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical\nanalysis of experimental chemistry manuscripts, featuring a hierarchical,\nmodular architecture (structured via Markdown) that defines detailed analysis\nworkflows. We develop this PWP prompt through iterative application of\nmeta-prompting techniques and meta-reasoning aimed at systematically codifying\nexpert review workflows, including tacit knowledge. Submitted once at the start\nof a session, this PWP prompt equips the LLM with persistent workflows\ntriggered by subsequent queries, guiding modern reasoning LLMs through\nsystematic, multimodal evaluations. Demonstrations show the PWP-guided LLM\nidentifying major methodological flaws in a test case while mitigating LLM\ninput bias and performing complex tasks, including distinguishing claims from\nevidence, integrating text/photo/figure analysis to infer parameters, executing\nquantitative feasibility checks, comparing estimates against claims, and\nassessing a priori plausibility. To ensure transparency and facilitate\nreplication, we provide full prompts, detailed demonstration analyses, and logs\nof interactive chats as supplementary resources. Beyond the specific\napplication, this work offers insights into the meta-development process\nitself, highlighting the potential of PWP, informed by detailed workflow\nformalization, to enable sophisticated analysis using readily available LLMs\nfor complex scientific tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical peer review of scientific manuscripts presents a significant\nchallenge for Large Language Models (LLMs), partly due to data limitations and\nthe complexity of expert reasoning. This report introduces Persistent Workflow\nPrompting (PWP), a potentially broadly applicable prompt engineering\nmethodology designed to bridge this gap using standard LLM chat interfaces\n(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical\nanalysis of experimental chemistry manuscripts, featuring a hierarchical,\nmodular architecture (structured via Markdown) that defines detailed analysis\nworkflows. We develop this PWP prompt through iterative application of\nmeta-prompting techniques and meta-reasoning aimed at systematically codifying\nexpert review workflows, including tacit knowledge. Submitted once at the start\nof a session, this PWP prompt equips the LLM with persistent workflows\ntriggered by subsequent queries, guiding modern reasoning LLMs through\nsystematic, multimodal evaluations. Demonstrations show the PWP-guided LLM\nidentifying major methodological flaws in a test case while mitigating LLM\ninput bias and performing complex tasks, including distinguishing claims from\nevidence, integrating text/photo/figure analysis to infer parameters, executing\nquantitative feasibility checks, comparing estimates against claims, and\nassessing a priori plausibility. To ensure transparency and facilitate\nreplication, we provide full prompts, detailed demonstration analyses, and logs\nof interactive chats as supplementary resources. Beyond the specific\napplication, this work offers insights into the meta-development process\nitself, highlighting the potential of PWP, informed by detailed workflow\nformalization, to enable sophisticated analysis using readily available LLMs\nfor complex scientific tasks."
                },
                "authors": [
                    {
                        "name": "Evgeny Markhasin"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Markhasin"
                },
                "author": "Evgeny Markhasin",
                "arxiv_comment": "22 pages, 36 pages (references and appendixes)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06103v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06103v3",
                "updated": "2025-05-06T09:06:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    6,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2024-08-12T12:43:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    43,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Method-of-Moments Inference for GLMs and Doubly Robust Functionals under\n  Proportional Asymptotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Method-of-Moments Inference for GLMs and Doubly Robust Functionals under\n  Proportional Asymptotics"
                },
                "summary": "In this paper, we consider the estimation of regression coefficients and\nsignal-to-noise (SNR) ratio in high-dimensional Generalized Linear Models\n(GLMs), and explore their implications in inferring popular estimands such as\naverage treatment effects in high-dimensional observational studies. Under the\n``proportional asymptotic'' regime and Gaussian covariates with known\n(population) covariance $\\Sigma$, we derive Consistent and Asymptotically\nNormal (CAN) estimators of our targets of inference through a Method-of-Moments\ntype of estimators that bypasses estimation of high dimensional nuisance\nfunctions and hyperparameter tuning altogether. Additionally, under\nnon-Gaussian covariates, we demonstrate universality of our results under\ncertain additional assumptions on the regression coefficients and $\\Sigma$. We\nalso demonstrate that knowing $\\Sigma$ is not essential to our proposed\nmethodology when the sample covariance matrix estimator is invertible. Finally,\nwe complement our theoretical results with numerical experiments and\ncomparisons with existing literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider the estimation of regression coefficients and\nsignal-to-noise (SNR) ratio in high-dimensional Generalized Linear Models\n(GLMs), and explore their implications in inferring popular estimands such as\naverage treatment effects in high-dimensional observational studies. Under the\n``proportional asymptotic'' regime and Gaussian covariates with known\n(population) covariance $\\Sigma$, we derive Consistent and Asymptotically\nNormal (CAN) estimators of our targets of inference through a Method-of-Moments\ntype of estimators that bypasses estimation of high dimensional nuisance\nfunctions and hyperparameter tuning altogether. Additionally, under\nnon-Gaussian covariates, we demonstrate universality of our results under\ncertain additional assumptions on the regression coefficients and $\\Sigma$. We\nalso demonstrate that knowing $\\Sigma$ is not essential to our proposed\nmethodology when the sample covariance matrix estimator is invertible. Finally,\nwe complement our theoretical results with numerical experiments and\ncomparisons with existing literature."
                },
                "authors": [
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Rajarshi Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Mukherjee"
                },
                "author": "Rajarshi Mukherjee",
                "arxiv_comment": "21 figures, 8 tables; updated funding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06103v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06103v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02768v2",
                "updated": "2025-05-06T09:02:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    2,
                    57,
                    1,
                    126,
                    0
                ],
                "published": "2024-09-17T05:17:37Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    5,
                    17,
                    37,
                    1,
                    261,
                    0
                ],
                "title": "Uncertainty-Guided Self-Questioning and Answering for Video-Language\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Guided Self-Questioning and Answering for Video-Language\n  Alignment"
                },
                "summary": "The development of multi-modal models has been rapidly advancing, with some\ndemonstrating remarkable capabilities. However, annotating video-text pairs\nremains expensive and insufficient. Take video question answering (VideoQA)\ntasks as an example, human annotated questions and answers often cover only\npart of the video, since the corresponding text is often short and monotonous,\nleading to underutilization of video. To address this, we propose a\nBootstrapping Video-Language Alignment framework (BoViLA), a self-training\nmethod that augments question samples during training process through LLM-based\nself-questioning and answering, which help model exploit video information and\nthe internal knowledge of LLMs more thoroughly to improve modality alignment.\nHowever, low-quality self-generated questions may instead contaminate the\nperformance, especially in the early stages of training, as we have observed in\nour experiments. To filter bad self-generated questions, we introduce\nEvidential Deep Learning (EDL) to estimate uncertainty and assess the quality\nof self-generated questions by evaluating the modality alignment within the\ncontext. To the best of our knowledge, this work is the first to explore\nLLM-based self-training frameworks for modality alignment. We evaluate BoViLA\non five strong VideoQA benchmarks, where it outperforms several\nstate-of-the-art methods and demonstrate its effectiveness and generality.\nAdditionally, we provide extensive analyses of the self-training framework and\nthe EDL-based uncertainty filtering mechanism. The code will be made available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of multi-modal models has been rapidly advancing, with some\ndemonstrating remarkable capabilities. However, annotating video-text pairs\nremains expensive and insufficient. Take video question answering (VideoQA)\ntasks as an example, human annotated questions and answers often cover only\npart of the video, since the corresponding text is often short and monotonous,\nleading to underutilization of video. To address this, we propose a\nBootstrapping Video-Language Alignment framework (BoViLA), a self-training\nmethod that augments question samples during training process through LLM-based\nself-questioning and answering, which help model exploit video information and\nthe internal knowledge of LLMs more thoroughly to improve modality alignment.\nHowever, low-quality self-generated questions may instead contaminate the\nperformance, especially in the early stages of training, as we have observed in\nour experiments. To filter bad self-generated questions, we introduce\nEvidential Deep Learning (EDL) to estimate uncertainty and assess the quality\nof self-generated questions by evaluating the modality alignment within the\ncontext. To the best of our knowledge, this work is the first to explore\nLLM-based self-training frameworks for modality alignment. We evaluate BoViLA\non five strong VideoQA benchmarks, where it outperforms several\nstate-of-the-art methods and demonstrate its effectiveness and generality.\nAdditionally, we provide extensive analyses of the self-training framework and\nthe EDL-based uncertainty filtering mechanism. The code will be made available."
                },
                "authors": [
                    {
                        "name": "Jin Chen"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Haojian Huang"
                    },
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Mehdi Hosseinzadeh"
                    },
                    {
                        "name": "Zhe Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Liu"
                },
                "author": "Zhe Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03315v1",
                "updated": "2025-05-06T08:45:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    45,
                    44,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T08:45:44Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    45,
                    44,
                    1,
                    126,
                    0
                ],
                "title": "Artificial Behavior Intelligence: Technology, Challenges, and Future\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Behavior Intelligence: Technology, Challenges, and Future\n  Directions"
                },
                "summary": "Understanding and predicting human behavior has emerged as a core capability\nin various AI application domains such as autonomous driving, smart healthcare,\nsurveillance systems, and social robotics. This paper defines the technical\nframework of Artificial Behavior Intelligence (ABI), which comprehensively\nanalyzes and interprets human posture, facial expressions, emotions, behavioral\nsequences, and contextual cues. It details the essential components of ABI,\nincluding pose estimation, face and emotion recognition, sequential behavior\nanalysis, and context-aware modeling. Furthermore, we highlight the\ntransformative potential of recent advances in large-scale pretrained models,\nsuch as large language models (LLMs), vision foundation models, and multimodal\nintegration models, in significantly improving the accuracy and\ninterpretability of behavior recognition. Our research team has a strong\ninterest in the ABI domain and is actively conducting research, particularly\nfocusing on the development of intelligent lightweight models capable of\nefficiently inferring complex human behaviors. This paper identifies several\ntechnical challenges that must be addressed to deploy ABI in real-world\napplications including learning behavioral intelligence from limited data,\nquantifying uncertainty in complex behavior prediction, and optimizing model\nstructures for low-power, real-time inference. To tackle these challenges, our\nteam is exploring various optimization strategies including lightweight\ntransformers, graph-based recognition architectures, energy-aware loss\nfunctions, and multimodal knowledge distillation, while validating their\napplicability in real-time environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and predicting human behavior has emerged as a core capability\nin various AI application domains such as autonomous driving, smart healthcare,\nsurveillance systems, and social robotics. This paper defines the technical\nframework of Artificial Behavior Intelligence (ABI), which comprehensively\nanalyzes and interprets human posture, facial expressions, emotions, behavioral\nsequences, and contextual cues. It details the essential components of ABI,\nincluding pose estimation, face and emotion recognition, sequential behavior\nanalysis, and context-aware modeling. Furthermore, we highlight the\ntransformative potential of recent advances in large-scale pretrained models,\nsuch as large language models (LLMs), vision foundation models, and multimodal\nintegration models, in significantly improving the accuracy and\ninterpretability of behavior recognition. Our research team has a strong\ninterest in the ABI domain and is actively conducting research, particularly\nfocusing on the development of intelligent lightweight models capable of\nefficiently inferring complex human behaviors. This paper identifies several\ntechnical challenges that must be addressed to deploy ABI in real-world\napplications including learning behavioral intelligence from limited data,\nquantifying uncertainty in complex behavior prediction, and optimizing model\nstructures for low-power, real-time inference. To tackle these challenges, our\nteam is exploring various optimization strategies including lightweight\ntransformers, graph-based recognition architectures, energy-aware loss\nfunctions, and multimodal knowledge distillation, while validating their\napplicability in real-time environments."
                },
                "authors": [
                    {
                        "name": "Kanghyun Jo"
                    },
                    {
                        "name": "Jehwan Choi"
                    },
                    {
                        "name": "Kwanho Kim"
                    },
                    {
                        "name": "Seongmin Kim"
                    },
                    {
                        "name": "Duy-Linh Nguyen"
                    },
                    {
                        "name": "Xuan-Thuy Vo"
                    },
                    {
                        "name": "Adri Priadana"
                    },
                    {
                        "name": "Tien-Dat Tran"
                    }
                ],
                "author_detail": {
                    "name": "Tien-Dat Tran"
                },
                "author": "Tien-Dat Tran",
                "arxiv_comment": "9 pages, 6 figures, Pre-print for IWIS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03308v1",
                "updated": "2025-05-06T08:41:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    41,
                    31,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T08:41:31Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    41,
                    31,
                    1,
                    126,
                    0
                ],
                "title": "An Active Inference perspective on Neurofeedback Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Active Inference perspective on Neurofeedback Training"
                },
                "summary": "Neurofeedback training (NFT) aims to teach self-regulation of brain activity\nthrough real-time feedback, but suffers from highly variable outcomes and\npoorly understood mechanisms, hampering its validation. To address these\nissues, we propose a formal computational model of the NFT closed loop. Using\nActive Inference, a Bayesian framework modelling perception, action, and\nlearning, we simulate agents interacting with an NFT environment. This enables\nus to test the impact of design choices (e.g., feedback quality, biomarker\nvalidity) and subject factors (e.g., prior beliefs) on training. Simulations\nshow that training effectiveness is sensitive to feedback noise or bias, and to\nprior beliefs (highlighting the importance of guiding instructions), but also\nreveal that perfect feedback is insufficient to guarantee high performance.\nThis approach provides a tool for assessing and predicting NFT variability,\ninterpret empirical data, and potentially develop personalized training\nprotocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neurofeedback training (NFT) aims to teach self-regulation of brain activity\nthrough real-time feedback, but suffers from highly variable outcomes and\npoorly understood mechanisms, hampering its validation. To address these\nissues, we propose a formal computational model of the NFT closed loop. Using\nActive Inference, a Bayesian framework modelling perception, action, and\nlearning, we simulate agents interacting with an NFT environment. This enables\nus to test the impact of design choices (e.g., feedback quality, biomarker\nvalidity) and subject factors (e.g., prior beliefs) on training. Simulations\nshow that training effectiveness is sensitive to feedback noise or bias, and to\nprior beliefs (highlighting the importance of guiding instructions), but also\nreveal that perfect feedback is insufficient to guarantee high performance.\nThis approach provides a tool for assessing and predicting NFT variability,\ninterpret empirical data, and potentially develop personalized training\nprotocols."
                },
                "authors": [
                    {
                        "name": "Côme Annicchiarico"
                    },
                    {
                        "name": "Fabien Lotte"
                    },
                    {
                        "name": "Jérémie Mattout"
                    }
                ],
                "author_detail": {
                    "name": "Jérémie Mattout"
                },
                "author": "Jérémie Mattout",
                "arxiv_comment": "Preprint, 43 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03303v1",
                "updated": "2025-05-06T08:36:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    36,
                    1,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T08:36:01Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    36,
                    1,
                    1,
                    126,
                    0
                ],
                "title": "Comparative Analysis of Lightweight Deep Learning Models for\n  Memory-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Lightweight Deep Learning Models for\n  Memory-Constrained Devices"
                },
                "summary": "This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms."
                },
                "authors": [
                    {
                        "name": "Tasnim Shahriar"
                    }
                ],
                "author_detail": {
                    "name": "Tasnim Shahriar"
                },
                "author": "Tasnim Shahriar",
                "arxiv_comment": "22 pages, 10 figures, 4 tables, submitted to Springer - Pattern\n  Recognition and Image Analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68-XX (Primary) 68Txx, 68T07 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02659v2",
                "updated": "2025-05-06T08:34:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    34,
                    46,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T14:05:15Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    5,
                    15,
                    0,
                    125,
                    0
                ],
                "title": "A Note on Statistically Accurate Tabular Data Generation Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Note on Statistically Accurate Tabular Data Generation Using Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have shown promise in synthetic tabular data\ngeneration, yet existing methods struggle to preserve complex feature\ndependencies, particularly among categorical variables. This work introduces a\nprobability-driven prompting approach that leverages LLMs to estimate\nconditional distributions, enabling more accurate and scalable data synthesis.\nThe results highlight the potential of prompting probability distributions to\nenhance the statistical fidelity of LLM-generated tabular data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise in synthetic tabular data\ngeneration, yet existing methods struggle to preserve complex feature\ndependencies, particularly among categorical variables. This work introduces a\nprobability-driven prompting approach that leverages LLMs to estimate\nconditional distributions, enabling more accurate and scalable data synthesis.\nThe results highlight the potential of prompting probability distributions to\nenhance the statistical fidelity of LLM-generated tabular data."
                },
                "authors": [
                    {
                        "name": "Andrey Sidorenko"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Sidorenko"
                },
                "author": "Andrey Sidorenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03300v1",
                "updated": "2025-05-06T08:31:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    31,
                    32,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T08:31:32Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    31,
                    32,
                    1,
                    126,
                    0
                ],
                "title": "3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds\n  Using Sensor-Intensity-Based 2D Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds\n  Using Sensor-Intensity-Based 2D Semantic Segmentation"
                },
                "summary": "Semantic segmentation of 3D LiDAR point clouds, essential for autonomous\ndriving and infrastructure management, is best achieved by supervised learning,\nwhich demands extensive annotated datasets and faces the problem of domain\nshifts. We introduce a new 3D semantic segmentation pipeline that leverages\naligned scenes and state-of-the-art 2D segmentation methods, avoiding the need\nfor direct 3D annotation or reliance on additional modalities such as camera\nimages at inference time. Our approach generates 2D views from LiDAR scans\ncolored by sensor intensity and applies 2D semantic segmentation to these views\nusing a camera-domain pretrained model. The segmented 2D outputs are then\nback-projected onto the 3D points, with a simple voting-based estimator that\nmerges the labels associated to each 3D point. Our main contribution is a\nglobal pipeline for 3D semantic segmentation requiring no prior 3D annotation\nand not other modality for inference, which can be used for pseudo-label\ngeneration. We conduct a thorough ablation study and demonstrate the potential\nof the generated pseudo-labels for the Unsupervised Domain Adaptation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic segmentation of 3D LiDAR point clouds, essential for autonomous\ndriving and infrastructure management, is best achieved by supervised learning,\nwhich demands extensive annotated datasets and faces the problem of domain\nshifts. We introduce a new 3D semantic segmentation pipeline that leverages\naligned scenes and state-of-the-art 2D segmentation methods, avoiding the need\nfor direct 3D annotation or reliance on additional modalities such as camera\nimages at inference time. Our approach generates 2D views from LiDAR scans\ncolored by sensor intensity and applies 2D semantic segmentation to these views\nusing a camera-domain pretrained model. The segmented 2D outputs are then\nback-projected onto the 3D points, with a simple voting-based estimator that\nmerges the labels associated to each 3D point. Our main contribution is a\nglobal pipeline for 3D semantic segmentation requiring no prior 3D annotation\nand not other modality for inference, which can be used for pseudo-label\ngeneration. We conduct a thorough ablation study and demonstrate the potential\nof the generated pseudo-labels for the Unsupervised Domain Adaptation task."
                },
                "authors": [
                    {
                        "name": "Andrew Caunes"
                    },
                    {
                        "name": "Thierry Chateau"
                    },
                    {
                        "name": "Vincent Frémont"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Frémont"
                },
                "author": "Vincent Frémont",
                "arxiv_doi": "10.1109/IV55156.2024.10588443",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IV55156.2024.10588443",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.03300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to IV2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07986v2",
                "updated": "2025-05-06T08:30:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    30,
                    46,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-07T02:42:07Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    2,
                    42,
                    7,
                    0,
                    97,
                    0
                ],
                "title": "SEAL: Steerable Reasoning Calibration of Large Language Models for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Steerable Reasoning Calibration of Large Language Models for Free"
                },
                "summary": "Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated\ncompelling capabilities for complex reasoning tasks via the extended\nchain-of-thought (CoT) reasoning mechanism. However, recent studies reveal\nsubstantial redundancy in the CoT reasoning traces, which not only increases\ninference latency but also negatively impacts model performance by diverting\nattention to unnecessary reasoning paths. To address this issue, we investigate\nthe internal reasoning structures of LLMs and categorize them into three\nprimary thought types: execution, reflection, and transition thoughts.\nMoreover, our analysis reveals that excessive reflection and transition\nthoughts are strongly correlated with failure cases and these thought\ncategories exhibit clear separation in the latent space. Based on these, we\nintroduce SEAL (Steerable reasoning calibration), a training-free approach that\nseamlessly calibrates the CoT process, improving accuracy while demonstrating\nsignificant efficiency gains. SEAL consists of an offline stage for extracting\nthe reasoning steering vector in the latent space, followed by an on-the-fly\ncalibration of the reasoning trace through representation intervention using\nthe steering vector. Notably, the steering vector exhibits strong\ntransferability across various tasks. Extensive experiments across multiple\nmodels (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500,\nGSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11%\nimprovement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our\ncode is publicly available at https://github.com/VITA-Group/SEAL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated\ncompelling capabilities for complex reasoning tasks via the extended\nchain-of-thought (CoT) reasoning mechanism. However, recent studies reveal\nsubstantial redundancy in the CoT reasoning traces, which not only increases\ninference latency but also negatively impacts model performance by diverting\nattention to unnecessary reasoning paths. To address this issue, we investigate\nthe internal reasoning structures of LLMs and categorize them into three\nprimary thought types: execution, reflection, and transition thoughts.\nMoreover, our analysis reveals that excessive reflection and transition\nthoughts are strongly correlated with failure cases and these thought\ncategories exhibit clear separation in the latent space. Based on these, we\nintroduce SEAL (Steerable reasoning calibration), a training-free approach that\nseamlessly calibrates the CoT process, improving accuracy while demonstrating\nsignificant efficiency gains. SEAL consists of an offline stage for extracting\nthe reasoning steering vector in the latent space, followed by an on-the-fly\ncalibration of the reasoning trace through representation intervention using\nthe steering vector. Notably, the steering vector exhibits strong\ntransferability across various tasks. Extensive experiments across multiple\nmodels (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500,\nGSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11%\nimprovement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our\ncode is publicly available at https://github.com/VITA-Group/SEAL."
                },
                "authors": [
                    {
                        "name": "Runjin Chen"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Junyuan Hong"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03296v1",
                "updated": "2025-05-06T08:27:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    27,
                    23,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T08:27:23Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    27,
                    23,
                    1,
                    126,
                    0
                ],
                "title": "The Unreasonable Effectiveness of Discrete-Time Gaussian Process\n  Mixtures for Robot Policy Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Unreasonable Effectiveness of Discrete-Time Gaussian Process\n  Mixtures for Robot Policy Learning"
                },
                "summary": "We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel\napproach for flexible policy representation and imitation learning in robot\nmanipulation. MiDiGap enables learning from as few as five demonstrations using\nonly camera observations and generalizes across a wide range of challenging\ntasks. It excels at long-horizon behaviors such as making coffee, highly\nconstrained motions such as opening doors, dynamic actions such as scooping\nwith a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns\nthese tasks on a CPU in less than a minute and scales linearly to large\ndatasets. We also develop a rich suite of tools for inference-time steering\nusing evidence such as collision signals and robot kinematic constraints. This\nsteering enables novel generalization capabilities, including obstacle\navoidance and cross-embodiment policy transfer. MiDiGap achieves\nstate-of-the-art performance on diverse few-shot manipulation benchmarks. On\nconstrained RLBench tasks, it improves policy success by 76 percentage points\nand reduces trajectory cost by 67%. On multimodal tasks, it improves policy\nsuccess by 48 percentage points and increases sample efficiency by a factor of\n20. In cross-embodiment transfer, it more than doubles policy success. We make\nthe code publicly available at https://midigap.cs.uni-freiburg.de.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel\napproach for flexible policy representation and imitation learning in robot\nmanipulation. MiDiGap enables learning from as few as five demonstrations using\nonly camera observations and generalizes across a wide range of challenging\ntasks. It excels at long-horizon behaviors such as making coffee, highly\nconstrained motions such as opening doors, dynamic actions such as scooping\nwith a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns\nthese tasks on a CPU in less than a minute and scales linearly to large\ndatasets. We also develop a rich suite of tools for inference-time steering\nusing evidence such as collision signals and robot kinematic constraints. This\nsteering enables novel generalization capabilities, including obstacle\navoidance and cross-embodiment policy transfer. MiDiGap achieves\nstate-of-the-art performance on diverse few-shot manipulation benchmarks. On\nconstrained RLBench tasks, it improves policy success by 76 percentage points\nand reduces trajectory cost by 67%. On multimodal tasks, it improves policy\nsuccess by 48 percentage points and increases sample efficiency by a factor of\n20. In cross-embodiment transfer, it more than doubles policy success. We make\nthe code publicly available at https://midigap.cs.uni-freiburg.de."
                },
                "authors": [
                    {
                        "name": "Jan Ole von Hartz"
                    },
                    {
                        "name": "Adrian Röfer"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Abhinav Valada"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Valada"
                },
                "author": "Abhinav Valada",
                "arxiv_comment": "Submitted for publication to IEEE Transaction on Robotics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03295v1",
                "updated": "2025-05-06T08:27:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    27,
                    4,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T08:27:04Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    27,
                    4,
                    1,
                    126,
                    0
                ],
                "title": "Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for\n  Reusing Existing Libraries and Interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for\n  Reusing Existing Libraries and Interfaces"
                },
                "summary": "Modern automation systems increasingly rely on modular architectures, with\ncapabilities and skills as one solution approach. Capabilities define the\nfunctions of resources in a machine-readable form and skills provide the\nconcrete implementations that realize those capabilities. However, the\ndevelopment of a skill implementation conforming to a corresponding capability\nremains a time-consuming and challenging task. In this paper, we present a\nmethod that treats capabilities as contracts for skill implementations and\nleverages large language models to generate executable code based on natural\nlanguage user input. A key feature of our approach is the integration of\nexisting software libraries and interface technologies, enabling the generation\nof skill implementations across different target languages. We introduce a\nframework that allows users to incorporate their own libraries and resource\ninterfaces into the code generation process through a retrieval-augmented\ngeneration architecture. The proposed method is evaluated using an autonomous\nmobile robot controlled via Python and ROS 2, demonstrating the feasibility and\nflexibility of the approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern automation systems increasingly rely on modular architectures, with\ncapabilities and skills as one solution approach. Capabilities define the\nfunctions of resources in a machine-readable form and skills provide the\nconcrete implementations that realize those capabilities. However, the\ndevelopment of a skill implementation conforming to a corresponding capability\nremains a time-consuming and challenging task. In this paper, we present a\nmethod that treats capabilities as contracts for skill implementations and\nleverages large language models to generate executable code based on natural\nlanguage user input. A key feature of our approach is the integration of\nexisting software libraries and interface technologies, enabling the generation\nof skill implementations across different target languages. We introduce a\nframework that allows users to incorporate their own libraries and resource\ninterfaces into the code generation process through a retrieval-augmented\ngeneration architecture. The proposed method is evaluated using an autonomous\nmobile robot controlled via Python and ROS 2, demonstrating the feasibility and\nflexibility of the approach."
                },
                "authors": [
                    {
                        "name": "Luis Miguel Vieira da Silva"
                    },
                    {
                        "name": "Aljosha Köcher"
                    },
                    {
                        "name": "Nicolas König"
                    },
                    {
                        "name": "Felix Gehlhoff"
                    },
                    {
                        "name": "Alexander Fay"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Fay"
                },
                "author": "Alexander Fay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03293v1",
                "updated": "2025-05-06T08:22:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    22,
                    51,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T08:22:51Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    22,
                    51,
                    1,
                    126,
                    0
                ],
                "title": "Ψ-Arena: Interactive Assessment and Optimization of LLM-based\n  Psychological Counselors with Tripartite Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ψ-Arena: Interactive Assessment and Optimization of LLM-based\n  Psychological Counselors with Tripartite Feedback"
                },
                "summary": "Large language models (LLMs) have shown promise in providing scalable mental\nhealth support, while evaluating their counseling capability remains crucial to\nensure both efficacy and safety. Existing evaluations are limited by the static\nassessment that focuses on knowledge tests, the single perspective that centers\non user experience, and the open-loop framework that lacks actionable feedback.\nTo address these issues, we propose {\\Psi}-Arena, an interactive framework for\ncomprehensive assessment and optimization of LLM-based counselors, featuring\nthree key characteristics: (1) Realistic arena interactions that simulate\nreal-world counseling through multi-stage dialogues with psychologically\nprofiled NPC clients, (2) Tripartite evaluation that integrates assessments\nfrom the client, counselor, and supervisor perspectives, and (3) Closed-loop\noptimization that iteratively improves LLM counselors using diagnostic\nfeedback. Experiments across eight state-of-the-art LLMs show significant\nperformance variations in different real-world scenarios and evaluation\nperspectives. Moreover, reflection-based optimization results in up to a 141%\nimprovement in counseling performance. We hope PsychoArena provides a\nfoundational resource for advancing reliable and human-aligned LLM applications\nin mental healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise in providing scalable mental\nhealth support, while evaluating their counseling capability remains crucial to\nensure both efficacy and safety. Existing evaluations are limited by the static\nassessment that focuses on knowledge tests, the single perspective that centers\non user experience, and the open-loop framework that lacks actionable feedback.\nTo address these issues, we propose {\\Psi}-Arena, an interactive framework for\ncomprehensive assessment and optimization of LLM-based counselors, featuring\nthree key characteristics: (1) Realistic arena interactions that simulate\nreal-world counseling through multi-stage dialogues with psychologically\nprofiled NPC clients, (2) Tripartite evaluation that integrates assessments\nfrom the client, counselor, and supervisor perspectives, and (3) Closed-loop\noptimization that iteratively improves LLM counselors using diagnostic\nfeedback. Experiments across eight state-of-the-art LLMs show significant\nperformance variations in different real-world scenarios and evaluation\nperspectives. Moreover, reflection-based optimization results in up to a 141%\nimprovement in counseling performance. We hope PsychoArena provides a\nfoundational resource for advancing reliable and human-aligned LLM applications\nin mental healthcare."
                },
                "authors": [
                    {
                        "name": "Shijing Zhu"
                    },
                    {
                        "name": "Zhuang Chen"
                    },
                    {
                        "name": "Guanqun Bi"
                    },
                    {
                        "name": "Binghang Li"
                    },
                    {
                        "name": "Yaxi Deng"
                    },
                    {
                        "name": "Dazhen Wan"
                    },
                    {
                        "name": "Libiao Peng"
                    },
                    {
                        "name": "Xiyao Xiao"
                    },
                    {
                        "name": "Rongsheng Zhang"
                    },
                    {
                        "name": "Tangjie Lv"
                    },
                    {
                        "name": "Zhipeng Hu"
                    },
                    {
                        "name": "FangFang Li"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10913v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10913v3",
                "updated": "2025-05-06T08:20:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    20,
                    2,
                    1,
                    126,
                    0
                ],
                "published": "2024-09-17T06:11:22Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    11,
                    22,
                    1,
                    261,
                    0
                ],
                "title": "ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of\n  Community Health Workers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of\n  Community Health Workers"
                },
                "summary": "Community health workers (CHWs) provide last-mile healthcare services but\nface challenges due to limited medical knowledge and training. This paper\ndescribes the design, deployment, and evaluation of ASHABot, an LLM-powered,\nexperts-in-the-loop, WhatsApp-based chatbot to address the information needs of\nCHWs in India. Through interviews with CHWs and their supervisors and log\nanalysis, we examine factors affecting their engagement with ASHABot, and\nASHABot's role in addressing CHWs' informational needs. We found that ASHABot\nprovided a private channel for CHWs to ask rudimentary and sensitive questions\nthey hesitated to ask supervisors. CHWs trusted the information they received\non ASHABot and treated it as an authoritative resource. CHWs' supervisors\nexpanded their knowledge by contributing answers to questions ASHABot failed to\nanswer, but were concerned about demands on their workload and increased\naccountability. We emphasize positioning LLMs as supplemental fallible\nresources within the community healthcare ecosystem, instead of as replacements\nfor supervisor support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Community health workers (CHWs) provide last-mile healthcare services but\nface challenges due to limited medical knowledge and training. This paper\ndescribes the design, deployment, and evaluation of ASHABot, an LLM-powered,\nexperts-in-the-loop, WhatsApp-based chatbot to address the information needs of\nCHWs in India. Through interviews with CHWs and their supervisors and log\nanalysis, we examine factors affecting their engagement with ASHABot, and\nASHABot's role in addressing CHWs' informational needs. We found that ASHABot\nprovided a private channel for CHWs to ask rudimentary and sensitive questions\nthey hesitated to ask supervisors. CHWs trusted the information they received\non ASHABot and treated it as an authoritative resource. CHWs' supervisors\nexpanded their knowledge by contributing answers to questions ASHABot failed to\nanswer, but were concerned about demands on their workload and increased\naccountability. We emphasize positioning LLMs as supplemental fallible\nresources within the community healthcare ecosystem, instead of as replacements\nfor supervisor support."
                },
                "authors": [
                    {
                        "name": "Pragnya Ramjee"
                    },
                    {
                        "name": "Mehak Chhokar"
                    },
                    {
                        "name": "Bhuvan Sachdeva"
                    },
                    {
                        "name": "Mahendra Meena"
                    },
                    {
                        "name": "Hamid Abdullah"
                    },
                    {
                        "name": "Aditya Vashistha"
                    },
                    {
                        "name": "Ruchit Nagar"
                    },
                    {
                        "name": "Mohit Jain"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Jain"
                },
                "author": "Mohit Jain",
                "arxiv_doi": "10.1145/3706598.3713680",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713680",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.10913v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10913v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "CHI '25: Proceedings of the 2025 CHI Conference on Human Factors\n  in Computing Systems, Article No.: 698, Pages 1-22",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.03739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03739v1",
                "updated": "2025-05-06T17:59:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    59,
                    53,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T17:59:53Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    59,
                    53,
                    1,
                    126,
                    0
                ],
                "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient\n  Large Speech-Language Model"
                },
                "summary": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks."
                },
                "authors": [
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Heting Gao"
                    },
                    {
                        "name": "Lijiang Li"
                    },
                    {
                        "name": "Peixian Chen"
                    },
                    {
                        "name": "Mengdan Zhang"
                    },
                    {
                        "name": "Hang Shao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Jinlong Peng"
                    },
                    {
                        "name": "Haoyu Cao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "Training and Inference Codes: https://github.com/VITA-MLLM/VITA-Audio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03733v1",
                "updated": "2025-05-06T17:59:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    59,
                    15,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T17:59:15Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    59,
                    15,
                    1,
                    126,
                    0
                ],
                "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch"
                },
                "summary": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model."
                },
                "authors": [
                    {
                        "name": "Zimu Lu"
                    },
                    {
                        "name": "Yunqiao Yang"
                    },
                    {
                        "name": "Houxing Ren"
                    },
                    {
                        "name": "Haotian Hou"
                    },
                    {
                        "name": "Han Xiao"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Weikang Shi"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Mingjie Zhan"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01153v3",
                "updated": "2025-05-06T17:40:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    40,
                    4,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-01T19:36:14Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    36,
                    14,
                    1,
                    91,
                    0
                ],
                "title": "Catch Me if You Search: When Contextual Web Search Results Affect the\n  Detection of Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catch Me if You Search: When Contextual Web Search Results Affect the\n  Detection of Hallucinations"
                },
                "summary": "While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or `hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N = 560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or `hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N = 560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts."
                },
                "authors": [
                    {
                        "name": "Mahjabin Nahar"
                    },
                    {
                        "name": "Eun-Ju Lee"
                    },
                    {
                        "name": "Jin Won Park"
                    },
                    {
                        "name": "Dongwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongwon Lee"
                },
                "author": "Dongwon Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20953v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20953v2",
                "updated": "2025-05-06T17:34:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    34,
                    29,
                    1,
                    126,
                    0
                ],
                "published": "2025-03-26T19:36:43Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    19,
                    36,
                    43,
                    2,
                    85,
                    0
                ],
                "title": "Clean & Clear: Feasibility of Safe LLM Clinical Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clean & Clear: Feasibility of Safe LLM Clinical Guidance"
                },
                "summary": "Background:\n  Clinical guidelines are central to safe evidence-based medicine in modern\nhealthcare, providing diagnostic criteria, treatment options and monitoring\nadvice for a wide range of illnesses. LLM-empowered chatbots have shown great\npromise in Healthcare Q&A tasks, offering the potential to provide quick and\naccurate responses to medical inquiries.\n  Our main objective was the development and preliminary assessment of an\nLLM-empowered chatbot software capable of reliably answering clinical guideline\nquestions using University College London Hospital (UCLH) clinical guidelines.\n  Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant\ninformation from the UCLH guidelines to answer questions. Our approach\nhighlights the safety and reliability of referencing information over its\ninterpretation and response generation. Seven doctors from the ward assessed\nthe chatbot's performance by comparing its answers to the gold standard.\n  Results: Our chatbot demonstrates promising performance in terms of\nrelevance, with ~73% of its responses rated as very relevant, showcasing a\nstrong understanding of the clinical context. Importantly, our chatbot achieves\na recall of 1.00 for extracted guideline lines, substantially minimising the\nrisk of missing critical information. Approximately 78% of responses were rated\nsatisfactory in terms of completeness. A small portion (~14.5%) contained minor\nunnecessary information, indicating occasional lapses in precision. The\nchatbot' showed high efficiency, with an average completion time of 10 seconds,\ncompared to 30 seconds for human respondents. Evaluation of clinical reasoning\nshowed that 72% of the chatbot's responses were without flaws. Our chatbot\ndemonstrates significant potential to speed up and improve the process of\naccessing locally relevant clinical information for healthcare professionals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background:\n  Clinical guidelines are central to safe evidence-based medicine in modern\nhealthcare, providing diagnostic criteria, treatment options and monitoring\nadvice for a wide range of illnesses. LLM-empowered chatbots have shown great\npromise in Healthcare Q&A tasks, offering the potential to provide quick and\naccurate responses to medical inquiries.\n  Our main objective was the development and preliminary assessment of an\nLLM-empowered chatbot software capable of reliably answering clinical guideline\nquestions using University College London Hospital (UCLH) clinical guidelines.\n  Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant\ninformation from the UCLH guidelines to answer questions. Our approach\nhighlights the safety and reliability of referencing information over its\ninterpretation and response generation. Seven doctors from the ward assessed\nthe chatbot's performance by comparing its answers to the gold standard.\n  Results: Our chatbot demonstrates promising performance in terms of\nrelevance, with ~73% of its responses rated as very relevant, showcasing a\nstrong understanding of the clinical context. Importantly, our chatbot achieves\na recall of 1.00 for extracted guideline lines, substantially minimising the\nrisk of missing critical information. Approximately 78% of responses were rated\nsatisfactory in terms of completeness. A small portion (~14.5%) contained minor\nunnecessary information, indicating occasional lapses in precision. The\nchatbot' showed high efficiency, with an average completion time of 10 seconds,\ncompared to 30 seconds for human respondents. Evaluation of clinical reasoning\nshowed that 72% of the chatbot's responses were without flaws. Our chatbot\ndemonstrates significant potential to speed up and improve the process of\naccessing locally relevant clinical information for healthcare professionals."
                },
                "authors": [
                    {
                        "name": "Julia Ive"
                    },
                    {
                        "name": "Felix Jozsa"
                    },
                    {
                        "name": "Nick Jackson"
                    },
                    {
                        "name": "Paulina Bondaronek"
                    },
                    {
                        "name": "Ciaran Scott Hill"
                    },
                    {
                        "name": "Richard Dobson"
                    }
                ],
                "author_detail": {
                    "name": "Richard Dobson"
                },
                "author": "Richard Dobson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20953v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20953v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10707v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10707v2",
                "updated": "2025-05-06T17:04:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    17,
                    4,
                    18,
                    1,
                    126,
                    0
                ],
                "published": "2025-03-12T18:36:41Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    18,
                    36,
                    41,
                    2,
                    71,
                    0
                ],
                "title": "CALLM: Understanding Cancer Survivors' Emotions and Intervention\n  Opportunities via Mobile Diaries and Context-Aware Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLM: Understanding Cancer Survivors' Emotions and Intervention\n  Opportunities via Mobile Diaries and Context-Aware Language Models"
                },
                "summary": "Cancer survivors face unique emotional challenges that impact their quality\nof life. Mobile diary entries provide a promising method for tracking emotional\nstates, improving self-awareness, and promoting well-being outcome. This paper\naims to, through mobile diaries, understand cancer survivors' emotional states\nand key variables related to just-in-time intervention opportunities, including\nthe desire to regulate emotions and the availability to engage in\ninterventions. Although emotion analysis tools show potential for recognizing\nemotions from text, current methods lack the contextual understanding necessary\nto interpret brief mobile diary narratives. Our analysis of diary entries from\ncancer survivors (N=407) reveals systematic relationships between described\ncontexts and emotional states, with administrative and health-related contexts\nassociated with negative affect and regulation needs, while leisure activities\npromote positive emotions. We propose CALLM, a Context-Aware framework\nleveraging Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG) to analyze these brief entries by integrating retrieved peer experiences\nand personal diary history. CALLM demonstrates strong performance with balanced\naccuracies reaching 72.96% for positive affect, 73.29% for negative affect,\n73.72% for emotion regulation desire, and 60.09% for intervention availability,\noutperforming language model baselines. Post-hoc analysis reveals that model\nconfidence strongly predicts accuracy, with longer diary entries generally\nenhancing performance, and brief personalization periods yielding meaningful\nimprovements. Our findings demonstrate how contextual information in mobile\ndiaries can be effectively leveraged to understand emotional experiences,\npredict key states, and identify optimal intervention moments for personalized\njust-in-time support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer survivors face unique emotional challenges that impact their quality\nof life. Mobile diary entries provide a promising method for tracking emotional\nstates, improving self-awareness, and promoting well-being outcome. This paper\naims to, through mobile diaries, understand cancer survivors' emotional states\nand key variables related to just-in-time intervention opportunities, including\nthe desire to regulate emotions and the availability to engage in\ninterventions. Although emotion analysis tools show potential for recognizing\nemotions from text, current methods lack the contextual understanding necessary\nto interpret brief mobile diary narratives. Our analysis of diary entries from\ncancer survivors (N=407) reveals systematic relationships between described\ncontexts and emotional states, with administrative and health-related contexts\nassociated with negative affect and regulation needs, while leisure activities\npromote positive emotions. We propose CALLM, a Context-Aware framework\nleveraging Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG) to analyze these brief entries by integrating retrieved peer experiences\nand personal diary history. CALLM demonstrates strong performance with balanced\naccuracies reaching 72.96% for positive affect, 73.29% for negative affect,\n73.72% for emotion regulation desire, and 60.09% for intervention availability,\noutperforming language model baselines. Post-hoc analysis reveals that model\nconfidence strongly predicts accuracy, with longer diary entries generally\nenhancing performance, and brief personalization periods yielding meaningful\nimprovements. Our findings demonstrate how contextual information in mobile\ndiaries can be effectively leveraged to understand emotional experiences,\npredict key states, and identify optimal intervention moments for personalized\njust-in-time support."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Katharine E. Daniel"
                    },
                    {
                        "name": "Laura E. Barnes"
                    },
                    {
                        "name": "Philip I. Chow"
                    }
                ],
                "author_detail": {
                    "name": "Philip I. Chow"
                },
                "author": "Philip I. Chow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10707v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10707v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02445v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02445v3",
                "updated": "2025-05-06T16:32:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    32,
                    17,
                    1,
                    126,
                    0
                ],
                "published": "2025-03-04T09:40:00Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    9,
                    40,
                    0,
                    1,
                    63,
                    0
                ],
                "title": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via\n  Multi-Agent Iterative Optimization and Diffusion Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via\n  Multi-Agent Iterative Optimization and Diffusion Modelling"
                },
                "summary": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yuhao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    },
                    {
                        "name": "Goran Nenadic"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02445v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02445v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03678v1",
                "updated": "2025-05-06T16:23:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    23,
                    42,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T16:23:42Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    23,
                    42,
                    1,
                    126,
                    0
                ],
                "title": "Graph Drawing for LLMs: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Drawing for LLMs: An Empirical Evaluation"
                },
                "summary": "Our work contributes to the fast-growing literature on the use of Large\nLanguage Models (LLMs) to perform graph-related tasks. In particular, we focus\non usage scenarios that rely on the visual modality, feeding the model with a\ndrawing of the graph under analysis. We investigate how the model's performance\nis affected by the chosen layout paradigm, the aesthetics of the drawing, and\nthe prompting technique used for the queries. We formulate three corresponding\nresearch questions and present the results of a thorough experimental analysis.\nOur findings reveal that choosing the right layout paradigm and optimizing the\nreadability of the input drawing from a human perspective can significantly\nimprove the performance of the model on the given task. Moreover, selecting the\nmost effective prompting technique is a challenging yet crucial task for\nachieving optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our work contributes to the fast-growing literature on the use of Large\nLanguage Models (LLMs) to perform graph-related tasks. In particular, we focus\non usage scenarios that rely on the visual modality, feeding the model with a\ndrawing of the graph under analysis. We investigate how the model's performance\nis affected by the chosen layout paradigm, the aesthetics of the drawing, and\nthe prompting technique used for the queries. We formulate three corresponding\nresearch questions and present the results of a thorough experimental analysis.\nOur findings reveal that choosing the right layout paradigm and optimizing the\nreadability of the input drawing from a human perspective can significantly\nimprove the performance of the model on the given task. Moreover, selecting the\nmost effective prompting technique is a challenging yet crucial task for\nachieving optimal performance."
                },
                "authors": [
                    {
                        "name": "Walter Didimo"
                    },
                    {
                        "name": "Fabrizio Montecchiani"
                    },
                    {
                        "name": "Tommaso Piselli"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Piselli"
                },
                "author": "Tommaso Piselli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03673v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03673v1",
                "updated": "2025-05-06T16:11:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    11,
                    49,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T16:11:49Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    16,
                    11,
                    49,
                    1,
                    126,
                    0
                ],
                "title": "RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and\n  Multi-Agent Collaboration"
                },
                "summary": "The dawn of embodied intelligence has ushered in an unprecedented imperative\nfor resilient, cognition-enabled multi-agent collaboration across\nnext-generation ecosystems, revolutionizing paradigms in autonomous\nmanufacturing, adaptive service robotics, and cyber-physical production\narchitectures. However, current robotic systems face significant limitations,\nsuch as limited cross-embodiment adaptability, inefficient task scheduling, and\ninsufficient dynamic error correction. While End-to-end VLA models demonstrate\ninadequate long-horizon planning and task generalization, hierarchical VLA\nmodels suffer from a lack of cross-embodiment and multi-agent coordination\ncapabilities. To address these challenges, we introduce RoboOS, the first\nopen-source embodied system built on a Brain-Cerebellum hierarchical\narchitecture, enabling a paradigm shift from single-agent to multi-agent\nintelligence. Specifically, RoboOS consists of three key components: (1)\nEmbodied Brain Model (RoboBrain), a MLLM designed for global perception and\nhigh-level decision-making; (2) Cerebellum Skill Library, a modular,\nplug-and-play toolkit that facilitates seamless execution of multiple skills;\nand (3) Real-Time Shared Memory, a spatiotemporal synchronization mechanism for\ncoordinating multi-agent states. By integrating hierarchical information flow,\nRoboOS bridges Embodied Brain and Cerebellum Skill Library, facilitating robust\nplanning, scheduling, and error correction for long-horizon tasks, while\nensuring efficient multi-agent collaboration through Real-Time Shared Memory.\nFurthermore, we enhance edge-cloud communication and cloud-based distributed\ninference to facilitate high-frequency interactions and enable scalable\ndeployment. Extensive real-world experiments across various scenarios,\ndemonstrate RoboOS's versatility in supporting heterogeneous embodiments.\nProject website: https://github.com/FlagOpen/RoboOS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dawn of embodied intelligence has ushered in an unprecedented imperative\nfor resilient, cognition-enabled multi-agent collaboration across\nnext-generation ecosystems, revolutionizing paradigms in autonomous\nmanufacturing, adaptive service robotics, and cyber-physical production\narchitectures. However, current robotic systems face significant limitations,\nsuch as limited cross-embodiment adaptability, inefficient task scheduling, and\ninsufficient dynamic error correction. While End-to-end VLA models demonstrate\ninadequate long-horizon planning and task generalization, hierarchical VLA\nmodels suffer from a lack of cross-embodiment and multi-agent coordination\ncapabilities. To address these challenges, we introduce RoboOS, the first\nopen-source embodied system built on a Brain-Cerebellum hierarchical\narchitecture, enabling a paradigm shift from single-agent to multi-agent\nintelligence. Specifically, RoboOS consists of three key components: (1)\nEmbodied Brain Model (RoboBrain), a MLLM designed for global perception and\nhigh-level decision-making; (2) Cerebellum Skill Library, a modular,\nplug-and-play toolkit that facilitates seamless execution of multiple skills;\nand (3) Real-Time Shared Memory, a spatiotemporal synchronization mechanism for\ncoordinating multi-agent states. By integrating hierarchical information flow,\nRoboOS bridges Embodied Brain and Cerebellum Skill Library, facilitating robust\nplanning, scheduling, and error correction for long-horizon tasks, while\nensuring efficient multi-agent collaboration through Real-Time Shared Memory.\nFurthermore, we enhance edge-cloud communication and cloud-based distributed\ninference to facilitate high-frequency interactions and enable scalable\ndeployment. Extensive real-world experiments across various scenarios,\ndemonstrate RoboOS's versatility in supporting heterogeneous embodiments.\nProject website: https://github.com/FlagOpen/RoboOS"
                },
                "authors": [
                    {
                        "name": "Huajie Tan"
                    },
                    {
                        "name": "Xiaoshuai Hao"
                    },
                    {
                        "name": "Minglan Lin"
                    },
                    {
                        "name": "Pengwei Wang"
                    },
                    {
                        "name": "Yaoxu Lyu"
                    },
                    {
                        "name": "Mingyu Cao"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "arxiv_comment": "22 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03673v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03673v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17761v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17761v3",
                "updated": "2025-05-06T15:58:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    58,
                    40,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-24T17:25:12Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    25,
                    12,
                    3,
                    114,
                    0
                ],
                "title": "Step1X-Edit: A Practical Framework for General Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step1X-Edit: A Practical Framework for General Image Editing"
                },
                "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing."
                },
                "authors": [
                    {
                        "name": "Shiyu Liu"
                    },
                    {
                        "name": "Yucheng Han"
                    },
                    {
                        "name": "Peng Xing"
                    },
                    {
                        "name": "Fukun Yin"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Jiaqi Liao"
                    },
                    {
                        "name": "Yingming Wang"
                    },
                    {
                        "name": "Honghao Fu"
                    },
                    {
                        "name": "Chunrui Han"
                    },
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Yuang Peng"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Jingwei Wu"
                    },
                    {
                        "name": "Yan Cai"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Ranchen Ming"
                    },
                    {
                        "name": "Lei Xia"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "code: https://github.com/stepfun-ai/Step1X-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17761v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17761v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15627v2",
                "updated": "2025-05-06T15:53:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    53,
                    34,
                    1,
                    126,
                    0
                ],
                "published": "2025-01-26T18:25:26Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    18,
                    25,
                    26,
                    6,
                    26,
                    0
                ],
                "title": "HardML: A Benchmark For Evaluating Data Science And Machine Learning\n  knowledge and reasoning in AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HardML: A Benchmark For Evaluating Data Science And Machine Learning\n  knowledge and reasoning in AI"
                },
                "summary": "We present HardML, a benchmark designed to evaluate the knowledge and\nreasoning abilities in the fields of data science and machine learning. HardML\ncomprises a diverse set of 100 challenging multiple-choice questions,\nhandcrafted over a period of 6 months, covering the most popular and modern\nbranches of data science and machine learning. These questions are challenging\neven for a typical Senior Machine Learning Engineer to answer correctly. To\nminimize the risk of data contamination, HardML uses mostly original content\ndevised by the author. Current state of the art AI models achieve a 30% error\nrate on this benchmark, which is about 3 times larger than the one achieved on\nthe equivalent, well known MMLU ML. While HardML is limited in scope and not\naiming to push the frontier, primarily due to its multiple choice nature, it\nserves as a rigorous and modern testbed to quantify and track the progress of\ntop AI. While plenty benchmarks and experimentation in LLM evaluation exist in\nother STEM fields like mathematics, physics and chemistry, the subfields of\ndata science and machine learning remain fairly underexplored.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present HardML, a benchmark designed to evaluate the knowledge and\nreasoning abilities in the fields of data science and machine learning. HardML\ncomprises a diverse set of 100 challenging multiple-choice questions,\nhandcrafted over a period of 6 months, covering the most popular and modern\nbranches of data science and machine learning. These questions are challenging\neven for a typical Senior Machine Learning Engineer to answer correctly. To\nminimize the risk of data contamination, HardML uses mostly original content\ndevised by the author. Current state of the art AI models achieve a 30% error\nrate on this benchmark, which is about 3 times larger than the one achieved on\nthe equivalent, well known MMLU ML. While HardML is limited in scope and not\naiming to push the frontier, primarily due to its multiple choice nature, it\nserves as a rigorous and modern testbed to quantify and track the progress of\ntop AI. While plenty benchmarks and experimentation in LLM evaluation exist in\nother STEM fields like mathematics, physics and chemistry, the subfields of\ndata science and machine learning remain fairly underexplored."
                },
                "authors": [
                    {
                        "name": "Tidor-Vlad Pricope"
                    }
                ],
                "author_detail": {
                    "name": "Tidor-Vlad Pricope"
                },
                "author": "Tidor-Vlad Pricope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03555v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03555v5",
                "updated": "2025-05-06T15:27:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    27,
                    49,
                    1,
                    126,
                    0
                ],
                "published": "2024-05-06T15:25:48Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    15,
                    25,
                    48,
                    0,
                    127,
                    0
                ],
                "title": "A Comprehensive Tutorial and Survey of O-RAN: Exploring Slicing-aware\n  Architecture, Deployment Options, Use Cases, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Tutorial and Survey of O-RAN: Exploring Slicing-aware\n  Architecture, Deployment Options, Use Cases, and Challenges"
                },
                "summary": "Open-radio access network (O-RAN) seeks to establish the principles of\nopenness, programmability, automation, intelligence, and hardware-software\ndisaggregation with interoperable and standard-compliant interfaces. It\nadvocates for multi-vendorism and multi-stakeholderism within a cloudified and\nvirtualized wireless infrastructure, aimed at enhancing the deployment,\noperation, and management of RAN architecture. These enhancements promise\nincreased flexibility, performance optimization, service innovation, energy\nefficiency, and cost effectiveness across fifth-generation (5G),\nsixth-generation (6G), and beyond networks. A silent feature of O-RAN\narchitecture is its support for network slicing, which entails interaction with\nother domains of the cellular network, notably the transport network (TN) and\nthe core network (CN), to realize end-to-end (E2E) network slicing. The study\nof this feature requires exploring the stances and contributions of diverse\nstandards development organizations (SDOs). In this context, we note that\ndespite the ongoing industrial deployments and standardization efforts, the\nresearch and standardization communities have yet to comprehensively address\nnetwork slicing in O-RAN. To address this gap, this paper provides a\ncomprehensive exploration of network slicing in O-RAN through an in-depth\nreview of specification documents from O-RAN Alliance and research papers from\nleading industry and academic institutions. The paper commences with an\noverview of the relevant standardization and open source contributions,\nsubsequently delving into the latest O-RAN architecture with an emphasis on its\nslicing aspects. Furthermore, the paper explores O-RAN deployment scenarios,\nexamining options for the deployment and orchestration of RAN and TN slice\nsubnets. It also discusses the slicing of the underlying infrastructure and\nprovides an overview of various use cases related...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-radio access network (O-RAN) seeks to establish the principles of\nopenness, programmability, automation, intelligence, and hardware-software\ndisaggregation with interoperable and standard-compliant interfaces. It\nadvocates for multi-vendorism and multi-stakeholderism within a cloudified and\nvirtualized wireless infrastructure, aimed at enhancing the deployment,\noperation, and management of RAN architecture. These enhancements promise\nincreased flexibility, performance optimization, service innovation, energy\nefficiency, and cost effectiveness across fifth-generation (5G),\nsixth-generation (6G), and beyond networks. A silent feature of O-RAN\narchitecture is its support for network slicing, which entails interaction with\nother domains of the cellular network, notably the transport network (TN) and\nthe core network (CN), to realize end-to-end (E2E) network slicing. The study\nof this feature requires exploring the stances and contributions of diverse\nstandards development organizations (SDOs). In this context, we note that\ndespite the ongoing industrial deployments and standardization efforts, the\nresearch and standardization communities have yet to comprehensively address\nnetwork slicing in O-RAN. To address this gap, this paper provides a\ncomprehensive exploration of network slicing in O-RAN through an in-depth\nreview of specification documents from O-RAN Alliance and research papers from\nleading industry and academic institutions. The paper commences with an\noverview of the relevant standardization and open source contributions,\nsubsequently delving into the latest O-RAN architecture with an emphasis on its\nslicing aspects. Furthermore, the paper explores O-RAN deployment scenarios,\nexamining options for the deployment and orchestration of RAN and TN slice\nsubnets. It also discusses the slicing of the underlying infrastructure and\nprovides an overview of various use cases related..."
                },
                "authors": [
                    {
                        "name": "Khurshid Alam"
                    },
                    {
                        "name": "Mohammad Asif Habibi"
                    },
                    {
                        "name": "Matthias Tammen"
                    },
                    {
                        "name": "Dennis Krummacker"
                    },
                    {
                        "name": "Walid Saad"
                    },
                    {
                        "name": "Marco Di Renzo"
                    },
                    {
                        "name": "Tommaso Melodia"
                    },
                    {
                        "name": "Xavier Costa-Pérez"
                    },
                    {
                        "name": "Mérouane Debbah"
                    },
                    {
                        "name": "Ashutosh Dutta"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "40 pages, 12 figures, 4 tables, submitted to the IEEE for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03555v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03555v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03621v1",
                "updated": "2025-05-06T15:18:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    18,
                    38,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T15:18:38Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    18,
                    38,
                    1,
                    126,
                    0
                ],
                "title": "PhysLLM: Harnessing Large Language Models for Cross-Modal Remote\n  Physiological Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysLLM: Harnessing Large Language Models for Cross-Modal Remote\n  Physiological Sensing"
                },
                "summary": "Remote photoplethysmography (rPPG) enables non-contact physiological\nmeasurement but remains highly susceptible to illumination changes, motion\nartifacts, and limited temporal modeling. Large Language Models (LLMs) excel at\ncapturing long-range dependencies, offering a potential solution but struggle\nwith the continuous, noise-sensitive nature of rPPG signals due to their\ntext-centric design. To bridge this gap, we introduce PhysLLM, a collaborative\noptimization framework that synergizes LLMs with domain-specific rPPG\ncomponents. Specifically, the Text Prototype Guidance (TPG) strategy is\nproposed to establish cross-modal alignment by projecting hemodynamic features\ninto LLM-interpretable semantic space, effectively bridging the\nrepresentational gap between physiological signals and linguistic tokens.\nBesides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for\nresolving signal instability through adaptive time-frequency domain feature\nre-weighting. Finally, rPPG task-specific cues systematically inject\nphysiological priors through physiological statistics, environmental contextual\nanswering, and task description, leveraging cross-modal learning to integrate\nboth visual and textual information, enabling dynamic adaptation to challenging\nscenarios like variable illumination and subject movements. Evaluation on four\nbenchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,\ndemonstrating superior generalization across lighting variations and motion\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote photoplethysmography (rPPG) enables non-contact physiological\nmeasurement but remains highly susceptible to illumination changes, motion\nartifacts, and limited temporal modeling. Large Language Models (LLMs) excel at\ncapturing long-range dependencies, offering a potential solution but struggle\nwith the continuous, noise-sensitive nature of rPPG signals due to their\ntext-centric design. To bridge this gap, we introduce PhysLLM, a collaborative\noptimization framework that synergizes LLMs with domain-specific rPPG\ncomponents. Specifically, the Text Prototype Guidance (TPG) strategy is\nproposed to establish cross-modal alignment by projecting hemodynamic features\ninto LLM-interpretable semantic space, effectively bridging the\nrepresentational gap between physiological signals and linguistic tokens.\nBesides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for\nresolving signal instability through adaptive time-frequency domain feature\nre-weighting. Finally, rPPG task-specific cues systematically inject\nphysiological priors through physiological statistics, environmental contextual\nanswering, and task description, leveraging cross-modal learning to integrate\nboth visual and textual information, enabling dynamic adaptation to challenging\nscenarios like variable illumination and subject movements. Evaluation on four\nbenchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,\ndemonstrating superior generalization across lighting variations and motion\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yiping Xie"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Mingtong Dai"
                    },
                    {
                        "name": "Jian-Ping Zhou"
                    },
                    {
                        "name": "Yue Sun"
                    },
                    {
                        "name": "Tao Tan"
                    },
                    {
                        "name": "Weicheng Xie"
                    },
                    {
                        "name": "Linlin Shen"
                    },
                    {
                        "name": "Zitong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zitong Yu"
                },
                "author": "Zitong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19187v2",
                "updated": "2025-05-06T15:11:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    11,
                    32,
                    1,
                    126,
                    0
                ],
                "published": "2025-02-26T14:50:50Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    50,
                    50,
                    2,
                    57,
                    0
                ],
                "title": "BIG-Bench Extra Hard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIG-Bench Extra Hard"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in everyday\napplications, demanding robust general reasoning capabilities and diverse\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\nwhich has served as a crucial benchmark for evaluating the general reasoning\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\nallowed for a comprehensive assessment of general reasoning across various\nskills within a unified framework. However, recent advances in LLMs have led to\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\nprobes a similar reasoning capability but exhibits significantly increased\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\naccuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best\nreasoning-specialized model, indicating substantial room for improvement and\nhighlighting the ongoing challenge of achieving robust general reasoning in\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in everyday\napplications, demanding robust general reasoning capabilities and diverse\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\nwhich has served as a crucial benchmark for evaluating the general reasoning\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\nallowed for a comprehensive assessment of general reasoning across various\nskills within a unified framework. However, recent advances in LLMs have led to\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\nprobes a similar reasoning capability but exhibits significantly increased\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\naccuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best\nreasoning-specialized model, indicating substantial room for improvement and\nhighlighting the ongoing challenge of achieving robust general reasoning in\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh."
                },
                "authors": [
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Bahare Fatemi"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "John Palowitch"
                    },
                    {
                        "name": "Chrysovalantis Anastasiou"
                    },
                    {
                        "name": "Sanket Vaibhav Mehta"
                    },
                    {
                        "name": "Lalit K. Jain"
                    },
                    {
                        "name": "Virginia Aglietti"
                    },
                    {
                        "name": "Disha Jindal"
                    },
                    {
                        "name": "Peter Chen"
                    },
                    {
                        "name": "Nishanth Dikkala"
                    },
                    {
                        "name": "Gladys Tyen"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Uri Shalit"
                    },
                    {
                        "name": "Silvia Chiappa"
                    },
                    {
                        "name": "Kate Olszewska"
                    },
                    {
                        "name": "Yi Tay"
                    },
                    {
                        "name": "Vinh Q. Tran"
                    },
                    {
                        "name": "Quoc V. Le"
                    },
                    {
                        "name": "Orhan Firat"
                    }
                ],
                "author_detail": {
                    "name": "Orhan Firat"
                },
                "author": "Orhan Firat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13379v2",
                "updated": "2025-05-06T14:56:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    56,
                    21,
                    1,
                    126,
                    0
                ],
                "published": "2025-01-23T04:43:10Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    4,
                    43,
                    10,
                    3,
                    23,
                    0
                ],
                "title": "A Quantitative Evaluation of Approximate Softmax Functions for Deep\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Quantitative Evaluation of Approximate Softmax Functions for Deep\n  Neural Networks"
                },
                "summary": "The softmax function is a widely used activation function in the output\nlayers of neural networks, responsible for converting raw scores into class\nprobabilities while introducing essential non-linearity. Implementing Softmax\nefficiently poses challenges on low-end FPGAs due to limited hardware resources\nand the computational complexity of exponential and division operations. This\nwork evaluates approximate computing techniques for softmax acceleration using\nTaylor series and interpolation methods using Look-Up Tables (LUTs). These\napproximations aim to reduce execution time and resource consumption while\nmaintaining acceptable levels of numerical precision. Our findings show that\nquadratic interpolation with LUTs yields the lowest numerical error. In\ncontrast, Taylor-based approximations offer significantly better performance in\nterms of execution time and resource efficiency due to their computational\nsimplicity. When applied to real-world deep learning models such as LeNet-5 and\nMobileNet v2, the first- and second-order Taylor approximations provided\nsubstantial trade-offs between accuracy and resource savings, achieving up to\n0.2% accuracy degradation and 14% resource reduction compared to exact\nimplementations. These results highlight the effectiveness of approximate\nSoftmax designs on resource-constrained FPGAs and lay the groundwork for their\nintegration into larger models, including large language models (LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The softmax function is a widely used activation function in the output\nlayers of neural networks, responsible for converting raw scores into class\nprobabilities while introducing essential non-linearity. Implementing Softmax\nefficiently poses challenges on low-end FPGAs due to limited hardware resources\nand the computational complexity of exponential and division operations. This\nwork evaluates approximate computing techniques for softmax acceleration using\nTaylor series and interpolation methods using Look-Up Tables (LUTs). These\napproximations aim to reduce execution time and resource consumption while\nmaintaining acceptable levels of numerical precision. Our findings show that\nquadratic interpolation with LUTs yields the lowest numerical error. In\ncontrast, Taylor-based approximations offer significantly better performance in\nterms of execution time and resource efficiency due to their computational\nsimplicity. When applied to real-world deep learning models such as LeNet-5 and\nMobileNet v2, the first- and second-order Taylor approximations provided\nsubstantial trade-offs between accuracy and resource savings, achieving up to\n0.2% accuracy degradation and 14% resource reduction compared to exact\nimplementations. These results highlight the effectiveness of approximate\nSoftmax designs on resource-constrained FPGAs and lay the groundwork for their\nintegration into larger models, including large language models (LLMs)."
                },
                "authors": [
                    {
                        "name": "Anthony Leiva-Valverde"
                    },
                    {
                        "name": "Fabricio Elizondo-Fernández"
                    },
                    {
                        "name": "Luis G. León-Vega"
                    },
                    {
                        "name": "Cristina Meinhardt"
                    },
                    {
                        "name": "Jorge Castro-Godínez"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Castro-Godínez"
                },
                "author": "Jorge Castro-Godínez",
                "arxiv_comment": "A new author has been added due to his contributions in the FPGA part\n  (Section IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01487v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01487v4",
                "updated": "2025-05-06T14:49:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    49,
                    11,
                    1,
                    126,
                    0
                ],
                "published": "2024-12-02T13:39:29Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    13,
                    39,
                    29,
                    0,
                    337,
                    0
                ],
                "title": "FastRM: An efficient and automatic explainability framework for\n  multimodal generative models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastRM: An efficient and automatic explainability framework for\n  multimodal generative models"
                },
                "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable reasoning\ncapabilities over textual and visual inputs. However, these models remain prone\nto generating misinformation. Identifying and mitigating ungrounded responses\nis crucial for developing trustworthy AI. Traditional explainability methods\nsuch as gradient-based relevancy maps, offer insight into the decision process\nof models, but are often computationally expensive and unsuitable for real-time\noutput validation. In this work, we introduce FastRM, an efficient method for\npredicting explainable Relevancy Maps of LVLMs. Furthermore, FastRM provides\nboth quantitative and qualitative assessment of model confidence. Experimental\nresults demonstrate that FastRM achieves a 99.8% reduction in computation time\nand a 44.4% reduction in memory footprint compared to traditional relevancy map\ngeneration. FastRM allows explainable AI to be more practical and scalable,\nthereby promoting its deployment in real-world applications and enabling users\nto more effectively evaluate the reliability of model outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision Language Models (LVLMs) have demonstrated remarkable reasoning\ncapabilities over textual and visual inputs. However, these models remain prone\nto generating misinformation. Identifying and mitigating ungrounded responses\nis crucial for developing trustworthy AI. Traditional explainability methods\nsuch as gradient-based relevancy maps, offer insight into the decision process\nof models, but are often computationally expensive and unsuitable for real-time\noutput validation. In this work, we introduce FastRM, an efficient method for\npredicting explainable Relevancy Maps of LVLMs. Furthermore, FastRM provides\nboth quantitative and qualitative assessment of model confidence. Experimental\nresults demonstrate that FastRM achieves a 99.8% reduction in computation time\nand a 44.4% reduction in memory footprint compared to traditional relevancy map\ngeneration. FastRM allows explainable AI to be more practical and scalable,\nthereby promoting its deployment in real-world applications and enabling users\nto more effectively evaluate the reliability of model outputs."
                },
                "authors": [
                    {
                        "name": "Gabriela Ben-Melech Stan"
                    },
                    {
                        "name": "Estelle Aflalo"
                    },
                    {
                        "name": "Man Luo"
                    },
                    {
                        "name": "Shachar Rosenman"
                    },
                    {
                        "name": "Tiep Le"
                    },
                    {
                        "name": "Sayak Paul"
                    },
                    {
                        "name": "Shao-Yen Tseng"
                    },
                    {
                        "name": "Vasudev Lal"
                    }
                ],
                "author_detail": {
                    "name": "Vasudev Lal"
                },
                "author": "Vasudev Lal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01487v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01487v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17279v2",
                "updated": "2025-05-06T14:34:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    34,
                    33,
                    1,
                    126,
                    0
                ],
                "published": "2025-03-21T16:27:12Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    16,
                    27,
                    12,
                    4,
                    80,
                    0
                ],
                "title": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic\n  Textual Similarity Measurement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic\n  Textual Similarity Measurement"
                },
                "summary": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance."
                },
                "authors": [
                    {
                        "name": "Gaifan Zhang"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Danushka Bollegala"
                    }
                ],
                "author_detail": {
                    "name": "Danushka Bollegala"
                },
                "author": "Danushka Bollegala",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03574v1",
                "updated": "2025-05-06T14:34:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    34,
                    21,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:34:21Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    34,
                    21,
                    1,
                    126,
                    0
                ],
                "title": "LlamaFirewall: An open source guardrail system for building secure AI\n  agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaFirewall: An open source guardrail system for building secure AI\n  agents"
                },
                "summary": "Large language models (LLMs) have evolved from simple chatbots into\nautonomous agents capable of performing complex tasks such as editing\nproduction code, orchestrating workflows, and taking higher-stakes actions\nbased on untrusted inputs like webpages and emails. These capabilities\nintroduce new security risks that existing security measures, such as model\nfine-tuning or chatbot-focused guardrails, do not fully address. Given the\nhigher stakes and the absence of deterministic solutions to mitigate these\nrisks, there is a critical need for a real-time guardrail monitor to serve as a\nfinal layer of defense, and support system level, use case specific safety\npolicy definition and enforcement. We introduce LlamaFirewall, an open-source\nsecurity focused guardrail framework designed to serve as a final layer of\ndefense against security risks associated with AI Agents. Our framework\nmitigates risks such as prompt injection, agent misalignment, and insecure code\nrisks through three powerful guardrails: PromptGuard 2, a universal jailbreak\ndetector that demonstrates clear state of the art performance; Agent Alignment\nChecks, a chain-of-thought auditor that inspects agent reasoning for prompt\ninjection and goal misalignment, which, while still experimental, shows\nstronger efficacy at preventing indirect injections in general scenarios than\npreviously proposed approaches; and CodeShield, an online static analysis\nengine that is both fast and extensible, aimed at preventing the generation of\ninsecure or dangerous code by coding agents. Additionally, we include\neasy-to-use customizable scanners that make it possible for any developer who\ncan write a regular expression or an LLM prompt to quickly update an agent's\nsecurity guardrails.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have evolved from simple chatbots into\nautonomous agents capable of performing complex tasks such as editing\nproduction code, orchestrating workflows, and taking higher-stakes actions\nbased on untrusted inputs like webpages and emails. These capabilities\nintroduce new security risks that existing security measures, such as model\nfine-tuning or chatbot-focused guardrails, do not fully address. Given the\nhigher stakes and the absence of deterministic solutions to mitigate these\nrisks, there is a critical need for a real-time guardrail monitor to serve as a\nfinal layer of defense, and support system level, use case specific safety\npolicy definition and enforcement. We introduce LlamaFirewall, an open-source\nsecurity focused guardrail framework designed to serve as a final layer of\ndefense against security risks associated with AI Agents. Our framework\nmitigates risks such as prompt injection, agent misalignment, and insecure code\nrisks through three powerful guardrails: PromptGuard 2, a universal jailbreak\ndetector that demonstrates clear state of the art performance; Agent Alignment\nChecks, a chain-of-thought auditor that inspects agent reasoning for prompt\ninjection and goal misalignment, which, while still experimental, shows\nstronger efficacy at preventing indirect injections in general scenarios than\npreviously proposed approaches; and CodeShield, an online static analysis\nengine that is both fast and extensible, aimed at preventing the generation of\ninsecure or dangerous code by coding agents. Additionally, we include\neasy-to-use customizable scanners that make it possible for any developer who\ncan write a regular expression or an LLM prompt to quickly update an agent's\nsecurity guardrails."
                },
                "authors": [
                    {
                        "name": "Sahana Chennabasappa"
                    },
                    {
                        "name": "Cyrus Nikolaidis"
                    },
                    {
                        "name": "Daniel Song"
                    },
                    {
                        "name": "David Molnar"
                    },
                    {
                        "name": "Stephanie Ding"
                    },
                    {
                        "name": "Shengye Wan"
                    },
                    {
                        "name": "Spencer Whitman"
                    },
                    {
                        "name": "Lauren Deason"
                    },
                    {
                        "name": "Nicholas Doucette"
                    },
                    {
                        "name": "Abraham Montilla"
                    },
                    {
                        "name": "Alekhya Gampa"
                    },
                    {
                        "name": "Beto de Paola"
                    },
                    {
                        "name": "Dominik Gabi"
                    },
                    {
                        "name": "James Crnkovich"
                    },
                    {
                        "name": "Jean-Christophe Testud"
                    },
                    {
                        "name": "Kat He"
                    },
                    {
                        "name": "Rashnil Chaturvedi"
                    },
                    {
                        "name": "Wu Zhou"
                    },
                    {
                        "name": "Joshua Saxe"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Saxe"
                },
                "author": "Joshua Saxe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15279v2",
                "updated": "2025-05-06T14:25:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    25,
                    0,
                    1,
                    126,
                    0
                ],
                "published": "2024-11-22T15:29:12Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    29,
                    12,
                    4,
                    327,
                    0
                ],
                "title": "Don't Mesh with Me: Generating Constructive Solid Geometry Instead of\n  Meshes by Fine-Tuning a Code-Generation LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Mesh with Me: Generating Constructive Solid Geometry Instead of\n  Meshes by Fine-Tuning a Code-Generation LLM"
                },
                "summary": "While recent advancements in machine learning, such as LLMs, are\nrevolutionizing software development and creative industries, they have had\nminimal impact on engineers designing mechanical parts, which remains largely a\nmanual process. Existing approaches to generating 3D geometry most commonly use\nmeshes as a 3D representation. While meshes are suitable for assets in video\ngames or animations, they lack sufficient precision and adaptability for\nmechanical engineering purposes. This paper introduces a novel approach for the\ngeneration of 3D geometry that generates surface-based Constructive Solid\nGeometry (CSG) by leveraging a code-generation LLM. First, we create a dataset\nof 3D mechanical parts represented as code scripts by converting Boundary\nRepresentation geometry (BREP) into CSG-based Python scripts. Second, we create\nannotations in natural language using GPT-4. The resulting dataset is used to\nfine-tune a code-generation LLM. The fine-tuned LLM can complete geometries\nbased on positional input and natural language in a plausible way,\ndemonstrating geometric understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent advancements in machine learning, such as LLMs, are\nrevolutionizing software development and creative industries, they have had\nminimal impact on engineers designing mechanical parts, which remains largely a\nmanual process. Existing approaches to generating 3D geometry most commonly use\nmeshes as a 3D representation. While meshes are suitable for assets in video\ngames or animations, they lack sufficient precision and adaptability for\nmechanical engineering purposes. This paper introduces a novel approach for the\ngeneration of 3D geometry that generates surface-based Constructive Solid\nGeometry (CSG) by leveraging a code-generation LLM. First, we create a dataset\nof 3D mechanical parts represented as code scripts by converting Boundary\nRepresentation geometry (BREP) into CSG-based Python scripts. Second, we create\nannotations in natural language using GPT-4. The resulting dataset is used to\nfine-tune a code-generation LLM. The fine-tuned LLM can complete geometries\nbased on positional input and natural language in a plausible way,\ndemonstrating geometric understanding."
                },
                "authors": [
                    {
                        "name": "Maximilian Mews"
                    },
                    {
                        "name": "Ansar Aynetdinov"
                    },
                    {
                        "name": "Vivian Schiller"
                    },
                    {
                        "name": "Peter Eisert"
                    },
                    {
                        "name": "Alan Akbik"
                    }
                ],
                "author_detail": {
                    "name": "Alan Akbik"
                },
                "author": "Alan Akbik",
                "arxiv_comment": "Accepted to the AI for Content Creation Workshop at CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03563v1",
                "updated": "2025-05-06T14:17:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    17,
                    30,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:17:30Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    17,
                    30,
                    1,
                    126,
                    0
                ],
                "title": "Say It Another Way: A Framework for User-Grounded Paraphrasing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Say It Another Way: A Framework for User-Grounded Paraphrasing"
                },
                "summary": "Small changes in how a prompt is worded can lead to meaningful differences in\nthe behavior of large language models (LLMs), raising concerns about the\nstability and reliability of their evaluations. While prior work has explored\nsimple formatting changes, these rarely capture the kinds of natural variation\nseen in real-world language use. We propose a controlled paraphrasing framework\nbased on a taxonomy of minimal linguistic transformations to systematically\ngenerate natural prompt variations. Using the BBQ dataset, we validate our\nmethod with both human annotations and automated checks, then use it to study\nhow LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our\nanalysis shows that even subtle prompt modifications can lead to substantial\nchanges in model behavior. These results highlight the need for robust,\nparaphrase-aware evaluation protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small changes in how a prompt is worded can lead to meaningful differences in\nthe behavior of large language models (LLMs), raising concerns about the\nstability and reliability of their evaluations. While prior work has explored\nsimple formatting changes, these rarely capture the kinds of natural variation\nseen in real-world language use. We propose a controlled paraphrasing framework\nbased on a taxonomy of minimal linguistic transformations to systematically\ngenerate natural prompt variations. Using the BBQ dataset, we validate our\nmethod with both human annotations and automated checks, then use it to study\nhow LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our\nanalysis shows that even subtle prompt modifications can lead to substantial\nchanges in model behavior. These results highlight the need for robust,\nparaphrase-aware evaluation protocols."
                },
                "authors": [
                    {
                        "name": "Cléa Chataigner"
                    },
                    {
                        "name": "Rebecca Ma"
                    },
                    {
                        "name": "Prakhar Ganesh"
                    },
                    {
                        "name": "Afaf Taïk"
                    },
                    {
                        "name": "Elliot Creager"
                    },
                    {
                        "name": "Golnoosh Farnadi"
                    }
                ],
                "author_detail": {
                    "name": "Golnoosh Farnadi"
                },
                "author": "Golnoosh Farnadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03562v1",
                "updated": "2025-05-06T14:13:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    13,
                    44,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:13:44Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    13,
                    44,
                    1,
                    126,
                    0
                ],
                "title": "Real-Time Person Image Synthesis Using a Flow Matching Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Person Image Synthesis Using a Flow Matching Model"
                },
                "summary": "Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images\nconditioned on a target pose and a source image. This task plays a key role in\nvarious real-world applications, such as sign language video generation, AR/VR,\ngaming, and live streaming. In these scenarios, real-time PGPIS is critical for\nproviding immediate visual feedback and maintaining user immersion.However,\nachieving real-time performance remains a significant challenge due to the\ncomplexity of synthesizing high-fidelity images from diverse and dynamic human\nposes. Recent diffusion-based methods have shown impressive image quality in\nPGPIS, but their slow sampling speeds hinder deployment in time-sensitive\napplications. This latency is particularly problematic in tasks like generating\nsign language videos during live broadcasts, where rapid image updates are\nrequired. Therefore, developing a fast and reliable PGPIS model is a crucial\nstep toward enabling real-time interactive systems. To address this challenge,\nwe propose a generative model based on flow matching (FM). Our approach enables\nfaster, more stable, and more efficient training and sampling. Furthermore, the\nproposed model supports conditional generation and can operate in latent space,\nmaking it especially suitable for real-time PGPIS applications where both speed\nand quality are critical. We evaluate our proposed method, Real-Time Person\nImage Synthesis Using a Flow Matching Model (RPFM), on the widely used\nDeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves\nnear-real-time sampling speeds while maintaining performance comparable to the\nstate-of-the-art models. Our methodology trades off a slight, acceptable\ndecrease in generated-image accuracy for over a twofold increase in generation\nspeed, thereby ensuring real-time performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images\nconditioned on a target pose and a source image. This task plays a key role in\nvarious real-world applications, such as sign language video generation, AR/VR,\ngaming, and live streaming. In these scenarios, real-time PGPIS is critical for\nproviding immediate visual feedback and maintaining user immersion.However,\nachieving real-time performance remains a significant challenge due to the\ncomplexity of synthesizing high-fidelity images from diverse and dynamic human\nposes. Recent diffusion-based methods have shown impressive image quality in\nPGPIS, but their slow sampling speeds hinder deployment in time-sensitive\napplications. This latency is particularly problematic in tasks like generating\nsign language videos during live broadcasts, where rapid image updates are\nrequired. Therefore, developing a fast and reliable PGPIS model is a crucial\nstep toward enabling real-time interactive systems. To address this challenge,\nwe propose a generative model based on flow matching (FM). Our approach enables\nfaster, more stable, and more efficient training and sampling. Furthermore, the\nproposed model supports conditional generation and can operate in latent space,\nmaking it especially suitable for real-time PGPIS applications where both speed\nand quality are critical. We evaluate our proposed method, Real-Time Person\nImage Synthesis Using a Flow Matching Model (RPFM), on the widely used\nDeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves\nnear-real-time sampling speeds while maintaining performance comparable to the\nstate-of-the-art models. Our methodology trades off a slight, acceptable\ndecrease in generated-image accuracy for over a twofold increase in generation\nspeed, thereby ensuring real-time performance."
                },
                "authors": [
                    {
                        "name": "Jiwoo Jeong"
                    },
                    {
                        "name": "Kirok Kim"
                    },
                    {
                        "name": "Wooju Kim"
                    },
                    {
                        "name": "Nam-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Nam-Joon Kim"
                },
                "author": "Nam-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03557v1",
                "updated": "2025-05-06T14:11:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    11,
                    2,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:11:02Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    11,
                    2,
                    1,
                    126,
                    0
                ],
                "title": "Generating Synthetic Data via Augmentations for Improved Facial\n  Resemblance in DreamBooth and InstantID",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Synthetic Data via Augmentations for Improved Facial\n  Resemblance in DreamBooth and InstantID"
                },
                "summary": "The personalization of Stable Diffusion for generating professional portraits\nfrom amateur photographs is a burgeoning area, with applications in various\ndownstream contexts. This paper investigates the impact of augmentations on\nimproving facial resemblance when using two prominent personalization\ntechniques: DreamBooth and InstantID. Through a series of experiments with\ndiverse subject datasets, we assessed the effectiveness of various augmentation\nstrategies on the generated headshots' fidelity to the original subject. We\nintroduce FaceDistance, a wrapper around FaceNet, to rank the generations based\non facial similarity, which aided in our assessment. Ultimately, this research\nprovides insights into the role of augmentations in enhancing facial\nresemblance in SDXL-generated portraits, informing strategies for their\neffective deployment in downstream applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The personalization of Stable Diffusion for generating professional portraits\nfrom amateur photographs is a burgeoning area, with applications in various\ndownstream contexts. This paper investigates the impact of augmentations on\nimproving facial resemblance when using two prominent personalization\ntechniques: DreamBooth and InstantID. Through a series of experiments with\ndiverse subject datasets, we assessed the effectiveness of various augmentation\nstrategies on the generated headshots' fidelity to the original subject. We\nintroduce FaceDistance, a wrapper around FaceNet, to rank the generations based\non facial similarity, which aided in our assessment. Ultimately, this research\nprovides insights into the role of augmentations in enhancing facial\nresemblance in SDXL-generated portraits, informing strategies for their\neffective deployment in downstream applications."
                },
                "authors": [
                    {
                        "name": "Koray Ulusan"
                    },
                    {
                        "name": "Benjamin Kiefer"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Kiefer"
                },
                "author": "Benjamin Kiefer",
                "arxiv_comment": "Accepted to CVPR 2025 Workshop \"Synthetic Data for Computer Vision\n  Workshop\", https://syndata4cv.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03556v1",
                "updated": "2025-05-06T14:09:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    9,
                    29,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:09:29Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    9,
                    29,
                    1,
                    126,
                    0
                ],
                "title": "A Comprehensive Survey of Large AI Models for Future Communications:\n  Foundations, Applications and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Large AI Models for Future Communications:\n  Foundations, Applications and Challenges"
                },
                "summary": "The 6G wireless communications aim to establish an intelligent world of\nubiquitous connectivity, providing an unprecedented communication experience.\nLarge artificial intelligence models (LAMs) are characterized by significantly\nlarger scales (e.g., billions or trillions of parameters) compared to typical\nartificial intelligence (AI) models. LAMs exhibit outstanding cognitive\nabilities, including strong generalization capabilities for fine-tuning to\ndownstream tasks, and emergent capabilities to handle tasks unseen during\ntraining. Therefore, LAMs efficiently provide AI services for diverse\ncommunication applications, making them crucial tools for addressing complex\nchallenges in future wireless communication systems. This study provides a\ncomprehensive review of the foundations, applications, and challenges of LAMs\nin communication. First, we introduce the current state of AI-based\ncommunication systems, emphasizing the motivation behind integrating LAMs into\ncommunications and summarizing the key contributions. We then present an\noverview of the essential concepts of LAMs in communication. This includes an\nintroduction to the main architectures of LAMs, such as transformer, diffusion\nmodels, and mamba. We also explore the classification of LAMs, including large\nlanguage models (LLMs), large vision models (LVMs), large multimodal models\n(LMMs), and world models, and examine their potential applications in\ncommunication. Additionally, we cover the training methods and evaluation\ntechniques for LAMs in communication systems. Lastly, we introduce optimization\nstrategies such as chain of thought (CoT), retrieval augmented generation\n(RAG), and agentic systems. Following this, we discuss the research\nadvancements of LAMs across various communication scenarios. Finally, we\nanalyze the challenges in the current research and provide insights into\npotential future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The 6G wireless communications aim to establish an intelligent world of\nubiquitous connectivity, providing an unprecedented communication experience.\nLarge artificial intelligence models (LAMs) are characterized by significantly\nlarger scales (e.g., billions or trillions of parameters) compared to typical\nartificial intelligence (AI) models. LAMs exhibit outstanding cognitive\nabilities, including strong generalization capabilities for fine-tuning to\ndownstream tasks, and emergent capabilities to handle tasks unseen during\ntraining. Therefore, LAMs efficiently provide AI services for diverse\ncommunication applications, making them crucial tools for addressing complex\nchallenges in future wireless communication systems. This study provides a\ncomprehensive review of the foundations, applications, and challenges of LAMs\nin communication. First, we introduce the current state of AI-based\ncommunication systems, emphasizing the motivation behind integrating LAMs into\ncommunications and summarizing the key contributions. We then present an\noverview of the essential concepts of LAMs in communication. This includes an\nintroduction to the main architectures of LAMs, such as transformer, diffusion\nmodels, and mamba. We also explore the classification of LAMs, including large\nlanguage models (LLMs), large vision models (LVMs), large multimodal models\n(LMMs), and world models, and examine their potential applications in\ncommunication. Additionally, we cover the training methods and evaluation\ntechniques for LAMs in communication systems. Lastly, we introduce optimization\nstrategies such as chain of thought (CoT), retrieval augmented generation\n(RAG), and agentic systems. Following this, we discuss the research\nadvancements of LAMs across various communication scenarios. Finally, we\nanalyze the challenges in the current research and provide insights into\npotential future research directions."
                },
                "authors": [
                    {
                        "name": "Feibo Jiang"
                    },
                    {
                        "name": "Cunhua Pan"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zhu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Han"
                },
                "author": "Zhu Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03553v1",
                "updated": "2025-05-06T14:05:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    5,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:05:12Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    5,
                    12,
                    1,
                    126,
                    0
                ],
                "title": "A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model\n  Reasoning"
                },
                "summary": "Inconsistent outputs and hallucinations from large language models (LLMs) are\nmajor obstacles to reliable AI systems. When different proprietary reasoning\nmodels (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,\nare given the same complex request, they often produce divergent results due to\nvariations in training and inference. This paper proposes a novel consensus\nmechanism, inspired by distributed ledger technology, to validate and converge\nthese outputs, treating each RM as a black-box peer. Building on the Hashgraph\nconsensus algorithm, our approach employs gossip-about-gossip communication and\nvirtual voting to achieve agreement among an ensemble of RMs. We present an\narchitectural design for a prototype system in which RMs iteratively exchange\nand update their answers, using information from each round to improve accuracy\nand confidence in subsequent rounds. This approach goes beyond simple majority\nvoting by incorporating the knowledge and cross-verification content of every\nmodel. We justify the feasibility of this Hashgraph-inspired consensus for AI\nensembles and outline its advantages over traditional ensembling techniques in\nreducing nonfactual outputs. Preliminary considerations for implementation,\nevaluation criteria for convergence and accuracy, and potential challenges are\ndiscussed. The proposed mechanism demonstrates a promising direction for\nmulti-agent AI systems to self-validate and deliver high-fidelity responses in\ncomplex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inconsistent outputs and hallucinations from large language models (LLMs) are\nmajor obstacles to reliable AI systems. When different proprietary reasoning\nmodels (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,\nare given the same complex request, they often produce divergent results due to\nvariations in training and inference. This paper proposes a novel consensus\nmechanism, inspired by distributed ledger technology, to validate and converge\nthese outputs, treating each RM as a black-box peer. Building on the Hashgraph\nconsensus algorithm, our approach employs gossip-about-gossip communication and\nvirtual voting to achieve agreement among an ensemble of RMs. We present an\narchitectural design for a prototype system in which RMs iteratively exchange\nand update their answers, using information from each round to improve accuracy\nand confidence in subsequent rounds. This approach goes beyond simple majority\nvoting by incorporating the knowledge and cross-verification content of every\nmodel. We justify the feasibility of this Hashgraph-inspired consensus for AI\nensembles and outline its advantages over traditional ensembling techniques in\nreducing nonfactual outputs. Preliminary considerations for implementation,\nevaluation criteria for convergence and accuracy, and potential challenges are\ndiscussed. The proposed mechanism demonstrates a promising direction for\nmulti-agent AI systems to self-validate and deliver high-fidelity responses in\ncomplex tasks."
                },
                "authors": [
                    {
                        "name": "Kolawole E. Ogunsina"
                    },
                    {
                        "name": "Morayo A. Ogunsina"
                    }
                ],
                "author_detail": {
                    "name": "Morayo A. Ogunsina"
                },
                "author": "Morayo A. Ogunsina",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03547v1",
                "updated": "2025-05-06T14:00:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    0,
                    41,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T14:00:41Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    14,
                    0,
                    41,
                    1,
                    126,
                    0
                ],
                "title": "STORY2GAME: Generating (Almost) Everything in an Interactive Fiction\n  Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STORY2GAME: Generating (Almost) Everything in an Interactive Fiction\n  Game"
                },
                "summary": "We introduce STORY2GAME, a novel approach to using Large Language Models to\ngenerate text-based interactive fiction games that starts by generating a\nstory, populates the world, and builds the code for actions in a game engine\nthat enables the story to play out interactively. Whereas a given set of\nhard-coded actions can artificially constrain story generation, the ability to\ngenerate actions means the story generation process can be more open-ended but\nstill allow for experiences that are grounded in a game state. The key to\nsuccessful action generation is to use LLM-generated preconditions and effects\nof actions in the stories as guides for what aspects of the game state must be\ntracked and changed by the game engine when a player performs an action. We\nalso introduce a technique for dynamically generating new actions to\naccommodate the player's desire to perform actions that they think of that are\nnot part of the story. Dynamic action generation may require on-the-fly updates\nto the game engine's state representation and revision of previously generated\nactions. We evaluate the success rate of action code generation with respect to\nwhether a player can interactively play through the entire generated story.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce STORY2GAME, a novel approach to using Large Language Models to\ngenerate text-based interactive fiction games that starts by generating a\nstory, populates the world, and builds the code for actions in a game engine\nthat enables the story to play out interactively. Whereas a given set of\nhard-coded actions can artificially constrain story generation, the ability to\ngenerate actions means the story generation process can be more open-ended but\nstill allow for experiences that are grounded in a game state. The key to\nsuccessful action generation is to use LLM-generated preconditions and effects\nof actions in the stories as guides for what aspects of the game state must be\ntracked and changed by the game engine when a player performs an action. We\nalso introduce a technique for dynamically generating new actions to\naccommodate the player's desire to perform actions that they think of that are\nnot part of the story. Dynamic action generation may require on-the-fly updates\nto the game engine's state representation and revision of previously generated\nactions. We evaluate the success rate of action code generation with respect to\nwhether a player can interactively play through the entire generated story."
                },
                "authors": [
                    {
                        "name": "Eric Zhou"
                    },
                    {
                        "name": "Shreyas Basavatia"
                    },
                    {
                        "name": "Moontashir Siam"
                    },
                    {
                        "name": "Zexin Chen"
                    },
                    {
                        "name": "Mark O. Riedl"
                    }
                ],
                "author_detail": {
                    "name": "Mark O. Riedl"
                },
                "author": "Mark O. Riedl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18991v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18991v2",
                "updated": "2025-05-06T13:47:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    47,
                    34,
                    1,
                    126,
                    0
                ],
                "published": "2025-03-23T16:40:29Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    16,
                    40,
                    29,
                    6,
                    82,
                    0
                ],
                "title": "HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective\n  Reasoning for LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective\n  Reasoning for LLM Alignment"
                },
                "summary": "The alignment of large language models (LLMs) with human values remains\ncritical yet hindered by four key challenges: (1) scarcity of balanced safety\ndatasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to\nshallow alignment, and (4) inability to dynamically adapt rewards according to\ntask difficulty. To address these limitations, we introduce HAIR\n(Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a\nnovel alignment approach inspired by shadow models in membership inference\nattacks. Our approach consists of two main components: (1) construction of a\nbalanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using\nstructured prompts that leverage the introspective reasoning capabilities of\nLLMs; and (2) training of category-specific reward models with Group Relative\nPolicy Optimization (GRPO), dynamically tuning optimization to task difficulty\nat both the data and model levels. Comprehensive experiments across four\nharmlessness and four usefulness benchmarks demonstrate that HAIR achieves\nstate-of-the-art performance, outperforming all baseline methods in safety\nwhile maintaining high levels of usefulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of large language models (LLMs) with human values remains\ncritical yet hindered by four key challenges: (1) scarcity of balanced safety\ndatasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to\nshallow alignment, and (4) inability to dynamically adapt rewards according to\ntask difficulty. To address these limitations, we introduce HAIR\n(Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a\nnovel alignment approach inspired by shadow models in membership inference\nattacks. Our approach consists of two main components: (1) construction of a\nbalanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using\nstructured prompts that leverage the introspective reasoning capabilities of\nLLMs; and (2) training of category-specific reward models with Group Relative\nPolicy Optimization (GRPO), dynamically tuning optimization to task difficulty\nat both the data and model levels. Comprehensive experiments across four\nharmlessness and four usefulness benchmarks demonstrate that HAIR achieves\nstate-of-the-art performance, outperforming all baseline methods in safety\nwhile maintaining high levels of usefulness."
                },
                "authors": [
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Haoxuan Ma"
                    },
                    {
                        "name": "Weixin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weixin Wang"
                },
                "author": "Weixin Wang",
                "arxiv_comment": "The three authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18991v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18991v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03531v1",
                "updated": "2025-05-06T13:41:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    41,
                    17,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T13:41:17Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    41,
                    17,
                    1,
                    126,
                    0
                ],
                "title": "Faster MoE LLM Inference for Extremely Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster MoE LLM Inference for Extremely Large Models"
                },
                "summary": "Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually\nbecoming the mainstream approach for ultra-large-scale models. Existing\noptimization efforts for MoE models have focused primarily on coarse-grained\nMoE architectures. With the emergence of DeepSeek Models, fine-grained MoE\nmodels are gaining popularity, yet research on them remains limited. Therefore,\nwe want to discuss the efficiency dynamic under different service loads.\nAdditionally, fine-grained models allow deployers to reduce the number of\nrouted experts, both activated counts and total counts, raising the question of\nhow this reduction affects the trade-off between MoE efficiency and\nperformance. Our findings indicate that while deploying MoE models presents\ngreater challenges, it also offers significant optimization opportunities.\nReducing the number of activated experts can lead to substantial efficiency\nimprovements in certain scenarios, with only minor performance degradation.\nReducing the total number of experts provides limited efficiency gains but\nresults in severe performance degradation. Our method can increase throughput\nby at least 10\\% without any performance degradation. Overall, we conclude that\nMoE inference optimization remains an area with substantial potential for\nexploration and improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually\nbecoming the mainstream approach for ultra-large-scale models. Existing\noptimization efforts for MoE models have focused primarily on coarse-grained\nMoE architectures. With the emergence of DeepSeek Models, fine-grained MoE\nmodels are gaining popularity, yet research on them remains limited. Therefore,\nwe want to discuss the efficiency dynamic under different service loads.\nAdditionally, fine-grained models allow deployers to reduce the number of\nrouted experts, both activated counts and total counts, raising the question of\nhow this reduction affects the trade-off between MoE efficiency and\nperformance. Our findings indicate that while deploying MoE models presents\ngreater challenges, it also offers significant optimization opportunities.\nReducing the number of activated experts can lead to substantial efficiency\nimprovements in certain scenarios, with only minor performance degradation.\nReducing the total number of experts provides limited efficiency gains but\nresults in severe performance degradation. Our method can increase throughput\nby at least 10\\% without any performance degradation. Overall, we conclude that\nMoE inference optimization remains an area with substantial potential for\nexploration and improvement."
                },
                "authors": [
                    {
                        "name": "Haoqi Yang"
                    },
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Qiwei Li"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Mengjia Shen"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03522v1",
                "updated": "2025-05-06T13:35:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    35,
                    59,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T13:35:59Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    35,
                    59,
                    1,
                    126,
                    0
                ],
                "title": "Optimization of Module Transferability in Single Image Super-Resolution:\n  Universality Assessment and Cycle Residual Blocks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization of Module Transferability in Single Image Super-Resolution:\n  Universality Assessment and Cycle Residual Blocks"
                },
                "summary": "Deep learning has substantially advanced the Single Image Super-Resolution\n(SISR). However, existing researches have predominantly focused on raw\nperformance gains, with little attention paid to quantifying the\ntransferability of architectural components. In this paper, we introduce the\nconcept of \"Universality\" and its associated definitions which extend the\ntraditional notion of \"Generalization\" to encompass the modules' ease of\ntransferability, thus revealing the relationships between module universality\nand model generalizability. Then we propose the Universality Assessment\nEquation (UAE), a metric for quantifying how readily a given module could be\ntransplanted across models. Guided by the UAE results of standard residual\nblocks and other plug-and-play modules, we further design two optimized\nmodules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).\nThrough comprehensive experiments on natural-scene benchmarks, remote-sensing\ndatasets, extreme-industrial imagery and on-device deployments, we demonstrate\nthat networks embedded with the proposed plug-and-play modules outperform\nseveral state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or\nenabling a 71.3% reduction in parameters with negligible loss in reconstruction\nfidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has substantially advanced the Single Image Super-Resolution\n(SISR). However, existing researches have predominantly focused on raw\nperformance gains, with little attention paid to quantifying the\ntransferability of architectural components. In this paper, we introduce the\nconcept of \"Universality\" and its associated definitions which extend the\ntraditional notion of \"Generalization\" to encompass the modules' ease of\ntransferability, thus revealing the relationships between module universality\nand model generalizability. Then we propose the Universality Assessment\nEquation (UAE), a metric for quantifying how readily a given module could be\ntransplanted across models. Guided by the UAE results of standard residual\nblocks and other plug-and-play modules, we further design two optimized\nmodules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).\nThrough comprehensive experiments on natural-scene benchmarks, remote-sensing\ndatasets, extreme-industrial imagery and on-device deployments, we demonstrate\nthat networks embedded with the proposed plug-and-play modules outperform\nseveral state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or\nenabling a 71.3% reduction in parameters with negligible loss in reconstruction\nfidelity."
                },
                "authors": [
                    {
                        "name": "Haotong Cheng"
                    },
                    {
                        "name": "Zhiqi Zhang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Xinshang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xinshang Zhang"
                },
                "author": "Xinshang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03513v1",
                "updated": "2025-05-06T13:22:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    22,
                    37,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T13:22:37Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    22,
                    37,
                    1,
                    126,
                    0
                ],
                "title": "Ruled by the Representation Space: On the University's Embrace of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ruled by the Representation Space: On the University's Embrace of Large\n  Language Models"
                },
                "summary": "This paper explores the implications of universities' rapid adoption of large\nlanguage models (LLMs) for studying, teaching, and research by analyzing the\nlogics underpinning their representation space. It argues that by uncritically\nadopting LLMs, the University surrenders its autonomy to a field of heteronomy,\nthat of generative AI, whose norms are not democratically shaped. Unlike\nearlier forms of rule-based AI, which sought to exclude human judgment and\ninterpretation, generative AI's new normative rationality is explicitly based\non the automation of moral judgment, valuation, and interpretation. By\nintegrating LLMs into pedagogical and research contexts before establishing a\ncritical framework for their use, the University subjects itself to being\ngoverned by contingent, ever-evolving, and domain-non-specific norms that\nstructure the model's virtual representation space and thus everything it\ngenerates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the implications of universities' rapid adoption of large\nlanguage models (LLMs) for studying, teaching, and research by analyzing the\nlogics underpinning their representation space. It argues that by uncritically\nadopting LLMs, the University surrenders its autonomy to a field of heteronomy,\nthat of generative AI, whose norms are not democratically shaped. Unlike\nearlier forms of rule-based AI, which sought to exclude human judgment and\ninterpretation, generative AI's new normative rationality is explicitly based\non the automation of moral judgment, valuation, and interpretation. By\nintegrating LLMs into pedagogical and research contexts before establishing a\ncritical framework for their use, the University subjects itself to being\ngoverned by contingent, ever-evolving, and domain-non-specific norms that\nstructure the model's virtual representation space and thus everything it\ngenerates."
                },
                "authors": [
                    {
                        "name": "Katia Schwerzmann"
                    }
                ],
                "author_detail": {
                    "name": "Katia Schwerzmann"
                },
                "author": "Katia Schwerzmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03501v1",
                "updated": "2025-05-06T13:07:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    7,
                    57,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T13:07:57Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    13,
                    7,
                    57,
                    1,
                    126,
                    0
                ],
                "title": "BadLingual: A Novel Lingual-Backdoor Attack against Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadLingual: A Novel Lingual-Backdoor Attack against Large Language\n  Models"
                },
                "summary": "In this paper, we present a new form of backdoor attack against Large\nLanguage Models (LLMs): lingual-backdoor attacks. The key novelty of\nlingual-backdoor attacks is that the language itself serves as the trigger to\nhijack the infected LLMs to generate inflammatory speech. They enable the\nprecise targeting of a specific language-speaking group, exacerbating racial\ndiscrimination by malicious entities. We first implement a baseline\nlingual-backdoor attack, which is carried out by poisoning a set of training\ndata for specific downstream tasks through translation into the trigger\nlanguage. However, this baseline attack suffers from poor task generalization\nand is impractical in real-world settings. To address this challenge, we design\nBadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any\ndownstream tasks within the chat LLMs, regardless of the specific questions of\nthese tasks. We design a new approach using PPL-constrained Greedy Coordinate\nGradient-based Search (PGCG) based adversarial training to expand the decision\nboundary of lingual-backdoor, thereby enhancing the generalization ability of\nlingual-backdoor across various tasks. We perform extensive experiments to\nvalidate the effectiveness of our proposed attacks. Specifically, the baseline\nattack achieves an ASR of over 90% on the specified tasks. However, its ASR\nreaches only 37.61% across six tasks in the task-agnostic scenario. In\ncontrast, BadLingual brings up to 37.35% improvement over the baseline. Our\nstudy sheds light on a new perspective of vulnerabilities in LLMs with\nmultilingual capabilities and is expected to promote future research on the\npotential defenses to enhance the LLMs' robustness",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a new form of backdoor attack against Large\nLanguage Models (LLMs): lingual-backdoor attacks. The key novelty of\nlingual-backdoor attacks is that the language itself serves as the trigger to\nhijack the infected LLMs to generate inflammatory speech. They enable the\nprecise targeting of a specific language-speaking group, exacerbating racial\ndiscrimination by malicious entities. We first implement a baseline\nlingual-backdoor attack, which is carried out by poisoning a set of training\ndata for specific downstream tasks through translation into the trigger\nlanguage. However, this baseline attack suffers from poor task generalization\nand is impractical in real-world settings. To address this challenge, we design\nBadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any\ndownstream tasks within the chat LLMs, regardless of the specific questions of\nthese tasks. We design a new approach using PPL-constrained Greedy Coordinate\nGradient-based Search (PGCG) based adversarial training to expand the decision\nboundary of lingual-backdoor, thereby enhancing the generalization ability of\nlingual-backdoor across various tasks. We perform extensive experiments to\nvalidate the effectiveness of our proposed attacks. Specifically, the baseline\nattack achieves an ASR of over 90% on the specified tasks. However, its ASR\nreaches only 37.61% across six tasks in the task-agnostic scenario. In\ncontrast, BadLingual brings up to 37.35% improvement over the baseline. Our\nstudy sheds light on a new perspective of vulnerabilities in LLMs with\nmultilingual capabilities and is expected to promote future research on the\npotential defenses to enhance the LLMs' robustness"
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Hongwei Li"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Wenbo Jiang"
                    },
                    {
                        "name": "Kangjie Chen"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Qingchuan Zhao"
                    },
                    {
                        "name": "Guowen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Guowen Xu"
                },
                "author": "Guowen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03492v1",
                "updated": "2025-05-06T12:48:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    48,
                    38,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T12:48:38Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    48,
                    38,
                    1,
                    126,
                    0
                ],
                "title": "Augmenting Human Cognition through Everyday AR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Human Cognition through Everyday AR"
                },
                "summary": "As spatial computing and multimodal LLMs mature, AR is tending to become an\nintuitive \"thinking tool,\" embedding semantic and context-aware intelligence\ndirectly into everyday environments. This paper explores how always-on AR can\nseamlessly bridge digital cognition and physical affordances, enabling\nproactive, context-sensitive interactions that enhance human task performance\nand understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As spatial computing and multimodal LLMs mature, AR is tending to become an\nintuitive \"thinking tool,\" embedding semantic and context-aware intelligence\ndirectly into everyday environments. This paper explores how always-on AR can\nseamlessly bridge digital cognition and physical affordances, enabling\nproactive, context-sensitive interactions that enhance human task performance\nand understanding."
                },
                "authors": [
                    {
                        "name": "Xiaoan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoan Liu"
                },
                "author": "Xiaoan Liu",
                "arxiv_comment": "3 pages, 4 figures. Position paper accepted to CHI'25 Workshop\n  'Everyday AR through AI-in-the-Loop'",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13754v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13754v3",
                "updated": "2025-05-06T12:45:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    45,
                    3,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-18T15:39:46Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    15,
                    39,
                    46,
                    4,
                    108,
                    0
                ],
                "title": "Towards Accurate and Interpretable Neuroblastoma Diagnosis via\n  Contrastive Multi-scale Pathological Image Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Accurate and Interpretable Neuroblastoma Diagnosis via\n  Contrastive Multi-scale Pathological Image Analysis"
                },
                "summary": "Neuroblastoma, adrenal-derived, is among the most common pediatric solid\nmalignancies, characterized by significant clinical heterogeneity. Timely and\naccurate pathological diagnosis from hematoxylin and eosin-stained whole-slide\nimages is critical for patient prognosis. However, current diagnostic practices\nprimarily rely on subjective manual examination by pathologists, leading to\ninconsistent accuracy. Existing automated whole-slide image classification\nmethods encounter challenges such as poor interpretability, limited feature\nextraction capabilities, and high computational costs, restricting their\npractical clinical deployment. To overcome these limitations, we propose\nCMSwinKAN, a contrastive-learning-based multi-scale feature fusion model\ntailored for pathological image classification, which enhances the Swin\nTransformer architecture by integrating a Kernel Activation Network within its\nmultilayer perceptron and classification head modules, significantly improving\nboth interpretability and accuracy. By fusing multi-scale features and\nleveraging contrastive learning strategies, CMSwinKAN mimics clinicians'\ncomprehensive approach, effectively capturing global and local tissue\ncharacteristics. Additionally, we introduce a heuristic soft voting mechanism\nguided by clinical insights to bridge patch-level predictions to whole-slide\nimage-level classifications seamlessly. We verified the CMSwinKAN on the\npublicly available BreakHis dataset and the PpNTs dataset, which was\nestablished by our hospital. Results demonstrate that CMSwinKAN performs better\nthan existing state-of-the-art pathology-specific models pre-trained on large\ndatasets. Our source code is available at\nhttps://github.com/JSLiam94/CMSwinKAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuroblastoma, adrenal-derived, is among the most common pediatric solid\nmalignancies, characterized by significant clinical heterogeneity. Timely and\naccurate pathological diagnosis from hematoxylin and eosin-stained whole-slide\nimages is critical for patient prognosis. However, current diagnostic practices\nprimarily rely on subjective manual examination by pathologists, leading to\ninconsistent accuracy. Existing automated whole-slide image classification\nmethods encounter challenges such as poor interpretability, limited feature\nextraction capabilities, and high computational costs, restricting their\npractical clinical deployment. To overcome these limitations, we propose\nCMSwinKAN, a contrastive-learning-based multi-scale feature fusion model\ntailored for pathological image classification, which enhances the Swin\nTransformer architecture by integrating a Kernel Activation Network within its\nmultilayer perceptron and classification head modules, significantly improving\nboth interpretability and accuracy. By fusing multi-scale features and\nleveraging contrastive learning strategies, CMSwinKAN mimics clinicians'\ncomprehensive approach, effectively capturing global and local tissue\ncharacteristics. Additionally, we introduce a heuristic soft voting mechanism\nguided by clinical insights to bridge patch-level predictions to whole-slide\nimage-level classifications seamlessly. We verified the CMSwinKAN on the\npublicly available BreakHis dataset and the PpNTs dataset, which was\nestablished by our hospital. Results demonstrate that CMSwinKAN performs better\nthan existing state-of-the-art pathology-specific models pre-trained on large\ndatasets. Our source code is available at\nhttps://github.com/JSLiam94/CMSwinKAN."
                },
                "authors": [
                    {
                        "name": "Zhu Zhu"
                    },
                    {
                        "name": "Shuo Jiang"
                    },
                    {
                        "name": "Jingyuan Zheng"
                    },
                    {
                        "name": "Yawen Li"
                    },
                    {
                        "name": "Yifei Chen"
                    },
                    {
                        "name": "Manli Zhao"
                    },
                    {
                        "name": "Weizhong Gu"
                    },
                    {
                        "name": "Feiwei Qin"
                    },
                    {
                        "name": "Jinhu Wang"
                    },
                    {
                        "name": "Gang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Gang Yu"
                },
                "author": "Gang Yu",
                "arxiv_comment": "10pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13754v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13754v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02138v2",
                "updated": "2025-05-06T12:35:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    35,
                    29,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-04T14:57:42Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    14,
                    57,
                    42,
                    6,
                    124,
                    0
                ],
                "title": "Efficient Multivariate Time Series Forecasting via Calibrated Language\n  Models with Privileged Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Multivariate Time Series Forecasting via Calibrated Language\n  Models with Privileged Knowledge Distillation"
                },
                "summary": "Multivariate time series forecasting (MTSF) endeavors to predict future\nobservations given historical data, playing a crucial role in time series data\nmanagement systems. With advancements in large language models (LLMs), recent\nstudies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF.\nHowever, the deployment of LLMs often suffers from low efficiency during the\ninference phase. To address this problem, we introduce TimeKD, an efficient\nMTSF framework that leverages the calibrated language models and privileged\nknowledge distillation. TimeKD aims to generate high-quality future\nrepresentations from the proposed cross-modality teacher model and cultivate an\neffective student model. The cross-modality teacher model adopts calibrated\nlanguage models (CLMs) with ground truth prompts, motivated by the paradigm of\nLearning Under Privileged Information (LUPI). In addition, we design a\nsubtractive cross attention (SCA) mechanism to refine these representations. To\ncultivate an effective student model, we propose an innovative privileged\nknowledge distillation (PKD) mechanism including correlation and feature\ndistillation. PKD enables the student to replicate the teacher's behavior while\nminimizing their output discrepancy. Extensive experiments on real data offer\ninsight into the effectiveness, efficiency, and scalability of the proposed\nTimeKD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time series forecasting (MTSF) endeavors to predict future\nobservations given historical data, playing a crucial role in time series data\nmanagement systems. With advancements in large language models (LLMs), recent\nstudies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF.\nHowever, the deployment of LLMs often suffers from low efficiency during the\ninference phase. To address this problem, we introduce TimeKD, an efficient\nMTSF framework that leverages the calibrated language models and privileged\nknowledge distillation. TimeKD aims to generate high-quality future\nrepresentations from the proposed cross-modality teacher model and cultivate an\neffective student model. The cross-modality teacher model adopts calibrated\nlanguage models (CLMs) with ground truth prompts, motivated by the paradigm of\nLearning Under Privileged Information (LUPI). In addition, we design a\nsubtractive cross attention (SCA) mechanism to refine these representations. To\ncultivate an effective student model, we propose an innovative privileged\nknowledge distillation (PKD) mechanism including correlation and feature\ndistillation. PKD enables the student to replicate the teacher's behavior while\nminimizing their output discrepancy. Extensive experiments on real data offer\ninsight into the effectiveness, efficiency, and scalability of the proposed\nTimeKD."
                },
                "authors": [
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Hao Miao"
                    },
                    {
                        "name": "Qianxiong Xu"
                    },
                    {
                        "name": "Shaowen Zhou"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Yan Zhao"
                    },
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Rui Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhao"
                },
                "author": "Rui Zhao",
                "arxiv_comment": "Accepted by ICDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03475v1",
                "updated": "2025-05-06T12:28:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    28,
                    50,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T12:28:50Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    28,
                    50,
                    1,
                    126,
                    0
                ],
                "title": "am-ELO: A Stable Framework for Arena-based LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "am-ELO: A Stable Framework for Arena-based LLM Evaluation"
                },
                "summary": "Arena-based evaluation is a fundamental yet significant evaluation paradigm\nfor modern AI models, especially large language models (LLMs). Existing\nframework based on ELO rating system suffers from the inevitable instability\nproblem due to ranking inconsistency and the lack of attention to the varying\nabilities of annotators. In this paper, we introduce a novel stable arena\nframework to address these issues by enhancing the ELO Rating System.\nSpecifically, we replace the iterative update method with a Maximum Likelihood\nEstimation (MLE) approach, m-ELO, and provide theoretical proof of the\nconsistency and stability of the MLE approach for model ranking. Additionally,\nwe proposed the am-ELO, which modify the Elo Rating's probability function to\nincorporate annotator abilities, enabling the simultaneous estimation of model\nscores and annotator reliability. Experiments demonstrate that this method\nensures stability, proving that this framework offers a more robust, accurate,\nand stable evaluation method for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arena-based evaluation is a fundamental yet significant evaluation paradigm\nfor modern AI models, especially large language models (LLMs). Existing\nframework based on ELO rating system suffers from the inevitable instability\nproblem due to ranking inconsistency and the lack of attention to the varying\nabilities of annotators. In this paper, we introduce a novel stable arena\nframework to address these issues by enhancing the ELO Rating System.\nSpecifically, we replace the iterative update method with a Maximum Likelihood\nEstimation (MLE) approach, m-ELO, and provide theoretical proof of the\nconsistency and stability of the MLE approach for model ranking. Additionally,\nwe proposed the am-ELO, which modify the Elo Rating's probability function to\nincorporate annotator abilities, enabling the simultaneous estimation of model\nscores and annotator reliability. Experiments demonstrate that this method\nensures stability, proving that this framework offers a more robust, accurate,\nand stable evaluation method for LLMs."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Yan Zhuang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Shuanghong Shen"
                    },
                    {
                        "name": "Jie Ouyang"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Shijin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shijin Wang"
                },
                "author": "Shijin Wang",
                "arxiv_comment": "ICML2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03473v1",
                "updated": "2025-05-06T12:25:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    25,
                    15,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T12:25:15Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    25,
                    15,
                    1,
                    126,
                    0
                ],
                "title": "Evaluation of LLMs on Long-tail Entity Linking in Historical Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of LLMs on Long-tail Entity Linking in Historical Documents"
                },
                "summary": "Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)\napplications, enabling the disambiguation of entity mentions by linking them to\ntheir corresponding entries in a reference knowledge base (KB). Thanks to their\ndeep contextual understanding capabilities, LLMs offer a new perspective to\ntackle EL, promising better results than traditional methods. Despite the\nimpressive generalization capabilities of LLMs, linking less popular, long-tail\nentities remains challenging as these entities are often underrepresented in\ntraining data and knowledge bases. Furthermore, the long-tail EL task is an\nunderstudied problem, and limited studies address it with LLMs. In the present\nwork, we assess the performance of two popular LLMs, GPT and LLama3, in a\nlong-tail entity linking scenario. Using MHERCL v0.1, a manually annotated\nbenchmark of sentences from domain-specific historical texts, we quantitatively\ncompare the performance of LLMs in identifying and linking entities to their\ncorresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity\nLinking and Relation Extraction framework. Our preliminary experiments reveal\nthat LLMs perform encouragingly well in long-tail EL, indicating that this\ntechnology can be a valuable adjunct in filling the gap between head and\nlong-tail EL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)\napplications, enabling the disambiguation of entity mentions by linking them to\ntheir corresponding entries in a reference knowledge base (KB). Thanks to their\ndeep contextual understanding capabilities, LLMs offer a new perspective to\ntackle EL, promising better results than traditional methods. Despite the\nimpressive generalization capabilities of LLMs, linking less popular, long-tail\nentities remains challenging as these entities are often underrepresented in\ntraining data and knowledge bases. Furthermore, the long-tail EL task is an\nunderstudied problem, and limited studies address it with LLMs. In the present\nwork, we assess the performance of two popular LLMs, GPT and LLama3, in a\nlong-tail entity linking scenario. Using MHERCL v0.1, a manually annotated\nbenchmark of sentences from domain-specific historical texts, we quantitatively\ncompare the performance of LLMs in identifying and linking entities to their\ncorresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity\nLinking and Relation Extraction framework. Our preliminary experiments reveal\nthat LLMs perform encouragingly well in long-tail EL, indicating that this\ntechnology can be a valuable adjunct in filling the gap between head and\nlong-tail EL."
                },
                "authors": [
                    {
                        "name": "Marta Boscariol"
                    },
                    {
                        "name": "Luana Bulla"
                    },
                    {
                        "name": "Lia Draetta"
                    },
                    {
                        "name": "Beatrice Fiumanò"
                    },
                    {
                        "name": "Emanuele Lenzi"
                    },
                    {
                        "name": "Leonardo Piano"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Piano"
                },
                "author": "Leonardo Piano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18116v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18116v3",
                "updated": "2025-05-06T12:24:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    24,
                    43,
                    1,
                    126,
                    0
                ],
                "published": "2024-12-24T02:54:56Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    54,
                    56,
                    1,
                    359,
                    0
                ],
                "title": "AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation"
                },
                "summary": "Large language models (LLMs) have brought exciting new advances to mobile UI\nagents, a long-standing research field that aims to complete arbitrary natural\nlanguage tasks through mobile UI interactions. However, existing UI agents\nusually demand powerful large language models that are difficult to be deployed\nlocally on end-users' devices, raising huge concerns about user privacy and\ncentralized serving cost. Inspired by the remarkable coding abilities of recent\nsmall language models (SLMs), we propose to convert the UI task automation\nproblem to a code generation problem, which can be effectively solved by an\non-device SLM and efficiently executed with an on-device code interpreter.\nUnlike normal coding tasks that can be extensively pre-trained with public\ndatasets, generating UI automation code is challenging due to the diversity,\ncomplexity, and variability of target apps. Therefore, we adopt a\ndocument-centered approach that automatically builds fine-grained API\ndocumentation for each app and generates diverse task samples based on this\ndocumentation. By guiding the agent with the synthetic documents and task\nsamples, it learns to generate precise and efficient scripts to complete unseen\ntasks. Based on detailed comparisons with state-of-the-art mobile UI agents,\nour approach effectively improves the mobile task automation with significantly\nhigher success rates and lower latency/token consumption. Code is open-sourced\nat https://github.com/MobileLLM/AutoDroid-V2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have brought exciting new advances to mobile UI\nagents, a long-standing research field that aims to complete arbitrary natural\nlanguage tasks through mobile UI interactions. However, existing UI agents\nusually demand powerful large language models that are difficult to be deployed\nlocally on end-users' devices, raising huge concerns about user privacy and\ncentralized serving cost. Inspired by the remarkable coding abilities of recent\nsmall language models (SLMs), we propose to convert the UI task automation\nproblem to a code generation problem, which can be effectively solved by an\non-device SLM and efficiently executed with an on-device code interpreter.\nUnlike normal coding tasks that can be extensively pre-trained with public\ndatasets, generating UI automation code is challenging due to the diversity,\ncomplexity, and variability of target apps. Therefore, we adopt a\ndocument-centered approach that automatically builds fine-grained API\ndocumentation for each app and generates diverse task samples based on this\ndocumentation. By guiding the agent with the synthetic documents and task\nsamples, it learns to generate precise and efficient scripts to complete unseen\ntasks. Based on detailed comparisons with state-of-the-art mobile UI agents,\nour approach effectively improves the mobile task automation with significantly\nhigher success rates and lower latency/token consumption. Code is open-sourced\nat https://github.com/MobileLLM/AutoDroid-V2."
                },
                "authors": [
                    {
                        "name": "Hao Wen"
                    },
                    {
                        "name": "Shizuo Tian"
                    },
                    {
                        "name": "Borislav Pavlov"
                    },
                    {
                        "name": "Wenjie Du"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Ge Chang"
                    },
                    {
                        "name": "Shanhui Zhao"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Yuanchun Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Li"
                },
                "author": "Yuanchun Li",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18116v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18116v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03472v1",
                "updated": "2025-05-06T12:23:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    23,
                    18,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T12:23:18Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    23,
                    18,
                    1,
                    126,
                    0
                ],
                "title": "Simulation to Reality: Testbeds and Architectures for Connected and\n  Automated Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation to Reality: Testbeds and Architectures for Connected and\n  Automated Vehicles"
                },
                "summary": "Ensuring the safe and efficient operation of CAVs relies heavily on the\nsoftware framework used. A software framework needs to ensure real-time\nproperties, reliable communication, and efficient resource utilization.\nFurthermore, a software framework needs to enable seamless transition between\ntesting stages, from simulation to small-scale to full-scale experiments. In\nthis paper, we survey prominent software frameworks used for in-vehicle and\ninter-vehicle communication in CAVs. We analyze these frameworks regarding\nopportunities and challenges, such as their real-time properties and\ntransitioning capabilities. Additionally, we delve into the tooling\nrequirements necessary for addressing the associated challenges. We illustrate\nthe practical implications of these challenges through case studies focusing on\ncritical areas such as perception, motion planning, and control. Furthermore,\nwe identify research gaps in the field, highlighting areas where further\ninvestigation is needed to advance the development and deployment of safe and\nefficient CAV systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safe and efficient operation of CAVs relies heavily on the\nsoftware framework used. A software framework needs to ensure real-time\nproperties, reliable communication, and efficient resource utilization.\nFurthermore, a software framework needs to enable seamless transition between\ntesting stages, from simulation to small-scale to full-scale experiments. In\nthis paper, we survey prominent software frameworks used for in-vehicle and\ninter-vehicle communication in CAVs. We analyze these frameworks regarding\nopportunities and challenges, such as their real-time properties and\ntransitioning capabilities. Additionally, we delve into the tooling\nrequirements necessary for addressing the associated challenges. We illustrate\nthe practical implications of these challenges through case studies focusing on\ncritical areas such as perception, motion planning, and control. Furthermore,\nwe identify research gaps in the field, highlighting areas where further\ninvestigation is needed to advance the development and deployment of safe and\nefficient CAV systems."
                },
                "authors": [
                    {
                        "name": "David Klüner"
                    },
                    {
                        "name": "Simon Schäfer"
                    },
                    {
                        "name": "Lucas Hegerath"
                    },
                    {
                        "name": "Jianye Xu"
                    },
                    {
                        "name": "Julius Kahle"
                    },
                    {
                        "name": "Hazem Ibrahim"
                    },
                    {
                        "name": "Alexandru Kampmann"
                    },
                    {
                        "name": "Bassam Alrifaee"
                    }
                ],
                "author_detail": {
                    "name": "Bassam Alrifaee"
                },
                "author": "Bassam Alrifaee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04430v3",
                "updated": "2025-05-06T12:19:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    19,
                    55,
                    1,
                    126,
                    0
                ],
                "published": "2024-08-08T12:57:14Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    12,
                    57,
                    14,
                    3,
                    221,
                    0
                ],
                "title": "The Struggles of LLMs in Cross-lingual Code Clone Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Struggles of LLMs in Cross-lingual Code Clone Detection"
                },
                "summary": "With the involvement of multiple programming languages in modern software\ndevelopment, cross-lingual code clone detection has gained traction within the\nsoftware engineering community. Numerous studies have explored this topic,\nproposing various promising approaches. Inspired by the significant advances in\nmachine learning in recent years, particularly Large Language Models (LLMs),\nwhich have demonstrated their ability to tackle various tasks, this paper\nrevisits cross-lingual code clone detection. We evaluate the performance of\nfive (05) LLMs and eight prompts (08) for the identification of cross-lingual\ncode clones. Additionally, we compare these results against two baseline\nmethods. Finally, we evaluate a pre-trained embedding model to assess the\neffectiveness of the generated representations for classifying clone and\nnon-clone pairs. The studies involving LLMs and Embedding models are evaluated\nusing two widely used cross-lingual datasets, XLCoST and CodeNet. Our results\nshow that LLMs can achieve high F1 scores, up to 0.99, for straightforward\nprogramming examples. However, they not only perform less well on programs\nassociated with complex programming challenges but also do not necessarily\nunderstand the meaning of \"code clones\" in a cross-lingual setting. We show\nthat embedding models used to represent code fragments from different\nprogramming languages in the same representation space enable the training of a\nbasic classifier that outperforms all LLMs by ~1 and ~20 percentage points on\nthe XLCoST and CodeNet datasets, respectively. This finding suggests that,\ndespite the apparent capabilities of LLMs, embeddings provided by embedding\nmodels offer suitable representations to achieve state-of-the-art performance\nin cross-lingual code clone detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the involvement of multiple programming languages in modern software\ndevelopment, cross-lingual code clone detection has gained traction within the\nsoftware engineering community. Numerous studies have explored this topic,\nproposing various promising approaches. Inspired by the significant advances in\nmachine learning in recent years, particularly Large Language Models (LLMs),\nwhich have demonstrated their ability to tackle various tasks, this paper\nrevisits cross-lingual code clone detection. We evaluate the performance of\nfive (05) LLMs and eight prompts (08) for the identification of cross-lingual\ncode clones. Additionally, we compare these results against two baseline\nmethods. Finally, we evaluate a pre-trained embedding model to assess the\neffectiveness of the generated representations for classifying clone and\nnon-clone pairs. The studies involving LLMs and Embedding models are evaluated\nusing two widely used cross-lingual datasets, XLCoST and CodeNet. Our results\nshow that LLMs can achieve high F1 scores, up to 0.99, for straightforward\nprogramming examples. However, they not only perform less well on programs\nassociated with complex programming challenges but also do not necessarily\nunderstand the meaning of \"code clones\" in a cross-lingual setting. We show\nthat embedding models used to represent code fragments from different\nprogramming languages in the same representation space enable the training of a\nbasic classifier that outperforms all LLMs by ~1 and ~20 percentage points on\nthe XLCoST and CodeNet datasets, respectively. This finding suggests that,\ndespite the apparent capabilities of LLMs, embeddings provided by embedding\nmodels offer suitable representations to achieve state-of-the-art performance\nin cross-lingual code clone detection."
                },
                "authors": [
                    {
                        "name": "Micheline Bénédicte Moumoula"
                    },
                    {
                        "name": "Abdoul Kader Kabore"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawendé Bissyande"
                    }
                ],
                "author_detail": {
                    "name": "Tegawendé Bissyande"
                },
                "author": "Tegawendé Bissyande",
                "arxiv_doi": "10.1145/3715764",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715764",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.04430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication at the ACM International Conference on the\n  Foundations of Software Engineering (FSE) 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03467v1",
                "updated": "2025-05-06T12:12:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    12,
                    48,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T12:12:48Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    12,
                    48,
                    1,
                    126,
                    0
                ],
                "title": "Uncertainty-Aware Large Language Models for Explainable Disease\n  Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Large Language Models for Explainable Disease\n  Diagnosis"
                },
                "summary": "Explainable disease diagnosis, which leverages patient information (e.g.,\nsigns and symptoms) and computational models to generate probable diagnoses and\nreasonings, offers clear clinical values. However, when clinical notes\nencompass insufficient evidence for a definite diagnosis, such as the absence\nof definitive symptoms, diagnostic uncertainty usually arises, increasing the\nrisk of misdiagnosis and adverse outcomes. Although explicitly identifying and\nexplaining diagnostic uncertainties is essential for trustworthy diagnostic\nsystems, it remains under-explored. To fill this gap, we introduce ConfiDx, an\nuncertainty-aware large language model (LLM) created by fine-tuning open-source\nLLMs with diagnostic criteria. We formalized the task and assembled richly\nannotated datasets that capture varying degrees of diagnostic ambiguity.\nEvaluating ConfiDx on real-world datasets demonstrated that it excelled in\nidentifying diagnostic uncertainties, achieving superior diagnostic\nperformance, and generating trustworthy explanations for diagnoses and\nuncertainties. To our knowledge, this is the first study to jointly address\ndiagnostic uncertainty recognition and explanation, substantially enhancing the\nreliability of automatic diagnostic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable disease diagnosis, which leverages patient information (e.g.,\nsigns and symptoms) and computational models to generate probable diagnoses and\nreasonings, offers clear clinical values. However, when clinical notes\nencompass insufficient evidence for a definite diagnosis, such as the absence\nof definitive symptoms, diagnostic uncertainty usually arises, increasing the\nrisk of misdiagnosis and adverse outcomes. Although explicitly identifying and\nexplaining diagnostic uncertainties is essential for trustworthy diagnostic\nsystems, it remains under-explored. To fill this gap, we introduce ConfiDx, an\nuncertainty-aware large language model (LLM) created by fine-tuning open-source\nLLMs with diagnostic criteria. We formalized the task and assembled richly\nannotated datasets that capture varying degrees of diagnostic ambiguity.\nEvaluating ConfiDx on real-world datasets demonstrated that it excelled in\nidentifying diagnostic uncertainties, achieving superior diagnostic\nperformance, and generating trustworthy explanations for diagnoses and\nuncertainties. To our knowledge, this is the first study to jointly address\ndiagnostic uncertainty recognition and explanation, substantially enhancing the\nreliability of automatic diagnostic systems."
                },
                "authors": [
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Jiashuo Wang"
                    },
                    {
                        "name": "Zidu Xu"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "David Brauer"
                    },
                    {
                        "name": "Lindsay Welton"
                    },
                    {
                        "name": "Jacob Cogan"
                    },
                    {
                        "name": "Yuen-Hei Chung"
                    },
                    {
                        "name": "Lei Tian"
                    },
                    {
                        "name": "Zaifu Zhan"
                    },
                    {
                        "name": "Yu Hou"
                    },
                    {
                        "name": "Mingquan Lin"
                    },
                    {
                        "name": "Genevieve B. Melton"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "arxiv_comment": "22 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03460v1",
                "updated": "2025-05-06T12:00:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    0,
                    49,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T12:00:49Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    12,
                    0,
                    49,
                    1,
                    126,
                    0
                ],
                "title": "LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal\n  Delivery Based on Agentic UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal\n  Delivery Based on Agentic UAVs"
                },
                "summary": "The growing demand for intelligent logistics, particularly fine-grained\nterminal delivery, underscores the need for autonomous UAV (Unmanned Aerial\nVehicle)-based delivery systems. However, most existing last-mile delivery\nstudies rely on ground robots, while current UAV-based Vision-Language\nNavigation (VLN) tasks primarily focus on coarse-grained, long-range goals,\nmaking them unsuitable for precise terminal delivery. To bridge this gap, we\npropose LogisticsVLN, a scalable aerial delivery system built on multimodal\nlarge language models (MLLMs) for autonomous terminal delivery. LogisticsVLN\nintegrates lightweight Large Language Models (LLMs) and Visual-Language Models\n(VLMs) in a modular pipeline for request understanding, floor localization,\nobject detection, and action-decision making. To support research and\nevaluation in this new setting, we construct the Vision-Language Delivery (VLD)\ndataset within the CARLA simulator. Experimental results on the VLD dataset\nshowcase the feasibility of the LogisticsVLN system. In addition, we conduct\nsubtask-level evaluations of each module of our system, offering valuable\ninsights for improving the robustness and real-world deployment of foundation\nmodel-based vision-language delivery systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for intelligent logistics, particularly fine-grained\nterminal delivery, underscores the need for autonomous UAV (Unmanned Aerial\nVehicle)-based delivery systems. However, most existing last-mile delivery\nstudies rely on ground robots, while current UAV-based Vision-Language\nNavigation (VLN) tasks primarily focus on coarse-grained, long-range goals,\nmaking them unsuitable for precise terminal delivery. To bridge this gap, we\npropose LogisticsVLN, a scalable aerial delivery system built on multimodal\nlarge language models (MLLMs) for autonomous terminal delivery. LogisticsVLN\nintegrates lightweight Large Language Models (LLMs) and Visual-Language Models\n(VLMs) in a modular pipeline for request understanding, floor localization,\nobject detection, and action-decision making. To support research and\nevaluation in this new setting, we construct the Vision-Language Delivery (VLD)\ndataset within the CARLA simulator. Experimental results on the VLD dataset\nshowcase the feasibility of the LogisticsVLN system. In addition, we conduct\nsubtask-level evaluations of each module of our system, offering valuable\ninsights for improving the robustness and real-world deployment of foundation\nmodel-based vision-language delivery systems."
                },
                "authors": [
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Yonglin Tian"
                    },
                    {
                        "name": "Fei Lin"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Kornélia Sára Szatmáry"
                    },
                    {
                        "name": "Fei-Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei-Yue Wang"
                },
                "author": "Fei-Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03455v1",
                "updated": "2025-05-06T11:52:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    52,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T11:52:12Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    52,
                    12,
                    1,
                    126,
                    0
                ],
                "title": "Mitigating Backdoor Triggered and Targeted Data Poisoning Attacks in\n  Voice Authentication Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Backdoor Triggered and Targeted Data Poisoning Attacks in\n  Voice Authentication Systems"
                },
                "summary": "Voice authentication systems remain susceptible to two major threats:\nbackdoor triggered attacks and targeted data poisoning attacks. This dual\nvulnerability is critical because conventional solutions typically address each\nthreat type separately, leaving systems exposed to adversaries who can exploit\nboth attacks simultaneously. We propose a unified defense framework that\neffectively addresses both BTA and TDPA. Our framework integrates a frequency\nfocused detection mechanism that flags covert pitch boosting and sound masking\nbackdoor attacks in near real time, followed by a convolutional neural network\nthat addresses TDPA. This dual layered defense approach utilizes\nmultidimensional acoustic features to isolate anomalous signals without\nrequiring costly model retraining. In particular, our PBSM detection mechanism\ncan seamlessly integrate into existing voice authentication pipelines and scale\neffectively for large scale deployments. Experimental results on benchmark\ndatasets and their compression with the state of the art algorithm demonstrate\nthat our PBSM detection mechanism outperforms the state of the art. Our\nframework reduces attack success rates to as low as five to fifteen percent\nwhile maintaining a recall rate of up to ninety five percent in recognizing\nTDPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voice authentication systems remain susceptible to two major threats:\nbackdoor triggered attacks and targeted data poisoning attacks. This dual\nvulnerability is critical because conventional solutions typically address each\nthreat type separately, leaving systems exposed to adversaries who can exploit\nboth attacks simultaneously. We propose a unified defense framework that\neffectively addresses both BTA and TDPA. Our framework integrates a frequency\nfocused detection mechanism that flags covert pitch boosting and sound masking\nbackdoor attacks in near real time, followed by a convolutional neural network\nthat addresses TDPA. This dual layered defense approach utilizes\nmultidimensional acoustic features to isolate anomalous signals without\nrequiring costly model retraining. In particular, our PBSM detection mechanism\ncan seamlessly integrate into existing voice authentication pipelines and scale\neffectively for large scale deployments. Experimental results on benchmark\ndatasets and their compression with the state of the art algorithm demonstrate\nthat our PBSM detection mechanism outperforms the state of the art. Our\nframework reduces attack success rates to as low as five to fifteen percent\nwhile maintaining a recall rate of up to ninety five percent in recognizing\nTDPA."
                },
                "authors": [
                    {
                        "name": "Alireza Mohammadi"
                    },
                    {
                        "name": "Keshav Sood"
                    },
                    {
                        "name": "Dhananjay Thiruvady"
                    },
                    {
                        "name": "Asef Nazari"
                    }
                ],
                "author_detail": {
                    "name": "Asef Nazari"
                },
                "author": "Asef Nazari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02560v2",
                "updated": "2025-05-06T11:44:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    44,
                    32,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T11:02:31Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    2,
                    31,
                    0,
                    125,
                    0
                ],
                "title": "Evaluating Contrastive Feedback for Effective User Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Contrastive Feedback for Effective User Simulations"
                },
                "summary": "The use of Large Language Models (LLMs) for simulating user behavior in the\ndomain of Interactive Information Retrieval has recently gained significant\npopularity. However, their application and capabilities remain highly debated\nand understudied. This study explores whether the underlying principles of\ncontrastive training techniques, which have been effective for fine-tuning\nLLMs, can also be applied beneficially in the area of prompt engineering for\nuser simulations.\n  Previous research has shown that LLMs possess comprehensive world knowledge,\nwhich can be leveraged to provide accurate estimates of relevant documents.\nThis study attempts to simulate a knowledge state by enhancing the model with\nadditional implicit contextual information gained during the simulation. This\napproach enables the model to refine the scope of desired documents further.\nThe primary objective of this study is to analyze how different modalities of\ncontextual information influence the effectiveness of user simulations.\n  Various user configurations were tested, where models are provided with\nsummaries of already judged relevant, irrelevant, or both types of documents in\na contrastive manner. The focus of this study is the assessment of the impact\nof the prompting techniques on the simulated user agent performance. We hereby\nlay the foundations for leveraging LLMs as part of more realistic simulated\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) for simulating user behavior in the\ndomain of Interactive Information Retrieval has recently gained significant\npopularity. However, their application and capabilities remain highly debated\nand understudied. This study explores whether the underlying principles of\ncontrastive training techniques, which have been effective for fine-tuning\nLLMs, can also be applied beneficially in the area of prompt engineering for\nuser simulations.\n  Previous research has shown that LLMs possess comprehensive world knowledge,\nwhich can be leveraged to provide accurate estimates of relevant documents.\nThis study attempts to simulate a knowledge state by enhancing the model with\nadditional implicit contextual information gained during the simulation. This\napproach enables the model to refine the scope of desired documents further.\nThe primary objective of this study is to analyze how different modalities of\ncontextual information influence the effectiveness of user simulations.\n  Various user configurations were tested, where models are provided with\nsummaries of already judged relevant, irrelevant, or both types of documents in\na contrastive manner. The focus of this study is the assessment of the impact\nof the prompting techniques on the simulated user agent performance. We hereby\nlay the foundations for leveraging LLMs as part of more realistic simulated\nusers."
                },
                "authors": [
                    {
                        "name": "Andreas Konstantin Kruff"
                    },
                    {
                        "name": "Timo Breuer"
                    },
                    {
                        "name": "Philipp Schaer"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Schaer"
                },
                "author": "Philipp Schaer",
                "arxiv_doi": "10.1145/3726302.3730189",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730189",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.02560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18135v2",
                "updated": "2025-05-06T11:41:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    41,
                    37,
                    1,
                    126,
                    0
                ],
                "published": "2024-12-24T03:43:15Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    3,
                    43,
                    15,
                    1,
                    359,
                    0
                ],
                "title": "LSAQ: Layer-Specific Adaptive Quantization for Large Language Model\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSAQ: Layer-Specific Adaptive Quantization for Large Language Model\n  Deployment"
                },
                "summary": "As Large Language Models (LLMs) demonstrate exceptional performance across\nvarious domains, deploying LLMs on edge devices has emerged as a new trend.\nQuantization techniques, which reduce the size and memory requirements of LLMs,\nare effective for deploying LLMs on resource-limited edge devices. However,\nexisting one-size-fits-all quantization methods often fail to dynamically\nadjust the memory requirements of LLMs, limiting their applications to\npractical edge devices with various computation resources. To tackle this\nissue, we propose Layer-Specific Adaptive Quantization (LSAQ), a system for\nadaptive quantization and dynamic deployment of LLMs based on layer importance.\nSpecifically, LSAQ evaluates the importance of LLMs' neural layers by\nconstructing top-k token sets from the inputs and outputs of each layer and\ncalculating their Jaccard similarity. Based on layer importance, our system\nadaptively adjusts quantization strategies in real time according to the\ncomputation resource of edge devices, which applies higher quantization\nprecision to layers with higher importance, and vice versa. {Experimental\nresults show that LSAQ consistently outperforms the selected quantization\nbaselines in terms of perplexity and zero-shot tasks. Additionally, it can\ndevise appropriate quantization schemes for different usage scenarios to\nfacilitate the deployment of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) demonstrate exceptional performance across\nvarious domains, deploying LLMs on edge devices has emerged as a new trend.\nQuantization techniques, which reduce the size and memory requirements of LLMs,\nare effective for deploying LLMs on resource-limited edge devices. However,\nexisting one-size-fits-all quantization methods often fail to dynamically\nadjust the memory requirements of LLMs, limiting their applications to\npractical edge devices with various computation resources. To tackle this\nissue, we propose Layer-Specific Adaptive Quantization (LSAQ), a system for\nadaptive quantization and dynamic deployment of LLMs based on layer importance.\nSpecifically, LSAQ evaluates the importance of LLMs' neural layers by\nconstructing top-k token sets from the inputs and outputs of each layer and\ncalculating their Jaccard similarity. Based on layer importance, our system\nadaptively adjusts quantization strategies in real time according to the\ncomputation resource of edge devices, which applies higher quantization\nprecision to layers with higher importance, and vice versa. {Experimental\nresults show that LSAQ consistently outperforms the selected quantization\nbaselines in terms of perplexity and zero-shot tasks. Additionally, it can\ndevise appropriate quantization schemes for different usage scenarios to\nfacilitate the deployment of LLMs."
                },
                "authors": [
                    {
                        "name": "Binrui Zeng"
                    },
                    {
                        "name": "Bin Ji"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Jie Yu"
                    },
                    {
                        "name": "Shasha Li"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Shangwen Wang"
                    },
                    {
                        "name": "Xinran Hong"
                    },
                    {
                        "name": "Yongtao Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yongtao Tang"
                },
                "author": "Yongtao Tang",
                "arxiv_comment": "8 pages, 4 figures, accepted to IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13551v3",
                "updated": "2025-05-06T11:38:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    38,
                    24,
                    1,
                    126,
                    0
                ],
                "published": "2025-03-16T15:18:40Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    15,
                    18,
                    40,
                    6,
                    75,
                    0
                ],
                "title": "Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in\n  Large Language Models"
                },
                "summary": "Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate step.\nIn addition, the cost of annotating reasoning processes for reward modeling is\nhigh, making large-scale collection of high-quality data challenging. To\naddress this, we propose a novel reward model approach called the Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps at both fine-grained and coarse-grained levels. HRM excels at assessing\nmulti-step reasoning coherence, especially when flawed steps are later\ncorrected through self-reflection. To further reduce the cost of generating\ntraining data, we introduce a lightweight and effective data augmentation\nstrategy called Hierarchical Node Compression (HNC), which merges two\nconsecutive reasoning steps into one within the tree structure. By applying HNC\nto MCTS-generated reasoning trajectories, we enhance the diversity and\nrobustness of HRM training data while introducing controlled noise with minimal\ncomputational overhead. Empirical results on the PRM800K dataset show that HRM,\ntogether with HNC, provides more stable and reliable evaluations than PRM.\nFurthermore, cross-domain evaluations on the MATH500 and GSM8K datasets\ndemonstrate HRM's strong generalization and robustness across a variety of\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate step.\nIn addition, the cost of annotating reasoning processes for reward modeling is\nhigh, making large-scale collection of high-quality data challenging. To\naddress this, we propose a novel reward model approach called the Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps at both fine-grained and coarse-grained levels. HRM excels at assessing\nmulti-step reasoning coherence, especially when flawed steps are later\ncorrected through self-reflection. To further reduce the cost of generating\ntraining data, we introduce a lightweight and effective data augmentation\nstrategy called Hierarchical Node Compression (HNC), which merges two\nconsecutive reasoning steps into one within the tree structure. By applying HNC\nto MCTS-generated reasoning trajectories, we enhance the diversity and\nrobustness of HRM training data while introducing controlled noise with minimal\ncomputational overhead. Empirical results on the PRM800K dataset show that HRM,\ntogether with HNC, provides more stable and reliable evaluations than PRM.\nFurthermore, cross-domain evaluations on the MATH500 and GSM8K datasets\ndemonstrate HRM's strong generalization and robustness across a variety of\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Zhangyi Jiang"
                    },
                    {
                        "name": "Zhenqi He"
                    },
                    {
                        "name": "Shenyang Tong"
                    },
                    {
                        "name": "Wenhan Yang"
                    },
                    {
                        "name": "Yanan Zheng"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Zifan He"
                    },
                    {
                        "name": "Hailei Gong"
                    }
                ],
                "author_detail": {
                    "name": "Hailei Gong"
                },
                "author": "Hailei Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03442v1",
                "updated": "2025-05-06T11:28:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    28,
                    28,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T11:28:28Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    28,
                    28,
                    1,
                    126,
                    0
                ],
                "title": "Knowledge Distillation for Speech Denoising by Latent Representation\n  Alignment with Cosine Distance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation for Speech Denoising by Latent Representation\n  Alignment with Cosine Distance"
                },
                "summary": "Speech denoising is a generally adopted and impactful task, appearing in many\ncommon and everyday-life use cases. Although there are very powerful methods\npublished, most of those are too complex for deployment in everyday and\nlow-resources computational environments, like hand-held devices, intelligent\nglasses, hearing aids, etc. Knowledge distillation (KD) is a prominent way for\nalleviating this complexity mismatch and is based on the\ntransferring/distilling of knowledge from a pre-trained complex model, the\nteacher, to another less complex one, the student. Existing KD methods for\nspeech denoising are based on processes that potentially hamper the KD by\nbounding the learning of the student to the distribution, information ordering,\nand feature dimensionality learned by the teacher. In this paper, we present\nand assess a method that tries to treat this issue, by exploiting the\nwell-known denoising-autoencoder framework, the linear inverted bottlenecks,\nand the properties of the cosine similarity. We use a public dataset and\nconduct repeated experiments with different mismatching scenarios between the\nteacher and the student, reporting the mean and standard deviation of the\nmetrics of our method and another, state-of-the-art method that is used as a\nbaseline. Our results show that with the proposed method, the student can\nperform better and can also retain greater mismatching conditions compared to\nthe teacher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech denoising is a generally adopted and impactful task, appearing in many\ncommon and everyday-life use cases. Although there are very powerful methods\npublished, most of those are too complex for deployment in everyday and\nlow-resources computational environments, like hand-held devices, intelligent\nglasses, hearing aids, etc. Knowledge distillation (KD) is a prominent way for\nalleviating this complexity mismatch and is based on the\ntransferring/distilling of knowledge from a pre-trained complex model, the\nteacher, to another less complex one, the student. Existing KD methods for\nspeech denoising are based on processes that potentially hamper the KD by\nbounding the learning of the student to the distribution, information ordering,\nand feature dimensionality learned by the teacher. In this paper, we present\nand assess a method that tries to treat this issue, by exploiting the\nwell-known denoising-autoencoder framework, the linear inverted bottlenecks,\nand the properties of the cosine similarity. We use a public dataset and\nconduct repeated experiments with different mismatching scenarios between the\nteacher and the student, reporting the mean and standard deviation of the\nmetrics of our method and another, state-of-the-art method that is used as a\nbaseline. Our results show that with the proposed method, the student can\nperform better and can also retain greater mismatching conditions compared to\nthe teacher."
                },
                "authors": [
                    {
                        "name": "Diep Luong"
                    },
                    {
                        "name": "Mikko Heikkinen"
                    },
                    {
                        "name": "Konstantinos Drossos"
                    },
                    {
                        "name": "Tuomas Virtanen"
                    }
                ],
                "author_detail": {
                    "name": "Tuomas Virtanen"
                },
                "author": "Tuomas Virtanen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03439v1",
                "updated": "2025-05-06T11:25:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    25,
                    52,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T11:25:52Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    25,
                    52,
                    1,
                    126,
                    0
                ],
                "title": "The Steganographic Potentials of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Steganographic Potentials of Language Models"
                },
                "summary": "The potential for large language models (LLMs) to hide messages within plain\ntext (steganography) poses a challenge to detection and thwarting of unaligned\nAI agents, and undermines faithfulness of LLMs reasoning. We explore the\nsteganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)\nto: (1) develop covert encoding schemes, (2) engage in steganography when\nprompted, and (3) utilize steganography in realistic scenarios where hidden\nreasoning is likely, but not prompted. In these scenarios, we detect the\nintention of LLMs to hide their reasoning as well as their steganography\nperformance. Our findings in the fine-tuning experiments as well as in\nbehavioral non fine-tuning evaluations reveal that while current models exhibit\nrudimentary steganographic abilities in terms of security and capacity,\nexplicit algorithmic guidance markedly enhances their capacity for information\nconcealment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The potential for large language models (LLMs) to hide messages within plain\ntext (steganography) poses a challenge to detection and thwarting of unaligned\nAI agents, and undermines faithfulness of LLMs reasoning. We explore the\nsteganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)\nto: (1) develop covert encoding schemes, (2) engage in steganography when\nprompted, and (3) utilize steganography in realistic scenarios where hidden\nreasoning is likely, but not prompted. In these scenarios, we detect the\nintention of LLMs to hide their reasoning as well as their steganography\nperformance. Our findings in the fine-tuning experiments as well as in\nbehavioral non fine-tuning evaluations reveal that while current models exhibit\nrudimentary steganographic abilities in terms of security and capacity,\nexplicit algorithmic guidance markedly enhances their capacity for information\nconcealment."
                },
                "authors": [
                    {
                        "name": "Artem Karpov"
                    },
                    {
                        "name": "Tinuade Adeleke"
                    },
                    {
                        "name": "Seong Hah Cho"
                    },
                    {
                        "name": "Natalia Perez-Campanero"
                    }
                ],
                "author_detail": {
                    "name": "Natalia Perez-Campanero"
                },
                "author": "Natalia Perez-Campanero",
                "arxiv_comment": "Published at Building Trust Workshop at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03434v1",
                "updated": "2025-05-06T11:18:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    18,
                    34,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T11:18:34Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    18,
                    34,
                    1,
                    126,
                    0
                ],
                "title": "Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in\n  LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in\n  LLM-Based Agents"
                },
                "summary": "Large Language Models (LLMs) represent a landmark achievement in Artificial\nIntelligence (AI), demonstrating unprecedented proficiency in procedural tasks\nsuch as text generation, code completion, and conversational coherence. These\ncapabilities stem from their architecture, which mirrors human procedural\nmemory -- the brain's ability to automate repetitive, pattern-driven tasks\nthrough practice. However, as LLMs are increasingly deployed in real-world\napplications, it becomes impossible to ignore their limitations operating in\ncomplex, unpredictable environments. This paper argues that LLMs, while\ntransformative, are fundamentally constrained by their reliance on procedural\nmemory. To create agents capable of navigating ``wicked'' learning environments\n-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must\naugment LLMs with semantic memory and associative learning systems. By adopting\na modular architecture that decouples these cognitive functions, we can bridge\nthe gap between narrow procedural expertise and the adaptive intelligence\nrequired for real-world problem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a landmark achievement in Artificial\nIntelligence (AI), demonstrating unprecedented proficiency in procedural tasks\nsuch as text generation, code completion, and conversational coherence. These\ncapabilities stem from their architecture, which mirrors human procedural\nmemory -- the brain's ability to automate repetitive, pattern-driven tasks\nthrough practice. However, as LLMs are increasingly deployed in real-world\napplications, it becomes impossible to ignore their limitations operating in\ncomplex, unpredictable environments. This paper argues that LLMs, while\ntransformative, are fundamentally constrained by their reliance on procedural\nmemory. To create agents capable of navigating ``wicked'' learning environments\n-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must\naugment LLMs with semantic memory and associative learning systems. By adopting\na modular architecture that decouples these cognitive functions, we can bridge\nthe gap between narrow procedural expertise and the adaptive intelligence\nrequired for real-world problem-solving."
                },
                "authors": [
                    {
                        "name": "Schaun Wheeler"
                    },
                    {
                        "name": "Olivier Jeunen"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Jeunen"
                },
                "author": "Olivier Jeunen",
                "arxiv_doi": "10.1145/3708319.3734172",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708319.3734172",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.03434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to the workshop on Hybrid AI for Human-Centric\n  Personalization (HyPer), co-located with ACM UMAP '25",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14196v2",
                "updated": "2025-05-06T11:13:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    13,
                    28,
                    1,
                    126,
                    0
                ],
                "published": "2024-09-21T16:53:56Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    16,
                    53,
                    56,
                    5,
                    265,
                    0
                ],
                "title": "Adversarial and Reactive Traffic Entities for Behavior-Realistic Driving\n  Simulation: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial and Reactive Traffic Entities for Behavior-Realistic Driving\n  Simulation: A Review"
                },
                "summary": "Despite advancements in perception and planning for autonomous vehicles\n(AVs), validating their performance remains a significant challenge. The\ndeployment of planning algorithms in real-world environments is often\nineffective due to discrepancies between simulations and real traffic\nconditions. Evaluating AVs planning algorithms in simulation typically involves\nreplaying driving logs from recorded real-world traffic. However, entities\nreplayed from offline data are not reactive, lack the ability to respond to\narbitrary AV behavior, and cannot behave in an adversarial manner to test\ncertain properties of the driving policy. Therefore, simulation with realistic\nand potentially adversarial entities represents a critical task for AV planning\nsoftware validation. In this work, we aim to review current research efforts in\nthe field of traffic simulation, focusing on the application of advanced\ntechniques for modeling realistic and adversarial behaviors of traffic\nentities. The objective of this work is to categorize existing approaches based\non the proposed classes of traffic entity behavior and scenario behavior\ncontrol. Moreover, we collect traffic datasets and examine existing traffic\nsimulations with respect to their employed default traffic entities. Finally,\nwe identify challenges and open questions that hold potential for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advancements in perception and planning for autonomous vehicles\n(AVs), validating their performance remains a significant challenge. The\ndeployment of planning algorithms in real-world environments is often\nineffective due to discrepancies between simulations and real traffic\nconditions. Evaluating AVs planning algorithms in simulation typically involves\nreplaying driving logs from recorded real-world traffic. However, entities\nreplayed from offline data are not reactive, lack the ability to respond to\narbitrary AV behavior, and cannot behave in an adversarial manner to test\ncertain properties of the driving policy. Therefore, simulation with realistic\nand potentially adversarial entities represents a critical task for AV planning\nsoftware validation. In this work, we aim to review current research efforts in\nthe field of traffic simulation, focusing on the application of advanced\ntechniques for modeling realistic and adversarial behaviors of traffic\nentities. The objective of this work is to categorize existing approaches based\non the proposed classes of traffic entity behavior and scenario behavior\ncontrol. Moreover, we collect traffic datasets and examine existing traffic\nsimulations with respect to their employed default traffic entities. Finally,\nwe identify challenges and open questions that hold potential for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Joshua Ransiek"
                    },
                    {
                        "name": "Philipp Reis"
                    },
                    {
                        "name": "Tobias Schürmann"
                    },
                    {
                        "name": "Eric Sax"
                    }
                ],
                "author_detail": {
                    "name": "Eric Sax"
                },
                "author": "Eric Sax",
                "arxiv_comment": "Submitted to the IEEE for possible publication, 8 pages, 1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03427v1",
                "updated": "2025-05-06T11:07:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    7,
                    26,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T11:07:26Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    7,
                    26,
                    1,
                    126,
                    0
                ],
                "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare."
                },
                "authors": [
                    {
                        "name": "Mouath Abu Daoud"
                    },
                    {
                        "name": "Chaimae Abouzahir"
                    },
                    {
                        "name": "Leen Kharouf"
                    },
                    {
                        "name": "Walid Al-Eisawi"
                    },
                    {
                        "name": "Nizar Habash"
                    },
                    {
                        "name": "Farah E. Shamout"
                    }
                ],
                "author_detail": {
                    "name": "Farah E. Shamout"
                },
                "author": "Farah E. Shamout",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03425v1",
                "updated": "2025-05-06T11:04:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    4,
                    7,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T11:04:07Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    4,
                    7,
                    1,
                    126,
                    0
                ],
                "title": "Directed Greybox Fuzzing via Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed Greybox Fuzzing via Large Language Model"
                },
                "summary": "Directed greybox fuzzing (DGF) focuses on efficiently reaching specific\nprogram locations or triggering particular behaviors, making it essential for\ntasks like vulnerability detection and crash reproduction. However, existing\nmethods often suffer from path explosion and randomness in input mutation,\nleading to inefficiencies in exploring and exploiting target paths. In this\npaper, we propose HGFuzzer, an automatic framework that leverages the large\nlanguage model (LLM) to address these challenges. HGFuzzer transforms path\nconstraint problems into targeted code generation tasks, systematically\ngenerating test harnesses and reachable inputs to reduce unnecessary\nexploration paths significantly. Additionally, we implement custom mutators\ndesigned specifically for target functions, minimizing randomness and improving\nthe precision of directed fuzzing. We evaluated HGFuzzer on 20 real-world\nvulnerabilities, successfully triggering 17, including 11 within the first\nminute, achieving a speedup of at least 24.8x compared to state-of-the-art\ndirected fuzzers. Furthermore, HGFuzzer discovered 9 previously unknown\nvulnerabilities, all of which were assigned CVE IDs, demonstrating the\neffectiveness of our approach in identifying real-world vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed greybox fuzzing (DGF) focuses on efficiently reaching specific\nprogram locations or triggering particular behaviors, making it essential for\ntasks like vulnerability detection and crash reproduction. However, existing\nmethods often suffer from path explosion and randomness in input mutation,\nleading to inefficiencies in exploring and exploiting target paths. In this\npaper, we propose HGFuzzer, an automatic framework that leverages the large\nlanguage model (LLM) to address these challenges. HGFuzzer transforms path\nconstraint problems into targeted code generation tasks, systematically\ngenerating test harnesses and reachable inputs to reduce unnecessary\nexploration paths significantly. Additionally, we implement custom mutators\ndesigned specifically for target functions, minimizing randomness and improving\nthe precision of directed fuzzing. We evaluated HGFuzzer on 20 real-world\nvulnerabilities, successfully triggering 17, including 11 within the first\nminute, achieving a speedup of at least 24.8x compared to state-of-the-art\ndirected fuzzers. Furthermore, HGFuzzer discovered 9 previously unknown\nvulnerabilities, all of which were assigned CVE IDs, demonstrating the\neffectiveness of our approach in identifying real-world vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Hanxiang Xu"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10148v2",
                "updated": "2025-05-06T11:03:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    11,
                    3,
                    22,
                    1,
                    126,
                    0
                ],
                "published": "2025-02-14T13:23:18Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    23,
                    18,
                    4,
                    45,
                    0
                ],
                "title": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis"
                },
                "summary": "Despite much progress in training distributed artificial intelligence (AI),\nbuilding cooperative multi-agent systems with multi-agent reinforcement\nlearning (MARL) faces challenges in sample efficiency, interpretability, and\ntransferability. Unlike traditional learning-based methods that require\nextensive interaction with the environment, large language models (LLMs)\ndemonstrate remarkable capabilities in zero-shot planning and complex\nreasoning. However, existing LLM-based approaches heavily rely on text-based\nobservations and struggle with the non-Markovian nature of multi-agent\ninteractions under partial observability. We present COMPASS, a novel\nmulti-agent architecture that integrates vision-language models (VLMs) with a\ndynamic skill library and structured communication for decentralized\nclosed-loop decision-making. The skill library, bootstrapped from\ndemonstrations, evolves via planner-guided tasks to enable adaptive strategies.\nCOMPASS propagates entity information through multi-hop communication under\npartial observability. Evaluations on the improved StarCraft Multi-Agent\nChallenge (SMACv2) demonstrate COMPASS's strong performance against\nstate-of-the-art MARL baselines across both symmetric and asymmetric scenarios.\nNotably, in the symmetric Protoss 5v5 task, COMPASS achieved a 57\\% win rate,\nrepresenting a 30 percentage point advantage over QMIX (27\\%). Project page can\nbe found at https://stellar-entremet-1720bb.netlify.app/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite much progress in training distributed artificial intelligence (AI),\nbuilding cooperative multi-agent systems with multi-agent reinforcement\nlearning (MARL) faces challenges in sample efficiency, interpretability, and\ntransferability. Unlike traditional learning-based methods that require\nextensive interaction with the environment, large language models (LLMs)\ndemonstrate remarkable capabilities in zero-shot planning and complex\nreasoning. However, existing LLM-based approaches heavily rely on text-based\nobservations and struggle with the non-Markovian nature of multi-agent\ninteractions under partial observability. We present COMPASS, a novel\nmulti-agent architecture that integrates vision-language models (VLMs) with a\ndynamic skill library and structured communication for decentralized\nclosed-loop decision-making. The skill library, bootstrapped from\ndemonstrations, evolves via planner-guided tasks to enable adaptive strategies.\nCOMPASS propagates entity information through multi-hop communication under\npartial observability. Evaluations on the improved StarCraft Multi-Agent\nChallenge (SMACv2) demonstrate COMPASS's strong performance against\nstate-of-the-art MARL baselines across both symmetric and asymmetric scenarios.\nNotably, in the symmetric Protoss 5v5 task, COMPASS achieved a 57\\% win rate,\nrepresenting a 30 percentage point advantage over QMIX (27\\%). Project page can\nbe found at https://stellar-entremet-1720bb.netlify.app/."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Wenshuai Zhao"
                    },
                    {
                        "name": "Joni Pajarinen"
                    }
                ],
                "author_detail": {
                    "name": "Joni Pajarinen"
                },
                "author": "Joni Pajarinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03418v1",
                "updated": "2025-05-06T10:53:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    53,
                    58,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T10:53:58Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    53,
                    58,
                    1,
                    126,
                    0
                ],
                "title": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Augmented Complex Problem Solving with Large Language Models:\n  A Survey"
                },
                "summary": "Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification."
                },
                "authors": [
                    {
                        "name": "Da Zheng"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Junwei Su"
                    },
                    {
                        "name": "Yuchen Tian"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Jintian Zhang"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03406v1",
                "updated": "2025-05-06T10:31:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    31,
                    54,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T10:31:54Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    31,
                    54,
                    1,
                    126,
                    0
                ],
                "title": "Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs\n  and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs\n  and Retrieval-Augmented Generation"
                },
                "summary": "This research paper investigates the application of Large Language Models\n(LLMs) in healthcare, specifically focusing on enhancing medical decision\nsupport through Retrieval-Augmented Generation (RAG) integrated with\nhospital-specific data and fine-tuning using Quantized Low-Rank Adaptation\n(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By\nembedding and retrieving context-relevant healthcare information, the system\nsignificantly improves response accuracy. QLoRA facilitates notable parameter\nefficiency and memory optimization, preserving the integrity of medical\ninformation through specialized quantization techniques. Our research also\nshows that our model performs relatively well on various medical benchmarks,\nindicating that it can be used to make basic medical suggestions. This paper\ndetails the system's technical components, including its architecture,\nquantization methods, and key healthcare applications such as enhanced disease\nprediction from patient symptoms and medical history, treatment suggestions,\nand efficient summarization of complex medical reports. We touch on the ethical\nconsiderations-patient privacy, data security, and the need for rigorous\nclinical validation-as well as the practical challenges of integrating such\nsystems into real-world healthcare workflows. Furthermore, the lightweight\nquantized weights ensure scalability and ease of deployment even in\nlow-resource hospital environments. Finally, the paper concludes with an\nanalysis of the broader impact of LLMs on healthcare and outlines future\ndirections for LLMs in medical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research paper investigates the application of Large Language Models\n(LLMs) in healthcare, specifically focusing on enhancing medical decision\nsupport through Retrieval-Augmented Generation (RAG) integrated with\nhospital-specific data and fine-tuning using Quantized Low-Rank Adaptation\n(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By\nembedding and retrieving context-relevant healthcare information, the system\nsignificantly improves response accuracy. QLoRA facilitates notable parameter\nefficiency and memory optimization, preserving the integrity of medical\ninformation through specialized quantization techniques. Our research also\nshows that our model performs relatively well on various medical benchmarks,\nindicating that it can be used to make basic medical suggestions. This paper\ndetails the system's technical components, including its architecture,\nquantization methods, and key healthcare applications such as enhanced disease\nprediction from patient symptoms and medical history, treatment suggestions,\nand efficient summarization of complex medical reports. We touch on the ethical\nconsiderations-patient privacy, data security, and the need for rigorous\nclinical validation-as well as the practical challenges of integrating such\nsystems into real-world healthcare workflows. Furthermore, the lightweight\nquantized weights ensure scalability and ease of deployment even in\nlow-resource hospital environments. Finally, the paper concludes with an\nanalysis of the broader impact of LLMs on healthcare and outlines future\ndirections for LLMs in medical settings."
                },
                "authors": [
                    {
                        "name": "Mohammad Shoaib Ansari"
                    },
                    {
                        "name": "Mohd Sohail Ali Khan"
                    },
                    {
                        "name": "Shubham Revankar"
                    },
                    {
                        "name": "Aditya Varma"
                    },
                    {
                        "name": "Anil S. Mokhade"
                    }
                ],
                "author_detail": {
                    "name": "Anil S. Mokhade"
                },
                "author": "Anil S. Mokhade",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03392v1",
                "updated": "2025-05-06T10:15:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    15,
                    5,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T10:15:05Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    10,
                    15,
                    5,
                    1,
                    126,
                    0
                ],
                "title": "Automatic Calibration for Membership Inference Attack on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Calibration for Membership Inference Attack on Large Language\n  Models"
                },
                "summary": "Membership Inference Attacks (MIAs) have recently been employed to determine\nwhether a specific text was part of the pre-training data of Large Language\nModels (LLMs). However, existing methods often misinfer non-members as members,\nleading to a high false positive rate, or depend on additional reference models\nfor probability calibration, which limits their practicality. To overcome these\nchallenges, we introduce a novel framework called Automatic Calibration\nMembership Inference Attack (ACMIA), which utilizes a tunable temperature to\ncalibrate output probabilities effectively. This approach is inspired by our\ntheoretical insights into maximum likelihood estimation during the pre-training\nof LLMs. We introduce ACMIA in three configurations designed to accommodate\ndifferent levels of model access and increase the probability gap between\nmembers and non-members, improving the reliability and robustness of membership\ninference. Extensive experiments on various open-source LLMs demonstrate that\nour proposed attack is highly effective, robust, and generalizable, surpassing\nstate-of-the-art baselines across three widely used benchmarks. Our code is\navailable at:\n\\href{https://github.com/Salehzz/ACMIA}{\\textcolor{blue}{Github}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks (MIAs) have recently been employed to determine\nwhether a specific text was part of the pre-training data of Large Language\nModels (LLMs). However, existing methods often misinfer non-members as members,\nleading to a high false positive rate, or depend on additional reference models\nfor probability calibration, which limits their practicality. To overcome these\nchallenges, we introduce a novel framework called Automatic Calibration\nMembership Inference Attack (ACMIA), which utilizes a tunable temperature to\ncalibrate output probabilities effectively. This approach is inspired by our\ntheoretical insights into maximum likelihood estimation during the pre-training\nof LLMs. We introduce ACMIA in three configurations designed to accommodate\ndifferent levels of model access and increase the probability gap between\nmembers and non-members, improving the reliability and robustness of membership\ninference. Extensive experiments on various open-source LLMs demonstrate that\nour proposed attack is highly effective, robust, and generalizable, surpassing\nstate-of-the-art baselines across three widely used benchmarks. Our code is\navailable at:\n\\href{https://github.com/Salehzz/ACMIA}{\\textcolor{blue}{Github}}."
                },
                "authors": [
                    {
                        "name": "Saleh Zare Zade"
                    },
                    {
                        "name": "Yao Qiang"
                    },
                    {
                        "name": "Xiangyu Zhou"
                    },
                    {
                        "name": "Hui Zhu"
                    },
                    {
                        "name": "Mohammad Amin Roshani"
                    },
                    {
                        "name": "Prashant Khanduri"
                    },
                    {
                        "name": "Dongxiao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiao Zhu"
                },
                "author": "Dongxiao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03373v1",
                "updated": "2025-05-06T09:47:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    47,
                    53,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:47:53Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    47,
                    53,
                    1,
                    126,
                    0
                ],
                "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty\n  Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAP: Structured Pruning via Alternating Optimization and Penalty\n  Methods"
                },
                "summary": "The deployment of large language models (LLMs) is often constrained by their\nsubstantial computational and memory demands. While structured pruning presents\na viable approach by eliminating entire network components, existing methods\nsuffer from performance degradation, reliance on heuristic metrics, or\nexpensive finetuning. To address these challenges, we propose SPAP (Structured\nPruning via Alternating Optimization and Penalty Methods), a novel and\nefficient structured pruning framework for LLMs grounded in optimization\ntheory. SPAP formulates the pruning problem through a mixed-integer\noptimization model, employs a penalty method that effectively makes pruning\ndecisions to minimize pruning errors, and introduces an alternating\nminimization algorithm tailored to the splittable problem structure for\nefficient weight updates and performance recovery. Extensive experiments on\nOPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over\nstate-of-the-art methods, delivering linear inference speedups (1.29$\\times$ at\n30% sparsity) and proportional memory reductions. Our work offers a practical,\noptimization-driven solution for pruning LLMs while preserving model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often constrained by their\nsubstantial computational and memory demands. While structured pruning presents\na viable approach by eliminating entire network components, existing methods\nsuffer from performance degradation, reliance on heuristic metrics, or\nexpensive finetuning. To address these challenges, we propose SPAP (Structured\nPruning via Alternating Optimization and Penalty Methods), a novel and\nefficient structured pruning framework for LLMs grounded in optimization\ntheory. SPAP formulates the pruning problem through a mixed-integer\noptimization model, employs a penalty method that effectively makes pruning\ndecisions to minimize pruning errors, and introduces an alternating\nminimization algorithm tailored to the splittable problem structure for\nefficient weight updates and performance recovery. Extensive experiments on\nOPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over\nstate-of-the-art methods, delivering linear inference speedups (1.29$\\times$ at\n30% sparsity) and proportional memory reductions. Our work offers a practical,\noptimization-driven solution for pruning LLMs while preserving model\nperformance."
                },
                "authors": [
                    {
                        "name": "Hanyu Hu"
                    },
                    {
                        "name": "Xiaoming Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Yuan"
                },
                "author": "Xiaoming Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03369v1",
                "updated": "2025-05-06T09:40:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    40,
                    47,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:40:47Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    40,
                    47,
                    1,
                    126,
                    0
                ],
                "title": "Validating the Effectiveness of a Large Language Model-based Approach\n  for Identifying Children's Development across Various Free Play Settings in\n  Kindergarten",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validating the Effectiveness of a Large Language Model-based Approach\n  for Identifying Children's Development across Various Free Play Settings in\n  Kindergarten"
                },
                "summary": "Free play is a fundamental aspect of early childhood education, supporting\nchildren's cognitive, social, emotional, and motor development. However,\nassessing children's development during free play poses significant challenges\ndue to the unstructured and spontaneous nature of the activity. Traditional\nassessment methods often rely on direct observations by teachers, parents, or\nresearchers, which may fail to capture comprehensive insights from free play\nand provide timely feedback to educators. This study proposes an innovative\napproach combining Large Language Models (LLMs) with learning analytics to\nanalyze children's self-narratives of their play experiences. The LLM\nidentifies developmental abilities, while performance scores across different\nplay settings are calculated using learning analytics techniques. We collected\n2,224 play narratives from 29 children in a kindergarten, covering four\ndistinct play areas over one semester. According to the evaluation results from\neight professionals, the LLM-based approach achieved high accuracy in\nidentifying cognitive, motor, and social abilities, with accuracy exceeding 90%\nin most domains. Moreover, significant differences in developmental outcomes\nwere observed across play settings, highlighting each area's unique\ncontributions to specific abilities. These findings confirm that the proposed\napproach is effective in identifying children's development across various free\nplay settings. This study demonstrates the potential of integrating LLMs and\nlearning analytics to provide child-centered insights into developmental\ntrajectories, offering educators valuable data to support personalized learning\nand enhance early childhood education practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free play is a fundamental aspect of early childhood education, supporting\nchildren's cognitive, social, emotional, and motor development. However,\nassessing children's development during free play poses significant challenges\ndue to the unstructured and spontaneous nature of the activity. Traditional\nassessment methods often rely on direct observations by teachers, parents, or\nresearchers, which may fail to capture comprehensive insights from free play\nand provide timely feedback to educators. This study proposes an innovative\napproach combining Large Language Models (LLMs) with learning analytics to\nanalyze children's self-narratives of their play experiences. The LLM\nidentifies developmental abilities, while performance scores across different\nplay settings are calculated using learning analytics techniques. We collected\n2,224 play narratives from 29 children in a kindergarten, covering four\ndistinct play areas over one semester. According to the evaluation results from\neight professionals, the LLM-based approach achieved high accuracy in\nidentifying cognitive, motor, and social abilities, with accuracy exceeding 90%\nin most domains. Moreover, significant differences in developmental outcomes\nwere observed across play settings, highlighting each area's unique\ncontributions to specific abilities. These findings confirm that the proposed\napproach is effective in identifying children's development across various free\nplay settings. This study demonstrates the potential of integrating LLMs and\nlearning analytics to provide child-centered insights into developmental\ntrajectories, offering educators valuable data to support personalized learning\nand enhance early childhood education practices."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Yang"
                    },
                    {
                        "name": "Yuan Shen"
                    },
                    {
                        "name": "Tianchen Sun"
                    },
                    {
                        "name": "Yangbin Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yangbin Xie"
                },
                "author": "Yangbin Xie",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03368v1",
                "updated": "2025-05-06T09:40:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    40,
                    6,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:40:06Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    40,
                    6,
                    1,
                    126,
                    0
                ],
                "title": "Geospatial Mechanistic Interpretability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geospatial Mechanistic Interpretability of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography."
                },
                "authors": [
                    {
                        "name": "Stef De Sabbata"
                    },
                    {
                        "name": "Stefano Mizzaro"
                    },
                    {
                        "name": "Kevin Roitero"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Roitero"
                },
                "author": "Kevin Roitero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02380v2",
                "updated": "2025-05-06T09:37:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    37,
                    57,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T05:42:14Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    5,
                    42,
                    14,
                    0,
                    125,
                    0
                ],
                "title": "EntroLLM: Entropy Encoded Weight Compression for Efficient Large\n  Language Model Inference on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EntroLLM: Entropy Encoded Weight Compression for Efficient Large\n  Language Model Inference on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional performance across\nvarious tasks, but their large storage and computational requirements constrain\ntheir deployment on edge devices. To address this, we propose EntroLLM, a novel\ncompression framework that integrates mixed quantization with entropy coding to\nreduce storage overhead while maintaining model accuracy. Our method applies a\nlayer-wise mixed quantization scheme - choosing between symmetric and\nasymmetric quantization based on individual layer weight distributions - to\noptimize compressibility. We then employ Huffman encoding for lossless\ncompression of the quantized weights, significantly reducing memory bandwidth\nrequirements. Furthermore, we introduce parallel Huffman decoding, which\nenables efficient retrieval of encoded weights during inference, ensuring\nminimal latency impact. Our experiments on edge-compatible LLMs, including\nsmolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct,\ndemonstrate that EntroLLM achieves up to $30\\%$ storage reduction compared to\nuint8 models and up to $65%$ storage reduction compared to uint4 models, while\npreserving perplexity and accuracy, on language benchmark tasks. We further\nshow that our method enables $31.9\\%$ - $146.6\\%$ faster inference throughput\non memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by\nreducing the required data movement. The proposed approach requires no\nadditional re-training and is fully compatible with existing post-training\nquantization methods, making it a practical solution for edge LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional performance across\nvarious tasks, but their large storage and computational requirements constrain\ntheir deployment on edge devices. To address this, we propose EntroLLM, a novel\ncompression framework that integrates mixed quantization with entropy coding to\nreduce storage overhead while maintaining model accuracy. Our method applies a\nlayer-wise mixed quantization scheme - choosing between symmetric and\nasymmetric quantization based on individual layer weight distributions - to\noptimize compressibility. We then employ Huffman encoding for lossless\ncompression of the quantized weights, significantly reducing memory bandwidth\nrequirements. Furthermore, we introduce parallel Huffman decoding, which\nenables efficient retrieval of encoded weights during inference, ensuring\nminimal latency impact. Our experiments on edge-compatible LLMs, including\nsmolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct,\ndemonstrate that EntroLLM achieves up to $30\\%$ storage reduction compared to\nuint8 models and up to $65%$ storage reduction compared to uint4 models, while\npreserving perplexity and accuracy, on language benchmark tasks. We further\nshow that our method enables $31.9\\%$ - $146.6\\%$ faster inference throughput\non memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by\nreducing the required data movement. The proposed approach requires no\nadditional re-training and is fully compatible with existing post-training\nquantization methods, making it a practical solution for edge LLMs."
                },
                "authors": [
                    {
                        "name": "Arnab Sanyal"
                    },
                    {
                        "name": "Prithwish Mukherjee"
                    },
                    {
                        "name": "Gourav Datta"
                    },
                    {
                        "name": "Sandeep P. Chinchali"
                    }
                ],
                "author_detail": {
                    "name": "Sandeep P. Chinchali"
                },
                "author": "Sandeep P. Chinchali",
                "arxiv_comment": "6 pages, 1 reference page. Under submission and review at ISLPED 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03361v1",
                "updated": "2025-05-06T09:30:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    30,
                    30,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:30:30Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    30,
                    30,
                    1,
                    126,
                    0
                ],
                "title": "Interpretable Zero-shot Learning with Infinite Class Concepts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Zero-shot Learning with Infinite Class Concepts"
                },
                "summary": "Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images\nwith intermediate class semantics, like human-annotated concepts or class\ndefinitions. An emerging alternative leverages Large-scale Language Models\n(LLMs) to automatically generate class documents. However, these methods often\nface challenges with transparency in the classification process and may suffer\nfrom the notorious hallucination problem in LLMs, resulting in non-visual class\nsemantics. This paper redefines class semantics in ZSL with a focus on\ntransferability and discriminability, introducing a novel framework called\nZero-shot Learning with Infinite Class Concepts (InfZSL). Our approach\nleverages the powerful capabilities of LLMs to dynamically generate an\nunlimited array of phrase-level class concepts. To address the hallucination\nchallenge, we introduce an entropy-based scoring process that incorporates a\n``goodness\" concept selection mechanism, ensuring that only the most\ntransferable and discriminative concepts are selected. Our InfZSL framework not\nonly demonstrates significant improvements on three popular benchmark datasets\nbut also generates highly interpretable, image-grounded concepts. Code will be\nreleased upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images\nwith intermediate class semantics, like human-annotated concepts or class\ndefinitions. An emerging alternative leverages Large-scale Language Models\n(LLMs) to automatically generate class documents. However, these methods often\nface challenges with transparency in the classification process and may suffer\nfrom the notorious hallucination problem in LLMs, resulting in non-visual class\nsemantics. This paper redefines class semantics in ZSL with a focus on\ntransferability and discriminability, introducing a novel framework called\nZero-shot Learning with Infinite Class Concepts (InfZSL). Our approach\nleverages the powerful capabilities of LLMs to dynamically generate an\nunlimited array of phrase-level class concepts. To address the hallucination\nchallenge, we introduce an entropy-based scoring process that incorporates a\n``goodness\" concept selection mechanism, ensuring that only the most\ntransferable and discriminative concepts are selected. Our InfZSL framework not\nonly demonstrates significant improvements on three popular benchmark datasets\nbut also generates highly interpretable, image-grounded concepts. Code will be\nreleased upon acceptance."
                },
                "authors": [
                    {
                        "name": "Zihan Ye"
                    },
                    {
                        "name": "Shreyank N Gowda"
                    },
                    {
                        "name": "Shiming Chen"
                    },
                    {
                        "name": "Yaochu Jin"
                    },
                    {
                        "name": "Kaizhu Huang"
                    },
                    {
                        "name": "Xiaobo Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobo Jin"
                },
                "author": "Xiaobo Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00693v2",
                "updated": "2025-05-06T09:24:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    24,
                    22,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-01T17:55:05Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    17,
                    55,
                    5,
                    3,
                    121,
                    0
                ],
                "title": "Robotic Visual Instruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic Visual Instruction"
                },
                "summary": "Recently, natural language has been the primary medium for human-robot\ninteraction. However, its inherent lack of spatial precision introduces\nchallenges for robotic task definition such as ambiguity and verbosity.\nMoreover, in some public settings where quiet is required, such as libraries or\nhospitals, verbal communication with robots is inappropriate. To address these\nlimitations, we introduce the Robotic Visual Instruction (RoVI), a novel\nparadigm to guide robotic tasks through an object-centric, hand-drawn symbolic\nrepresentation. RoVI effectively encodes spatial-temporal information into\nhuman-interpretable visual instructions through 2D sketches, utilizing arrows,\ncircles, colors, and numbers to direct 3D robotic manipulation. To enable\nrobots to understand RoVI better and generate precise actions based on RoVI, we\npresent Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for\nRoVI-conditioned policies. This approach leverages Vision-Language Models\n(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from\n2D pixel space via keypoint extraction, and then transform them into executable\n3D action sequences. We additionally curate a specialized dataset of 15K\ninstances to fine-tune small VLMs for edge deployment,enabling them to\neffectively learn RoVI capabilities. Our approach is rigorously validated\nacross 11 novel tasks in both real and simulated environments, demonstrating\nsignificant generalization capability. Notably, VIEW achieves an 87.5% success\nrate in real-world scenarios involving unseen tasks that feature multi-step\nactions, with disturbances, and trajectory-following requirements. Project\nwebsite: https://robotic-visual-instruction.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, natural language has been the primary medium for human-robot\ninteraction. However, its inherent lack of spatial precision introduces\nchallenges for robotic task definition such as ambiguity and verbosity.\nMoreover, in some public settings where quiet is required, such as libraries or\nhospitals, verbal communication with robots is inappropriate. To address these\nlimitations, we introduce the Robotic Visual Instruction (RoVI), a novel\nparadigm to guide robotic tasks through an object-centric, hand-drawn symbolic\nrepresentation. RoVI effectively encodes spatial-temporal information into\nhuman-interpretable visual instructions through 2D sketches, utilizing arrows,\ncircles, colors, and numbers to direct 3D robotic manipulation. To enable\nrobots to understand RoVI better and generate precise actions based on RoVI, we\npresent Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for\nRoVI-conditioned policies. This approach leverages Vision-Language Models\n(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from\n2D pixel space via keypoint extraction, and then transform them into executable\n3D action sequences. We additionally curate a specialized dataset of 15K\ninstances to fine-tune small VLMs for edge deployment,enabling them to\neffectively learn RoVI capabilities. Our approach is rigorously validated\nacross 11 novel tasks in both real and simulated environments, demonstrating\nsignificant generalization capability. Notably, VIEW achieves an 87.5% success\nrate in real-world scenarios involving unseen tasks that feature multi-step\nactions, with disturbances, and trajectory-following requirements. Project\nwebsite: https://robotic-visual-instruction.github.io/"
                },
                "authors": [
                    {
                        "name": "Yanbang Li"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Xiaoqi Huang"
                    },
                    {
                        "name": "Haolan Kang"
                    },
                    {
                        "name": "Guangping Bai"
                    },
                    {
                        "name": "Xianzheng Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xianzheng Ma"
                },
                "author": "Xianzheng Ma",
                "arxiv_comment": "Project website: https://robotic-visual-instruction.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03354v1",
                "updated": "2025-05-06T09:23:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    23,
                    0,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:23:00Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    23,
                    0,
                    1,
                    126,
                    0
                ],
                "title": "Physics-Informed Neural Networks in Electromagnetic and Nanophotonic\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Neural Networks in Electromagnetic and Nanophotonic\n  Design"
                },
                "summary": "The fusion of artificial intelligence (AI) with physics-guided frameworks has\nopened transformative avenues for advancing the design and optimization of\nelectromagnetic and nanophotonic systems. Innovations in deep neural networks\n(DNNs) and physics-informed neural networks (PINNs) now provide robust tools to\ntackle longstanding challenges in light scattering engineering, meta-optics,\nand nonlinear photonics. This review outlines recent progress in leveraging\nthese computational methodologies to enhance device performance across domains\nsuch as dynamic light modulation, antenna design, and nonlinear optical\nphenomena. We systematically survey advancements in AI-driven forward and\ninverse design strategies, which bypass conventional trial-and-error approaches\nby embedding physical laws directly into optimization workflows. Furthermore,\nthe integration of AI accelerates electromagnetic simulations and enables\nprecise modelling of complex optical effects, including topological photonic\nstates and nonlinear interactions. A comparative evaluation of algorithmic\nframeworks highlights their strengths in balancing computational efficiency,\nmulti-objective optimization, and fabrication feasibility. Challenges such as\nlimited interpretability of AI models and data scarcity for unconventional\noptical modes are critically addressed. Finally, we emphasize future\nopportunities in scalable multi-physics modelling, adaptive architectures, and\npractical deployment of AI-optimized photonic devices. This work underscores\nthe pivotal role of AI in transcending traditional design limitations, thereby\npropelling the development of next-generation photonic technologies with\nunprecedented functionality and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fusion of artificial intelligence (AI) with physics-guided frameworks has\nopened transformative avenues for advancing the design and optimization of\nelectromagnetic and nanophotonic systems. Innovations in deep neural networks\n(DNNs) and physics-informed neural networks (PINNs) now provide robust tools to\ntackle longstanding challenges in light scattering engineering, meta-optics,\nand nonlinear photonics. This review outlines recent progress in leveraging\nthese computational methodologies to enhance device performance across domains\nsuch as dynamic light modulation, antenna design, and nonlinear optical\nphenomena. We systematically survey advancements in AI-driven forward and\ninverse design strategies, which bypass conventional trial-and-error approaches\nby embedding physical laws directly into optimization workflows. Furthermore,\nthe integration of AI accelerates electromagnetic simulations and enables\nprecise modelling of complex optical effects, including topological photonic\nstates and nonlinear interactions. A comparative evaluation of algorithmic\nframeworks highlights their strengths in balancing computational efficiency,\nmulti-objective optimization, and fabrication feasibility. Challenges such as\nlimited interpretability of AI models and data scarcity for unconventional\noptical modes are critically addressed. Finally, we emphasize future\nopportunities in scalable multi-physics modelling, adaptive architectures, and\npractical deployment of AI-optimized photonic devices. This work underscores\nthe pivotal role of AI in transcending traditional design limitations, thereby\npropelling the development of next-generation photonic technologies with\nunprecedented functionality and efficiency."
                },
                "authors": [
                    {
                        "name": "Omar A. M. Abdelraouf"
                    },
                    {
                        "name": "Abdulrahman M. A. Ahmed"
                    },
                    {
                        "name": "Emadeldeen Eldele"
                    },
                    {
                        "name": "Ahmed A. Omar"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed A. Omar"
                },
                "author": "Ahmed A. Omar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03345v1",
                "updated": "2025-05-06T09:13:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    13,
                    32,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:13:32Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    13,
                    32,
                    1,
                    126,
                    0
                ],
                "title": "Elevating Cyber Threat Intelligence against Disinformation Campaigns\n  with LLM-based Concept Extraction and the FakeCTI Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elevating Cyber Threat Intelligence against Disinformation Campaigns\n  with LLM-based Concept Extraction and the FakeCTI Dataset"
                },
                "summary": "The swift spread of fake news and disinformation campaigns poses a\nsignificant threat to public trust, political stability, and cybersecurity.\nTraditional Cyber Threat Intelligence (CTI) approaches, which rely on low-level\nindicators such as domain names and social media handles, are easily evaded by\nadversaries who frequently modify their online infrastructure. To address these\nlimitations, we introduce a novel CTI framework that focuses on high-level,\nsemantic indicators derived from recurrent narratives and relationships of\ndisinformation campaigns. Our approach extracts structured CTI indicators from\nunstructured disinformation content, capturing key entities and their\ncontextual dependencies within fake news using Large Language Models (LLMs). We\nfurther introduce FakeCTI, the first dataset that systematically links fake\nnews to disinformation campaigns and threat actors. To evaluate the\neffectiveness of our CTI framework, we analyze multiple fake news attribution\ntechniques, spanning from traditional Natural Language Processing (NLP) to\nfine-tuned LLMs. This work shifts the focus from low-level artifacts to\npersistent conceptual structures, establishing a scalable and adaptive approach\nto tracking and countering disinformation campaigns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The swift spread of fake news and disinformation campaigns poses a\nsignificant threat to public trust, political stability, and cybersecurity.\nTraditional Cyber Threat Intelligence (CTI) approaches, which rely on low-level\nindicators such as domain names and social media handles, are easily evaded by\nadversaries who frequently modify their online infrastructure. To address these\nlimitations, we introduce a novel CTI framework that focuses on high-level,\nsemantic indicators derived from recurrent narratives and relationships of\ndisinformation campaigns. Our approach extracts structured CTI indicators from\nunstructured disinformation content, capturing key entities and their\ncontextual dependencies within fake news using Large Language Models (LLMs). We\nfurther introduce FakeCTI, the first dataset that systematically links fake\nnews to disinformation campaigns and threat actors. To evaluate the\neffectiveness of our CTI framework, we analyze multiple fake news attribution\ntechniques, spanning from traditional Natural Language Processing (NLP) to\nfine-tuned LLMs. This work shifts the focus from low-level artifacts to\npersistent conceptual structures, establishing a scalable and adaptive approach\nto tracking and countering disinformation campaigns."
                },
                "authors": [
                    {
                        "name": "Domenico Cotroneo"
                    },
                    {
                        "name": "Roberto Natella"
                    },
                    {
                        "name": "Vittorio Orbinato"
                    }
                ],
                "author_detail": {
                    "name": "Vittorio Orbinato"
                },
                "author": "Vittorio Orbinato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03344v1",
                "updated": "2025-05-06T09:12:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    12,
                    37,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:12:37Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    12,
                    37,
                    1,
                    126,
                    0
                ],
                "title": "RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic\n  Simulation"
                },
                "summary": "Achieving both realism and controllability in interactive closed-loop traffic\nsimulation remains a key challenge in autonomous driving. Data-driven\nsimulation methods reproduce realistic trajectories but suffer from covariate\nshift in closed-loop deployment, compounded by simplified dynamics models that\nfurther reduce reliability. Conversely, physics-based simulation methods\nenhance reliable and controllable closed-loop interactions but often lack\nexpert demonstrations, compromising realism. To address these challenges, we\nintroduce a dual-stage AV-centered simulation framework that conducts open-loop\nimitation learning pre-training in a data-driven simulator to capture\ntrajectory-level realism and multimodality, followed by closed-loop\nreinforcement learning fine-tuning in a physics-based simulator to enhance\ncontrollability and mitigate covariate shift. In the fine-tuning stage, we\npropose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that\npreserves the trajectory-level multimodality through a GRPO-style\ngroup-relative advantage formulation, while enhancing controllability and\ntraining stability by replacing KL regularization with the dual-clip mechanism.\nExtensive experiments demonstrate that RIFT significantly improves the realism\nand controllability of generated traffic scenarios, providing a robust platform\nfor evaluating autonomous vehicle performance in diverse and interactive\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving both realism and controllability in interactive closed-loop traffic\nsimulation remains a key challenge in autonomous driving. Data-driven\nsimulation methods reproduce realistic trajectories but suffer from covariate\nshift in closed-loop deployment, compounded by simplified dynamics models that\nfurther reduce reliability. Conversely, physics-based simulation methods\nenhance reliable and controllable closed-loop interactions but often lack\nexpert demonstrations, compromising realism. To address these challenges, we\nintroduce a dual-stage AV-centered simulation framework that conducts open-loop\nimitation learning pre-training in a data-driven simulator to capture\ntrajectory-level realism and multimodality, followed by closed-loop\nreinforcement learning fine-tuning in a physics-based simulator to enhance\ncontrollability and mitigate covariate shift. In the fine-tuning stage, we\npropose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that\npreserves the trajectory-level multimodality through a GRPO-style\ngroup-relative advantage formulation, while enhancing controllability and\ntraining stability by replacing KL regularization with the dual-clip mechanism.\nExtensive experiments demonstrate that RIFT significantly improves the realism\nand controllability of generated traffic scenarios, providing a robust platform\nfor evaluating autonomous vehicle performance in diverse and interactive\nscenarios."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Wenchao Sun"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Sifa Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Sifa Zheng"
                },
                "author": "Sifa Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03341v1",
                "updated": "2025-05-06T09:11:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    11,
                    56,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:11:56Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    11,
                    56,
                    1,
                    126,
                    0
                ],
                "title": "Electrically Reconfigurable NbOCl$_2$ Metasurface for Quantum\n  Technologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrically Reconfigurable NbOCl$_2$ Metasurface for Quantum\n  Technologies"
                },
                "summary": "Entangled photon-pair sources are foundational to advancing quantum\ntechnologies, including secure communication, quantum sensing, and imaging. For\ndeployment in space-constrained environments such as satellite-based quantum\nnetworks or portable devices, compact, reconfigurable, and efficient\nentanglement sources are essential. Here, we present an electrically tunable\nentangled photon-pair source utilizing a nanostructured NbOCl$_2$ crystal,\nengineered for operation in the telecommunication C-band. The inherent\nnon-centrosymmetric lattice symmetry of NbOCl$_2$ enables direct generation of\npolarization-entangled Bell states without the need for post-selection,\nleveraging its exceptional second-order nonlinear susceptibility, which\nsurpasses conventional nonlinear materials. By nanopatterning NbOCl$_2$ into a\nhigh-quality-factor metasurface, we achieve three orders of magnitude\nenhancement in photon-pair generation efficiency via resonant excitation of\nbound states in the continuum resonance, which intensify light-matter\ninteractions. Furthermore, we demonstrate in situ electrical tunability of the\nphoton-pair emission wavelength over a 250 nm range from 1450 nm to 1700 nm by\ndynamically modulating surrounding liquid crystal layer. Remarkably, the\ndecoupling of photon-pair generation rate and spectral tunability ensures high\nbrightness, above 10,000 coincidences, under active tuning. The air stability\nand mechanical robustness of NbOCl$_2$ further enhance its practicality for\nreal-world deployment. This work establishes NbOCl$_2$ as a superior material\nfor scalable, on-chip quantum light sources, paving the way for integrated\nquantum communication systems, adaptive sensors, and portable quantum devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entangled photon-pair sources are foundational to advancing quantum\ntechnologies, including secure communication, quantum sensing, and imaging. For\ndeployment in space-constrained environments such as satellite-based quantum\nnetworks or portable devices, compact, reconfigurable, and efficient\nentanglement sources are essential. Here, we present an electrically tunable\nentangled photon-pair source utilizing a nanostructured NbOCl$_2$ crystal,\nengineered for operation in the telecommunication C-band. The inherent\nnon-centrosymmetric lattice symmetry of NbOCl$_2$ enables direct generation of\npolarization-entangled Bell states without the need for post-selection,\nleveraging its exceptional second-order nonlinear susceptibility, which\nsurpasses conventional nonlinear materials. By nanopatterning NbOCl$_2$ into a\nhigh-quality-factor metasurface, we achieve three orders of magnitude\nenhancement in photon-pair generation efficiency via resonant excitation of\nbound states in the continuum resonance, which intensify light-matter\ninteractions. Furthermore, we demonstrate in situ electrical tunability of the\nphoton-pair emission wavelength over a 250 nm range from 1450 nm to 1700 nm by\ndynamically modulating surrounding liquid crystal layer. Remarkably, the\ndecoupling of photon-pair generation rate and spectral tunability ensures high\nbrightness, above 10,000 coincidences, under active tuning. The air stability\nand mechanical robustness of NbOCl$_2$ further enhance its practicality for\nreal-world deployment. This work establishes NbOCl$_2$ as a superior material\nfor scalable, on-chip quantum light sources, paving the way for integrated\nquantum communication systems, adaptive sensors, and portable quantum devices."
                },
                "authors": [
                    {
                        "name": "Omar A. M. Abdelraouf"
                    }
                ],
                "author_detail": {
                    "name": "Omar A. M. Abdelraouf"
                },
                "author": "Omar A. M. Abdelraouf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03336v1",
                "updated": "2025-05-06T09:08:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    8,
                    36,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:08:36Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    8,
                    36,
                    1,
                    126,
                    0
                ],
                "title": "Avoid Recommending Out-of-Domain Items: Constrained Generative\n  Recommendation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avoid Recommending Out-of-Domain Items: Constrained Generative\n  Recommendation with LLMs"
                },
                "summary": "Large Language Models (LLMs) have shown promise for generative recommender\nsystems due to their transformative capabilities in user interaction. However,\nensuring they do not recommend out-of-domain (OOD) items remains a challenge.\nWe study two distinct methods to address this issue: RecLM-ret, a\nretrieval-based method, and RecLM-cgen, a constrained generation method. Both\nmethods integrate seamlessly with existing LLMs to ensure in-domain\nrecommendations. Comprehensive experiments on three recommendation datasets\ndemonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing\nLLM-based recommender models in accuracy while eliminating OOD recommendations,\nmaking it the preferred method for adoption. Additionally, RecLM-cgen maintains\nstrong generalist capabilities and is a lightweight plug-and-play module for\neasy integration into LLMs, offering valuable practical benefits for the\ncommunity. Source code is available at https://github.com/microsoft/RecAI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promise for generative recommender\nsystems due to their transformative capabilities in user interaction. However,\nensuring they do not recommend out-of-domain (OOD) items remains a challenge.\nWe study two distinct methods to address this issue: RecLM-ret, a\nretrieval-based method, and RecLM-cgen, a constrained generation method. Both\nmethods integrate seamlessly with existing LLMs to ensure in-domain\nrecommendations. Comprehensive experiments on three recommendation datasets\ndemonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing\nLLM-based recommender models in accuracy while eliminating OOD recommendations,\nmaking it the preferred method for adoption. Additionally, RecLM-cgen maintains\nstrong generalist capabilities and is a lightweight plug-and-play module for\neasy integration into LLMs, offering valuable practical benefits for the\ncommunity. Source code is available at https://github.com/microsoft/RecAI"
                },
                "authors": [
                    {
                        "name": "Hao Liao"
                    },
                    {
                        "name": "Wensheng Lu"
                    },
                    {
                        "name": "Jianxun Lian"
                    },
                    {
                        "name": "Mingqi Wu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Yitian Huang"
                    },
                    {
                        "name": "Mingyang Zhou"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03332v1",
                "updated": "2025-05-06T09:06:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    6,
                    18,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T09:06:18Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    6,
                    18,
                    1,
                    126,
                    0
                ],
                "title": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting,\n  Meta-Prompting, and Meta-Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting,\n  Meta-Prompting, and Meta-Reasoning"
                },
                "summary": "Critical peer review of scientific manuscripts presents a significant\nchallenge for Large Language Models (LLMs), partly due to data limitations and\nthe complexity of expert reasoning. This report introduces Persistent Workflow\nPrompting (PWP), a potentially broadly applicable prompt engineering\nmethodology designed to bridge this gap using standard LLM chat interfaces\n(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical\nanalysis of experimental chemistry manuscripts, featuring a hierarchical,\nmodular architecture (structured via Markdown) that defines detailed analysis\nworkflows. We develop this PWP prompt through iterative application of\nmeta-prompting techniques and meta-reasoning aimed at systematically codifying\nexpert review workflows, including tacit knowledge. Submitted once at the start\nof a session, this PWP prompt equips the LLM with persistent workflows\ntriggered by subsequent queries, guiding modern reasoning LLMs through\nsystematic, multimodal evaluations. Demonstrations show the PWP-guided LLM\nidentifying major methodological flaws in a test case while mitigating LLM\ninput bias and performing complex tasks, including distinguishing claims from\nevidence, integrating text/photo/figure analysis to infer parameters, executing\nquantitative feasibility checks, comparing estimates against claims, and\nassessing a priori plausibility. To ensure transparency and facilitate\nreplication, we provide full prompts, detailed demonstration analyses, and logs\nof interactive chats as supplementary resources. Beyond the specific\napplication, this work offers insights into the meta-development process\nitself, highlighting the potential of PWP, informed by detailed workflow\nformalization, to enable sophisticated analysis using readily available LLMs\nfor complex scientific tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical peer review of scientific manuscripts presents a significant\nchallenge for Large Language Models (LLMs), partly due to data limitations and\nthe complexity of expert reasoning. This report introduces Persistent Workflow\nPrompting (PWP), a potentially broadly applicable prompt engineering\nmethodology designed to bridge this gap using standard LLM chat interfaces\n(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical\nanalysis of experimental chemistry manuscripts, featuring a hierarchical,\nmodular architecture (structured via Markdown) that defines detailed analysis\nworkflows. We develop this PWP prompt through iterative application of\nmeta-prompting techniques and meta-reasoning aimed at systematically codifying\nexpert review workflows, including tacit knowledge. Submitted once at the start\nof a session, this PWP prompt equips the LLM with persistent workflows\ntriggered by subsequent queries, guiding modern reasoning LLMs through\nsystematic, multimodal evaluations. Demonstrations show the PWP-guided LLM\nidentifying major methodological flaws in a test case while mitigating LLM\ninput bias and performing complex tasks, including distinguishing claims from\nevidence, integrating text/photo/figure analysis to infer parameters, executing\nquantitative feasibility checks, comparing estimates against claims, and\nassessing a priori plausibility. To ensure transparency and facilitate\nreplication, we provide full prompts, detailed demonstration analyses, and logs\nof interactive chats as supplementary resources. Beyond the specific\napplication, this work offers insights into the meta-development process\nitself, highlighting the potential of PWP, informed by detailed workflow\nformalization, to enable sophisticated analysis using readily available LLMs\nfor complex scientific tasks."
                },
                "authors": [
                    {
                        "name": "Evgeny Markhasin"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Markhasin"
                },
                "author": "Evgeny Markhasin",
                "arxiv_comment": "22 pages, 36 pages (references and appendixes)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02768v2",
                "updated": "2025-05-06T09:02:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    9,
                    2,
                    57,
                    1,
                    126,
                    0
                ],
                "published": "2024-09-17T05:17:37Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    5,
                    17,
                    37,
                    1,
                    261,
                    0
                ],
                "title": "Uncertainty-Guided Self-Questioning and Answering for Video-Language\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Guided Self-Questioning and Answering for Video-Language\n  Alignment"
                },
                "summary": "The development of multi-modal models has been rapidly advancing, with some\ndemonstrating remarkable capabilities. However, annotating video-text pairs\nremains expensive and insufficient. Take video question answering (VideoQA)\ntasks as an example, human annotated questions and answers often cover only\npart of the video, since the corresponding text is often short and monotonous,\nleading to underutilization of video. To address this, we propose a\nBootstrapping Video-Language Alignment framework (BoViLA), a self-training\nmethod that augments question samples during training process through LLM-based\nself-questioning and answering, which help model exploit video information and\nthe internal knowledge of LLMs more thoroughly to improve modality alignment.\nHowever, low-quality self-generated questions may instead contaminate the\nperformance, especially in the early stages of training, as we have observed in\nour experiments. To filter bad self-generated questions, we introduce\nEvidential Deep Learning (EDL) to estimate uncertainty and assess the quality\nof self-generated questions by evaluating the modality alignment within the\ncontext. To the best of our knowledge, this work is the first to explore\nLLM-based self-training frameworks for modality alignment. We evaluate BoViLA\non five strong VideoQA benchmarks, where it outperforms several\nstate-of-the-art methods and demonstrate its effectiveness and generality.\nAdditionally, we provide extensive analyses of the self-training framework and\nthe EDL-based uncertainty filtering mechanism. The code will be made available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of multi-modal models has been rapidly advancing, with some\ndemonstrating remarkable capabilities. However, annotating video-text pairs\nremains expensive and insufficient. Take video question answering (VideoQA)\ntasks as an example, human annotated questions and answers often cover only\npart of the video, since the corresponding text is often short and monotonous,\nleading to underutilization of video. To address this, we propose a\nBootstrapping Video-Language Alignment framework (BoViLA), a self-training\nmethod that augments question samples during training process through LLM-based\nself-questioning and answering, which help model exploit video information and\nthe internal knowledge of LLMs more thoroughly to improve modality alignment.\nHowever, low-quality self-generated questions may instead contaminate the\nperformance, especially in the early stages of training, as we have observed in\nour experiments. To filter bad self-generated questions, we introduce\nEvidential Deep Learning (EDL) to estimate uncertainty and assess the quality\nof self-generated questions by evaluating the modality alignment within the\ncontext. To the best of our knowledge, this work is the first to explore\nLLM-based self-training frameworks for modality alignment. We evaluate BoViLA\non five strong VideoQA benchmarks, where it outperforms several\nstate-of-the-art methods and demonstrate its effectiveness and generality.\nAdditionally, we provide extensive analyses of the self-training framework and\nthe EDL-based uncertainty filtering mechanism. The code will be made available."
                },
                "authors": [
                    {
                        "name": "Jin Chen"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Haojian Huang"
                    },
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Mehdi Hosseinzadeh"
                    },
                    {
                        "name": "Zhe Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Liu"
                },
                "author": "Zhe Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13840v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13840v2",
                "updated": "2025-05-06T08:55:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    55,
                    42,
                    1,
                    126,
                    0
                ],
                "published": "2024-12-18T13:33:28Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    13,
                    33,
                    28,
                    2,
                    353,
                    0
                ],
                "title": "Unleashing the Power of Continual Learning on Non-Centralized Devices: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Power of Continual Learning on Non-Centralized Devices: A\n  Survey"
                },
                "summary": "Non-Centralized Continual Learning (NCCL) has become an emerging paradigm for\nenabling distributed devices such as vehicles and servers to handle streaming\ndata from a joint non-stationary environment. To achieve high reliability and\nscalability in deploying this paradigm in distributed systems, it is essential\nto conquer challenges stemming from both spatial and temporal dimensions,\nmanifesting as distribution shifts, catastrophic forgetting, heterogeneity, and\nprivacy issues. This survey focuses on a comprehensive examination of the\ndevelopment of the non-centralized continual learning algorithms and the\nreal-world deployment across distributed devices. We begin with an introduction\nto the background and fundamentals of non-centralized learning and continual\nlearning. Then, we review existing solutions from three levels to represent how\nexisting techniques alleviate the catastrophic forgetting and distribution\nshift. Additionally, we delve into the various types of heterogeneity issues,\nsecurity, and privacy attributes, as well as real-world applications across\nthree prevalent scenarios. Furthermore, we establish a large-scale benchmark to\nrevisit this problem and analyze the performance of the state-of-the-art NCCL\napproaches. Finally, we discuss the important challenges and future research\ndirections in NCCL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Centralized Continual Learning (NCCL) has become an emerging paradigm for\nenabling distributed devices such as vehicles and servers to handle streaming\ndata from a joint non-stationary environment. To achieve high reliability and\nscalability in deploying this paradigm in distributed systems, it is essential\nto conquer challenges stemming from both spatial and temporal dimensions,\nmanifesting as distribution shifts, catastrophic forgetting, heterogeneity, and\nprivacy issues. This survey focuses on a comprehensive examination of the\ndevelopment of the non-centralized continual learning algorithms and the\nreal-world deployment across distributed devices. We begin with an introduction\nto the background and fundamentals of non-centralized learning and continual\nlearning. Then, we review existing solutions from three levels to represent how\nexisting techniques alleviate the catastrophic forgetting and distribution\nshift. Additionally, we delve into the various types of heterogeneity issues,\nsecurity, and privacy attributes, as well as real-world applications across\nthree prevalent scenarios. Furthermore, we establish a large-scale benchmark to\nrevisit this problem and analyze the performance of the state-of-the-art NCCL\napproaches. Finally, we discuss the important challenges and future research\ndirections in NCCL."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Wenchao Xu"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Minzhu Tu"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Shui Yu"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13840v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17529v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17529v2",
                "updated": "2025-05-06T08:47:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    47,
                    32,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-24T13:17:18Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    17,
                    18,
                    3,
                    114,
                    0
                ],
                "title": "IRA: Adaptive Interest-aware Representation and Alignment for\n  Personalized Multi-interest Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRA: Adaptive Interest-aware Representation and Alignment for\n  Personalized Multi-interest Retrieval"
                },
                "summary": "Online community platforms require dynamic personalized retrieval and\nrecommendation that can continuously adapt to evolving user interests and new\ndocuments. However, optimizing models to handle such changes in real-time\nremains a major challenge in large-scale industrial settings. To address this,\nwe propose the Interest-aware Representation and Alignment (IRA) framework, an\nefficient and scalable approach that dynamically adapts to new interactions\nthrough a cumulative structure. IRA leverages two key mechanisms: (1) Interest\nUnits that capture diverse user interests as contextual texts, while\nreinforcing or fading over time through cumulative updates, and (2) a retrieval\nprocess that measures the relevance between Interest Units and documents based\nsolely on semantic relationships, eliminating dependence on click signals to\nmitigate temporal biases. By integrating cumulative Interest Unit updates with\nthe retrieval process, IRA continuously adapts to evolving user preferences,\nensuring robust and fine-grained personalization without being constrained by\npast training distributions. We validate the effectiveness of IRA through\nextensive experiments on real-world datasets, including its deployment in the\nHome Section of NAVER's CAFE, South Korea's leading community platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online community platforms require dynamic personalized retrieval and\nrecommendation that can continuously adapt to evolving user interests and new\ndocuments. However, optimizing models to handle such changes in real-time\nremains a major challenge in large-scale industrial settings. To address this,\nwe propose the Interest-aware Representation and Alignment (IRA) framework, an\nefficient and scalable approach that dynamically adapts to new interactions\nthrough a cumulative structure. IRA leverages two key mechanisms: (1) Interest\nUnits that capture diverse user interests as contextual texts, while\nreinforcing or fading over time through cumulative updates, and (2) a retrieval\nprocess that measures the relevance between Interest Units and documents based\nsolely on semantic relationships, eliminating dependence on click signals to\nmitigate temporal biases. By integrating cumulative Interest Unit updates with\nthe retrieval process, IRA continuously adapts to evolving user preferences,\nensuring robust and fine-grained personalization without being constrained by\npast training distributions. We validate the effectiveness of IRA through\nextensive experiments on real-world datasets, including its deployment in the\nHome Section of NAVER's CAFE, South Korea's leading community platform."
                },
                "authors": [
                    {
                        "name": "Youngjune Lee"
                    },
                    {
                        "name": "Haeyu Jeong"
                    },
                    {
                        "name": "Changgeon Lim"
                    },
                    {
                        "name": "Jeong Choi"
                    },
                    {
                        "name": "Hongjun Lim"
                    },
                    {
                        "name": "Hangon Kim"
                    },
                    {
                        "name": "Jiyoon Kwon"
                    },
                    {
                        "name": "Saehun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Saehun Kim"
                },
                "author": "Saehun Kim",
                "arxiv_comment": "Accepted to SIGIR 2025 Industry Track. First two authors contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17529v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03315v1",
                "updated": "2025-05-06T08:45:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    45,
                    44,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T08:45:44Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    45,
                    44,
                    1,
                    126,
                    0
                ],
                "title": "Artificial Behavior Intelligence: Technology, Challenges, and Future\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Behavior Intelligence: Technology, Challenges, and Future\n  Directions"
                },
                "summary": "Understanding and predicting human behavior has emerged as a core capability\nin various AI application domains such as autonomous driving, smart healthcare,\nsurveillance systems, and social robotics. This paper defines the technical\nframework of Artificial Behavior Intelligence (ABI), which comprehensively\nanalyzes and interprets human posture, facial expressions, emotions, behavioral\nsequences, and contextual cues. It details the essential components of ABI,\nincluding pose estimation, face and emotion recognition, sequential behavior\nanalysis, and context-aware modeling. Furthermore, we highlight the\ntransformative potential of recent advances in large-scale pretrained models,\nsuch as large language models (LLMs), vision foundation models, and multimodal\nintegration models, in significantly improving the accuracy and\ninterpretability of behavior recognition. Our research team has a strong\ninterest in the ABI domain and is actively conducting research, particularly\nfocusing on the development of intelligent lightweight models capable of\nefficiently inferring complex human behaviors. This paper identifies several\ntechnical challenges that must be addressed to deploy ABI in real-world\napplications including learning behavioral intelligence from limited data,\nquantifying uncertainty in complex behavior prediction, and optimizing model\nstructures for low-power, real-time inference. To tackle these challenges, our\nteam is exploring various optimization strategies including lightweight\ntransformers, graph-based recognition architectures, energy-aware loss\nfunctions, and multimodal knowledge distillation, while validating their\napplicability in real-time environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and predicting human behavior has emerged as a core capability\nin various AI application domains such as autonomous driving, smart healthcare,\nsurveillance systems, and social robotics. This paper defines the technical\nframework of Artificial Behavior Intelligence (ABI), which comprehensively\nanalyzes and interprets human posture, facial expressions, emotions, behavioral\nsequences, and contextual cues. It details the essential components of ABI,\nincluding pose estimation, face and emotion recognition, sequential behavior\nanalysis, and context-aware modeling. Furthermore, we highlight the\ntransformative potential of recent advances in large-scale pretrained models,\nsuch as large language models (LLMs), vision foundation models, and multimodal\nintegration models, in significantly improving the accuracy and\ninterpretability of behavior recognition. Our research team has a strong\ninterest in the ABI domain and is actively conducting research, particularly\nfocusing on the development of intelligent lightweight models capable of\nefficiently inferring complex human behaviors. This paper identifies several\ntechnical challenges that must be addressed to deploy ABI in real-world\napplications including learning behavioral intelligence from limited data,\nquantifying uncertainty in complex behavior prediction, and optimizing model\nstructures for low-power, real-time inference. To tackle these challenges, our\nteam is exploring various optimization strategies including lightweight\ntransformers, graph-based recognition architectures, energy-aware loss\nfunctions, and multimodal knowledge distillation, while validating their\napplicability in real-time environments."
                },
                "authors": [
                    {
                        "name": "Kanghyun Jo"
                    },
                    {
                        "name": "Jehwan Choi"
                    },
                    {
                        "name": "Kwanho Kim"
                    },
                    {
                        "name": "Seongmin Kim"
                    },
                    {
                        "name": "Duy-Linh Nguyen"
                    },
                    {
                        "name": "Xuan-Thuy Vo"
                    },
                    {
                        "name": "Adri Priadana"
                    },
                    {
                        "name": "Tien-Dat Tran"
                    }
                ],
                "author_detail": {
                    "name": "Tien-Dat Tran"
                },
                "author": "Tien-Dat Tran",
                "arxiv_comment": "9 pages, 6 figures, Pre-print for IWIS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01420v2",
                "updated": "2025-05-06T08:40:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    40,
                    17,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-02T17:57:14Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    17,
                    57,
                    14,
                    4,
                    122,
                    0
                ],
                "title": "Evaluating Frontier Models for Stealth and Situational Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Frontier Models for Stealth and Situational Awareness"
                },
                "summary": "Recent work has demonstrated the plausibility of frontier AI models scheming\n-- knowingly and covertly pursuing an objective misaligned with its developer's\nintentions. Such behavior could be very hard to detect, and if present in\nfuture advanced systems, could pose severe loss of control risk. It is\ntherefore important for AI developers to rule out harm from scheming prior to\nmodel deployment. In this paper, we present a suite of scheming reasoning\nevaluations measuring two types of reasoning capabilities that we believe are\nprerequisites for successful scheming: First, we propose five evaluations of\nability to reason about and circumvent oversight (stealth). Second, we present\neleven evaluations for measuring a model's ability to instrumentally reason\nabout itself, its environment and its deployment (situational awareness). We\ndemonstrate how these evaluations can be used as part of a scheming inability\nsafety case: a model that does not succeed on these evaluations is almost\ncertainly incapable of causing severe harm via scheming in real deployment. We\nrun our evaluations on current frontier models and find that none of them show\nconcerning levels of either situational awareness or stealth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has demonstrated the plausibility of frontier AI models scheming\n-- knowingly and covertly pursuing an objective misaligned with its developer's\nintentions. Such behavior could be very hard to detect, and if present in\nfuture advanced systems, could pose severe loss of control risk. It is\ntherefore important for AI developers to rule out harm from scheming prior to\nmodel deployment. In this paper, we present a suite of scheming reasoning\nevaluations measuring two types of reasoning capabilities that we believe are\nprerequisites for successful scheming: First, we propose five evaluations of\nability to reason about and circumvent oversight (stealth). Second, we present\neleven evaluations for measuring a model's ability to instrumentally reason\nabout itself, its environment and its deployment (situational awareness). We\ndemonstrate how these evaluations can be used as part of a scheming inability\nsafety case: a model that does not succeed on these evaluations is almost\ncertainly incapable of causing severe harm via scheming in real deployment. We\nrun our evaluations on current frontier models and find that none of them show\nconcerning levels of either situational awareness or stealth."
                },
                "authors": [
                    {
                        "name": "Mary Phuong"
                    },
                    {
                        "name": "Roland S. Zimmermann"
                    },
                    {
                        "name": "Ziyue Wang"
                    },
                    {
                        "name": "David Lindner"
                    },
                    {
                        "name": "Victoria Krakovna"
                    },
                    {
                        "name": "Sarah Cogan"
                    },
                    {
                        "name": "Allan Dafoe"
                    },
                    {
                        "name": "Lewis Ho"
                    },
                    {
                        "name": "Rohin Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rohin Shah"
                },
                "author": "Rohin Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03303v1",
                "updated": "2025-05-06T08:36:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    36,
                    1,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T08:36:01Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    36,
                    1,
                    1,
                    126,
                    0
                ],
                "title": "Comparative Analysis of Lightweight Deep Learning Models for\n  Memory-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Lightweight Deep Learning Models for\n  Memory-Constrained Devices"
                },
                "summary": "This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms."
                },
                "authors": [
                    {
                        "name": "Tasnim Shahriar"
                    }
                ],
                "author_detail": {
                    "name": "Tasnim Shahriar"
                },
                "author": "Tasnim Shahriar",
                "arxiv_comment": "22 pages, 10 figures, 4 tables, submitted to Springer - Pattern\n  Recognition and Image Analysis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68-XX (Primary) 68Txx, 68T07 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02659v2",
                "updated": "2025-05-06T08:34:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    34,
                    46,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T14:05:15Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    14,
                    5,
                    15,
                    0,
                    125,
                    0
                ],
                "title": "A Note on Statistically Accurate Tabular Data Generation Using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Note on Statistically Accurate Tabular Data Generation Using Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have shown promise in synthetic tabular data\ngeneration, yet existing methods struggle to preserve complex feature\ndependencies, particularly among categorical variables. This work introduces a\nprobability-driven prompting approach that leverages LLMs to estimate\nconditional distributions, enabling more accurate and scalable data synthesis.\nThe results highlight the potential of prompting probability distributions to\nenhance the statistical fidelity of LLM-generated tabular data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise in synthetic tabular data\ngeneration, yet existing methods struggle to preserve complex feature\ndependencies, particularly among categorical variables. This work introduces a\nprobability-driven prompting approach that leverages LLMs to estimate\nconditional distributions, enabling more accurate and scalable data synthesis.\nThe results highlight the potential of prompting probability distributions to\nenhance the statistical fidelity of LLM-generated tabular data."
                },
                "authors": [
                    {
                        "name": "Andrey Sidorenko"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Sidorenko"
                },
                "author": "Andrey Sidorenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07986v2",
                "updated": "2025-05-06T08:30:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    30,
                    46,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-07T02:42:07Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    2,
                    42,
                    7,
                    0,
                    97,
                    0
                ],
                "title": "SEAL: Steerable Reasoning Calibration of Large Language Models for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Steerable Reasoning Calibration of Large Language Models for Free"
                },
                "summary": "Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated\ncompelling capabilities for complex reasoning tasks via the extended\nchain-of-thought (CoT) reasoning mechanism. However, recent studies reveal\nsubstantial redundancy in the CoT reasoning traces, which not only increases\ninference latency but also negatively impacts model performance by diverting\nattention to unnecessary reasoning paths. To address this issue, we investigate\nthe internal reasoning structures of LLMs and categorize them into three\nprimary thought types: execution, reflection, and transition thoughts.\nMoreover, our analysis reveals that excessive reflection and transition\nthoughts are strongly correlated with failure cases and these thought\ncategories exhibit clear separation in the latent space. Based on these, we\nintroduce SEAL (Steerable reasoning calibration), a training-free approach that\nseamlessly calibrates the CoT process, improving accuracy while demonstrating\nsignificant efficiency gains. SEAL consists of an offline stage for extracting\nthe reasoning steering vector in the latent space, followed by an on-the-fly\ncalibration of the reasoning trace through representation intervention using\nthe steering vector. Notably, the steering vector exhibits strong\ntransferability across various tasks. Extensive experiments across multiple\nmodels (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500,\nGSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11%\nimprovement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our\ncode is publicly available at https://github.com/VITA-Group/SEAL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated\ncompelling capabilities for complex reasoning tasks via the extended\nchain-of-thought (CoT) reasoning mechanism. However, recent studies reveal\nsubstantial redundancy in the CoT reasoning traces, which not only increases\ninference latency but also negatively impacts model performance by diverting\nattention to unnecessary reasoning paths. To address this issue, we investigate\nthe internal reasoning structures of LLMs and categorize them into three\nprimary thought types: execution, reflection, and transition thoughts.\nMoreover, our analysis reveals that excessive reflection and transition\nthoughts are strongly correlated with failure cases and these thought\ncategories exhibit clear separation in the latent space. Based on these, we\nintroduce SEAL (Steerable reasoning calibration), a training-free approach that\nseamlessly calibrates the CoT process, improving accuracy while demonstrating\nsignificant efficiency gains. SEAL consists of an offline stage for extracting\nthe reasoning steering vector in the latent space, followed by an on-the-fly\ncalibration of the reasoning trace through representation intervention using\nthe steering vector. Notably, the steering vector exhibits strong\ntransferability across various tasks. Extensive experiments across multiple\nmodels (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500,\nGSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11%\nimprovement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our\ncode is publicly available at https://github.com/VITA-Group/SEAL."
                },
                "authors": [
                    {
                        "name": "Runjin Chen"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Junyuan Hong"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03295v1",
                "updated": "2025-05-06T08:27:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    27,
                    4,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T08:27:04Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    27,
                    4,
                    1,
                    126,
                    0
                ],
                "title": "Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for\n  Reusing Existing Libraries and Interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for\n  Reusing Existing Libraries and Interfaces"
                },
                "summary": "Modern automation systems increasingly rely on modular architectures, with\ncapabilities and skills as one solution approach. Capabilities define the\nfunctions of resources in a machine-readable form and skills provide the\nconcrete implementations that realize those capabilities. However, the\ndevelopment of a skill implementation conforming to a corresponding capability\nremains a time-consuming and challenging task. In this paper, we present a\nmethod that treats capabilities as contracts for skill implementations and\nleverages large language models to generate executable code based on natural\nlanguage user input. A key feature of our approach is the integration of\nexisting software libraries and interface technologies, enabling the generation\nof skill implementations across different target languages. We introduce a\nframework that allows users to incorporate their own libraries and resource\ninterfaces into the code generation process through a retrieval-augmented\ngeneration architecture. The proposed method is evaluated using an autonomous\nmobile robot controlled via Python and ROS 2, demonstrating the feasibility and\nflexibility of the approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern automation systems increasingly rely on modular architectures, with\ncapabilities and skills as one solution approach. Capabilities define the\nfunctions of resources in a machine-readable form and skills provide the\nconcrete implementations that realize those capabilities. However, the\ndevelopment of a skill implementation conforming to a corresponding capability\nremains a time-consuming and challenging task. In this paper, we present a\nmethod that treats capabilities as contracts for skill implementations and\nleverages large language models to generate executable code based on natural\nlanguage user input. A key feature of our approach is the integration of\nexisting software libraries and interface technologies, enabling the generation\nof skill implementations across different target languages. We introduce a\nframework that allows users to incorporate their own libraries and resource\ninterfaces into the code generation process through a retrieval-augmented\ngeneration architecture. The proposed method is evaluated using an autonomous\nmobile robot controlled via Python and ROS 2, demonstrating the feasibility and\nflexibility of the approach."
                },
                "authors": [
                    {
                        "name": "Luis Miguel Vieira da Silva"
                    },
                    {
                        "name": "Aljosha Köcher"
                    },
                    {
                        "name": "Nicolas König"
                    },
                    {
                        "name": "Felix Gehlhoff"
                    },
                    {
                        "name": "Alexander Fay"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Fay"
                },
                "author": "Alexander Fay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03293v1",
                "updated": "2025-05-06T08:22:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    22,
                    51,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T08:22:51Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    22,
                    51,
                    1,
                    126,
                    0
                ],
                "title": "Ψ-Arena: Interactive Assessment and Optimization of LLM-based\n  Psychological Counselors with Tripartite Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ψ-Arena: Interactive Assessment and Optimization of LLM-based\n  Psychological Counselors with Tripartite Feedback"
                },
                "summary": "Large language models (LLMs) have shown promise in providing scalable mental\nhealth support, while evaluating their counseling capability remains crucial to\nensure both efficacy and safety. Existing evaluations are limited by the static\nassessment that focuses on knowledge tests, the single perspective that centers\non user experience, and the open-loop framework that lacks actionable feedback.\nTo address these issues, we propose {\\Psi}-Arena, an interactive framework for\ncomprehensive assessment and optimization of LLM-based counselors, featuring\nthree key characteristics: (1) Realistic arena interactions that simulate\nreal-world counseling through multi-stage dialogues with psychologically\nprofiled NPC clients, (2) Tripartite evaluation that integrates assessments\nfrom the client, counselor, and supervisor perspectives, and (3) Closed-loop\noptimization that iteratively improves LLM counselors using diagnostic\nfeedback. Experiments across eight state-of-the-art LLMs show significant\nperformance variations in different real-world scenarios and evaluation\nperspectives. Moreover, reflection-based optimization results in up to a 141%\nimprovement in counseling performance. We hope PsychoArena provides a\nfoundational resource for advancing reliable and human-aligned LLM applications\nin mental healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise in providing scalable mental\nhealth support, while evaluating their counseling capability remains crucial to\nensure both efficacy and safety. Existing evaluations are limited by the static\nassessment that focuses on knowledge tests, the single perspective that centers\non user experience, and the open-loop framework that lacks actionable feedback.\nTo address these issues, we propose {\\Psi}-Arena, an interactive framework for\ncomprehensive assessment and optimization of LLM-based counselors, featuring\nthree key characteristics: (1) Realistic arena interactions that simulate\nreal-world counseling through multi-stage dialogues with psychologically\nprofiled NPC clients, (2) Tripartite evaluation that integrates assessments\nfrom the client, counselor, and supervisor perspectives, and (3) Closed-loop\noptimization that iteratively improves LLM counselors using diagnostic\nfeedback. Experiments across eight state-of-the-art LLMs show significant\nperformance variations in different real-world scenarios and evaluation\nperspectives. Moreover, reflection-based optimization results in up to a 141%\nimprovement in counseling performance. We hope PsychoArena provides a\nfoundational resource for advancing reliable and human-aligned LLM applications\nin mental healthcare."
                },
                "authors": [
                    {
                        "name": "Shijing Zhu"
                    },
                    {
                        "name": "Zhuang Chen"
                    },
                    {
                        "name": "Guanqun Bi"
                    },
                    {
                        "name": "Binghang Li"
                    },
                    {
                        "name": "Yaxi Deng"
                    },
                    {
                        "name": "Dazhen Wan"
                    },
                    {
                        "name": "Libiao Peng"
                    },
                    {
                        "name": "Xiyao Xiao"
                    },
                    {
                        "name": "Rongsheng Zhang"
                    },
                    {
                        "name": "Tangjie Lv"
                    },
                    {
                        "name": "Zhipeng Hu"
                    },
                    {
                        "name": "FangFang Li"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "arxiv_comment": "in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10913v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10913v3",
                "updated": "2025-05-06T08:20:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    20,
                    2,
                    1,
                    126,
                    0
                ],
                "published": "2024-09-17T06:11:22Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    6,
                    11,
                    22,
                    1,
                    261,
                    0
                ],
                "title": "ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of\n  Community Health Workers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of\n  Community Health Workers"
                },
                "summary": "Community health workers (CHWs) provide last-mile healthcare services but\nface challenges due to limited medical knowledge and training. This paper\ndescribes the design, deployment, and evaluation of ASHABot, an LLM-powered,\nexperts-in-the-loop, WhatsApp-based chatbot to address the information needs of\nCHWs in India. Through interviews with CHWs and their supervisors and log\nanalysis, we examine factors affecting their engagement with ASHABot, and\nASHABot's role in addressing CHWs' informational needs. We found that ASHABot\nprovided a private channel for CHWs to ask rudimentary and sensitive questions\nthey hesitated to ask supervisors. CHWs trusted the information they received\non ASHABot and treated it as an authoritative resource. CHWs' supervisors\nexpanded their knowledge by contributing answers to questions ASHABot failed to\nanswer, but were concerned about demands on their workload and increased\naccountability. We emphasize positioning LLMs as supplemental fallible\nresources within the community healthcare ecosystem, instead of as replacements\nfor supervisor support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Community health workers (CHWs) provide last-mile healthcare services but\nface challenges due to limited medical knowledge and training. This paper\ndescribes the design, deployment, and evaluation of ASHABot, an LLM-powered,\nexperts-in-the-loop, WhatsApp-based chatbot to address the information needs of\nCHWs in India. Through interviews with CHWs and their supervisors and log\nanalysis, we examine factors affecting their engagement with ASHABot, and\nASHABot's role in addressing CHWs' informational needs. We found that ASHABot\nprovided a private channel for CHWs to ask rudimentary and sensitive questions\nthey hesitated to ask supervisors. CHWs trusted the information they received\non ASHABot and treated it as an authoritative resource. CHWs' supervisors\nexpanded their knowledge by contributing answers to questions ASHABot failed to\nanswer, but were concerned about demands on their workload and increased\naccountability. We emphasize positioning LLMs as supplemental fallible\nresources within the community healthcare ecosystem, instead of as replacements\nfor supervisor support."
                },
                "authors": [
                    {
                        "name": "Pragnya Ramjee"
                    },
                    {
                        "name": "Mehak Chhokar"
                    },
                    {
                        "name": "Bhuvan Sachdeva"
                    },
                    {
                        "name": "Mahendra Meena"
                    },
                    {
                        "name": "Hamid Abdullah"
                    },
                    {
                        "name": "Aditya Vashistha"
                    },
                    {
                        "name": "Ruchit Nagar"
                    },
                    {
                        "name": "Mohit Jain"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Jain"
                },
                "author": "Mohit Jain",
                "arxiv_doi": "10.1145/3706598.3713680",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713680",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.10913v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10913v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "CHI '25: Proceedings of the 2025 CHI Conference on Human Factors\n  in Computing Systems, Article No.: 698, Pages 1-22",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03275v1",
                "updated": "2025-05-06T08:05:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    5,
                    35,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T08:05:35Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    5,
                    35,
                    1,
                    126,
                    0
                ],
                "title": "RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via\n  Retrieval-Augmented Generation"
                },
                "summary": "Large language models (LLMs) struggle to effectively utilize a growing number\nof external tools, such as those defined by the Model Context Protocol\n(MCP)\\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We\nintroduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes\nthis challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to\nidentify the most relevant MCP(s) for a given query from an external index\nbefore engaging the LLM. Only the selected tool descriptions are passed to the\nmodel, drastically reducing prompt size and simplifying decision-making.\nExperiments, including an MCP stress test, demonstrate RAG-MCP significantly\ncuts prompt tokens (e.g., by over 50%) and more than triples tool selection\naccuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables\nscalable and accurate tool integration for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle to effectively utilize a growing number\nof external tools, such as those defined by the Model Context Protocol\n(MCP)\\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We\nintroduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes\nthis challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to\nidentify the most relevant MCP(s) for a given query from an external index\nbefore engaging the LLM. Only the selected tool descriptions are passed to the\nmodel, drastically reducing prompt size and simplifying decision-making.\nExperiments, including an MCP stress test, demonstrate RAG-MCP significantly\ncuts prompt tokens (e.g., by over 50%) and more than triples tool selection\naccuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables\nscalable and accurate tool integration for LLMs."
                },
                "authors": [
                    {
                        "name": "Tiantian Gan"
                    },
                    {
                        "name": "Qiyao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Qiyao Sun"
                },
                "author": "Qiyao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03273v1",
                "updated": "2025-05-06T08:04:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    4,
                    37,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T08:04:37Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    8,
                    4,
                    37,
                    1,
                    126,
                    0
                ],
                "title": "SepALM: Audio Language Models Are Error Correctors for Robust Speech\n  Separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepALM: Audio Language Models Are Error Correctors for Robust Speech\n  Separation"
                },
                "summary": "While contemporary speech separation technologies adeptly process lengthy\nmixed audio waveforms, they are frequently challenged by the intricacies of\nreal-world environments, including noisy and reverberant settings, which can\nresult in artifacts or distortions in the separated speech. To overcome these\nlimitations, we introduce SepALM, a pioneering approach that employs audio\nlanguage models (ALMs) to rectify and re-synthesize speech within the text\ndomain following preliminary separation. SepALM comprises four core components:\na separator, a corrector, a synthesizer, and an aligner. By integrating an\nALM-based end-to-end error correction mechanism, we mitigate the risk of error\naccumulation and circumvent the optimization hurdles typically encountered in\nconventional methods that amalgamate automatic speech recognition (ASR) with\nlarge language models (LLMs). Additionally, we have developed Chain-of-Thought\n(CoT) prompting and knowledge distillation techniques to facilitate the\nreasoning and training processes of the ALM. Our experiments substantiate that\nSepALM not only elevates the precision of speech separation but also markedly\nbolsters adaptability in novel acoustic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While contemporary speech separation technologies adeptly process lengthy\nmixed audio waveforms, they are frequently challenged by the intricacies of\nreal-world environments, including noisy and reverberant settings, which can\nresult in artifacts or distortions in the separated speech. To overcome these\nlimitations, we introduce SepALM, a pioneering approach that employs audio\nlanguage models (ALMs) to rectify and re-synthesize speech within the text\ndomain following preliminary separation. SepALM comprises four core components:\na separator, a corrector, a synthesizer, and an aligner. By integrating an\nALM-based end-to-end error correction mechanism, we mitigate the risk of error\naccumulation and circumvent the optimization hurdles typically encountered in\nconventional methods that amalgamate automatic speech recognition (ASR) with\nlarge language models (LLMs). Additionally, we have developed Chain-of-Thought\n(CoT) prompting and knowledge distillation techniques to facilitate the\nreasoning and training processes of the ALM. Our experiments substantiate that\nSepALM not only elevates the precision of speech separation but also markedly\nbolsters adaptability in novel acoustic environments."
                },
                "authors": [
                    {
                        "name": "Zhaoxi Mu"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Gang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gang Wang"
                },
                "author": "Gang Wang",
                "arxiv_comment": "Appears in IJCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03266v1",
                "updated": "2025-05-06T07:58:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    7,
                    58,
                    26,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T07:58:26Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    7,
                    58,
                    26,
                    1,
                    126,
                    0
                ],
                "title": "Rapid diagnostics of reconfigurable intelligent surfaces using\n  space-time-coding modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid diagnostics of reconfigurable intelligent surfaces using\n  space-time-coding modulation"
                },
                "summary": "Reconfigurable intelligent surfaces (RISs) have emerged as a key technology\nfor shaping smart wireless environments in next-generation wireless\ncommunication systems. To support the large-scale deployment of RISs, a\nreliable and efficient diagnostic method is essential to ensure optimal\nperformance. In this work, a robust and efficient approach for RIS diagnostics\nis proposed using a space-time coding strategy with orthogonal codes. The\nmethod encodes the reflected signals from individual RIS elements into distinct\ncode channels, enabling the recovery of channel power at the receiving\nterminals for fault identification. Theoretical analysis shows that the\nnormally functioning elements generate high power in their respective code\nchannels, whereas the faulty elements exhibit significantly lower power. This\ndistinction enables rapid and accurate diagnostics of elements' operational\nstates through simple signal processing techniques. Simulation results validate\nthe effectiveness of the proposed method, even under high fault ratios and\nvarying reception angles. Proof-of-principle experiments on two RIS prototypes\nare conducted, implementing two coding strategies: direct and segmented.\nExperimental results in a realistic scenario confirm the reliability of the\ndiagnostic method, demonstrating its potential for large-scale RIS deployment\nin future wireless communication systems and radar applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surfaces (RISs) have emerged as a key technology\nfor shaping smart wireless environments in next-generation wireless\ncommunication systems. To support the large-scale deployment of RISs, a\nreliable and efficient diagnostic method is essential to ensure optimal\nperformance. In this work, a robust and efficient approach for RIS diagnostics\nis proposed using a space-time coding strategy with orthogonal codes. The\nmethod encodes the reflected signals from individual RIS elements into distinct\ncode channels, enabling the recovery of channel power at the receiving\nterminals for fault identification. Theoretical analysis shows that the\nnormally functioning elements generate high power in their respective code\nchannels, whereas the faulty elements exhibit significantly lower power. This\ndistinction enables rapid and accurate diagnostics of elements' operational\nstates through simple signal processing techniques. Simulation results validate\nthe effectiveness of the proposed method, even under high fault ratios and\nvarying reception angles. Proof-of-principle experiments on two RIS prototypes\nare conducted, implementing two coding strategies: direct and segmented.\nExperimental results in a realistic scenario confirm the reliability of the\ndiagnostic method, demonstrating its potential for large-scale RIS deployment\nin future wireless communication systems and radar applications."
                },
                "authors": [
                    {
                        "name": "Yi Ning Zheng"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Xiao Qing Chen"
                    },
                    {
                        "name": "Marco Rossi"
                    },
                    {
                        "name": "Giuseppe Castaldi"
                    },
                    {
                        "name": "Shuo Liu"
                    },
                    {
                        "name": "Tie Jun Cui"
                    },
                    {
                        "name": "Vincenzo Galdi"
                    }
                ],
                "author_detail": {
                    "name": "Vincenzo Galdi"
                },
                "author": "Vincenzo Galdi",
                "arxiv_comment": "30 pages, 6 figures, 1 table, supporting information",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05675v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05675v4",
                "updated": "2025-05-06T07:57:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    7,
                    57,
                    8,
                    1,
                    126,
                    0
                ],
                "published": "2025-01-10T02:57:08Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    2,
                    57,
                    8,
                    4,
                    10,
                    0
                ],
                "title": "Synergizing Large Language Models and Task-specific Models for Time\n  Series Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing Large Language Models and Task-specific Models for Time\n  Series Anomaly Detection"
                },
                "summary": "In anomaly detection, methods based on large language models (LLMs) can\nincorporate expert knowledge by reading professional document, while\ntask-specific small models excel at extracting normal data patterns and\ndetecting value fluctuations from training data of target applications.\nInspired by the human nervous system, where the brain stores expert knowledge\nand the peripheral nervous system and spinal cord handle specific tasks like\nwithdrawal and knee-jerk reflexes, we propose CoLLaTe, a framework designed to\nfacilitate collaboration between LLMs and task-specific models, leveraging the\nstrengths of both models for anomaly detection.\n  In particular, we first formulate the collaboration process and identify two\nkey challenges in the collaboration:\n  (1) the misalignment between the expression domains of the LLMs and\ntask-specific small models, and (2) error accumulation arising from the\npredictions of both models.\n  To address these challenges, we then introduce two key components in CoLLaTe:\na model alignment module and a collaborative loss function. Through theoretical\nanalysis and experimental validation, we demonstrate that these components\neffectively mitigate the identified challenges and achieve better performance\nthan both LLM-based and task-specific models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In anomaly detection, methods based on large language models (LLMs) can\nincorporate expert knowledge by reading professional document, while\ntask-specific small models excel at extracting normal data patterns and\ndetecting value fluctuations from training data of target applications.\nInspired by the human nervous system, where the brain stores expert knowledge\nand the peripheral nervous system and spinal cord handle specific tasks like\nwithdrawal and knee-jerk reflexes, we propose CoLLaTe, a framework designed to\nfacilitate collaboration between LLMs and task-specific models, leveraging the\nstrengths of both models for anomaly detection.\n  In particular, we first formulate the collaboration process and identify two\nkey challenges in the collaboration:\n  (1) the misalignment between the expression domains of the LLMs and\ntask-specific small models, and (2) error accumulation arising from the\npredictions of both models.\n  To address these challenges, we then introduce two key components in CoLLaTe:\na model alignment module and a collaborative loss function. Through theoretical\nanalysis and experimental validation, we demonstrate that these components\neffectively mitigate the identified challenges and achieve better performance\nthan both LLM-based and task-specific models."
                },
                "authors": [
                    {
                        "name": "Feiyi Chen"
                    },
                    {
                        "name": "Leilei Zhang"
                    },
                    {
                        "name": "Guansong Pang"
                    },
                    {
                        "name": "Roger Zimmermann"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05675v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05675v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10498v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10498v3",
                "updated": "2025-05-06T07:41:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    7,
                    41,
                    59,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-07T13:43:53Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    43,
                    53,
                    0,
                    97,
                    0
                ],
                "title": "CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation\n  for Large Language Models"
                },
                "summary": "The performance of large language models (LLMs) in Q&A task increased\nsubstantially through Retrieval-Augmented Generation (RAG) which brings in\nexternal knowledge. However, the main difficulty lies in balancing the inherent\nself-knowledge of LLMs with external information retrieval (IR). The current\nthreshold-based methods apply one-dimensional static mechanisms with single\ncriterion. As a result, their IR decisions might be irrelevant to the LLMs'\nresponse under difficult queries. To alleviate this problem, we propose\nCognitive Convection of Self-Knowledge (CCSK). Different from traditional\nmethods that maintain single fixed IR activation criteria, CCSK implements a\ndynamic joint decision process via a Siamese Network module and a Response\nQuality Model. The Siamese Network calculates the cosine similarity between the\ncurrent query and the historical queries. The Response Quality Model evaluates\nthe responses of LLMs through LightGBM. The final decision of the CCSK is\nderived from the outputs of the two modules, as well as text features fused\nusing a multi-head attention mechanism. Extensive experiments on real-world\ndatasets show that CCSK significantly enhances the model's effectiveness in\ninformation retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) in Q&A task increased\nsubstantially through Retrieval-Augmented Generation (RAG) which brings in\nexternal knowledge. However, the main difficulty lies in balancing the inherent\nself-knowledge of LLMs with external information retrieval (IR). The current\nthreshold-based methods apply one-dimensional static mechanisms with single\ncriterion. As a result, their IR decisions might be irrelevant to the LLMs'\nresponse under difficult queries. To alleviate this problem, we propose\nCognitive Convection of Self-Knowledge (CCSK). Different from traditional\nmethods that maintain single fixed IR activation criteria, CCSK implements a\ndynamic joint decision process via a Siamese Network module and a Response\nQuality Model. The Siamese Network calculates the cosine similarity between the\ncurrent query and the historical queries. The Response Quality Model evaluates\nthe responses of LLMs through LightGBM. The final decision of the CCSK is\nderived from the outputs of the two modules, as well as text features fused\nusing a multi-head attention mechanism. Extensive experiments on real-world\ndatasets show that CCSK significantly enhances the model's effectiveness in\ninformation retrieval."
                },
                "authors": [
                    {
                        "name": "Jianling Lu"
                    },
                    {
                        "name": "Mingqi Lv"
                    },
                    {
                        "name": "Tieming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tieming Chen"
                },
                "author": "Tieming Chen",
                "arxiv_comment": "All authors of this paper have unanimously decided to withdraw its\n  preprint from arXiv. As one of the authors, I cannot unilaterally decide its\n  retention. In accordance with the collective decision, we formally request\n  the complete deletion of the paper from arXiv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10498v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10498v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13481v2",
                "updated": "2025-05-06T07:26:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    7,
                    26,
                    13,
                    1,
                    126,
                    0
                ],
                "published": "2025-02-19T07:10:23Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    10,
                    23,
                    2,
                    50,
                    0
                ],
                "title": "LLM4Tag: Automatic Tagging System for Information Retrieval via Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Tag: Automatic Tagging System for Information Retrieval via Large\n  Language Models"
                },
                "summary": "Tagging systems play an essential role in various information retrieval\napplications such as search engines and recommender systems. Recently, Large\nLanguage Models (LLMs) have been applied in tagging systems due to their\nextensive world knowledge, semantic understanding, and reasoning capabilities.\nDespite achieving remarkable performance, existing methods still have\nlimitations, including difficulties in retrieving relevant candidate tags\ncomprehensively, challenges in adapting to emerging domain-specific knowledge,\nand the lack of reliable tag confidence quantification. To address these three\nlimitations above, we propose an automatic tagging system LLM4Tag. First, a\ngraph-based tag recall module is designed to effectively and comprehensively\nconstruct a small-scale highly relevant candidate tag set. Subsequently, a\nknowledge-enhanced tag generation module is employed to generate accurate tags\nwith long-term and short-term knowledge injection. Finally, a tag confidence\ncalibration module is introduced to generate reliable tag confidence scores.\nExtensive experiments over three large-scale industrial datasets show that\nLLM4Tag significantly outperforms the state-of-the-art baselines and LLM4Tag\nhas been deployed online for content tagging to serve hundreds of millions of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tagging systems play an essential role in various information retrieval\napplications such as search engines and recommender systems. Recently, Large\nLanguage Models (LLMs) have been applied in tagging systems due to their\nextensive world knowledge, semantic understanding, and reasoning capabilities.\nDespite achieving remarkable performance, existing methods still have\nlimitations, including difficulties in retrieving relevant candidate tags\ncomprehensively, challenges in adapting to emerging domain-specific knowledge,\nand the lack of reliable tag confidence quantification. To address these three\nlimitations above, we propose an automatic tagging system LLM4Tag. First, a\ngraph-based tag recall module is designed to effectively and comprehensively\nconstruct a small-scale highly relevant candidate tag set. Subsequently, a\nknowledge-enhanced tag generation module is employed to generate accurate tags\nwith long-term and short-term knowledge injection. Finally, a tag confidence\ncalibration module is introduced to generate reliable tag confidence scores.\nExtensive experiments over three large-scale industrial datasets show that\nLLM4Tag significantly outperforms the state-of-the-art baselines and LLM4Tag\nhas been deployed online for content tagging to serve hundreds of millions of\nusers."
                },
                "authors": [
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Chenxu Zhu"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Weipeng Zhang"
                    },
                    {
                        "name": "Menghui Zhu"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Huifeng Guo"
                    }
                ],
                "author_detail": {
                    "name": "Huifeng Guo"
                },
                "author": "Huifeng Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03244v1",
                "updated": "2025-05-06T07:15:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    7,
                    15,
                    4,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T07:15:04Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    7,
                    15,
                    4,
                    1,
                    126,
                    0
                ],
                "title": "SonicRAG : High Fidelity Sound Effects Synthesis Based on Retrival\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SonicRAG : High Fidelity Sound Effects Synthesis Based on Retrival\n  Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing (NLP) and multimodal learning, with successful\napplications in text generation and speech synthesis, enabling a deeper\nunderstanding and generation of multimodal content. In the field of sound\neffects (SFX) generation, LLMs have been leveraged to orchestrate multiple\nmodels for audio synthesis. However, due to the scarcity of annotated datasets,\nand the complexity of temproal modeling. current SFX generation techniques\nstill fall short in achieving high-fidelity audio. To address these\nlimitations, this paper introduces a novel framework that integrates LLMs with\nexisting sound effect databases, allowing for the retrieval, recombination, and\nsynthesis of audio based on user requirements. By leveraging this approach, we\nenhance the diversity and quality of generated sound effects while eliminating\nthe need for additional recording costs, offering a flexible and efficient\nsolution for sound design and application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing (NLP) and multimodal learning, with successful\napplications in text generation and speech synthesis, enabling a deeper\nunderstanding and generation of multimodal content. In the field of sound\neffects (SFX) generation, LLMs have been leveraged to orchestrate multiple\nmodels for audio synthesis. However, due to the scarcity of annotated datasets,\nand the complexity of temproal modeling. current SFX generation techniques\nstill fall short in achieving high-fidelity audio. To address these\nlimitations, this paper introduces a novel framework that integrates LLMs with\nexisting sound effect databases, allowing for the retrieval, recombination, and\nsynthesis of audio based on user requirements. By leveraging this approach, we\nenhance the diversity and quality of generated sound effects while eliminating\nthe need for additional recording costs, offering a flexible and efficient\nsolution for sound design and application."
                },
                "authors": [
                    {
                        "name": "Yu-Ren Guo"
                    },
                    {
                        "name": "Wen-Kai Tai"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Kai Tai"
                },
                "author": "Wen-Kai Tai",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03238v1",
                "updated": "2025-05-06T07:07:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    7,
                    7,
                    28,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T07:07:28Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    7,
                    7,
                    28,
                    1,
                    126,
                    0
                ],
                "title": "RobotxR1: Enabling Embodied Robotic Intelligence on Large Language\n  Models through Closed-Loop Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobotxR1: Enabling Embodied Robotic Intelligence on Large Language\n  Models through Closed-Loop Reinforcement Learning"
                },
                "summary": "Future robotic systems operating in real-world environments will require\non-board embodied intelligence without continuous cloud connection, balancing\ncapabilities with constraints on computational power and memory. This work\npresents an extension of the R1-zero approach, which enables the usage of low\nparameter-count Large Language Models (LLMs) in the robotic domain. The R1-Zero\napproach was originally developed to enable mathematical reasoning in LLMs\nusing static datasets. We extend it to the robotics domain through integration\nin a closed-loop Reinforcement Learning (RL) framework. This extension enhances\nreasoning in Embodied Artificial Intelligence (Embodied AI) settings without\nrelying solely on distillation of large models through Supervised Fine-Tuning\n(SFT). We show that small-scale LLMs can achieve effective reasoning\nperformance by learning through closed-loop interaction with their environment,\nwhich enables tasks that previously required significantly larger models. In an\nautonomous driving setting, a performance gain of 20.2%-points over the\nSFT-based baseline is observed with a Qwen2.5-1.5B model. Using the proposed\ntraining procedure, Qwen2.5-3B achieves a 63.3% control adaptability score,\nsurpassing the 58.5% obtained by the much larger, cloud-bound GPT-4o. These\nresults highlight that practical, on-board deployment of small LLMs is not only\nfeasible but can outperform larger models if trained through environmental\nfeedback, underscoring the importance of an interactive learning framework for\nrobotic Embodied AI, one grounded in practical experience rather than static\nsupervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future robotic systems operating in real-world environments will require\non-board embodied intelligence without continuous cloud connection, balancing\ncapabilities with constraints on computational power and memory. This work\npresents an extension of the R1-zero approach, which enables the usage of low\nparameter-count Large Language Models (LLMs) in the robotic domain. The R1-Zero\napproach was originally developed to enable mathematical reasoning in LLMs\nusing static datasets. We extend it to the robotics domain through integration\nin a closed-loop Reinforcement Learning (RL) framework. This extension enhances\nreasoning in Embodied Artificial Intelligence (Embodied AI) settings without\nrelying solely on distillation of large models through Supervised Fine-Tuning\n(SFT). We show that small-scale LLMs can achieve effective reasoning\nperformance by learning through closed-loop interaction with their environment,\nwhich enables tasks that previously required significantly larger models. In an\nautonomous driving setting, a performance gain of 20.2%-points over the\nSFT-based baseline is observed with a Qwen2.5-1.5B model. Using the proposed\ntraining procedure, Qwen2.5-3B achieves a 63.3% control adaptability score,\nsurpassing the 58.5% obtained by the much larger, cloud-bound GPT-4o. These\nresults highlight that practical, on-board deployment of small LLMs is not only\nfeasible but can outperform larger models if trained through environmental\nfeedback, underscoring the importance of an interactive learning framework for\nrobotic Embodied AI, one grounded in practical experience rather than static\nsupervision."
                },
                "authors": [
                    {
                        "name": "Liam Boyle"
                    },
                    {
                        "name": "Nicolas Baumann"
                    },
                    {
                        "name": "Paviththiren Sivasothilingam"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02118v2",
                "updated": "2025-05-06T07:07:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    7,
                    7,
                    13,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-04T14:00:04Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    14,
                    0,
                    4,
                    6,
                    124,
                    0
                ],
                "title": "Adversarial Cooperative Rationalization: The Risk of Spurious\n  Correlations in Even Clean Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Cooperative Rationalization: The Risk of Spurious\n  Correlations in Even Clean Datasets"
                },
                "summary": "This study investigates the self-rationalization framework constructed with a\ncooperative game, where a generator initially extracts the most informative\nsegment from raw input, and a subsequent predictor utilizes the selected subset\nfor its input. The generator and predictor are trained collaboratively to\nmaximize prediction accuracy. In this paper, we first uncover a potential\ncaveat: such a cooperative game could unintentionally introduce a sampling bias\nduring rationale extraction. Specifically, the generator might inadvertently\ncreate an incorrect correlation between the selected rationale candidate and\nthe label, even when they are semantically unrelated in the original dataset.\nSubsequently, we elucidate the origins of this bias using both detailed\ntheoretical analysis and empirical evidence. Our findings suggest a direction\nfor inspecting these correlations through attacks, based on which we further\nintroduce an instruction to prevent the predictor from learning the\ncorrelations. Through experiments on six text classification datasets and two\ngraph classification datasets using three network architectures (GRUs, BERT,\nand GCN), we show that our method not only significantly outperforms recent\nrationalization methods, but also achieves comparable or even better results\nthan a representative LLM (llama3.1-8b-instruct).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the self-rationalization framework constructed with a\ncooperative game, where a generator initially extracts the most informative\nsegment from raw input, and a subsequent predictor utilizes the selected subset\nfor its input. The generator and predictor are trained collaboratively to\nmaximize prediction accuracy. In this paper, we first uncover a potential\ncaveat: such a cooperative game could unintentionally introduce a sampling bias\nduring rationale extraction. Specifically, the generator might inadvertently\ncreate an incorrect correlation between the selected rationale candidate and\nthe label, even when they are semantically unrelated in the original dataset.\nSubsequently, we elucidate the origins of this bias using both detailed\ntheoretical analysis and empirical evidence. Our findings suggest a direction\nfor inspecting these correlations through attacks, based on which we further\nintroduce an instruction to prevent the predictor from learning the\ncorrelations. Through experiments on six text classification datasets and two\ngraph classification datasets using three network architectures (GRUs, BERT,\nand GCN), we show that our method not only significantly outperforms recent\nrationalization methods, but also achieves comparable or even better results\nthan a representative LLM (llama3.1-8b-instruct)."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Zhongyu Niu"
                    },
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Zhiying Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02737v2",
                "updated": "2025-05-06T06:44:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    6,
                    44,
                    35,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T15:40:24Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    15,
                    40,
                    24,
                    0,
                    125,
                    0
                ],
                "title": "Knowledge Graphs for Enhancing Large Language Models in Entity\n  Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graphs for Enhancing Large Language Models in Entity\n  Disambiguation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have positioned them as a\nprominent solution for Natural Language Processing tasks. Notably, they can\napproach these problems in a zero or few-shot manner, thereby eliminating the\nneed for training or fine-tuning task-specific models. However, LLMs face some\nchallenges, including hallucination and the presence of outdated knowledge or\nmissing information from specific domains in the training data. These problems\ncannot be easily solved by retraining the models with new data as it is a\ntime-consuming and expensive process. To mitigate these issues, Knowledge\nGraphs (KGs) have been proposed as a structured external source of information\nto enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for\nzero-shot Entity Disambiguation (ED). For that purpose, we leverage the\nhierarchical representation of the entities' classes in a KG to gradually prune\nthe candidate space as well as the entities' descriptions to enrich the input\nprompt with additional factual knowledge. Our evaluation on popular ED datasets\nshows that the proposed method outperforms non-enhanced and description-only\nenhanced LLMs, and has a higher degree of adaptability than task-specific\nmodels. Furthermore, we conduct an error analysis and discuss the impact of the\nleveraged KG's semantic expressivity on the ED performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have positioned them as a\nprominent solution for Natural Language Processing tasks. Notably, they can\napproach these problems in a zero or few-shot manner, thereby eliminating the\nneed for training or fine-tuning task-specific models. However, LLMs face some\nchallenges, including hallucination and the presence of outdated knowledge or\nmissing information from specific domains in the training data. These problems\ncannot be easily solved by retraining the models with new data as it is a\ntime-consuming and expensive process. To mitigate these issues, Knowledge\nGraphs (KGs) have been proposed as a structured external source of information\nto enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for\nzero-shot Entity Disambiguation (ED). For that purpose, we leverage the\nhierarchical representation of the entities' classes in a KG to gradually prune\nthe candidate space as well as the entities' descriptions to enrich the input\nprompt with additional factual knowledge. Our evaluation on popular ED datasets\nshows that the proposed method outperforms non-enhanced and description-only\nenhanced LLMs, and has a higher degree of adaptability than task-specific\nmodels. Furthermore, we conduct an error analysis and discuss the impact of the\nleveraged KG's semantic expressivity on the ED performance."
                },
                "authors": [
                    {
                        "name": "Gerard Pons"
                    },
                    {
                        "name": "Besim Bilalli"
                    },
                    {
                        "name": "Anna Queralt"
                    }
                ],
                "author_detail": {
                    "name": "Anna Queralt"
                },
                "author": "Anna Queralt",
                "arxiv_doi": "10.1007/978-3-031-77844-5_9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-77844-5_9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.02737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Pre-print submitted to ISWC 2024",
                "arxiv_journal_ref": "Proc. 23rd Int. Semantic Web Conf. (ISWC 2024), LNCS, Springer,\n  2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02579v2",
                "updated": "2025-05-06T06:26:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    6,
                    26,
                    11,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-05T11:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    30,
                    46,
                    0,
                    125,
                    0
                ],
                "title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning"
                },
                "summary": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives."
                },
                "authors": [
                    {
                        "name": "Lingxiao Kong"
                    },
                    {
                        "name": "Cong Yang"
                    },
                    {
                        "name": "Susanne Neufang"
                    },
                    {
                        "name": "Oya Deniz Beyan"
                    },
                    {
                        "name": "Zeyd Boukhers"
                    }
                ],
                "author_detail": {
                    "name": "Zeyd Boukhers"
                },
                "author": "Zeyd Boukhers",
                "arxiv_comment": "13 pages, 9 figures, submitted to SIGDIAL 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03209v1",
                "updated": "2025-05-06T05:53:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    5,
                    53,
                    9,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T05:53:09Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    5,
                    53,
                    9,
                    1,
                    126,
                    0
                ],
                "title": "DYSTIL: Dynamic Strategy Induction with Large Language Models for\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DYSTIL: Dynamic Strategy Induction with Large Language Models for\n  Reinforcement Learning"
                },
                "summary": "Reinforcement learning from expert demonstrations has long remained a\nchallenging research problem, and existing state-of-the-art methods using\nbehavioral cloning plus further RL training often suffer from poor\ngeneralization, low sample efficiency, and poor model interpretability.\nInspired by the strong reasoning abilities of large language models (LLMs), we\npropose a novel strategy-based reinforcement learning framework integrated with\nLLMs called DYnamic STrategy Induction with Llms for reinforcement learning\n(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a\nstrategy-generating LLM to induce textual strategies based on advantage\nestimations and expert demonstrations, and gradually internalizes induced\nstrategies into the RL agent through policy optimization to improve its\nperformance through boosting policy generalization and enhancing sample\nefficiency. It also provides a direct textual channel to observe and interpret\nthe evolution of the policy's underlying strategies during training. We test\nDYSTIL over challenging RL environments from Minigrid and BabyAI, and\nempirically demonstrate that DYSTIL significantly outperforms state-of-the-art\nbaseline methods by 17.75% in average success rate while also enjoying higher\nsample efficiency during the learning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from expert demonstrations has long remained a\nchallenging research problem, and existing state-of-the-art methods using\nbehavioral cloning plus further RL training often suffer from poor\ngeneralization, low sample efficiency, and poor model interpretability.\nInspired by the strong reasoning abilities of large language models (LLMs), we\npropose a novel strategy-based reinforcement learning framework integrated with\nLLMs called DYnamic STrategy Induction with Llms for reinforcement learning\n(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a\nstrategy-generating LLM to induce textual strategies based on advantage\nestimations and expert demonstrations, and gradually internalizes induced\nstrategies into the RL agent through policy optimization to improve its\nperformance through boosting policy generalization and enhancing sample\nefficiency. It also provides a direct textual channel to observe and interpret\nthe evolution of the policy's underlying strategies during training. We test\nDYSTIL over challenging RL environments from Minigrid and BabyAI, and\nempirically demonstrate that DYSTIL significantly outperforms state-of-the-art\nbaseline methods by 17.75% in average success rate while also enjoying higher\nsample efficiency during the learning process."
                },
                "authors": [
                    {
                        "name": "Borui Wang"
                    },
                    {
                        "name": "Kathleen McKeown"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06269v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06269v2",
                "updated": "2025-05-06T05:48:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    5,
                    48,
                    7,
                    1,
                    126,
                    0
                ],
                "published": "2025-03-08T16:29:45Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    16,
                    29,
                    45,
                    5,
                    67,
                    0
                ],
                "title": "Using Mechanistic Interpretability to Craft Adversarial Attacks against\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Mechanistic Interpretability to Craft Adversarial Attacks against\n  Large Language Models"
                },
                "summary": "Traditional white-box methods for creating adversarial perturbations against\nLLMs typically rely only on gradient computation from the targeted model,\nignoring the internal mechanisms responsible for attack success or failure.\nConversely, interpretability studies that analyze these internal mechanisms\nlack practical applications beyond runtime interventions. We bridge this gap by\nintroducing a novel white-box approach that leverages mechanistic\ninterpretability techniques to craft practical adversarial inputs.\nSpecifically, we first identify acceptance subspaces - sets of feature vectors\nthat do not trigger the model's refusal mechanisms - then use gradient-based\noptimization to reroute embeddings from refusal subspaces to acceptance\nsubspaces, effectively achieving jailbreaks. This targeted approach\nsignificantly reduces computation cost, achieving attack success rates of\n80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5\nwithin minutes or even seconds, compared to existing techniques that often fail\nor require hours of computation. We believe this approach opens a new direction\nfor both attack research and defense development. Furthermore, it showcases a\npractical application of mechanistic interpretability where other methods are\nless efficient, which highlights its utility. The code and generated datasets\nare available at https://github.com/Sckathach/subspace-rerouting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional white-box methods for creating adversarial perturbations against\nLLMs typically rely only on gradient computation from the targeted model,\nignoring the internal mechanisms responsible for attack success or failure.\nConversely, interpretability studies that analyze these internal mechanisms\nlack practical applications beyond runtime interventions. We bridge this gap by\nintroducing a novel white-box approach that leverages mechanistic\ninterpretability techniques to craft practical adversarial inputs.\nSpecifically, we first identify acceptance subspaces - sets of feature vectors\nthat do not trigger the model's refusal mechanisms - then use gradient-based\noptimization to reroute embeddings from refusal subspaces to acceptance\nsubspaces, effectively achieving jailbreaks. This targeted approach\nsignificantly reduces computation cost, achieving attack success rates of\n80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5\nwithin minutes or even seconds, compared to existing techniques that often fail\nor require hours of computation. We believe this approach opens a new direction\nfor both attack research and defense development. Furthermore, it showcases a\npractical application of mechanistic interpretability where other methods are\nless efficient, which highlights its utility. The code and generated datasets\nare available at https://github.com/Sckathach/subspace-rerouting."
                },
                "authors": [
                    {
                        "name": "Thomas Winninger"
                    },
                    {
                        "name": "Boussad Addad"
                    },
                    {
                        "name": "Katarzyna Kapusta"
                    }
                ],
                "author_detail": {
                    "name": "Katarzyna Kapusta"
                },
                "author": "Katarzyna Kapusta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06269v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06269v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03196v1",
                "updated": "2025-05-06T05:32:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    5,
                    32,
                    46,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T05:32:46Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    5,
                    32,
                    46,
                    1,
                    126,
                    0
                ],
                "title": "A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case"
                },
                "summary": "Large Language Models (LLMs) demonstrate strong potential across a variety of\ntasks in communications and networking due to their advanced reasoning\ncapabilities. However, because different LLMs have different model structures\nand are trained using distinct corpora and methods, they may offer varying\noptimization strategies for the same network issues. Moreover, the limitations\nof an individual LLM's training data, aggravated by the potential maliciousness\nof its hosting device, can result in responses with low confidence or even\nbias. To address these challenges, we propose a blockchain-enabled\ncollaborative framework that connects multiple LLMs into a Trustworthy\nMulti-LLM Network (MultiLLMN). This architecture enables the cooperative\nevaluation and selection of the most reliable and high-quality responses to\ncomplex network optimization problems. Specifically, we begin by reviewing\nrelated work and highlighting the limitations of existing LLMs in collaboration\nand trust, emphasizing the need for trustworthiness in LLM-based systems. We\nthen introduce the workflow and design of the proposed Trustworthy MultiLLMN\nframework. Given the severity of False Base Station (FBS) attacks in B5G and 6G\ncommunication systems and the difficulty of addressing such threats through\ntraditional modeling techniques, we present FBS defense as a case study to\nempirically validate the effectiveness of our approach. Finally, we outline\npromising future research directions in this emerging area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate strong potential across a variety of\ntasks in communications and networking due to their advanced reasoning\ncapabilities. However, because different LLMs have different model structures\nand are trained using distinct corpora and methods, they may offer varying\noptimization strategies for the same network issues. Moreover, the limitations\nof an individual LLM's training data, aggravated by the potential maliciousness\nof its hosting device, can result in responses with low confidence or even\nbias. To address these challenges, we propose a blockchain-enabled\ncollaborative framework that connects multiple LLMs into a Trustworthy\nMulti-LLM Network (MultiLLMN). This architecture enables the cooperative\nevaluation and selection of the most reliable and high-quality responses to\ncomplex network optimization problems. Specifically, we begin by reviewing\nrelated work and highlighting the limitations of existing LLMs in collaboration\nand trust, emphasizing the need for trustworthiness in LLM-based systems. We\nthen introduce the workflow and design of the proposed Trustworthy MultiLLMN\nframework. Given the severity of False Base Station (FBS) attacks in B5G and 6G\ncommunication systems and the difficulty of addressing such threats through\ntraditional modeling techniques, we present FBS defense as a case study to\nempirically validate the effectiveness of our approach. Finally, we outline\npromising future research directions in this emerging area."
                },
                "authors": [
                    {
                        "name": "Haoxiang Luo"
                    },
                    {
                        "name": "Gang Sun"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Hongfang Yu"
                    },
                    {
                        "name": "Mohammed Atiquzzaman"
                    },
                    {
                        "name": "Schahram Dustdar"
                    }
                ],
                "author_detail": {
                    "name": "Schahram Dustdar"
                },
                "author": "Schahram Dustdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03189v1",
                "updated": "2025-05-06T05:15:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    5,
                    15,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T05:15:12Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    5,
                    15,
                    12,
                    1,
                    126,
                    0
                ],
                "title": "Patterns and Mechanisms of Contrastive Activation Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patterns and Mechanisms of Contrastive Activation Engineering"
                },
                "summary": "Controlling the behavior of Large Language Models (LLMs) remains a\nsignificant challenge due to their inherent complexity and opacity. While\ntechniques like fine-tuning can modify model behavior, they typically require\nextensive computational resources. Recent work has introduced a class of\ncontrastive activation engineering (CAE) techniques as promising approaches for\nsteering LLM outputs through targeted modifications to their internal\nrepresentations. Applied at inference-time with zero cost, CAE has the\npotential to introduce a new paradigm of flexible, task-specific LLM behavior\ntuning. We analyze the performance of CAE in in-distribution,\nout-of-distribution settings, evaluate drawbacks, and begin to develop\ncomprehensive guidelines for its effective deployment. We find that 1. CAE is\nonly reliably effective when applied to in-distribution contexts. 2. Increasing\nthe number of samples used to generate steering vectors has diminishing returns\nat around 80 samples. 3. Steering vectors are susceptible to adversarial inputs\nthat reverses the behavior that is steered for. 4. Steering vectors harm the\noverall model perplexity. 5. Larger models are more resistant to\nsteering-induced degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling the behavior of Large Language Models (LLMs) remains a\nsignificant challenge due to their inherent complexity and opacity. While\ntechniques like fine-tuning can modify model behavior, they typically require\nextensive computational resources. Recent work has introduced a class of\ncontrastive activation engineering (CAE) techniques as promising approaches for\nsteering LLM outputs through targeted modifications to their internal\nrepresentations. Applied at inference-time with zero cost, CAE has the\npotential to introduce a new paradigm of flexible, task-specific LLM behavior\ntuning. We analyze the performance of CAE in in-distribution,\nout-of-distribution settings, evaluate drawbacks, and begin to develop\ncomprehensive guidelines for its effective deployment. We find that 1. CAE is\nonly reliably effective when applied to in-distribution contexts. 2. Increasing\nthe number of samples used to generate steering vectors has diminishing returns\nat around 80 samples. 3. Steering vectors are susceptible to adversarial inputs\nthat reverses the behavior that is steered for. 4. Steering vectors harm the\noverall model perplexity. 5. Larger models are more resistant to\nsteering-induced degradation."
                },
                "authors": [
                    {
                        "name": "Yixiong Hao"
                    },
                    {
                        "name": "Ayush Panda"
                    },
                    {
                        "name": "Stepan Shabalin"
                    },
                    {
                        "name": "Sheikh Abdur Raheem Ali"
                    }
                ],
                "author_detail": {
                    "name": "Sheikh Abdur Raheem Ali"
                },
                "author": "Sheikh Abdur Raheem Ali",
                "arxiv_comment": "Published at the ICLR 2025 Bi-Align, HAIC, and Building Trust\n  workshops",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13460v3",
                "updated": "2025-05-06T05:00:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    5,
                    0,
                    15,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-18T04:35:35Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    4,
                    35,
                    35,
                    4,
                    108,
                    0
                ],
                "title": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action\n  Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action\n  Localization"
                },
                "summary": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark."
                },
                "authors": [
                    {
                        "name": "Hongwei Ji"
                    },
                    {
                        "name": "Wulian Yun"
                    },
                    {
                        "name": "Mengshi Qi"
                    },
                    {
                        "name": "Huadong Ma"
                    }
                ],
                "author_detail": {
                    "name": "Huadong Ma"
                },
                "author": "Huadong Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03181v1",
                "updated": "2025-05-06T04:51:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    4,
                    51,
                    57,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T04:51:57Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    4,
                    51,
                    57,
                    1,
                    126,
                    0
                ],
                "title": "VLM Q-Learning: Aligning Vision-Language Models for Interactive\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM Q-Learning: Aligning Vision-Language Models for Interactive\n  Decision-Making"
                },
                "summary": "Recent research looks to harness the general knowledge and reasoning of large\nlanguage models (LLMs) into agents that accomplish user-specified goals in\ninteractive environments. Vision-language models (VLMs) extend LLMs to\nmulti-modal data and provide agents with the visual reasoning necessary for new\napplications in areas such as computer automation. However, agent tasks\nemphasize skills where accessible open-weight VLMs lag behind their LLM\nequivalents. For example, VLMs are less capable of following an environment's\nstrict output syntax requirements and are more focused on open-ended question\nanswering. Overcoming these limitations requires supervised fine-tuning (SFT)\non task-specific expert demonstrations. Our work approaches these challenges\nfrom an offline-to-online reinforcement learning (RL) perspective. RL lets us\nfine-tune VLMs to agent tasks while learning from the unsuccessful decisions of\nour own model or more capable (larger) models. We explore an off-policy RL\nsolution that retains the stability and simplicity of the widely used SFT\nworkflow while allowing our agent to self-improve and learn from low-quality\ndatasets. We demonstrate this technique with two open-weight VLMs across three\nmulti-modal agent domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research looks to harness the general knowledge and reasoning of large\nlanguage models (LLMs) into agents that accomplish user-specified goals in\ninteractive environments. Vision-language models (VLMs) extend LLMs to\nmulti-modal data and provide agents with the visual reasoning necessary for new\napplications in areas such as computer automation. However, agent tasks\nemphasize skills where accessible open-weight VLMs lag behind their LLM\nequivalents. For example, VLMs are less capable of following an environment's\nstrict output syntax requirements and are more focused on open-ended question\nanswering. Overcoming these limitations requires supervised fine-tuning (SFT)\non task-specific expert demonstrations. Our work approaches these challenges\nfrom an offline-to-online reinforcement learning (RL) perspective. RL lets us\nfine-tune VLMs to agent tasks while learning from the unsuccessful decisions of\nour own model or more capable (larger) models. We explore an off-policy RL\nsolution that retains the stability and simplicity of the widely used SFT\nworkflow while allowing our agent to self-improve and learn from low-quality\ndatasets. We demonstrate this technique with two open-weight VLMs across three\nmulti-modal agent domains."
                },
                "authors": [
                    {
                        "name": "Jake Grigsby"
                    },
                    {
                        "name": "Yuke Zhu"
                    },
                    {
                        "name": "Michael Ryoo"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    }
                ],
                "author_detail": {
                    "name": "Juan Carlos Niebles"
                },
                "author": "Juan Carlos Niebles",
                "arxiv_comment": "SSI-FM Workshop ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03179v1",
                "updated": "2025-05-06T04:47:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    4,
                    47,
                    52,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T04:47:52Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    4,
                    47,
                    52,
                    1,
                    126,
                    0
                ],
                "title": "Bridging Expertise Gaps: The Role of LLMs in Human-AI Collaboration for\n  Cybersecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Expertise Gaps: The Role of LLMs in Human-AI Collaboration for\n  Cybersecurity"
                },
                "summary": "This study investigates whether large language models (LLMs) can function as\nintelligent collaborators to bridge expertise gaps in cybersecurity\ndecision-making. We examine two representative tasks-phishing email detection\nand intrusion detection-that differ in data modality, cognitive complexity, and\nuser familiarity. Through a controlled mixed-methods user study, n = 58\n(phishing, n = 34; intrusion, n = 24), we find that human-AI collaboration\nimproves task performance,reducing false positives in phishing detection and\nfalse negatives in intrusion detection. A learning effect is also observed when\nparticipants transition from collaboration to independent work, suggesting that\nLLMs can support long-term skill development. Our qualitative analysis shows\nthat interaction dynamics-such as LLM definitiveness, explanation style, and\ntone-influence user trust, prompting strategies, and decision revision. Users\nengaged in more analytic questioning and showed greater reliance on LLM\nfeedback in high-complexity settings. These results provide design guidance for\nbuilding interpretable, adaptive, and trustworthy human-AI teaming systems, and\ndemonstrate that LLMs can meaningfully support non-experts in reasoning through\ncomplex cybersecurity problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates whether large language models (LLMs) can function as\nintelligent collaborators to bridge expertise gaps in cybersecurity\ndecision-making. We examine two representative tasks-phishing email detection\nand intrusion detection-that differ in data modality, cognitive complexity, and\nuser familiarity. Through a controlled mixed-methods user study, n = 58\n(phishing, n = 34; intrusion, n = 24), we find that human-AI collaboration\nimproves task performance,reducing false positives in phishing detection and\nfalse negatives in intrusion detection. A learning effect is also observed when\nparticipants transition from collaboration to independent work, suggesting that\nLLMs can support long-term skill development. Our qualitative analysis shows\nthat interaction dynamics-such as LLM definitiveness, explanation style, and\ntone-influence user trust, prompting strategies, and decision revision. Users\nengaged in more analytic questioning and showed greater reliance on LLM\nfeedback in high-complexity settings. These results provide design guidance for\nbuilding interpretable, adaptive, and trustworthy human-AI teaming systems, and\ndemonstrate that LLMs can meaningfully support non-experts in reasoning through\ncomplex cybersecurity problems."
                },
                "authors": [
                    {
                        "name": "Shahroz Tariq"
                    },
                    {
                        "name": "Ronal Singh"
                    },
                    {
                        "name": "Mohan Baruwal Chhetri"
                    },
                    {
                        "name": "Surya Nepal"
                    },
                    {
                        "name": "Cecile Paris"
                    }
                ],
                "author_detail": {
                    "name": "Cecile Paris"
                },
                "author": "Cecile Paris",
                "arxiv_comment": "20 pages, 10 figures, 2 tables, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03171v1",
                "updated": "2025-05-06T04:32:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    4,
                    32,
                    17,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T04:32:17Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    4,
                    32,
                    17,
                    1,
                    126,
                    0
                ],
                "title": "CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics"
                },
                "summary": "Neurosymbolic approaches integrating large language models with formal\nreasoning have recently achieved human-level performance on mathematics\ncompetition problems in algebra, geometry and number theory. In comparison,\ncombinatorics remains a challenging domain, characterized by a lack of\nappropriate benchmarks and theorem libraries. To address this gap, we introduce\nCombiBench, a comprehensive benchmark comprising 100 combinatorial problems,\neach formalized in Lean~4 and paired with its corresponding informal statement.\nThe problem set covers a wide spectrum of difficulty levels, ranging from\nmiddle school to IMO and university level, and span over ten combinatorial\ntopics. CombiBench is suitable for testing IMO solving capabilities since it\nincludes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its\nstatement contain an images). Furthermore, we provide a comprehensive and\nstandardized evaluation framework, dubbed Fine-Eval (for\n$\\textbf{F}$ill-in-the-blank $\\textbf{in}$ L$\\textbf{e}$an Evaluation), for\nformal mathematics. It accommodates not only proof-based problems but also, for\nthe first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval\nas the evaluation method and Kimina Lean Server as the backend, we benchmark\nseveral LLMs on CombiBench and observe that their capabilities for formally\nsolving combinatorial problems remain limited. Among all models tested (none of\nwhich has been trained for this particular task), Kimina-Prover attains the\nbest results, solving 7 problems (out of 100) under both ``with solution'' and\n``without solution'' scenarios. We open source the benchmark dataset alongside\nwith the code of the proposed evaluation method at\nhttps://github.com/MoonshotAI/CombiBench/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neurosymbolic approaches integrating large language models with formal\nreasoning have recently achieved human-level performance on mathematics\ncompetition problems in algebra, geometry and number theory. In comparison,\ncombinatorics remains a challenging domain, characterized by a lack of\nappropriate benchmarks and theorem libraries. To address this gap, we introduce\nCombiBench, a comprehensive benchmark comprising 100 combinatorial problems,\neach formalized in Lean~4 and paired with its corresponding informal statement.\nThe problem set covers a wide spectrum of difficulty levels, ranging from\nmiddle school to IMO and university level, and span over ten combinatorial\ntopics. CombiBench is suitable for testing IMO solving capabilities since it\nincludes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its\nstatement contain an images). Furthermore, we provide a comprehensive and\nstandardized evaluation framework, dubbed Fine-Eval (for\n$\\textbf{F}$ill-in-the-blank $\\textbf{in}$ L$\\textbf{e}$an Evaluation), for\nformal mathematics. It accommodates not only proof-based problems but also, for\nthe first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval\nas the evaluation method and Kimina Lean Server as the backend, we benchmark\nseveral LLMs on CombiBench and observe that their capabilities for formally\nsolving combinatorial problems remain limited. Among all models tested (none of\nwhich has been trained for this particular task), Kimina-Prover attains the\nbest results, solving 7 problems (out of 100) under both ``with solution'' and\n``without solution'' scenarios. We open source the benchmark dataset alongside\nwith the code of the proposed evaluation method at\nhttps://github.com/MoonshotAI/CombiBench/."
                },
                "authors": [
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Xiaohan Lin"
                    },
                    {
                        "name": "Jonas Bayer"
                    },
                    {
                        "name": "Yael Dillies"
                    },
                    {
                        "name": "Weijie Jiang"
                    },
                    {
                        "name": "Xiaodan Liang"
                    },
                    {
                        "name": "Roman Soletskyi"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Yunzhou Xie"
                    },
                    {
                        "name": "Beibei Xiong"
                    },
                    {
                        "name": "Zhengfeng Yang"
                    },
                    {
                        "name": "Jujian Zhang"
                    },
                    {
                        "name": "Lihong Zhi"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Zhengying Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengying Liu"
                },
                "author": "Zhengying Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03163v1",
                "updated": "2025-05-06T04:14:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    4,
                    14,
                    32,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T04:14:32Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    4,
                    14,
                    32,
                    1,
                    126,
                    0
                ],
                "title": "The Impact of Large Language Models on K-12 Education in Rural India: A\n  Thematic Analysis of Student Volunteer's Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Large Language Models on K-12 Education in Rural India: A\n  Thematic Analysis of Student Volunteer's Perspectives"
                },
                "summary": "AI-driven education, particularly Large Language Models (LLMs), has the\npotential to address learning disparities in rural K-12 schools. However,\nresearch on AI adoption in rural India remains limited, with existing studies\nfocusing primarily on urban settings. This study examines the perceptions of\nvolunteer teachers on AI integration in rural education, identifying key\nchallenges and opportunities. Through semi-structured interviews with 23\nvolunteer educators in Rajasthan and Delhi, we conducted a thematic analysis to\nexplore infrastructure constraints, teacher preparedness, and digital literacy\ngaps. Findings indicate that while LLMs could enhance personalized learning and\nreduce teacher workload, barriers such as poor connectivity, lack of AI\ntraining, and parental skepticism hinder adoption. Despite concerns over\nover-reliance and ethical risks, volunteers emphasize that AI should be seen as\na complementary tool rather than a replacement for traditional teaching. Given\nthe potential benefits, LLM-based tutors merit further exploration in rural\nclassrooms, with structured implementation and localized adaptations to ensure\naccessibility and equity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-driven education, particularly Large Language Models (LLMs), has the\npotential to address learning disparities in rural K-12 schools. However,\nresearch on AI adoption in rural India remains limited, with existing studies\nfocusing primarily on urban settings. This study examines the perceptions of\nvolunteer teachers on AI integration in rural education, identifying key\nchallenges and opportunities. Through semi-structured interviews with 23\nvolunteer educators in Rajasthan and Delhi, we conducted a thematic analysis to\nexplore infrastructure constraints, teacher preparedness, and digital literacy\ngaps. Findings indicate that while LLMs could enhance personalized learning and\nreduce teacher workload, barriers such as poor connectivity, lack of AI\ntraining, and parental skepticism hinder adoption. Despite concerns over\nover-reliance and ethical risks, volunteers emphasize that AI should be seen as\na complementary tool rather than a replacement for traditional teaching. Given\nthe potential benefits, LLM-based tutors merit further exploration in rural\nclassrooms, with structured implementation and localized adaptations to ensure\naccessibility and equity."
                },
                "authors": [
                    {
                        "name": "Harshita Goyal"
                    },
                    {
                        "name": "Garima Garg"
                    },
                    {
                        "name": "Prisha Mordia"
                    },
                    {
                        "name": "Veena Ramachandran"
                    },
                    {
                        "name": "Dhruv Kumar"
                    },
                    {
                        "name": "Jagat Sesh Challa"
                    }
                ],
                "author_detail": {
                    "name": "Jagat Sesh Challa"
                },
                "author": "Jagat Sesh Challa",
                "arxiv_comment": "Under review, 23 pages, 1 figure, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03161v1",
                "updated": "2025-05-06T04:14:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    4,
                    14,
                    13,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T04:14:13Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    4,
                    14,
                    13,
                    1,
                    126,
                    0
                ],
                "title": "An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground\n  Integrated Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground\n  Integrated Networks"
                },
                "summary": "Recently emerged 6G space-air-ground integrated networks (SAGINs), which\nintegrate satellites, aerial networks, and terrestrial communications, offer\nubiquitous coverage for various mobile applications. However, the highly\ndynamic, open, and heterogeneous nature of SAGINs poses severe security issues.\nForming a defense line of SAGINs suffers from two preliminary challenges: 1)\naccurately understanding massive unstructured multi-dimensional threat\ninformation to generate defense strategies against various malicious attacks,\n2) rapidly adapting to potential unknown threats to yield more effective\nsecurity strategies. To tackle the above two challenges, we propose a novel\nsecurity framework for SAGINs based on Large Language Models (LLMs), which\nconsists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG\nleverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent\nmechanisms to analyze massive unstructured multi-dimensional threat data and\ngenerate comprehensive security strategies, thus addressing the first\nchallenge. Our proposed 6G-INST relies on a novel self-evolving method to\nautomatically update LLM-6GNG, enabling it to accommodate unknown threats under\ndynamic communication environments, thereby addressing the second challenge.\nAdditionally, we prototype the proposed framework with ns-3, OpenAirInterface\n(OAI), and software-defined radio (SDR). Experiments on three benchmarks\ndemonstrate the effectiveness of our framework. The results show that our\nframework produces highly accurate security strategies that remain robust\nagainst a variety of unknown attacks. We will release our code to contribute to\nthe community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently emerged 6G space-air-ground integrated networks (SAGINs), which\nintegrate satellites, aerial networks, and terrestrial communications, offer\nubiquitous coverage for various mobile applications. However, the highly\ndynamic, open, and heterogeneous nature of SAGINs poses severe security issues.\nForming a defense line of SAGINs suffers from two preliminary challenges: 1)\naccurately understanding massive unstructured multi-dimensional threat\ninformation to generate defense strategies against various malicious attacks,\n2) rapidly adapting to potential unknown threats to yield more effective\nsecurity strategies. To tackle the above two challenges, we propose a novel\nsecurity framework for SAGINs based on Large Language Models (LLMs), which\nconsists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG\nleverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent\nmechanisms to analyze massive unstructured multi-dimensional threat data and\ngenerate comprehensive security strategies, thus addressing the first\nchallenge. Our proposed 6G-INST relies on a novel self-evolving method to\nautomatically update LLM-6GNG, enabling it to accommodate unknown threats under\ndynamic communication environments, thereby addressing the second challenge.\nAdditionally, we prototype the proposed framework with ns-3, OpenAirInterface\n(OAI), and software-defined radio (SDR). Experiments on three benchmarks\ndemonstrate the effectiveness of our framework. The results show that our\nframework produces highly accurate security strategies that remain robust\nagainst a variety of unknown attacks. We will release our code to contribute to\nthe community."
                },
                "authors": [
                    {
                        "name": "Qi Qin"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Guoshun Nan"
                    },
                    {
                        "name": "Sihan Chen"
                    },
                    {
                        "name": "Rushan Li"
                    },
                    {
                        "name": "Li Su"
                    },
                    {
                        "name": "Haitao Du"
                    },
                    {
                        "name": "Qimei Cui"
                    },
                    {
                        "name": "Pengxuan Mao"
                    },
                    {
                        "name": "Xiaofeng Tao"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03147v1",
                "updated": "2025-05-06T03:43:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    3,
                    43,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T03:43:12Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    3,
                    43,
                    12,
                    1,
                    126,
                    0
                ],
                "title": "Towards Effective Identification of Attack Techniques in Cyber Threat\n  Intelligence Reports using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Effective Identification of Attack Techniques in Cyber Threat\n  Intelligence Reports using Large Language Models"
                },
                "summary": "This work evaluates the performance of Cyber Threat Intelligence (CTI)\nextraction methods in identifying attack techniques from threat reports\navailable on the web using the MITRE ATT&CK framework. We analyse four\nconfigurations utilising state-of-the-art tools, including the Threat Report\nATT&CK Mapper (TRAM) and open-source Large Language Models (LLMs) such as\nLlama2. Our findings reveal significant challenges, including class imbalance,\noverfitting, and domain-specific complexity, which impede accurate technique\nextraction. To mitigate these issues, we propose a novel two-step pipeline:\nfirst, an LLM summarises the reports, and second, a retrained SciBERT model\nprocesses a rebalanced dataset augmented with LLM-generated data. This approach\nachieves an improvement in F1-scores compared to baseline models, with several\nattack techniques surpassing an F1-score of 0.90. Our contributions enhance the\nefficiency of web-based CTI systems and support collaborative cybersecurity\noperations in an interconnected digital landscape, paving the way for future\nresearch on integrating human-AI collaboration platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work evaluates the performance of Cyber Threat Intelligence (CTI)\nextraction methods in identifying attack techniques from threat reports\navailable on the web using the MITRE ATT&CK framework. We analyse four\nconfigurations utilising state-of-the-art tools, including the Threat Report\nATT&CK Mapper (TRAM) and open-source Large Language Models (LLMs) such as\nLlama2. Our findings reveal significant challenges, including class imbalance,\noverfitting, and domain-specific complexity, which impede accurate technique\nextraction. To mitigate these issues, we propose a novel two-step pipeline:\nfirst, an LLM summarises the reports, and second, a retrained SciBERT model\nprocesses a rebalanced dataset augmented with LLM-generated data. This approach\nachieves an improvement in F1-scores compared to baseline models, with several\nattack techniques surpassing an F1-score of 0.90. Our contributions enhance the\nefficiency of web-based CTI systems and support collaborative cybersecurity\noperations in an interconnected digital landscape, paving the way for future\nresearch on integrating human-AI collaboration platforms."
                },
                "authors": [
                    {
                        "name": "Hoang Cuong Nguyen"
                    },
                    {
                        "name": "Shahroz Tariq"
                    },
                    {
                        "name": "Mohan Baruwal Chhetri"
                    },
                    {
                        "name": "Bao Quoc Vo"
                    }
                ],
                "author_detail": {
                    "name": "Bao Quoc Vo"
                },
                "author": "Bao Quoc Vo",
                "arxiv_doi": "10.1145/3701716.3715469",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715469",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.03147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 2 figures 4 tables, accepted for publication at the Web\n  Conference 2025 (WWW'25)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03139v1",
                "updated": "2025-05-06T03:27:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    3,
                    27,
                    54,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T03:27:54Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    3,
                    27,
                    54,
                    1,
                    126,
                    0
                ],
                "title": "Edge Large AI Models: Collaborative Deployment and IoT Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Large AI Models: Collaborative Deployment and IoT Applications"
                },
                "summary": "Large artificial intelligence models (LAMs) emulate human-like\nproblem-solving capabilities across diverse domains, modalities, and tasks. By\nleveraging the communication and computation resources of geographically\ndistributed edge devices, edge LAMs enable real-time intelligent services at\nthe network edge. Unlike conventional edge AI, which relies on small or\nmoderate-sized models for direct feature-to-prediction mappings, edge LAMs\nleverage the intricate coordination of modular components to enable\ncontext-aware generative tasks and multi-modal inference. We shall propose a\ncollaborative deployment framework for edge LAM by characterizing the LAM\nintelligent capabilities and limited edge network resources. Specifically, we\npropose a collaborative training framework over heterogeneous edge networks\nthat adaptively decomposes LAMs according to computation resources, data\nmodalities, and training objectives, reducing communication and computation\noverheads during the fine-tuning process. Furthermore, we introduce a\nmicroservice-based inference framework that virtualizes the functional modules\nof edge LAMs according to their architectural characteristics, thereby\nimproving resource utilization and reducing inference latency. The developed\nedge LAM will provide actionable solutions to enable diversified\nInternet-of-Things (IoT) applications, facilitated by constructing mappings\nfrom diverse sensor data to token representations and fine-tuning based on\ndomain knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large artificial intelligence models (LAMs) emulate human-like\nproblem-solving capabilities across diverse domains, modalities, and tasks. By\nleveraging the communication and computation resources of geographically\ndistributed edge devices, edge LAMs enable real-time intelligent services at\nthe network edge. Unlike conventional edge AI, which relies on small or\nmoderate-sized models for direct feature-to-prediction mappings, edge LAMs\nleverage the intricate coordination of modular components to enable\ncontext-aware generative tasks and multi-modal inference. We shall propose a\ncollaborative deployment framework for edge LAM by characterizing the LAM\nintelligent capabilities and limited edge network resources. Specifically, we\npropose a collaborative training framework over heterogeneous edge networks\nthat adaptively decomposes LAMs according to computation resources, data\nmodalities, and training objectives, reducing communication and computation\noverheads during the fine-tuning process. Furthermore, we introduce a\nmicroservice-based inference framework that virtualizes the functional modules\nof edge LAMs according to their architectural characteristics, thereby\nimproving resource utilization and reducing inference latency. The developed\nedge LAM will provide actionable solutions to enable diversified\nInternet-of-Things (IoT) applications, facilitated by constructing mappings\nfrom diverse sensor data to token representations and fine-tuning based on\ndomain knowledge."
                },
                "authors": [
                    {
                        "name": "Zixin Wang"
                    },
                    {
                        "name": "Yuanming Shi"
                    },
                    {
                        "name": "Khaled. B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled. B. Letaief"
                },
                "author": "Khaled. B. Letaief",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03135v1",
                "updated": "2025-05-06T03:19:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    3,
                    19,
                    51,
                    1,
                    126,
                    0
                ],
                "published": "2025-05-06T03:19:51Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    3,
                    19,
                    51,
                    1,
                    126,
                    0
                ],
                "title": "Holmes: Automated Fact Check with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holmes: Automated Fact Check with Large Language Models"
                },
                "summary": "The rise of Internet connectivity has accelerated the spread of\ndisinformation, threatening societal trust, decision-making, and national\nsecurity. Disinformation has evolved from simple text to complex multimodal\nforms combining images and text, challenging existing detection methods.\nTraditional deep learning models struggle to capture the complexity of\nmultimodal disinformation. Inspired by advances in AI, this study explores\nusing Large Language Models (LLMs) for automated disinformation detection. The\nempirical study shows that (1) LLMs alone cannot reliably assess the\ntruthfulness of claims; (2) providing relevant evidence significantly improves\ntheir performance; (3) however, LLMs cannot autonomously search for accurate\nevidence. To address this, we propose Holmes, an end-to-end framework featuring\na novel evidence retrieval method that assists LLMs in collecting high-quality\nevidence. Our approach uses (1) LLM-powered summarization to extract key\ninformation from open sources and (2) a new algorithm and metrics to evaluate\nevidence quality. Holmes enables LLMs to verify claims and generate\njustifications effectively. Experiments show Holmes achieves 88.3% accuracy on\ntwo open-source datasets and 90.2% in real-time verification tasks. Notably,\nour improved evidence retrieval boosts fact-checking accuracy by 30.8% over\nexisting methods",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Internet connectivity has accelerated the spread of\ndisinformation, threatening societal trust, decision-making, and national\nsecurity. Disinformation has evolved from simple text to complex multimodal\nforms combining images and text, challenging existing detection methods.\nTraditional deep learning models struggle to capture the complexity of\nmultimodal disinformation. Inspired by advances in AI, this study explores\nusing Large Language Models (LLMs) for automated disinformation detection. The\nempirical study shows that (1) LLMs alone cannot reliably assess the\ntruthfulness of claims; (2) providing relevant evidence significantly improves\ntheir performance; (3) however, LLMs cannot autonomously search for accurate\nevidence. To address this, we propose Holmes, an end-to-end framework featuring\na novel evidence retrieval method that assists LLMs in collecting high-quality\nevidence. Our approach uses (1) LLM-powered summarization to extract key\ninformation from open sources and (2) a new algorithm and metrics to evaluate\nevidence quality. Holmes enables LLMs to verify claims and generate\njustifications effectively. Experiments show Holmes achieves 88.3% accuracy on\ntwo open-source datasets and 90.2% in real-time verification tasks. Notably,\nour improved evidence retrieval boosts fact-checking accuracy by 30.8% over\nexisting methods"
                },
                "authors": [
                    {
                        "name": "Haoran Ou"
                    },
                    {
                        "name": "Gelei Deng"
                    },
                    {
                        "name": "Xingshuo Han"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Xinlei He"
                    },
                    {
                        "name": "Han Qiu"
                    },
                    {
                        "name": "Shangwei Guo"
                    },
                    {
                        "name": "Tianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianwei Zhang"
                },
                "author": "Tianwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]