[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.12959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v2",
                "updated": "2025-02-05T09:35:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    35,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v2",
                "updated": "2025-02-05T08:22:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    22,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "Accepted to NAACL2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v2",
                "updated": "2025-02-05T08:10:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    10,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring"
                },
                "summary": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02818v1",
                "updated": "2025-02-05T01:36:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T01:36:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL"
                },
                "summary": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments."
                },
                "authors": [
                    {
                        "name": "Wenbo Sun"
                    },
                    {
                        "name": "Qiming Guo"
                    },
                    {
                        "name": "Wenlu Wang"
                    },
                    {
                        "name": "Rihan Hai"
                    }
                ],
                "author_detail": {
                    "name": "Rihan Hai"
                },
                "author": "Rihan Hai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v1",
                "updated": "2025-02-04T23:26:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02750v1",
                "updated": "2025-02-04T22:37:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T22:37:17Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "title": "Cache is King: Smart Page Eviction with eBPF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache is King: Smart Page Eviction with eBPF"
                },
                "summary": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency."
                },
                "authors": [
                    {
                        "name": "Tal Zussman"
                    },
                    {
                        "name": "Ioannis Zarkadas"
                    },
                    {
                        "name": "Jeremy Carin"
                    },
                    {
                        "name": "Andrew Cheng"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Jonas Pfefferle"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02564v1",
                "updated": "2025-02-04T18:39:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:39:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing"
                },
                "summary": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%."
                },
                "authors": [
                    {
                        "name": "Atiyeh Javaheri"
                    },
                    {
                        "name": "Ali Bohlooli"
                    },
                    {
                        "name": "Kamal Jamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Kamal Jamshidi"
                },
                "author": "Kamal Jamshidi",
                "arxiv_comment": "18 pages, 14 figures, submit to Digital Communications and Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v2",
                "updated": "2025-02-04T17:14:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    14,
                    22,
                    1,
                    35,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Oral Presentation at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02493v1",
                "updated": "2025-02-04T17:09:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization"
                },
                "summary": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models."
                },
                "authors": [
                    {
                        "name": "Yize Wu"
                    },
                    {
                        "name": "Ke Gao"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02437v1",
                "updated": "2025-02-04T16:03:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:03:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems"
                },
                "summary": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher."
                },
                "authors": [
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonalo Moreira"
                    },
                    {
                        "name": "Jos Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v1",
                "updated": "2025-02-04T15:55:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Rbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "Andrs Gyrgy"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02349v1",
                "updated": "2025-02-04T14:33:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:33:44Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "title": "Random Adaptive Cache Placement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Adaptive Cache Placement Policy"
                },
                "summary": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times."
                },
                "authors": [
                    {
                        "name": "Vrushank Ahire"
                    },
                    {
                        "name": "Pranav Menon"
                    },
                    {
                        "name": "Aniruddh Muley"
                    },
                    {
                        "name": "Abhinandan S. Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Abhinandan S. Prasad"
                },
                "author": "Abhinandan S. Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v2",
                "updated": "2025-02-04T13:45:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    45,
                    37,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation"
                },
                "summary": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v1",
                "updated": "2025-02-04T09:48:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02617v1",
                "updated": "2025-02-04T08:52:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T08:52:13Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "title": "PolarQuant: Quantizing KV Caches with Polar Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Quantizing KV Caches with Polar Transformation"
                },
                "summary": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Praneeth Kacham"
                    },
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v4",
                "updated": "2025-02-04T08:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    16,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02069v1",
                "updated": "2025-02-04T07:40:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:40:26Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "title": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models"
                },
                "summary": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache."
                },
                "authors": [
                    {
                        "name": "Yuto Kojima"
                    },
                    {
                        "name": "Jiarui Xu"
                    },
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v1",
                "updated": "2025-02-04T03:13:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "14 pages, 11 figures, the first version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01941v1",
                "updated": "2025-02-04T02:23:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T02:23:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
                },
                "summary": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v2",
                "updated": "2025-02-03T21:45:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    21,
                    45,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "18 pages, 7 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v1",
                "updated": "2025-02-03T20:30:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "19 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01637v1",
                "updated": "2025-02-03T18:59:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T18:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "title": "Scaling Embedding Layers in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Embedding Layers in Language Models"
                },
                "summary": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS."
                },
                "authors": [
                    {
                        "name": "Da Yu"
                    },
                    {
                        "name": "Edith Cohen"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Pritish Kamath"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "Daogao Liu"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiyuan Zhang"
                },
                "author": "Chiyuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01449v1",
                "updated": "2025-02-03T15:38:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    38,
                    53,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T15:38:53Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    38,
                    53,
                    0,
                    34,
                    0
                ],
                "title": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies"
                },
                "summary": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs."
                },
                "authors": [
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Benigna Bruggmann"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v2",
                "updated": "2025-02-03T15:15:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    15,
                    58,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v1",
                "updated": "2025-02-03T05:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08784v2",
                "updated": "2025-02-02T14:38:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    38,
                    15,
                    6,
                    33,
                    0
                ],
                "published": "2023-10-12T07:35:30Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    7,
                    35,
                    30,
                    3,
                    285,
                    0
                ],
                "title": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction"
                },
                "summary": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches."
                },
                "authors": [
                    {
                        "name": "Pol Caselles"
                    },
                    {
                        "name": "Eduard Ramon"
                    },
                    {
                        "name": "Jaime Garcia"
                    },
                    {
                        "name": "Gil Triginer"
                    },
                    {
                        "name": "Francesc Moreno-Noguer"
                    }
                ],
                "author_detail": {
                    "name": "Francesc Moreno-Noguer"
                },
                "author": "Francesc Moreno-Noguer",
                "arxiv_comment": "Accepted at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v2",
                "updated": "2025-02-02T03:04:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    4,
                    54,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00527v1",
                "updated": "2025-02-01T18:59:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T18:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "title": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration"
                },
                "summary": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models."
                },
                "authors": [
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v3",
                "updated": "2025-02-01T16:00:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    16,
                    0,
                    50,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00439v1",
                "updated": "2025-02-01T14:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T14:16:31Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "title": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs"
                },
                "summary": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}."
                },
                "authors": [
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Xin Ye"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Haoran Lian"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "11 pages, 4 figures. Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00433v1",
                "updated": "2025-02-01T13:46:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T13:46:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models"
                },
                "summary": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning"
                },
                "authors": [
                    {
                        "name": "Xinle Cheng"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00382v1",
                "updated": "2025-02-01T09:41:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T09:41:01Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "title": "Masked Generative Nested Transformers with Decode Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Nested Transformers with Decode Time Scaling"
                },
                "summary": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Sahil Goyal"
                    },
                    {
                        "name": "Debapriya Tula"
                    },
                    {
                        "name": "Gagan Jain"
                    },
                    {
                        "name": "Pradeep Shenoy"
                    },
                    {
                        "name": "Prateek Jain"
                    },
                    {
                        "name": "Sujoy Paul"
                    }
                ],
                "author_detail": {
                    "name": "Sujoy Paul"
                },
                "author": "Sujoy Paul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v2",
                "updated": "2025-02-01T04:24:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    4,
                    24,
                    16,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v1",
                "updated": "2025-02-01T03:49:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v2",
                "updated": "2025-02-01T03:40:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    40,
                    37,
                    5,
                    32,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v2",
                "updated": "2025-01-31T19:09:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    19,
                    9,
                    19,
                    4,
                    31,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_doi": "10.1109/IPCCC59868.2024.10850382",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IPCCC59868.2024.10850382",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.12178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "pp. 1-9, doi: 10.1109/IPCCC59868.2024.10850382. keywords:\n  {Accuracy;Prefetching;Large language models;Computational\n  modeling;Companies;Transformers;User experience;Time\n  factors;Tuning;Guidelines;Large Language Models (LLMs);AI\n  Compression;Activation Sparsity;Edge LLM},",
                "arxiv_journal_ref": "2024 IEEE International Performance, Computing, and Communications\n  Conference (IPCCC), Orlando, FL, USA, 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v1",
                "updated": "2025-01-31T18:47:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v1",
                "updated": "2025-01-31T16:56:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee-Joe Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v1",
                "updated": "2025-01-31T16:22:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Mao Xun Huang"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v1",
                "updated": "2025-01-31T15:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v3",
                "updated": "2025-01-31T14:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    26,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Our code will be released upon acceptance. The Change Logs on Page 9\n  reveal our significant changes compared with v1 and v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17426v3",
                "updated": "2025-01-31T14:13:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    13,
                    49,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-26T13:34:02Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    34,
                    2,
                    1,
                    331,
                    0
                ],
                "title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning"
                },
                "summary": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Fan jiang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/GraphPKU/PiSSA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19051v1",
                "updated": "2025-01-31T11:25:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:25:40Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "title": "Swift: Rethinking RDMA Control Plane for Elastic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swift: Rethinking RDMA Control Plane for Elastic Computing"
                },
                "summary": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions."
                },
                "authors": [
                    {
                        "name": "Junxue Zhang"
                    },
                    {
                        "name": "Han Tian"
                    },
                    {
                        "name": "Xinyang Huang"
                    },
                    {
                        "name": "Wenxue Li"
                    },
                    {
                        "name": "Kaiqiang Xu"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19021v1",
                "updated": "2025-01-31T10:43:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T10:43:00Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "title": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode"
                },
                "summary": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model."
                },
                "authors": [
                    {
                        "name": "Emilio Corte"
                    },
                    {
                        "name": "Alberto Bortone"
                    },
                    {
                        "name": "Elena Nieto Hernndez"
                    },
                    {
                        "name": "Carlo Ceresa"
                    },
                    {
                        "name": "Georgios Provatas"
                    },
                    {
                        "name": "Karla Ivankovi Nizi"
                    },
                    {
                        "name": "Milko Jaksi"
                    },
                    {
                        "name": "Ettore Vittone"
                    },
                    {
                        "name": "Sviatoslav Ditalia Tchernij"
                    }
                ],
                "author_detail": {
                    "name": "Sviatoslav Ditalia Tchernij"
                },
                "author": "Sviatoslav Ditalia Tchernij",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18824v1",
                "updated": "2025-01-31T00:43:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T00:43:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Fine-Tuning of Transformers via Token Selection"
                },
                "summary": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune."
                },
                "authors": [
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Namyong Park"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Grey Yang"
                    }
                ],
                "author_detail": {
                    "name": "Grey Yang"
                },
                "author": "Grey Yang",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v3",
                "updated": "2025-01-30T18:23:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    23,
                    46,
                    3,
                    30,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18356v1",
                "updated": "2025-01-30T14:03:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:03:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence"
                },
                "summary": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Thea Aviss"
                    }
                ],
                "author_detail": {
                    "name": "Thea Aviss"
                },
                "author": "Thea Aviss",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v2",
                "updated": "2025-01-30T13:07:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    7,
                    37,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices"
                },
                "summary": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05172v2",
                "updated": "2025-01-30T06:02:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    6,
                    2,
                    11,
                    3,
                    30,
                    0
                ],
                "published": "2023-10-08T14:06:06Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    14,
                    6,
                    6,
                    6,
                    281,
                    0
                ],
                "title": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy"
                },
                "summary": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v5",
                "updated": "2025-01-29T16:44:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    44,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04437v3",
                "updated": "2025-01-29T04:10:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    10,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-07T16:00:32Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    16,
                    0,
                    32,
                    1,
                    128,
                    0
                ],
                "title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention"
                },
                "summary": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer."
                },
                "authors": [
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14770v2",
                "updated": "2025-01-28T20:35:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    35,
                    23,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:37:18Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    37,
                    18,
                    6,
                    364,
                    0
                ],
                "title": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches"
                },
                "summary": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing citations in Sections 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14771v2",
                "updated": "2025-01-28T20:33:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    33,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:39:37Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    39,
                    37,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching"
                },
                "summary": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing proper citations in Sections 2 and 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17123v1",
                "updated": "2025-01-28T18:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis"
                },
                "summary": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats."
                },
                "authors": [
                    {
                        "name": "Tejal Joshi"
                    },
                    {
                        "name": "Aarya Kawalay"
                    },
                    {
                        "name": "Anvi Jamkhande"
                    },
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "8 pages, 4 figures. Accepted in IEEE's 2nd International Conference\n  on Computational Intelligence and Network Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v2",
                "updated": "2025-01-28T16:19:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    19,
                    24,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tlli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tlli"
                },
                "author": "Antti Tlli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v1",
                "updated": "2025-01-28T12:57:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16597v1",
                "updated": "2025-01-28T00:22:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T00:22:34Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs"
                },
                "summary": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed Saif"
                    },
                    {
                        "name": "Md. Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16535v1",
                "updated": "2025-01-27T22:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T22:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "title": "Latency Guarantees for Caching with Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency Guarantees for Caching with Delayed Hits"
                },
                "summary": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm."
                },
                "authors": [
                    {
                        "name": "Keerthana Gurushankar"
                    },
                    {
                        "name": "Noah G. Singer"
                    },
                    {
                        "name": "Bernardo Subercaseaux"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Subercaseaux"
                },
                "author": "Bernardo Subercaseaux",
                "arxiv_comment": "Accepted at INFOCOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16245v1",
                "updated": "2025-01-27T17:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:42:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis"
                },
                "summary": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonalo Moreira"
                    },
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Jos Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v4",
                "updated": "2025-01-27T14:55:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    55,
                    40,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v2",
                "updated": "2025-01-27T14:37:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    37,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tlli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tlli"
                },
                "author": "Antti Tlli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16055v1",
                "updated": "2025-01-27T13:53:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "title": "Random Reshuffling for Stochastic Gradient Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Reshuffling for Stochastic Gradient Langevin Dynamics"
                },
                "summary": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent."
                },
                "authors": [
                    {
                        "name": "Luke Shaw"
                    },
                    {
                        "name": "Peter A. Whalley"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Whalley"
                },
                "author": "Peter A. Whalley",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65C05, 82C31, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v2",
                "updated": "2025-01-27T13:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    39,
                    25,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization"
                },
                "summary": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "PrefixQuant improves quantization accuracy across various precision\n  and quantization settings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v3",
                "updated": "2025-01-27T06:47:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    47,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15782v1",
                "updated": "2025-01-27T05:02:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T05:02:05Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "title": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare"
                },
                "summary": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets."
                },
                "authors": [
                    {
                        "name": "Faraz Zargari"
                    },
                    {
                        "name": "Hossein Nekouyan Jazi"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Xiaoqi Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Tan"
                },
                "author": "Xiaoqi Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15570v1",
                "updated": "2025-01-26T15:56:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T15:56:56Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer"
                },
                "summary": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}."
                },
                "authors": [
                    {
                        "name": "Lin Yueyu"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Peter Yue"
                    },
                    {
                        "name": "Liu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Liu Xiao"
                },
                "author": "Liu Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15481v1",
                "updated": "2025-01-26T11:01:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T11:01:10Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "title": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems"
                },
                "summary": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one."
                },
                "authors": [
                    {
                        "name": "Joaqun Gayoso-Cabada"
                    },
                    {
                        "name": "Mercedes Gmez-Albarrn"
                    },
                    {
                        "name": "Jos-Luis Sierra"
                    }
                ],
                "author_detail": {
                    "name": "Jos-Luis Sierra"
                },
                "author": "Jos-Luis Sierra",
                "arxiv_doi": "10.1007/978-3-030-04257-8_4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-030-04257-8_4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.15481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "camera-ready",
                "arxiv_journal_ref": "MATURITY AND INNOVATION IN DIGITAL LIBRARIES, ICADL 2018",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v4",
                "updated": "2025-01-26T07:29:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    7,
                    29,
                    6,
                    6,
                    26,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v2",
                "updated": "2025-01-26T01:43:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    1,
                    43,
                    46,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15348v1",
                "updated": "2025-01-25T23:16:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T23:16:03Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "title": "ReInc: Scaling Training of Dynamic Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReInc: Scaling Training of Dynamic Graph Neural Networks"
                },
                "summary": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets."
                },
                "authors": [
                    {
                        "name": "Mingyu Guan"
                    },
                    {
                        "name": "Saumia Singhal"
                    },
                    {
                        "name": "Taesoo Kim"
                    },
                    {
                        "name": "Anand Padmanabha Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Anand Padmanabha Iyer"
                },
                "author": "Anand Padmanabha Iyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v2",
                "updated": "2025-01-25T12:17:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    12,
                    17,
                    41,
                    5,
                    25,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v2",
                "updated": "2025-01-25T10:38:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    10,
                    38,
                    11,
                    5,
                    25,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15126v1",
                "updated": "2025-01-25T08:27:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T08:27:26Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "title": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs"
                },
                "summary": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x."
                },
                "authors": [
                    {
                        "name": "Deniz Elbek"
                    },
                    {
                        "name": "Kamer Kaya"
                    }
                ],
                "author_detail": {
                    "name": "Kamer Kaya"
                },
                "author": "Kamer Kaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15113v1",
                "updated": "2025-01-25T07:28:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T07:28:13Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "title": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads"
                },
                "summary": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Xingyang He"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Shaowei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Chen"
                },
                "author": "Shaowei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v2",
                "updated": "2025-01-25T04:21:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    4,
                    21,
                    57,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15021v1",
                "updated": "2025-01-25T02:01:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T02:01:56Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "title": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models"
                },
                "summary": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v2",
                "updated": "2025-01-24T19:13:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    19,
                    13,
                    12,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v2",
                "updated": "2025-01-24T15:16:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    16,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v2",
                "updated": "2025-01-24T14:32:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    32,
                    34,
                    4,
                    24,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Lucas Brgi"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14387v1",
                "updated": "2025-01-24T10:39:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:39:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers"
                },
                "summary": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform."
                },
                "authors": [
                    {
                        "name": "Simone Bolettieri"
                    },
                    {
                        "name": "Raffaele Bruno"
                    },
                    {
                        "name": "Enzo Mingozzi"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Mingozzi"
                },
                "author": "Enzo Mingozzi",
                "arxiv_doi": "10.1016/j.jnca.2021.103020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jnca.2021.103020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Network and Computer Applications, Volume 181, 1 May\n  2021, 103020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14367v1",
                "updated": "2025-01-24T10:00:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:00:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks"
                },
                "summary": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Kexin Shi"
                    },
                    {
                        "name": "Yaru Fu"
                    },
                    {
                        "name": "Yongna Guo"
                    },
                    {
                        "name": "Fu Lee Wang"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14312v1",
                "updated": "2025-01-24T08:12:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:12:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Locality-aware Fair Scheduling in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locality-aware Fair Scheduling in LLM Serving"
                },
                "summary": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency."
                },
                "authors": [
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Liangsheng Yin"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14205v1",
                "updated": "2025-01-24T03:21:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T03:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading"
                },
                "summary": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v1",
                "updated": "2025-01-23T12:58:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13998v1",
                "updated": "2025-01-23T11:18:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T11:18:42Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "title": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry"
                },
                "summary": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique."
                },
                "authors": [
                    {
                        "name": "E. Chamizo"
                    },
                    {
                        "name": "M. C. Jimnez-Ramos"
                    },
                    {
                        "name": "S. M. Enamorado"
                    },
                    {
                        "name": "M. Garca-Len"
                    },
                    {
                        "name": "R. Garca-Tenorio"
                    },
                    {
                        "name": "J. L. Mas"
                    },
                    {
                        "name": "P. Masqu"
                    },
                    {
                        "name": "J. Merino"
                    },
                    {
                        "name": "J. A. Sanchez-Cabeza"
                    }
                ],
                "author_detail": {
                    "name": "J. A. Sanchez-Cabeza"
                },
                "author": "J. A. Sanchez-Cabeza",
                "arxiv_doi": "10.1016/j.nimb.2009.10.151",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nimb.2009.10.151",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 1 table, 3 figures",
                "arxiv_journal_ref": "Nuclear Instruments and Methods in Physics Research Section B:\n  Beam Interactions with Materials and Atoms, Volume 268, Issues 7-8, April\n  2010, Pages 1273-1276",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13540v1",
                "updated": "2025-01-23T10:40:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:40:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks"
                },
                "summary": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Harel Berger"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    }
                ],
                "author_detail": {
                    "name": "Anat Bremler-Barr"
                },
                "author": "Anat Bremler-Barr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v3",
                "updated": "2025-01-23T07:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    25,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v2",
                "updated": "2025-01-23T06:48:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11745v2",
                "updated": "2025-01-22T16:25:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    25,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-20T21:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    7,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching"
                },
                "summary": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Hayssam Dahrouj"
                    },
                    {
                        "name": "Nour Kouzayha"
                    },
                    {
                        "name": "Hesham ElSawy"
                    },
                    {
                        "name": "Tareq Y. Al-Naffouri"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Y. Al-Naffouri"
                },
                "author": "Tareq Y. Al-Naffouri",
                "arxiv_comment": "accepted for publication in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v5",
                "updated": "2025-01-22T15:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    9,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v3",
                "updated": "2025-01-22T15:05:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    5,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "To appear in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v2",
                "updated": "2025-01-22T10:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    39,
                    50,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-Gonzlez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martn"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12744v1",
                "updated": "2025-01-22T09:25:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:25:29Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "title": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity"
                },
                "summary": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips."
                },
                "authors": [
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Vincent Calvo"
                    },
                    {
                        "name": "Christian Elssser"
                    },
                    {
                        "name": "Giuliano Coppola"
                    },
                    {
                        "name": "Frdric Mazen"
                    },
                    {
                        "name": "Sbastien Kerdils"
                    },
                    {
                        "name": "Flix Cache"
                    },
                    {
                        "name": "Anas Drau"
                    },
                    {
                        "name": "Jean-Michel Grard"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Grard"
                },
                "arxiv_affiliation": "Univ. Grenoble Alpes, CEA, Grenoble INP, IRIG, PHELIQS",
                "author": "Jean-Michel Grard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12528v1",
                "updated": "2025-01-21T22:33:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T22:33:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System"
                },
                "summary": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT."
                },
                "authors": [
                    {
                        "name": "Junyi Wang"
                    },
                    {
                        "name": "Quan Zang"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Minquan Cheng"
                },
                "author": "Minquan Cheng",
                "arxiv_comment": "14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v1",
                "updated": "2025-01-21T12:19:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11940v1",
                "updated": "2025-01-21T07:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:32:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "Build Optimization: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build Optimization: A Systematic Literature Review"
                },
                "summary": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research."
                },
                "authors": [
                    {
                        "name": "Henri Adasso"
                    },
                    {
                        "name": "Mohammed Sayagh"
                    },
                    {
                        "name": "Francis Bordeleau"
                    }
                ],
                "author_detail": {
                    "name": "Francis Bordeleau"
                },
                "author": "Francis Bordeleau",
                "arxiv_comment": "An earlier version of this work was submitted to ACM CSUR in November\n  2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11834v1",
                "updated": "2025-01-21T02:35:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:35:31Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "title": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching"
                },
                "summary": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "35 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v1",
                "updated": "2025-01-20T23:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference"
                },
                "summary": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11502v1",
                "updated": "2025-01-20T14:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T14:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "title": "Hierarchical Coded Caching in High Memory Regime with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching in High Memory Regime with Coded Placement"
                },
                "summary": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages, 3 figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v3",
                "updated": "2025-01-20T08:44:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    8,
                    44,
                    1,
                    0,
                    20,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.03463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03463v1",
                "updated": "2025-02-05T18:59:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    59,
                    8,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:59:08Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    59,
                    8,
                    2,
                    36,
                    0
                ],
                "title": "Cosmic Calipers: Precise and Accurate Neutron Star Radius Measurements\n  with Next-Generation Gravitational Wave Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmic Calipers: Precise and Accurate Neutron Star Radius Measurements\n  with Next-Generation Gravitational Wave Detectors"
                },
                "summary": "Gravitational waves from merging binary neutron stars carry characteristic\ninformation about their astrophysical properties, including masses and tidal\ndeformabilities, that are needed to infer their radii. In this study, we use\nBayesian inference to quantify the precision with which radius can inferred\nwith upgrades in the current gravitational wave detectors and next-generation\nobservatories such as the Einstein Telescope and Cosmic Explorer. We assign\nevidences for a set of plausible equations of state, which are then used as\nweights to obtain radius posteriors. We find that prior choices and the\nloudness of observed signals limit the precision and accuracy of inferred radii\nby current detectors. In contrast, next-generation observatories can resolve\nthe radius precisely and accurately, across most of the mass range to within\n$\\lesssim 5\\%$ for both soft and stiff equations of state. We also explore how\nthe choice of the neutron star mass prior can influence the inferred masses and\npotentially affect radii measurements, finding that choosing an astrophysically\nmotivated prior does not notably impact an individual neutron star's radius\nmeasurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves from merging binary neutron stars carry characteristic\ninformation about their astrophysical properties, including masses and tidal\ndeformabilities, that are needed to infer their radii. In this study, we use\nBayesian inference to quantify the precision with which radius can inferred\nwith upgrades in the current gravitational wave detectors and next-generation\nobservatories such as the Einstein Telescope and Cosmic Explorer. We assign\nevidences for a set of plausible equations of state, which are then used as\nweights to obtain radius posteriors. We find that prior choices and the\nloudness of observed signals limit the precision and accuracy of inferred radii\nby current detectors. In contrast, next-generation observatories can resolve\nthe radius precisely and accurately, across most of the mass range to within\n$\\lesssim 5\\%$ for both soft and stiff equations of state. We also explore how\nthe choice of the neutron star mass prior can influence the inferred masses and\npotentially affect radii measurements, finding that choosing an astrophysically\nmotivated prior does not notably impact an individual neutron star's radius\nmeasurements."
                },
                "authors": [
                    {
                        "name": "Sanika Khadkikar"
                    },
                    {
                        "name": "Ish Gupta"
                    },
                    {
                        "name": "Rahul Kashyap"
                    },
                    {
                        "name": "Koustav Chandra"
                    },
                    {
                        "name": "Rossella Gamba"
                    },
                    {
                        "name": "Bangalore Sathyaprakash"
                    }
                ],
                "author_detail": {
                    "name": "Bangalore Sathyaprakash"
                },
                "author": "Bangalore Sathyaprakash",
                "arxiv_comment": "15 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03455v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03455v2",
                "updated": "2025-02-05T18:58:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    58,
                    56,
                    2,
                    36,
                    0
                ],
                "published": "2024-08-06T21:25:42Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    21,
                    25,
                    42,
                    1,
                    219,
                    0
                ],
                "title": "Bayesian learning with Gaussian processes for low-dimensional\n  representations of time-dependent nonlinear systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian learning with Gaussian processes for low-dimensional\n  representations of time-dependent nonlinear systems"
                },
                "summary": "This work presents a data-driven method for learning low-dimensional\ntime-dependent physics-based surrogate models whose predictions are endowed\nwith uncertainty estimates. We use the operator inference approach to model\nreduction that poses the problem of learning low-dimensional model terms as a\nregression of state space data and corresponding time derivatives by minimizing\nthe residual of reduced system equations. Standard operator inference models\nperform well with accurate training data that are dense in time, but producing\nstable and accurate models when the state data are noisy and/or sparse in time\nremains a challenge. Another challenge is the lack of uncertainty estimation\nfor the predictions from the operator inference models. Our approach addresses\nthese challenges by incorporating Gaussian process surrogates into the operator\ninference framework to (1) probabilistically describe uncertainties in the\nstate predictions and (2) procure analytical time derivative estimates with\nquantified uncertainties. The formulation leads to a generalized least-squares\nregression and, ultimately, reduced-order models that are described\nprobabilistically with a closed-form expression for the posterior distribution\nof the operators. The resulting probabilistic surrogate model propagates\nuncertainties from the observed state data to reduced-order predictions. We\ndemonstrate the method is effective for constructing low-dimensional models of\ntwo nonlinear partial differential equations representing a compressible flow\nand a nonlinear diffusion-reaction process, as well as for estimating the\nparameters of a low-dimensional system of nonlinear ordinary differential\nequations representing compartmental models in epidemiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a data-driven method for learning low-dimensional\ntime-dependent physics-based surrogate models whose predictions are endowed\nwith uncertainty estimates. We use the operator inference approach to model\nreduction that poses the problem of learning low-dimensional model terms as a\nregression of state space data and corresponding time derivatives by minimizing\nthe residual of reduced system equations. Standard operator inference models\nperform well with accurate training data that are dense in time, but producing\nstable and accurate models when the state data are noisy and/or sparse in time\nremains a challenge. Another challenge is the lack of uncertainty estimation\nfor the predictions from the operator inference models. Our approach addresses\nthese challenges by incorporating Gaussian process surrogates into the operator\ninference framework to (1) probabilistically describe uncertainties in the\nstate predictions and (2) procure analytical time derivative estimates with\nquantified uncertainties. The formulation leads to a generalized least-squares\nregression and, ultimately, reduced-order models that are described\nprobabilistically with a closed-form expression for the posterior distribution\nof the operators. The resulting probabilistic surrogate model propagates\nuncertainties from the observed state data to reduced-order predictions. We\ndemonstrate the method is effective for constructing low-dimensional models of\ntwo nonlinear partial differential equations representing a compressible flow\nand a nonlinear diffusion-reaction process, as well as for estimating the\nparameters of a low-dimensional system of nonlinear ordinary differential\nequations representing compartmental models in epidemiology."
                },
                "authors": [
                    {
                        "name": "Shane A. McQuarrie"
                    },
                    {
                        "name": "Anirban Chaudhuri"
                    },
                    {
                        "name": "Karen E. Willcox"
                    },
                    {
                        "name": "Mengwu Guo"
                    }
                ],
                "author_detail": {
                    "name": "Mengwu Guo"
                },
                "author": "Mengwu Guo",
                "arxiv_comment": "https://github.com/Sandialabs/GP-BayesOpInf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03455v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03455v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 60G15, 65C05, 35B30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03461v1",
                "updated": "2025-02-05T18:58:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    58,
                    19,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:58:19Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    58,
                    19,
                    2,
                    36,
                    0
                ],
                "title": "Do Large Language Model Benchmarks Test Reliability?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Model Benchmarks Test Reliability?"
                },
                "summary": "When deploying large language models (LLMs), it is important to ensure that\nthese models are not only capable, but also reliable. Many benchmarks have been\ncreated to track LLMs' growing capabilities, however there has been no similar\nfocus on measuring their reliability. To understand the potential ramifications\nof this gap, we investigate how well current benchmarks quantify model\nreliability. We find that pervasive label errors can compromise these\nevaluations, obscuring lingering model failures and hiding unreliable behavior.\n  Motivated by this gap in the evaluation of reliability, we then propose the\nconcept of so-called platinum benchmarks, i.e., benchmarks carefully curated to\nminimize label errors and ambiguity. As a first attempt at constructing such\nbenchmarks, we revise examples from fifteen existing popular benchmarks. We\nevaluate a wide range of models on these platinum benchmarks and find that,\nindeed, frontier LLMs still exhibit failures on simple tasks such as\nelementary-level math word problems. Analyzing these failures further reveals\npreviously unidentified patterns of problems on which frontier models\nconsistently struggle. We provide code at\nhttps://github.com/MadryLab/platinum-benchmarks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When deploying large language models (LLMs), it is important to ensure that\nthese models are not only capable, but also reliable. Many benchmarks have been\ncreated to track LLMs' growing capabilities, however there has been no similar\nfocus on measuring their reliability. To understand the potential ramifications\nof this gap, we investigate how well current benchmarks quantify model\nreliability. We find that pervasive label errors can compromise these\nevaluations, obscuring lingering model failures and hiding unreliable behavior.\n  Motivated by this gap in the evaluation of reliability, we then propose the\nconcept of so-called platinum benchmarks, i.e., benchmarks carefully curated to\nminimize label errors and ambiguity. As a first attempt at constructing such\nbenchmarks, we revise examples from fifteen existing popular benchmarks. We\nevaluate a wide range of models on these platinum benchmarks and find that,\nindeed, frontier LLMs still exhibit failures on simple tasks such as\nelementary-level math word problems. Analyzing these failures further reveals\npreviously unidentified patterns of problems on which frontier models\nconsistently struggle. We provide code at\nhttps://github.com/MadryLab/platinum-benchmarks"
                },
                "authors": [
                    {
                        "name": "Joshua Vendrow"
                    },
                    {
                        "name": "Edward Vendrow"
                    },
                    {
                        "name": "Sara Beery"
                    },
                    {
                        "name": "Aleksander Madry"
                    }
                ],
                "author_detail": {
                    "name": "Aleksander Madry"
                },
                "author": "Aleksander Madry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03460v1",
                "updated": "2025-02-05T18:57:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    57,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:57:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    57,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language\n  Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language\n  Model Training"
                },
                "summary": "Small language models (SLMs) have attracted considerable attention from both\nacademia and industry due to their broad range of applications in edge devices.\nTo obtain SLMs with strong performance, conventional approaches either\npre-train the models from scratch, which incurs substantial computational\ncosts, or compress/prune existing large language models (LLMs), which results\nin performance drops and falls short in comparison to pre-training. In this\npaper, we investigate the family of acceleration methods that involve both\nstructured pruning and model training. We found 1) layer-wise adaptive pruning\n(Adapt-Pruner) is extremely effective in LLMs and yields significant\nimprovements over existing pruning techniques, 2) adaptive pruning equipped\nwith further training leads to models comparable to those pre-training from\nscratch, 3) incremental pruning brings non-trivial performance gain by\ninterleaving pruning with training and only removing a small portion of neurons\n($\\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that\nAdapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner,\nFLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense\nbenchmarks. Additionally, Adapt-Pruner restores the performance of\nMobileLLM-125M to 600M on the MMLU benchmark with 200$\\times$ fewer tokens via\npruning from its larger counterparts, and discovers a new 1B model that\nsurpasses LLaMA-3.2-1B in multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs) have attracted considerable attention from both\nacademia and industry due to their broad range of applications in edge devices.\nTo obtain SLMs with strong performance, conventional approaches either\npre-train the models from scratch, which incurs substantial computational\ncosts, or compress/prune existing large language models (LLMs), which results\nin performance drops and falls short in comparison to pre-training. In this\npaper, we investigate the family of acceleration methods that involve both\nstructured pruning and model training. We found 1) layer-wise adaptive pruning\n(Adapt-Pruner) is extremely effective in LLMs and yields significant\nimprovements over existing pruning techniques, 2) adaptive pruning equipped\nwith further training leads to models comparable to those pre-training from\nscratch, 3) incremental pruning brings non-trivial performance gain by\ninterleaving pruning with training and only removing a small portion of neurons\n($\\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that\nAdapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner,\nFLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense\nbenchmarks. Additionally, Adapt-Pruner restores the performance of\nMobileLLM-125M to 600M on the MMLU benchmark with 200$\\times$ fewer tokens via\npruning from its larger counterparts, and discovers a new 1B model that\nsurpasses LLaMA-3.2-1B in multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Boyao Wang"
                    },
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Xingyuan Pan"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Renjie Pi"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03459v1",
                "updated": "2025-02-05T18:57:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    57,
                    4,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:57:04Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    57,
                    4,
                    2,
                    36,
                    0
                ],
                "title": "SKI Models: Skeleton Induced Vision-Language Embeddings for\n  Understanding Activities of Daily Living",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKI Models: Skeleton Induced Vision-Language Embeddings for\n  Understanding Activities of Daily Living"
                },
                "summary": "The introduction of vision-language models like CLIP has enabled the\ndevelopment of foundational video models capable of generalizing to unseen\nvideos and human actions. However, these models are typically trained on web\nvideos, which often fail to capture the challenges present in Activities of\nDaily Living (ADL) videos. Existing works address ADL-specific challenges, such\nas similar appearances, subtle motion patterns, and multiple viewpoints, by\ncombining 3D skeletons and RGB videos. However, these approaches are not\nintegrated with language, limiting their ability to generalize to unseen action\nclasses. In this paper, we introduce SKI models, which integrate 3D skeletons\ninto the vision-language embedding space. SKI models leverage a\nskeleton-language model, SkeletonCLIP, to infuse skeleton information into\nVision Language Models (VLMs) and Large Vision Language Models (LVLMs) through\ncollaborative training. Notably, SKI models do not require skeleton data during\ninference, enhancing their robustness for real-world applications. The\neffectiveness of SKI models is validated on three popular ADL datasets for\nzero-shot action recognition and video caption generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The introduction of vision-language models like CLIP has enabled the\ndevelopment of foundational video models capable of generalizing to unseen\nvideos and human actions. However, these models are typically trained on web\nvideos, which often fail to capture the challenges present in Activities of\nDaily Living (ADL) videos. Existing works address ADL-specific challenges, such\nas similar appearances, subtle motion patterns, and multiple viewpoints, by\ncombining 3D skeletons and RGB videos. However, these approaches are not\nintegrated with language, limiting their ability to generalize to unseen action\nclasses. In this paper, we introduce SKI models, which integrate 3D skeletons\ninto the vision-language embedding space. SKI models leverage a\nskeleton-language model, SkeletonCLIP, to infuse skeleton information into\nVision Language Models (VLMs) and Large Vision Language Models (LVLMs) through\ncollaborative training. Notably, SKI models do not require skeleton data during\ninference, enhancing their robustness for real-world applications. The\neffectiveness of SKI models is validated on three popular ADL datasets for\nzero-shot action recognition and video caption generation tasks."
                },
                "authors": [
                    {
                        "name": "Arkaprava Sinha"
                    },
                    {
                        "name": "Dominick Reilly"
                    },
                    {
                        "name": "Francois Bremond"
                    },
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Srijan Das"
                    }
                ],
                "author_detail": {
                    "name": "Srijan Das"
                },
                "author": "Srijan Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03452v1",
                "updated": "2025-02-05T18:51:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    51,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:51:45Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    51,
                    45,
                    2,
                    36,
                    0
                ],
                "title": "Unconventional anomalous Hall effect in hexagonal polar magnet\n  Y_3Co_8Sn_4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unconventional anomalous Hall effect in hexagonal polar magnet\n  Y_3Co_8Sn_4"
                },
                "summary": "We report a rare realization of unconventional anomalous Hall effect (UAHE)\nboth below and above the magnetic transition temperature (T_C) in a hexagonal\nnoncentrosymmetric magnet Y_3Co_8Sn_4, using a combined experimental and\nab-initio calculations. Occurrence of such UAHE is mainly attributed to the\nreciprocal (KS) topology (i.e. the presence of topological Weyl points at/near\nthe Fermi level), along with some contribution from the topological magnetic\ntexture, as inferred from the measured field-dependent ac susceptibility. The\neffect of UAHE on the measured transport behavior however evolves differently\nwith temperature above and below T_C, suggesting different physical mechanism\nresponsible in the two phases. A unique planar ferrimagnetic ordering is found\nto be the most stable state with ab-plane as the easy plane below TC, as\nobserved experimentally. The simulated net magnetization and the moment per Co\natom agrees fairly well with the measured values. A reasonably large AHC is\nalso observed in both the phases (above and below and T_C) of the present\ncompound, which is again not so ubiquitous. Our results underscore the family\nof R_3Co_8Sn_4 (R= rare earth) polar magnets as a compelling backdrop for\nexploring the synergy of topological magnetism and non-trivial electronic\nbands, pivotal for spintronic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report a rare realization of unconventional anomalous Hall effect (UAHE)\nboth below and above the magnetic transition temperature (T_C) in a hexagonal\nnoncentrosymmetric magnet Y_3Co_8Sn_4, using a combined experimental and\nab-initio calculations. Occurrence of such UAHE is mainly attributed to the\nreciprocal (KS) topology (i.e. the presence of topological Weyl points at/near\nthe Fermi level), along with some contribution from the topological magnetic\ntexture, as inferred from the measured field-dependent ac susceptibility. The\neffect of UAHE on the measured transport behavior however evolves differently\nwith temperature above and below T_C, suggesting different physical mechanism\nresponsible in the two phases. A unique planar ferrimagnetic ordering is found\nto be the most stable state with ab-plane as the easy plane below TC, as\nobserved experimentally. The simulated net magnetization and the moment per Co\natom agrees fairly well with the measured values. A reasonably large AHC is\nalso observed in both the phases (above and below and T_C) of the present\ncompound, which is again not so ubiquitous. Our results underscore the family\nof R_3Co_8Sn_4 (R= rare earth) polar magnets as a compelling backdrop for\nexploring the synergy of topological magnetism and non-trivial electronic\nbands, pivotal for spintronic applications."
                },
                "authors": [
                    {
                        "name": "Afsar Ahmed"
                    },
                    {
                        "name": "Jyoti Sharma"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    },
                    {
                        "name": "Anis Biswas"
                    },
                    {
                        "name": "Tukai Singha"
                    },
                    {
                        "name": "Yaroslav Mudryk"
                    },
                    {
                        "name": "Aftab Alam"
                    },
                    {
                        "name": "I. Das"
                    }
                ],
                "author_detail": {
                    "name": "I. Das"
                },
                "author": "I. Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03450v1",
                "updated": "2025-02-05T18:50:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    50,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:50:38Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    50,
                    38,
                    2,
                    36,
                    0
                ],
                "title": "A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene\n  Graphs with Large-Language-Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene\n  Graphs with Large-Language-Models (LLMs)"
                },
                "summary": "Scene graphs have emerged as a structured and serializable environment\nrepresentation for grounded spatial reasoning with Large Language Models\n(LLMs). In this work, we propose SG-RwR, a Schema-Guided Retrieve-while-Reason\nframework for reasoning and planning with scene graphs. Our approach employs\ntwo cooperative, code-writing LLM agents: a (1) Reasoner for task planning and\ninformation queries generation, and a (2) Retriever for extracting\ncorresponding graph information following the queries. Two agents collaborate\niteratively, enabling sequential reasoning and adaptive attention to graph\ninformation. Unlike prior works, both agents are prompted only with the scene\ngraph schema rather than the full graph data, which reduces the hallucination\nby limiting input tokens, and drives the Reasoner to generate reasoning trace\nabstractly.Following the trace, the Retriever programmatically query the scene\ngraph data based on the schema understanding, allowing dynamic and global\nattention on the graph that enhances alignment between reasoning and retrieval.\nThrough experiments in multiple simulation environments, we show that our\nframework surpasses existing LLM-based approaches in numerical Q\\&A and\nplanning tasks, and can benefit from task-level few-shot examples, even in the\nabsence of agent-level demonstrations. Project code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene graphs have emerged as a structured and serializable environment\nrepresentation for grounded spatial reasoning with Large Language Models\n(LLMs). In this work, we propose SG-RwR, a Schema-Guided Retrieve-while-Reason\nframework for reasoning and planning with scene graphs. Our approach employs\ntwo cooperative, code-writing LLM agents: a (1) Reasoner for task planning and\ninformation queries generation, and a (2) Retriever for extracting\ncorresponding graph information following the queries. Two agents collaborate\niteratively, enabling sequential reasoning and adaptive attention to graph\ninformation. Unlike prior works, both agents are prompted only with the scene\ngraph schema rather than the full graph data, which reduces the hallucination\nby limiting input tokens, and drives the Reasoner to generate reasoning trace\nabstractly.Following the trace, the Retriever programmatically query the scene\ngraph data based on the schema understanding, allowing dynamic and global\nattention on the graph that enhances alignment between reasoning and retrieval.\nThrough experiments in multiple simulation environments, we show that our\nframework surpasses existing LLM-based approaches in numerical Q\\&A and\nplanning tasks, and can benefit from task-level few-shot examples, even in the\nabsence of agent-level demonstrations. Project code will be released."
                },
                "authors": [
                    {
                        "name": "Yiye Chen"
                    },
                    {
                        "name": "Harpreet Sawhney"
                    },
                    {
                        "name": "Nicholas Gyd"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Jack Saunders"
                    },
                    {
                        "name": "Patricio Vela"
                    },
                    {
                        "name": "Ben Lundell"
                    }
                ],
                "author_detail": {
                    "name": "Ben Lundell"
                },
                "author": "Ben Lundell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03447v1",
                "updated": "2025-02-05T18:45:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    45,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:45:38Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    45,
                    38,
                    2,
                    36,
                    0
                ],
                "title": "Designing LLM-simulated Immersive Spaces to Enhance Autistic Children's\n  Social Affordances Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing LLM-simulated Immersive Spaces to Enhance Autistic Children's\n  Social Affordances Understanding"
                },
                "summary": "One of the key challenges faced by autistic children is understanding social\naffordances in complex environments, which further impacts their ability to\nrespond appropriately to social signals. In traffic scenarios, this impairment\ncan even lead to safety concerns. In this paper, we introduce an LLM-simulated\nimmersive projection environment designed to improve this ability in autistic\nchildren while ensuring their safety. We first propose 17 design considerations\nacross four major categories, derived from a comprehensive review of previous\nresearch. Next, we developed a system called AIroad, which leverages LLMs to\nsimulate drivers with varying social intents, expressed through explicit\nmultimodal social signals. AIroad helps autistic children bridge the gap in\nrecognizing the intentions behind behaviors and learning appropriate responses\nthrough various stimuli. A user study involving 14 participants demonstrated\nthat this technology effectively engages autistic children and leads to\nsignificant improvements in their comprehension of social affordances in\ntraffic scenarios. Additionally, parents reported high perceived usability of\nthe system. These findings highlight the potential of combining LLM technology\nwith immersive environments for the functional rehabilitation of autistic\nchildren in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the key challenges faced by autistic children is understanding social\naffordances in complex environments, which further impacts their ability to\nrespond appropriately to social signals. In traffic scenarios, this impairment\ncan even lead to safety concerns. In this paper, we introduce an LLM-simulated\nimmersive projection environment designed to improve this ability in autistic\nchildren while ensuring their safety. We first propose 17 design considerations\nacross four major categories, derived from a comprehensive review of previous\nresearch. Next, we developed a system called AIroad, which leverages LLMs to\nsimulate drivers with varying social intents, expressed through explicit\nmultimodal social signals. AIroad helps autistic children bridge the gap in\nrecognizing the intentions behind behaviors and learning appropriate responses\nthrough various stimuli. A user study involving 14 participants demonstrated\nthat this technology effectively engages autistic children and leads to\nsignificant improvements in their comprehension of social affordances in\ntraffic scenarios. Additionally, parents reported high perceived usability of\nthe system. These findings highlight the potential of combining LLM technology\nwith immersive environments for the functional rehabilitation of autistic\nchildren in the future."
                },
                "authors": [
                    {
                        "name": "Yancheng Cao"
                    },
                    {
                        "name": "Yangyang HE"
                    },
                    {
                        "name": "Yonglin Chen"
                    },
                    {
                        "name": "Menghan Chen"
                    },
                    {
                        "name": "Shanhe You"
                    },
                    {
                        "name": "Yulin Qiu"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Chuan Luo"
                    },
                    {
                        "name": "Chen Zheng"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Jing Liang"
                    },
                    {
                        "name": "Jiangtao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiangtao Gong"
                },
                "author": "Jiangtao Gong",
                "arxiv_doi": "10.1145/3708359.3712142.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712142.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.03447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "iui2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03444v1",
                "updated": "2025-02-05T18:42:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    42,
                    4,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:42:04Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    42,
                    4,
                    2,
                    36,
                    0
                ],
                "title": "Masked Autoencoders Are Effective Tokenizers for Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Autoencoders Are Effective Tokenizers for Diffusion Models"
                },
                "summary": "Recent advances in latent diffusion models have demonstrated their\neffectiveness for high-resolution image synthesis. However, the properties of\nthe latent space from tokenizer for better learning and generation of diffusion\nmodels remain under-explored. Theoretically and empirically, we find that\nimproved generation quality is closely tied to the latent distributions with\nbetter structure, such as the ones with fewer Gaussian Mixture modes and more\ndiscriminative features. Motivated by these insights, we propose MAETok, an\nautoencoder (AE) leveraging mask modeling to learn semantically rich latent\nspace while maintaining reconstruction fidelity. Extensive experiments validate\nour analysis, demonstrating that the variational form of autoencoders is not\nnecessary, and a discriminative latent space from AE alone enables\nstate-of-the-art performance on ImageNet generation using only 128 tokens.\nMAETok achieves significant practical improvements, enabling a gFID of 1.69\nwith 76x faster training and 31x higher inference throughput for 512x512\ngeneration. Our findings show that the structure of the latent space, rather\nthan variational constraints, is crucial for effective diffusion models. Code\nand trained models are released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in latent diffusion models have demonstrated their\neffectiveness for high-resolution image synthesis. However, the properties of\nthe latent space from tokenizer for better learning and generation of diffusion\nmodels remain under-explored. Theoretically and empirically, we find that\nimproved generation quality is closely tied to the latent distributions with\nbetter structure, such as the ones with fewer Gaussian Mixture modes and more\ndiscriminative features. Motivated by these insights, we propose MAETok, an\nautoencoder (AE) leveraging mask modeling to learn semantically rich latent\nspace while maintaining reconstruction fidelity. Extensive experiments validate\nour analysis, demonstrating that the variational form of autoencoders is not\nnecessary, and a discriminative latent space from AE alone enables\nstate-of-the-art performance on ImageNet generation using only 128 tokens.\nMAETok achieves significant practical improvements, enabling a gFID of 1.69\nwith 76x faster training and 31x higher inference throughput for 512x512\ngeneration. Our findings show that the structure of the latent space, rather\nthan variational constraints, is crucial for effective diffusion models. Code\nand trained models are released."
                },
                "authors": [
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yujin Han"
                    },
                    {
                        "name": "Fangyi Chen"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Difan Zou"
                    },
                    {
                        "name": "Bhiksha Raj"
                    }
                ],
                "author_detail": {
                    "name": "Bhiksha Raj"
                },
                "author": "Bhiksha Raj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v7",
                "updated": "2025-02-05T18:39:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    39,
                    43,
                    2,
                    36,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads"
                },
                "summary": "Sparse attention, which selectively attends to a subset of tokens in the\ncontext was supposed to be efficient. However, its theoretical reduction in\nFLOPs has rarely translated into wall-clock speed-up over its dense attention\ncounterparts due to the lack of hardware-aware optimizations like\nFlashAttention. Meanwhile, it remains unclear whether sparse attention can\nmaintain the model's quality at a scale of today's large language models (LLMs)\nand how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library\nthat provides kernel optimization for sparse attention customizable at both\nper-head and per-context-range levels. S2-Attention enables the exploration of\nnovel and high-performance sparse attention techniques, which we demonstrate\nthrough extensive ablations across a wide range of sparse attention designs at\nvarious model scales. From these insights, we present several basic guidelines\nto design sparse attention that can achieve not only practical efficiency\nimprovements, but also strong downstream performance. To achieve high\nparallelization and optimized memory IO, sparse attention should shard the\ncontext heterogeneously across attention heads, where each head attends to a\ndifferent subset of tokens while collectively covering the full context.\nMeanwhile, we find hybrid architectures combining sparse and dense attention\nparticularly beneficial in practice. S2-Attention achieves wall-clock speedup\nof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with\nstrong downstream performance on-par with full attention and perfect retrieval\nperformance at a 128k context length. At inference, for 7B models, our model,\nwith the help of our S2-Attention kernel, achieves 4.5x speed-up compared to\ndense counterparts. S2-Attention is released with easy-to-customize APIs for\ndirect usage in Megatron and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse attention, which selectively attends to a subset of tokens in the\ncontext was supposed to be efficient. However, its theoretical reduction in\nFLOPs has rarely translated into wall-clock speed-up over its dense attention\ncounterparts due to the lack of hardware-aware optimizations like\nFlashAttention. Meanwhile, it remains unclear whether sparse attention can\nmaintain the model's quality at a scale of today's large language models (LLMs)\nand how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library\nthat provides kernel optimization for sparse attention customizable at both\nper-head and per-context-range levels. S2-Attention enables the exploration of\nnovel and high-performance sparse attention techniques, which we demonstrate\nthrough extensive ablations across a wide range of sparse attention designs at\nvarious model scales. From these insights, we present several basic guidelines\nto design sparse attention that can achieve not only practical efficiency\nimprovements, but also strong downstream performance. To achieve high\nparallelization and optimized memory IO, sparse attention should shard the\ncontext heterogeneously across attention heads, where each head attends to a\ndifferent subset of tokens while collectively covering the full context.\nMeanwhile, we find hybrid architectures combining sparse and dense attention\nparticularly beneficial in practice. S2-Attention achieves wall-clock speedup\nof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with\nstrong downstream performance on-par with full attention and perfect retrieval\nperformance at a 128k context length. At inference, for 7B models, our model,\nwith the help of our S2-Attention kernel, achieves 4.5x speed-up compared to\ndense counterparts. S2-Attention is released with easy-to-customize APIs for\ndirect usage in Megatron and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Liliang Ren"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03438v1",
                "updated": "2025-02-05T18:33:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    33,
                    36,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:33:36Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    33,
                    36,
                    2,
                    36,
                    0
                ],
                "title": "BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic\n  Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic\n  Theorem Proving"
                },
                "summary": "Recent advancements in large language models (LLMs) have spurred growing\ninterest in automatic theorem proving using Lean4, where effective tree search\nmethods are crucial for navigating proof search spaces. While the existing\napproaches primarily rely on value functions and Monte Carlo Tree Search\n(MCTS), the potential of simpler methods like Best-First Search (BFS) remains\nunderexplored. This paper investigates whether BFS can achieve competitive\nperformance in large-scale theorem proving tasks. We present\n\\texttt{BFS-Prover}, a scalable expert iteration framework, featuring three key\ninnovations. First, we implement strategic data filtering at each expert\niteration round, excluding problems solvable via beam search node expansion to\nfocus on harder cases. Second, we improve the sample efficiency of BFS through\nDirect Preference Optimization (DPO) applied to state-tactic pairs\nautomatically annotated with compiler error feedback, refining the LLM's policy\nto prioritize productive expansions. Third, we employ length normalization in\nBFS to encourage exploration of deeper proof paths. \\texttt{BFS-Prover}\nachieves a score of $71.31$ on the MiniF2F test set and therefore challenges\nthe perceived necessity of complex tree search methods, demonstrating that BFS\ncan achieve competitive performance when properly scaled.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have spurred growing\ninterest in automatic theorem proving using Lean4, where effective tree search\nmethods are crucial for navigating proof search spaces. While the existing\napproaches primarily rely on value functions and Monte Carlo Tree Search\n(MCTS), the potential of simpler methods like Best-First Search (BFS) remains\nunderexplored. This paper investigates whether BFS can achieve competitive\nperformance in large-scale theorem proving tasks. We present\n\\texttt{BFS-Prover}, a scalable expert iteration framework, featuring three key\ninnovations. First, we implement strategic data filtering at each expert\niteration round, excluding problems solvable via beam search node expansion to\nfocus on harder cases. Second, we improve the sample efficiency of BFS through\nDirect Preference Optimization (DPO) applied to state-tactic pairs\nautomatically annotated with compiler error feedback, refining the LLM's policy\nto prioritize productive expansions. Third, we employ length normalization in\nBFS to encourage exploration of deeper proof paths. \\texttt{BFS-Prover}\nachieves a score of $71.31$ on the MiniF2F test set and therefore challenges\nthe perceived necessity of complex tree search methods, demonstrating that BFS\ncan achieve competitive performance when properly scaled."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Chenguang Xi"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Xia Xiao"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Shen Zheng"
                    },
                    {
                        "name": "Kai Shen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shen"
                },
                "author": "Kai Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2202.03513v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2202.03513v3",
                "updated": "2025-02-05T18:29:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    29,
                    55,
                    2,
                    36,
                    0
                ],
                "published": "2022-02-07T20:57:03Z",
                "published_parsed": [
                    2022,
                    2,
                    7,
                    20,
                    57,
                    3,
                    0,
                    38,
                    0
                ],
                "title": "Causal survival analysis under competing risks using longitudinal\n  modified treatment policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal survival analysis under competing risks using longitudinal\n  modified treatment policies"
                },
                "summary": "Longitudinal modified treatment policies (LMTP) have been recently developed\nas a novel method to define and estimate causal parameters that depend on the\nnatural value of treatment. LMTPs represent an important advancement in causal\ninference for longitudinal studies as they allow the non-parametric definition\nand estimation of the joint effect of multiple categorical, numerical, or\ncontinuous exposures measured at several time points. We extend the LMTP\nmethodology to problems in which the outcome is a time-to-event variable\nsubject to right-censoring and competing risks. We present identification\nresults and non-parametric locally efficient estimators that use flexible\ndata-adaptive regression techniques to alleviate model misspecification bias,\nwhile retaining important asymptotic properties such as $\\sqrt{n}$-consistency.\nWe present an application to the estimation of the effect of the\ntime-to-intubation on acute kidney injury amongst COVID-19 hospitalized\npatients, where death by other causes is taken to be the competing event.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Longitudinal modified treatment policies (LMTP) have been recently developed\nas a novel method to define and estimate causal parameters that depend on the\nnatural value of treatment. LMTPs represent an important advancement in causal\ninference for longitudinal studies as they allow the non-parametric definition\nand estimation of the joint effect of multiple categorical, numerical, or\ncontinuous exposures measured at several time points. We extend the LMTP\nmethodology to problems in which the outcome is a time-to-event variable\nsubject to right-censoring and competing risks. We present identification\nresults and non-parametric locally efficient estimators that use flexible\ndata-adaptive regression techniques to alleviate model misspecification bias,\nwhile retaining important asymptotic properties such as $\\sqrt{n}$-consistency.\nWe present an application to the estimation of the effect of the\ntime-to-intubation on acute kidney injury amongst COVID-19 hospitalized\npatients, where death by other causes is taken to be the competing event."
                },
                "authors": [
                    {
                        "name": "Ivn Daz"
                    },
                    {
                        "name": "Katherine L Hoffman"
                    },
                    {
                        "name": "Nima S. Hejazi"
                    }
                ],
                "author_detail": {
                    "name": "Nima S. Hejazi"
                },
                "author": "Nima S. Hejazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2202.03513v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2202.03513v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03425v1",
                "updated": "2025-02-05T18:15:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    15,
                    9,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:15:09Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    15,
                    9,
                    2,
                    36,
                    0
                ],
                "title": "Harnessing Large Language Models for Curated Code Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language Models for Curated Code Reviews"
                },
                "summary": "In code review, generating structured and relevant comments is crucial for\nidentifying code issues and facilitating accurate code changes that ensure an\nefficient code review process. Well-crafted comments not only streamline the\ncode review itself but are also essential for subsequent tasks like code\nrefinement, where the code is modified to satisfy the input review comment.\nAlthough various AI-based approaches aimed to automate comment generation,\ntheir effectiveness remains limited by the quality of the training data.\nExisting code review datasets are often noisy and unrefined, posing limitations\nto the learning potential of AI models and hindering the automation process.\n  To address these challenges, we propose a curation pipeline designed to\nenhance the quality of the largest publicly available code review dataset. We\nbegin by establishing an evaluation framework, incorporating specific criteria\nand categories to empirically study the initial quality of the dataset. Using a\nlarge language model (LLM)-driven approach, we then apply our curation pipeline\nto refine the dataset. A comparative analysis of the newly curated dataset,\nbased on the same evaluation framework, demonstrates substantial improvements\nin the clarity and conciseness of the comments. Additionally, we assess the\nimpact of the curated dataset on automating downstream tasks, specifically\ncomment generation and code refinement. Our findings show that the curated\ndataset leads to enhanced model performance in generating more accurate\ncomments. Curated comments are also more useful as they lead to more accurate\ncode refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In code review, generating structured and relevant comments is crucial for\nidentifying code issues and facilitating accurate code changes that ensure an\nefficient code review process. Well-crafted comments not only streamline the\ncode review itself but are also essential for subsequent tasks like code\nrefinement, where the code is modified to satisfy the input review comment.\nAlthough various AI-based approaches aimed to automate comment generation,\ntheir effectiveness remains limited by the quality of the training data.\nExisting code review datasets are often noisy and unrefined, posing limitations\nto the learning potential of AI models and hindering the automation process.\n  To address these challenges, we propose a curation pipeline designed to\nenhance the quality of the largest publicly available code review dataset. We\nbegin by establishing an evaluation framework, incorporating specific criteria\nand categories to empirically study the initial quality of the dataset. Using a\nlarge language model (LLM)-driven approach, we then apply our curation pipeline\nto refine the dataset. A comparative analysis of the newly curated dataset,\nbased on the same evaluation framework, demonstrates substantial improvements\nin the clarity and conciseness of the comments. Additionally, we assess the\nimpact of the curated dataset on automating downstream tasks, specifically\ncomment generation and code refinement. Our findings show that the curated\ndataset leads to enhanced model performance in generating more accurate\ncomments. Curated comments are also more useful as they lead to more accurate\ncode refinement."
                },
                "authors": [
                    {
                        "name": "Oussama Ben Sghaier"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Houari Sahraoui"
                    }
                ],
                "author_detail": {
                    "name": "Houari Sahraoui"
                },
                "author": "Houari Sahraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03298v3",
                "updated": "2025-02-05T18:13:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    13,
                    41,
                    2,
                    36,
                    0
                ],
                "published": "2024-06-05T14:08:13Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    14,
                    8,
                    13,
                    2,
                    157,
                    0
                ],
                "title": "L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap\n  Multiview Point Cloud Registration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap\n  Multiview Point Cloud Registration"
                },
                "summary": "Point cloud registration is a prerequisite for many applications in computer\nvision and robotics. Most existing methods focus on pairwise registration of\ntwo point clouds with high overlap. Although there have been some methods for\nlow overlap cases, they struggle in degraded scenarios. This paper introduces a\nnovel framework dubbed L-PR, designed to register unordered low overlap\nmultiview point clouds leveraging LiDAR fiducial markers. We refer to them as\nLiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco\nmarkers, thin sheets of paper that do not affect the 3D geometry of the\nenvironment. We first propose an improved adaptive threshold marker detection\nmethod to provide robust detection results when the viewpoints among point\nclouds change dramatically. Then, we formulate the unordered multiview point\ncloud registration problem as a maximum a-posteriori (MAP) problem and develop\na framework consisting of two levels of graphs to address it. The first-level\ngraph, constructed as a weighted graph, is designed to efficiently and\noptimally infer initial values of scan poses from the unordered set. The\nsecond-level graph is constructed as a factor graph. By globally optimizing the\nvariables on the graph, including scan poses, marker poses, and marker corner\npositions, we tackle the MAP problem. We conduct both qualitative and\nquantitative experiments to demonstrate that the proposed method surpasses\nprevious state-of-the-art (SOTA) methods and to showcase that L-PR can serve as\na low-cost and efficient tool for 3D asset collection and training data\ncollection. In particular, we collect a new dataset named Livox-3DMatch using\nL-PR and incorporate it into the training of the SOTA learning-based method,\nSGHR, which brings evident improvements for SGHR on various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud registration is a prerequisite for many applications in computer\nvision and robotics. Most existing methods focus on pairwise registration of\ntwo point clouds with high overlap. Although there have been some methods for\nlow overlap cases, they struggle in degraded scenarios. This paper introduces a\nnovel framework dubbed L-PR, designed to register unordered low overlap\nmultiview point clouds leveraging LiDAR fiducial markers. We refer to them as\nLiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco\nmarkers, thin sheets of paper that do not affect the 3D geometry of the\nenvironment. We first propose an improved adaptive threshold marker detection\nmethod to provide robust detection results when the viewpoints among point\nclouds change dramatically. Then, we formulate the unordered multiview point\ncloud registration problem as a maximum a-posteriori (MAP) problem and develop\na framework consisting of two levels of graphs to address it. The first-level\ngraph, constructed as a weighted graph, is designed to efficiently and\noptimally infer initial values of scan poses from the unordered set. The\nsecond-level graph is constructed as a factor graph. By globally optimizing the\nvariables on the graph, including scan poses, marker poses, and marker corner\npositions, we tackle the MAP problem. We conduct both qualitative and\nquantitative experiments to demonstrate that the proposed method surpasses\nprevious state-of-the-art (SOTA) methods and to showcase that L-PR can serve as\na low-cost and efficient tool for 3D asset collection and training data\ncollection. In particular, we collect a new dataset named Livox-3DMatch using\nL-PR and incorporate it into the training of the SOTA learning-based method,\nSGHR, which brings evident improvements for SGHR on various benchmarks."
                },
                "authors": [
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Jinjun Shan"
                    },
                    {
                        "name": "Amaldev Haridevan"
                    },
                    {
                        "name": "Shuo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Zhang"
                },
                "author": "Shuo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03418v1",
                "updated": "2025-02-05T18:04:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    4,
                    29,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:04:29Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    4,
                    29,
                    2,
                    36,
                    0
                ],
                "title": "Think or Step-by-Step? UnZIPping the Black Box in Zero-Shot Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think or Step-by-Step? UnZIPping the Black Box in Zero-Shot Prompts"
                },
                "summary": "Zero-shot prompting techniques have significantly improved the performance of\nLarge Language Models (LLMs). However, we lack a clear understanding of why\nzero-shot prompts are so effective. For example, in the prompt \"Let's think\nstep-by-step,\" is \"think\" or \"step-by-step\" more crucial to its success?\nExisting interpretability methods, such as gradient-based and attention-based\napproaches, are computationally intensive and restricted to open-source models.\nWe introduce the ZIP score (Zero-shot Importance of Perturbation score), a\nversatile metric applicable to both open and closed-source models, based on\nsystematic input word perturbations. Our experiments across four recent LLMs,\nseven widely-used prompts, and several tasks, reveal interesting patterns in\nword importance. For instance, while both 'step-by-step' and 'think' show high\nZIP scores, which one is more influential depends on the model and task. We\nvalidate our method using controlled experiments and compare our results with\nhuman judgments, finding that proprietary models align more closely with human\nintuition regarding word significance. These findings enhance our understanding\nof LLM behavior and contribute to developing more effective zero-shot prompts\nand improved model analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot prompting techniques have significantly improved the performance of\nLarge Language Models (LLMs). However, we lack a clear understanding of why\nzero-shot prompts are so effective. For example, in the prompt \"Let's think\nstep-by-step,\" is \"think\" or \"step-by-step\" more crucial to its success?\nExisting interpretability methods, such as gradient-based and attention-based\napproaches, are computationally intensive and restricted to open-source models.\nWe introduce the ZIP score (Zero-shot Importance of Perturbation score), a\nversatile metric applicable to both open and closed-source models, based on\nsystematic input word perturbations. Our experiments across four recent LLMs,\nseven widely-used prompts, and several tasks, reveal interesting patterns in\nword importance. For instance, while both 'step-by-step' and 'think' show high\nZIP scores, which one is more influential depends on the model and task. We\nvalidate our method using controlled experiments and compare our results with\nhuman judgments, finding that proprietary models align more closely with human\nintuition regarding word significance. These findings enhance our understanding\nof LLM behavior and contribute to developing more effective zero-shot prompts\nand improved model analysis."
                },
                "authors": [
                    {
                        "name": "Nikta Gohari Sadr"
                    },
                    {
                        "name": "Sangmitra Madhusudan"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "arxiv_comment": "8 pages (excluding references)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03417v1",
                "updated": "2025-02-05T18:02:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    2,
                    1,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:02:01Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    2,
                    1,
                    2,
                    36,
                    0
                ],
                "title": "From Features to Transformers: Redefining Ranking for Scalable Impact",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Features to Transformers: Redefining Ranking for Scalable Impact"
                },
                "summary": "We present LiGR, a large-scale ranking framework developed at LinkedIn that\nbrings state-of-the-art transformer-based modeling architectures into\nproduction. We introduce a modified transformer architecture that incorporates\nlearned normalization and simultaneous set-wise attention to user history and\nranked items. This architecture enables several breakthrough achievements,\nincluding: (1) the deprecation of most manually designed feature engineering,\noutperforming the prior state-of-the-art system using only few features\n(compared to hundreds in the baseline), (2) validation of the scaling law for\nranking systems, showing improved performance with larger models, more training\ndata, and longer context sequences, and (3) simultaneous joint scoring of items\nin a set-wise manner, leading to automated improvements in diversity. To enable\nefficient serving of large ranking models, we describe techniques to scale\ninference effectively using single-pass processing of user history and set-wise\nattention. We also summarize key insights from various ablation studies and A/B\ntests, highlighting the most impactful technical approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LiGR, a large-scale ranking framework developed at LinkedIn that\nbrings state-of-the-art transformer-based modeling architectures into\nproduction. We introduce a modified transformer architecture that incorporates\nlearned normalization and simultaneous set-wise attention to user history and\nranked items. This architecture enables several breakthrough achievements,\nincluding: (1) the deprecation of most manually designed feature engineering,\noutperforming the prior state-of-the-art system using only few features\n(compared to hundreds in the baseline), (2) validation of the scaling law for\nranking systems, showing improved performance with larger models, more training\ndata, and longer context sequences, and (3) simultaneous joint scoring of items\nin a set-wise manner, leading to automated improvements in diversity. To enable\nefficient serving of large ranking models, we describe techniques to scale\ninference effectively using single-pass processing of user history and set-wise\nattention. We also summarize key insights from various ablation studies and A/B\ntests, highlighting the most impactful technical approaches."
                },
                "authors": [
                    {
                        "name": "Fedor Borisyuk"
                    },
                    {
                        "name": "Lars Hertel"
                    },
                    {
                        "name": "Ganesh Parameswaran"
                    },
                    {
                        "name": "Gaurav Srivastava"
                    },
                    {
                        "name": "Sudarshan Srinivasa Ramanujam"
                    },
                    {
                        "name": "Borja Ocejo"
                    },
                    {
                        "name": "Peng Du"
                    },
                    {
                        "name": "Andrei Akterskii"
                    },
                    {
                        "name": "Neil Daftary"
                    },
                    {
                        "name": "Shao Tang"
                    },
                    {
                        "name": "Daqi Sun"
                    },
                    {
                        "name": "Qiang Charles Xiao"
                    },
                    {
                        "name": "Deepesh Nathani"
                    },
                    {
                        "name": "Mohit Kothari"
                    },
                    {
                        "name": "Yun Dai"
                    },
                    {
                        "name": "Aman Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Aman Gupta"
                },
                "author": "Aman Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02542v2",
                "updated": "2025-02-05T17:58:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    58,
                    46,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-04T18:12:41Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    12,
                    41,
                    1,
                    35,
                    0
                ],
                "title": "OverThink: Slowdown Attacks on Reasoning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OverThink: Slowdown Attacks on Reasoning LLMs"
                },
                "summary": "We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 18x slowdown on FreshQA dataset and 46x\nslowdown on SQuAD dataset. The attack also shows high transferability across\nmodels. To protect applications, we discuss and implement defenses leveraging\nLLM-based and system design approaches. Finally, we discuss societal,\nfinancial, and energy impacts of OVERTHINK attack which could amplify the costs\nfor third-party applications operating reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 18x slowdown on FreshQA dataset and 46x\nslowdown on SQuAD dataset. The attack also shows high transferability across\nmodels. To protect applications, we discuss and implement defenses leveraging\nLLM-based and system design approaches. Finally, we discuss societal,\nfinancial, and energy impacts of OVERTHINK attack which could amplify the costs\nfor third-party applications operating reasoning models."
                },
                "authors": [
                    {
                        "name": "Abhinav Kumar"
                    },
                    {
                        "name": "Jaechul Roh"
                    },
                    {
                        "name": "Ali Naseh"
                    },
                    {
                        "name": "Marzena Karpinska"
                    },
                    {
                        "name": "Mohit Iyyer"
                    },
                    {
                        "name": "Amir Houmansadr"
                    },
                    {
                        "name": "Eugene Bagdasarian"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Bagdasarian"
                },
                "author": "Eugene Bagdasarian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03414v1",
                "updated": "2025-02-05T17:55:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    55,
                    22,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T17:55:22Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    55,
                    22,
                    2,
                    36,
                    0
                ],
                "title": "Estimating causal effects using difference-in-differences under network\n  dependency and interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating causal effects using difference-in-differences under network\n  dependency and interference"
                },
                "summary": "Differences-in-differences (DiD) is a causal inference method for\nobservational longitudinal data that assumes parallel expected outcome\ntrajectories between treatment groups under the (possible) counterfactual of\nreceiving a specific treatment. In this paper DiD is extended to allow for (i)\nnetwork dependency where outcomes, treatments, and covariates may exhibit\nbetween-unit latent correlation, and (ii) interference, where treatments can\naffect outcomes in neighboring units. In this setting, the causal estimand of\ninterest is the average exposure effect among units with a specific exposure\nlevel, where the exposure is a function of treatments from potentially many\nunits. Under a conditional parallel trends assumption and suitable network\ndependency conditions, a doubly robust estimator allowing for data-adaptive\nnuisance function estimation is proposed and shown to be consistent and\nasymptotically normal with variance reaching the semiparametric efficiency\nbound. The proposed methods are evaluated in simulations and applied to study\nthe effects of adopting emission control technologies in coal power plants on\ncounty-level mortality due to cardiovascular disease.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differences-in-differences (DiD) is a causal inference method for\nobservational longitudinal data that assumes parallel expected outcome\ntrajectories between treatment groups under the (possible) counterfactual of\nreceiving a specific treatment. In this paper DiD is extended to allow for (i)\nnetwork dependency where outcomes, treatments, and covariates may exhibit\nbetween-unit latent correlation, and (ii) interference, where treatments can\naffect outcomes in neighboring units. In this setting, the causal estimand of\ninterest is the average exposure effect among units with a specific exposure\nlevel, where the exposure is a function of treatments from potentially many\nunits. Under a conditional parallel trends assumption and suitable network\ndependency conditions, a doubly robust estimator allowing for data-adaptive\nnuisance function estimation is proposed and shown to be consistent and\nasymptotically normal with variance reaching the semiparametric efficiency\nbound. The proposed methods are evaluated in simulations and applied to study\nthe effects of adopting emission control technologies in coal power plants on\ncounty-level mortality due to cardiovascular disease."
                },
                "authors": [
                    {
                        "name": "Michael Jetsupphasuk"
                    },
                    {
                        "name": "Didong Li"
                    },
                    {
                        "name": "Michael G. Hudgens"
                    }
                ],
                "author_detail": {
                    "name": "Michael G. Hudgens"
                },
                "author": "Michael G. Hudgens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20724v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20724v4",
                "updated": "2025-02-05T17:45:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    45,
                    24,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-28T04:39:32Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    4,
                    39,
                    32,
                    0,
                    302,
                    0
                ],
                "title": "Simple Is Effective: The Roles of Graphs and Large Language Models in\n  Knowledge-Graph-Based Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple Is Effective: The Roles of Graphs and Large Language Models in\n  Knowledge-Graph-Based Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) demonstrate strong reasoning abilities but face\nlimitations such as hallucinations and outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by\ngrounding LLM outputs in structured external knowledge from KGs. However,\ncurrent KG-based RAG frameworks still struggle to optimize the trade-off\nbetween retrieval effectiveness and efficiency in identifying a suitable amount\nof relevant graph information for the LLM to digest. We introduce SubgraphRAG,\nextending the KG-based RAG framework that retrieves subgraphs and leverages\nLLMs for reasoning and answer prediction. Our approach innovatively integrates\na lightweight multilayer perceptron with a parallel triple-scoring mechanism\nfor efficient and flexible subgraph retrieval while encoding directional\nstructural distances to enhance retrieval effectiveness. The size of retrieved\nsubgraphs can be flexibly adjusted to match the query's need and the downstream\nLLM's capabilities. This design strikes a balance between model complexity and\nreasoning power, enabling scalable and generalizable retrieval processes.\nNotably, based on our retrieved subgraphs, smaller LLMs like\nLlama3.1-8B-Instruct deliver competitive results with explainable reasoning,\nwhile larger models like GPT-4o achieve state-of-the-art accuracy compared with\nprevious baselines -- all without fine-tuning. Extensive evaluations on the\nWebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,\naccuracy, and reliability by reducing hallucinations and improving response\ngrounding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate strong reasoning abilities but face\nlimitations such as hallucinations and outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by\ngrounding LLM outputs in structured external knowledge from KGs. However,\ncurrent KG-based RAG frameworks still struggle to optimize the trade-off\nbetween retrieval effectiveness and efficiency in identifying a suitable amount\nof relevant graph information for the LLM to digest. We introduce SubgraphRAG,\nextending the KG-based RAG framework that retrieves subgraphs and leverages\nLLMs for reasoning and answer prediction. Our approach innovatively integrates\na lightweight multilayer perceptron with a parallel triple-scoring mechanism\nfor efficient and flexible subgraph retrieval while encoding directional\nstructural distances to enhance retrieval effectiveness. The size of retrieved\nsubgraphs can be flexibly adjusted to match the query's need and the downstream\nLLM's capabilities. This design strikes a balance between model complexity and\nreasoning power, enabling scalable and generalizable retrieval processes.\nNotably, based on our retrieved subgraphs, smaller LLMs like\nLlama3.1-8B-Instruct deliver competitive results with explainable reasoning,\nwhile larger models like GPT-4o achieve state-of-the-art accuracy compared with\nprevious baselines -- all without fine-tuning. Extensive evaluations on the\nWebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,\naccuracy, and reliability by reducing hallucinations and improving response\ngrounding."
                },
                "authors": [
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Siqi Miao"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "arxiv_comment": "Accepted by ICLR 2025; Code available at\n  https://github.com/Graph-COM/SubgraphRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20724v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20724v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09662v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09662v3",
                "updated": "2025-02-05T17:41:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    41,
                    42,
                    2,
                    36,
                    0
                ],
                "published": "2024-09-15T08:25:24Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    8,
                    25,
                    24,
                    6,
                    259,
                    0
                ],
                "title": "ExploreSelf: Fostering User-driven Exploration and Reflection on\n  Personal Challenges with Adaptive Guidance by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExploreSelf: Fostering User-driven Exploration and Reflection on\n  Personal Challenges with Adaptive Guidance by Large Language Models"
                },
                "summary": "Expressing stressful experiences in words is proven to improve mental and\nphysical health, but individuals often disengage with writing interventions as\nthey struggle to organize their thoughts and emotions. Reflective prompts have\nbeen used to provide direction, and large language models (LLMs) have\ndemonstrated the potential to provide tailored guidance. However, current\nsystems often limit users' flexibility to direct their reflections. We thus\npresent ExploreSelf, an LLM-driven application designed to empower users to\ncontrol their reflective journey, providing adaptive support through\ndynamically generated questions. Through an exploratory study with 19\nparticipants, we examine how participants explore and reflect on personal\nchallenges using ExploreSelf. Our findings demonstrate that participants valued\nthe flexible navigation of adaptive guidance to control their reflective\njourney, leading to deeper engagement and insight. Building on our findings, we\ndiscuss the implications of designing LLM-driven tools that facilitate\nuser-driven and effective reflection of personal challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expressing stressful experiences in words is proven to improve mental and\nphysical health, but individuals often disengage with writing interventions as\nthey struggle to organize their thoughts and emotions. Reflective prompts have\nbeen used to provide direction, and large language models (LLMs) have\ndemonstrated the potential to provide tailored guidance. However, current\nsystems often limit users' flexibility to direct their reflections. We thus\npresent ExploreSelf, an LLM-driven application designed to empower users to\ncontrol their reflective journey, providing adaptive support through\ndynamically generated questions. Through an exploratory study with 19\nparticipants, we examine how participants explore and reflect on personal\nchallenges using ExploreSelf. Our findings demonstrate that participants valued\nthe flexible navigation of adaptive guidance to control their reflective\njourney, leading to deeper engagement and insight. Building on our findings, we\ndiscuss the implications of designing LLM-driven tools that facilitate\nuser-driven and effective reflection of personal challenges."
                },
                "authors": [
                    {
                        "name": "Inhwa Song"
                    },
                    {
                        "name": "SoHyun Park"
                    },
                    {
                        "name": "Sachin R. Pendse"
                    },
                    {
                        "name": "Jessica Lee Schleider"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    },
                    {
                        "name": "Young-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young-Ho Kim"
                },
                "author": "Young-Ho Kim",
                "arxiv_comment": "17 pages excluding reference and appendix. Accepted at ACM CHI 2025.\n  https://naver-ai.github.io/exploreself",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09662v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09662v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03397v1",
                "updated": "2025-02-05T17:32:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    32,
                    29,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T17:32:29Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    32,
                    29,
                    2,
                    36,
                    0
                ],
                "title": "SPRI: Aligning Large Language Models with Context-Situated Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPRI: Aligning Large Language Models with Context-Situated Principles"
                },
                "summary": "Aligning Large Language Models to integrate and reflect human values,\nespecially for tasks that demand intricate human oversight, is arduous since it\nis resource-intensive and time-consuming to depend on human expertise for\ncontext-specific guidance. Prior work has utilized predefined sets of rules or\nprinciples to steer the behavior of models (Bai et al., 2022; Sun et al.,\n2023). However, these principles tend to be generic, making it challenging to\nadapt them to each individual input query or context. In this work, we present\nSituated-PRInciples (SPRI), a framework requiring minimal or no human effort\nthat is designed to automatically generate guiding principles in real-time for\neach input query and utilize them to align each response. We evaluate SPRI on\nthree tasks, and show that 1) SPRI can derive principles in a complex\ndomain-specific task that leads to on-par performance as expert-crafted ones;\n2) SPRI-generated principles lead to instance-specific rubrics that outperform\nprior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data\nleads to substantial improvement on truthfulness. We release our code and model\ngenerations at https://github.com/honglizhan/SPRI-public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models to integrate and reflect human values,\nespecially for tasks that demand intricate human oversight, is arduous since it\nis resource-intensive and time-consuming to depend on human expertise for\ncontext-specific guidance. Prior work has utilized predefined sets of rules or\nprinciples to steer the behavior of models (Bai et al., 2022; Sun et al.,\n2023). However, these principles tend to be generic, making it challenging to\nadapt them to each individual input query or context. In this work, we present\nSituated-PRInciples (SPRI), a framework requiring minimal or no human effort\nthat is designed to automatically generate guiding principles in real-time for\neach input query and utilize them to align each response. We evaluate SPRI on\nthree tasks, and show that 1) SPRI can derive principles in a complex\ndomain-specific task that leads to on-par performance as expert-crafted ones;\n2) SPRI-generated principles lead to instance-specific rubrics that outperform\nprior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data\nleads to substantial improvement on truthfulness. We release our code and model\ngenerations at https://github.com/honglizhan/SPRI-public."
                },
                "authors": [
                    {
                        "name": "Hongli Zhan"
                    },
                    {
                        "name": "Muneeza Azmat"
                    },
                    {
                        "name": "Raya Horesh"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Yurochkin"
                },
                "author": "Mikhail Yurochkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03395v1",
                "updated": "2025-02-05T17:30:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    30,
                    31,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T17:30:31Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    30,
                    31,
                    2,
                    36,
                    0
                ],
                "title": "Benchmarking Time Series Forecasting Models: From Statistical Techniques\n  to Foundation Models in Real-World Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Time Series Forecasting Models: From Statistical Techniques\n  to Foundation Models in Real-World Applications"
                },
                "summary": "Time series forecasting is essential for operational intelligence in the\nhospitality industry, and particularly challenging in large-scale, distributed\nsystems. This study evaluates the performance of statistical, machine learning\n(ML), deep learning, and foundation models in forecasting hourly sales over a\n14-day horizon using real-world data from a network of thousands of restaurants\nacross Germany. The forecasting solution includes features such as weather\nconditions, calendar events, and time-of-day patterns. Results demonstrate the\nstrong performance of ML-based meta-models and highlight the emerging potential\nof foundation models like Chronos and TimesFM, which deliver competitive\nperformance with minimal feature engineering, leveraging only the pre-trained\nmodel (zero-shot inference). Additionally, a hybrid PySpark-Pandas approach\nproves to be a robust solution for achieving horizontal scalability in\nlarge-scale deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting is essential for operational intelligence in the\nhospitality industry, and particularly challenging in large-scale, distributed\nsystems. This study evaluates the performance of statistical, machine learning\n(ML), deep learning, and foundation models in forecasting hourly sales over a\n14-day horizon using real-world data from a network of thousands of restaurants\nacross Germany. The forecasting solution includes features such as weather\nconditions, calendar events, and time-of-day patterns. Results demonstrate the\nstrong performance of ML-based meta-models and highlight the emerging potential\nof foundation models like Chronos and TimesFM, which deliver competitive\nperformance with minimal feature engineering, leveraging only the pre-trained\nmodel (zero-shot inference). Additionally, a hybrid PySpark-Pandas approach\nproves to be a robust solution for achieving horizontal scalability in\nlarge-scale deployments."
                },
                "authors": [
                    {
                        "name": "Issar Arab"
                    },
                    {
                        "name": "Rodrigo Benitez"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Benitez"
                },
                "author": "Rodrigo Benitez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01976v2",
                "updated": "2025-02-05T17:26:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    26,
                    35,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-04T03:36:44Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    36,
                    44,
                    1,
                    35,
                    0
                ],
                "title": "CITER: Collaborative Inference for Efficient Large Language Model\n  Decoding with Token-Level Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITER: Collaborative Inference for Efficient Large Language Model\n  Decoding with Token-Level Routing"
                },
                "summary": "Large language models have achieved remarkable success in various tasks but\nsuffer from high computational costs during inference, limiting their\ndeployment in resource-constrained applications. To address this issue, we\npropose a novel CITER (\\textbf{C}ollaborative \\textbf{I}nference with\n\\textbf{T}oken-l\\textbf{E}vel \\textbf{R}outing) framework that enables\nefficient collaboration between small and large language models (SLMs & LLMs)\nthrough a token-level routing strategy. Specifically, CITER routes non-critical\ntokens to an SLM for efficiency and routes critical tokens to an LLM for\ngeneralization quality. We formulate router training as a policy optimization,\nwhere the router receives rewards based on both the quality of predictions and\nthe inference costs of generation. This allows the router to learn to predict\ntoken-level routing scores and make routing decisions based on both the current\ntoken and the future impact of its decisions. To further accelerate the reward\nevaluation process, we introduce a shortcut which significantly reduces the\ncosts of the reward estimation and improving the practicality of our approach.\nExtensive experiments on five benchmark datasets demonstrate that CITER reduces\nthe inference costs while preserving high-quality generation, offering a\npromising solution for real-time and resource-constrained applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved remarkable success in various tasks but\nsuffer from high computational costs during inference, limiting their\ndeployment in resource-constrained applications. To address this issue, we\npropose a novel CITER (\\textbf{C}ollaborative \\textbf{I}nference with\n\\textbf{T}oken-l\\textbf{E}vel \\textbf{R}outing) framework that enables\nefficient collaboration between small and large language models (SLMs & LLMs)\nthrough a token-level routing strategy. Specifically, CITER routes non-critical\ntokens to an SLM for efficiency and routes critical tokens to an LLM for\ngeneralization quality. We formulate router training as a policy optimization,\nwhere the router receives rewards based on both the quality of predictions and\nthe inference costs of generation. This allows the router to learn to predict\ntoken-level routing scores and make routing decisions based on both the current\ntoken and the future impact of its decisions. To further accelerate the reward\nevaluation process, we introduce a shortcut which significantly reduces the\ncosts of the reward estimation and improving the practicality of our approach.\nExtensive experiments on five benchmark datasets demonstrate that CITER reduces\nthe inference costs while preserving high-quality generation, offering a\npromising solution for real-time and resource-constrained applications."
                },
                "authors": [
                    {
                        "name": "Wenhao Zheng"
                    },
                    {
                        "name": "Yixiao Chen"
                    },
                    {
                        "name": "Weitong Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric P. Xing"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03382v1",
                "updated": "2025-02-05T17:18:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    18,
                    55,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T17:18:55Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    18,
                    55,
                    2,
                    36,
                    0
                ],
                "title": "High-Fidelity Simultaneous Speech-To-Speech Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Fidelity Simultaneous Speech-To-Speech Translation"
                },
                "summary": "We introduce Hibiki, a decoder-only model for simultaneous speech\ntranslation. Hibiki leverages a multistream language model to synchronously\nprocess source and target speech, and jointly produces text and audio tokens to\nperform speech-to-text and speech-to-speech translation. We furthermore address\nthe fundamental challenge of simultaneous interpretation, which unlike its\nconsecutive counterpart, where one waits for the end of the source utterance to\nstart translating, adapts its flow to accumulate just enough context to produce\na correct translation in real-time, chunk by chunk. To do so, we introduce a\nweakly-supervised method that leverages the perplexity of an off-the-shelf text\ntranslation system to identify optimal delays on a per-word basis and create\naligned synthetic data. After supervised training, Hibiki performs adaptive,\nsimultaneous speech translation with vanilla temperature sampling. On a\nFrench-English simultaneous speech translation task, Hibiki demonstrates\nstate-of-the-art performance in translation quality, speaker fidelity and\nnaturalness. Moreover, the simplicity of its inference process makes it\ncompatible with batched translation and even real-time on-device deployment. We\nprovide examples as well as models and inference code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Hibiki, a decoder-only model for simultaneous speech\ntranslation. Hibiki leverages a multistream language model to synchronously\nprocess source and target speech, and jointly produces text and audio tokens to\nperform speech-to-text and speech-to-speech translation. We furthermore address\nthe fundamental challenge of simultaneous interpretation, which unlike its\nconsecutive counterpart, where one waits for the end of the source utterance to\nstart translating, adapts its flow to accumulate just enough context to produce\na correct translation in real-time, chunk by chunk. To do so, we introduce a\nweakly-supervised method that leverages the perplexity of an off-the-shelf text\ntranslation system to identify optimal delays on a per-word basis and create\naligned synthetic data. After supervised training, Hibiki performs adaptive,\nsimultaneous speech translation with vanilla temperature sampling. On a\nFrench-English simultaneous speech translation task, Hibiki demonstrates\nstate-of-the-art performance in translation quality, speaker fidelity and\nnaturalness. Moreover, the simplicity of its inference process makes it\ncompatible with batched translation and even real-time on-device deployment. We\nprovide examples as well as models and inference code."
                },
                "authors": [
                    {
                        "name": "Tom Labiausse"
                    },
                    {
                        "name": "Laurent Mazar"
                    },
                    {
                        "name": "Edouard Grave"
                    },
                    {
                        "name": "Patrick Prez"
                    },
                    {
                        "name": "Alexandre Dfossez"
                    },
                    {
                        "name": "Neil Zeghidour"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zeghidour"
                },
                "author": "Neil Zeghidour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03373v1",
                "updated": "2025-02-05T17:13:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    13,
                    32,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T17:13:32Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    13,
                    32,
                    2,
                    36,
                    0
                ],
                "title": "Demystifying Long Chain-of-Thought Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Long Chain-of-Thought Reasoning in LLMs"
                },
                "summary": "Scaling inference compute enhances reasoning in large language models (LLMs),\nwith long chains-of-thought (CoTs) enabling strategies like backtracking and\nerror correction. Reinforcement learning (RL) has emerged as a crucial method\nfor developing these capabilities, yet the conditions under which long CoTs\nemerge remain unclear, and RL training requires careful design choices. In this\nstudy, we systematically investigate the mechanics of long CoT reasoning,\nidentifying the key factors that enable models to generate long CoT\ntrajectories. Through extensive supervised fine-tuning (SFT) and RL\nexperiments, we present four main findings: (1) While SFT is not strictly\nnecessary, it simplifies training and improves efficiency; (2) Reasoning\ncapabilities tend to emerge with increased training compute, but their\ndevelopment is not guaranteed, making reward shaping crucial for stabilizing\nCoT length growth; (3) Scaling verifiable reward signals is critical for RL. We\nfind that leveraging noisy, web-extracted solutions with filtering mechanisms\nshows strong potential, particularly for out-of-distribution (OOD) tasks such\nas STEM reasoning; and (4) Core abilities like error correction are inherently\npresent in base models, but incentivizing these skills effectively for complex\ntasks via RL demands significant compute, and measuring their emergence\nrequires a nuanced approach. These insights provide practical guidance for\noptimizing training strategies to enhance long CoT reasoning in LLMs. Our code\nis available at: https://github.com/eddycmu/demystify-long-cot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling inference compute enhances reasoning in large language models (LLMs),\nwith long chains-of-thought (CoTs) enabling strategies like backtracking and\nerror correction. Reinforcement learning (RL) has emerged as a crucial method\nfor developing these capabilities, yet the conditions under which long CoTs\nemerge remain unclear, and RL training requires careful design choices. In this\nstudy, we systematically investigate the mechanics of long CoT reasoning,\nidentifying the key factors that enable models to generate long CoT\ntrajectories. Through extensive supervised fine-tuning (SFT) and RL\nexperiments, we present four main findings: (1) While SFT is not strictly\nnecessary, it simplifies training and improves efficiency; (2) Reasoning\ncapabilities tend to emerge with increased training compute, but their\ndevelopment is not guaranteed, making reward shaping crucial for stabilizing\nCoT length growth; (3) Scaling verifiable reward signals is critical for RL. We\nfind that leveraging noisy, web-extracted solutions with filtering mechanisms\nshows strong potential, particularly for out-of-distribution (OOD) tasks such\nas STEM reasoning; and (4) Core abilities like error correction are inherently\npresent in base models, but incentivizing these skills effectively for complex\ntasks via RL demands significant compute, and measuring their emergence\nrequires a nuanced approach. These insights provide practical guidance for\noptimizing training strategies to enhance long CoT reasoning in LLMs. Our code\nis available at: https://github.com/eddycmu/demystify-long-cot."
                },
                "authors": [
                    {
                        "name": "Edward Yeo"
                    },
                    {
                        "name": "Yuxuan Tong"
                    },
                    {
                        "name": "Morry Niu"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Xiang Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Yue"
                },
                "author": "Xiang Yue",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00326v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00326v8",
                "updated": "2025-02-05T17:08:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    8,
                    17,
                    2,
                    36,
                    0
                ],
                "published": "2023-12-01T03:44:54Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    3,
                    44,
                    54,
                    4,
                    335,
                    0
                ],
                "title": "Agent-OM: Leveraging LLM Agents for Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-OM: Leveraging LLM Agents for Ontology Matching"
                },
                "summary": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Kerry Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Kerry Taylor"
                },
                "author": "Kerry Taylor",
                "arxiv_comment": "19 pages, 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00326v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00326v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03368v1",
                "updated": "2025-02-05T17:06:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    6,
                    59,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T17:06:59Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    6,
                    59,
                    2,
                    36,
                    0
                ],
                "title": "PalimpChat: Declarative and Interactive AI analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PalimpChat: Declarative and Interactive AI analytics"
                },
                "summary": "Thanks to the advances in generative architectures and large language models,\ndata scientists can now code pipelines of machine-learning operations to\nprocess large collections of unstructured data. Recent progress has seen the\nrise of declarative AI frameworks (e.g., Palimpzest, Lotus, and DocETL) to\nbuild optimized and increasingly complex pipelines, but these systems often\nremain accessible only to expert programmers. In this demonstration, we present\nPalimpChat, a chat-based interface to Palimpzest that bridges this gap by\nletting users create and run sophisticated AI pipelines through natural\nlanguage alone. By integrating Archytas, a ReAct-based reasoning agent, and\nPalimpzest's suite of relational and LLM-based operators, PalimpChat provides a\npractical illustration of how a chat interface can make declarative AI\nframeworks truly accessible to non-experts.\n  Our demo system is publicly available online. At SIGMOD'25, participants can\nexplore three real-world scenarios--scientific discovery, legal discovery, and\nreal estate search--or apply PalimpChat to their own datasets. In this paper,\nwe focus on how PalimpChat, supported by the Palimpzest optimizer, simplifies\ncomplex AI workflows such as extracting and analyzing biomedical data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thanks to the advances in generative architectures and large language models,\ndata scientists can now code pipelines of machine-learning operations to\nprocess large collections of unstructured data. Recent progress has seen the\nrise of declarative AI frameworks (e.g., Palimpzest, Lotus, and DocETL) to\nbuild optimized and increasingly complex pipelines, but these systems often\nremain accessible only to expert programmers. In this demonstration, we present\nPalimpChat, a chat-based interface to Palimpzest that bridges this gap by\nletting users create and run sophisticated AI pipelines through natural\nlanguage alone. By integrating Archytas, a ReAct-based reasoning agent, and\nPalimpzest's suite of relational and LLM-based operators, PalimpChat provides a\npractical illustration of how a chat interface can make declarative AI\nframeworks truly accessible to non-experts.\n  Our demo system is publicly available online. At SIGMOD'25, participants can\nexplore three real-world scenarios--scientific discovery, legal discovery, and\nreal estate search--or apply PalimpChat to their own datasets. In this paper,\nwe focus on how PalimpChat, supported by the Palimpzest optimizer, simplifies\ncomplex AI workflows such as extracting and analyzing biomedical data."
                },
                "authors": [
                    {
                        "name": "Chunwei Liu"
                    },
                    {
                        "name": "Gerardo Vitagliano"
                    },
                    {
                        "name": "Brandon Rose"
                    },
                    {
                        "name": "Matt Prinz"
                    },
                    {
                        "name": "David Andrew Samson"
                    },
                    {
                        "name": "Michael Cafarella"
                    }
                ],
                "author_detail": {
                    "name": "Michael Cafarella"
                },
                "author": "Michael Cafarella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03366v1",
                "updated": "2025-02-05T17:03:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    3,
                    49,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T17:03:49Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    3,
                    49,
                    2,
                    36,
                    0
                ],
                "title": "Rethinking Approximate Gaussian Inference in Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Approximate Gaussian Inference in Classification"
                },
                "summary": "In classification tasks, softmax functions are ubiquitously used as output\nactivations to produce predictive probabilities. Such outputs only capture\naleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian\ninference methods have been proposed, which output Gaussian distributions over\nthe logit space. Predictives are then obtained as the expectations of the\nGaussian distributions pushed forward through the softmax. However, such\nsoftmax Gaussian integrals cannot be solved analytically, and Monte Carlo (MC)\napproximations can be costly and noisy. We propose a simple change in the\nlearning objective which allows the exact computation of predictives and enjoys\nimproved training dynamics, with no runtime or memory overhead. This framework\nis compatible with a family of output activation functions that includes the\nsoftmax, as well as element-wise normCDF and sigmoid. Moreover, it allows for\napproximating the Gaussian pushforwards with Dirichlet distributions by\nanalytic moment matching. We evaluate our approach combined with several\napproximate Gaussian inference methods (Laplace, HET, SNGP) on large- and\nsmall-scale datasets (ImageNet, CIFAR-10), demonstrating improved uncertainty\nquantification capabilities compared to softmax MC sampling. Code is available\nat https://github.com/bmucsanyi/probit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In classification tasks, softmax functions are ubiquitously used as output\nactivations to produce predictive probabilities. Such outputs only capture\naleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian\ninference methods have been proposed, which output Gaussian distributions over\nthe logit space. Predictives are then obtained as the expectations of the\nGaussian distributions pushed forward through the softmax. However, such\nsoftmax Gaussian integrals cannot be solved analytically, and Monte Carlo (MC)\napproximations can be costly and noisy. We propose a simple change in the\nlearning objective which allows the exact computation of predictives and enjoys\nimproved training dynamics, with no runtime or memory overhead. This framework\nis compatible with a family of output activation functions that includes the\nsoftmax, as well as element-wise normCDF and sigmoid. Moreover, it allows for\napproximating the Gaussian pushforwards with Dirichlet distributions by\nanalytic moment matching. We evaluate our approach combined with several\napproximate Gaussian inference methods (Laplace, HET, SNGP) on large- and\nsmall-scale datasets (ImageNet, CIFAR-10), demonstrating improved uncertainty\nquantification capabilities compared to softmax MC sampling. Code is available\nat https://github.com/bmucsanyi/probit."
                },
                "authors": [
                    {
                        "name": "Blint Mucsnyi"
                    },
                    {
                        "name": "Nathal Da Costa"
                    },
                    {
                        "name": "Philipp Hennig"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Hennig"
                },
                "author": "Philipp Hennig",
                "arxiv_comment": "29 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03360v1",
                "updated": "2025-02-05T16:56:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    56,
                    17,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:56:17Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    56,
                    17,
                    2,
                    36,
                    0
                ],
                "title": "A Beam's Eye View to Fluence Maps 3D Network for Ultra Fast VMAT\n  Radiotherapy Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Beam's Eye View to Fluence Maps 3D Network for Ultra Fast VMAT\n  Radiotherapy Planning"
                },
                "summary": "Volumetric Modulated Arc Therapy (VMAT) revolutionizes cancer treatment by\nprecisely delivering radiation while sparing healthy tissues. Fluence maps\ngeneration, crucial in VMAT planning, traditionally involves complex and\niterative, and thus time consuming processes. These fluence maps are\nsubsequently leveraged for leaf-sequence. The deep-learning approach presented\nin this article aims to expedite this by directly predicting fluence maps from\npatient data. We developed a 3D network which we trained in a supervised way\nusing a combination of L1 and L2 losses, and RT plans generated by Eclipse and\nfrom the REQUITE dataset, taking the RT dose map as input and the fluence maps\ncomputed from the corresponding RT plans as target. Our network predicts\njointly the 180 fluence maps corresponding to the 180 control points (CP) of\nsingle arc VMAT plans. In order to help the network, we pre-process the input\ndose by computing the projections of the 3D dose map to the beam's eye view\n(BEV) of the 180 CPs, in the same coordinate system as the fluence maps. We\ngenerated over 2000 VMAT plans using Eclipse to scale up the dataset size.\nAdditionally, we evaluated various network architectures and analyzed the\nimpact of increasing the dataset size. We are measuring the performance in the\n2D fluence maps domain using image metrics (PSNR, SSIM), as well as in the 3D\ndose domain using the dose-volume histogram (DVH) on a validation dataset. The\nnetwork inference, which does not include the data loading and processing, is\nless than 20ms. Using our proposed 3D network architecture as well as\nincreasing the dataset size using Eclipse improved the fluence map\nreconstruction performance by approximately 8 dB in PSNR compared to a U-Net\narchitecture trained on the original REQUITE dataset. The resulting DVHs are\nvery close to the one of the input target dose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Volumetric Modulated Arc Therapy (VMAT) revolutionizes cancer treatment by\nprecisely delivering radiation while sparing healthy tissues. Fluence maps\ngeneration, crucial in VMAT planning, traditionally involves complex and\niterative, and thus time consuming processes. These fluence maps are\nsubsequently leveraged for leaf-sequence. The deep-learning approach presented\nin this article aims to expedite this by directly predicting fluence maps from\npatient data. We developed a 3D network which we trained in a supervised way\nusing a combination of L1 and L2 losses, and RT plans generated by Eclipse and\nfrom the REQUITE dataset, taking the RT dose map as input and the fluence maps\ncomputed from the corresponding RT plans as target. Our network predicts\njointly the 180 fluence maps corresponding to the 180 control points (CP) of\nsingle arc VMAT plans. In order to help the network, we pre-process the input\ndose by computing the projections of the 3D dose map to the beam's eye view\n(BEV) of the 180 CPs, in the same coordinate system as the fluence maps. We\ngenerated over 2000 VMAT plans using Eclipse to scale up the dataset size.\nAdditionally, we evaluated various network architectures and analyzed the\nimpact of increasing the dataset size. We are measuring the performance in the\n2D fluence maps domain using image metrics (PSNR, SSIM), as well as in the 3D\ndose domain using the dose-volume histogram (DVH) on a validation dataset. The\nnetwork inference, which does not include the data loading and processing, is\nless than 20ms. Using our proposed 3D network architecture as well as\nincreasing the dataset size using Eclipse improved the fluence map\nreconstruction performance by approximately 8 dB in PSNR compared to a U-Net\narchitecture trained on the original REQUITE dataset. The resulting DVHs are\nvery close to the one of the input target dose."
                },
                "authors": [
                    {
                        "name": "Simon Arberet"
                    },
                    {
                        "name": "Florin C. Ghesu"
                    },
                    {
                        "name": "Riqiang Gao"
                    },
                    {
                        "name": "Martin Kraus"
                    },
                    {
                        "name": "Jonathan Sackett"
                    },
                    {
                        "name": "Esa Kuusela"
                    },
                    {
                        "name": "Ali Kamen"
                    }
                ],
                "author_detail": {
                    "name": "Ali Kamen"
                },
                "author": "Ali Kamen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10121v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10121v2",
                "updated": "2025-02-05T16:54:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    54,
                    42,
                    2,
                    36,
                    0
                ],
                "published": "2024-05-16T14:21:33Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    14,
                    21,
                    33,
                    3,
                    137,
                    0
                ],
                "title": "Distilling Implicit Multimodal Knowledge into Large Language Models for\n  Zero-Resource Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Implicit Multimodal Knowledge into Large Language Models for\n  Zero-Resource Dialogue Generation"
                },
                "summary": "Integrating multimodal knowledge into large language models (LLMs) represents\na significant advancement in dialogue generation capabilities. However, the\neffective incorporation of such knowledge in zero-resource scenarios remains a\nsubstantial challenge due to the scarcity of diverse, high-quality dialogue\ndatasets. To address this, we propose the Visual Implicit Knowledge\nDistillation Framework (VIKDF), an innovative approach aimed at enhancing LLMs\nfor enriched dialogue generation in zero-resource contexts by leveraging\nimplicit multimodal knowledge. VIKDF comprises two main stages: knowledge\ndistillation, using an Implicit Query Transformer to extract and encode visual\nimplicit knowledge from image-text pairs into knowledge vectors; and knowledge\nintegration, employing a novel Bidirectional Variational Information Fusion\ntechnique to seamlessly integrate these distilled vectors into LLMs. This\nenables the LLMs to generate dialogues that are not only coherent and engaging\nbut also exhibit a deep understanding of the context through implicit\nmultimodal cues, effectively overcoming the limitations of zero-resource\nscenarios. Our extensive experimentation across two dialogue datasets shows\nthat VIKDF outperforms existing state-of-the-art models in generating\nhigh-quality dialogues. The code is available at\nhttps://github.com/zhangbo-nlp/VIKDF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating multimodal knowledge into large language models (LLMs) represents\na significant advancement in dialogue generation capabilities. However, the\neffective incorporation of such knowledge in zero-resource scenarios remains a\nsubstantial challenge due to the scarcity of diverse, high-quality dialogue\ndatasets. To address this, we propose the Visual Implicit Knowledge\nDistillation Framework (VIKDF), an innovative approach aimed at enhancing LLMs\nfor enriched dialogue generation in zero-resource contexts by leveraging\nimplicit multimodal knowledge. VIKDF comprises two main stages: knowledge\ndistillation, using an Implicit Query Transformer to extract and encode visual\nimplicit knowledge from image-text pairs into knowledge vectors; and knowledge\nintegration, employing a novel Bidirectional Variational Information Fusion\ntechnique to seamlessly integrate these distilled vectors into LLMs. This\nenables the LLMs to generate dialogues that are not only coherent and engaging\nbut also exhibit a deep understanding of the context through implicit\nmultimodal cues, effectively overcoming the limitations of zero-resource\nscenarios. Our extensive experimentation across two dialogue datasets shows\nthat VIKDF outperforms existing state-of-the-art models in generating\nhigh-quality dialogues. The code is available at\nhttps://github.com/zhangbo-nlp/VIKDF."
                },
                "authors": [
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Hui Ma"
                    },
                    {
                        "name": "Jian Ding"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Hongfei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Hongfei Lin"
                },
                "author": "Hongfei Lin",
                "arxiv_doi": "10.1016/j.inffus.2025.102985",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.inffus.2025.102985",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.10121v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10121v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by Information Fusion. The code is available at\n  https://github.com/zhangbo-nlp/VIKDF",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03358v1",
                "updated": "2025-02-05T16:53:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    53,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:53:45Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    53,
                    45,
                    2,
                    36,
                    0
                ],
                "title": "Minerva: A Programmable Memory Test Benchmark for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minerva: A Programmable Memory Test Benchmark for Language Models"
                },
                "summary": "How effectively can LLM-based AI assistants utilize their memory (context) to\nperform various tasks? Traditional data benchmarks, which are often manually\ncrafted, suffer from several limitations: they are static, susceptible to\noverfitting, difficult to interpret, and lack actionable insights--failing to\npinpoint the specific capabilities a model lacks when it does not pass a test.\nIn this paper, we present a framework for automatically generating a\ncomprehensive set of tests to evaluate models' abilities to use their memory\neffectively. Our framework extends the range of capability tests beyond the\ncommonly explored (passkey, key-value, needle in the haystack) search, a\ndominant focus in the literature. Specifically, we evaluate models on atomic\ntasks such as searching, recalling, editing, matching, comparing information in\ncontext memory, and performing basic operations when inputs are structured into\ndistinct blocks, simulating real-world data. Additionally, we design composite\ntests to investigate the models' ability to maintain state while operating on\nmemory. Our benchmark enables an interpretable, detailed assessment of memory\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How effectively can LLM-based AI assistants utilize their memory (context) to\nperform various tasks? Traditional data benchmarks, which are often manually\ncrafted, suffer from several limitations: they are static, susceptible to\noverfitting, difficult to interpret, and lack actionable insights--failing to\npinpoint the specific capabilities a model lacks when it does not pass a test.\nIn this paper, we present a framework for automatically generating a\ncomprehensive set of tests to evaluate models' abilities to use their memory\neffectively. Our framework extends the range of capability tests beyond the\ncommonly explored (passkey, key-value, needle in the haystack) search, a\ndominant focus in the literature. Specifically, we evaluate models on atomic\ntasks such as searching, recalling, editing, matching, comparing information in\ncontext memory, and performing basic operations when inputs are structured into\ndistinct blocks, simulating real-world data. Additionally, we design composite\ntests to investigate the models' ability to maintain state while operating on\nmemory. Our benchmark enables an interpretable, detailed assessment of memory\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Menglin Xia"
                    },
                    {
                        "name": "Victor Ruehle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Reza Shokri"
                    }
                ],
                "author_detail": {
                    "name": "Reza Shokri"
                },
                "author": "Reza Shokri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03356v1",
                "updated": "2025-02-05T16:53:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    53,
                    34,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:53:34Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    53,
                    34,
                    2,
                    36,
                    0
                ],
                "title": "Inverse Mixed Strategy Games with Generative Trajectory Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Mixed Strategy Games with Generative Trajectory Models"
                },
                "summary": "Game-theoretic models are effective tools for modeling multi-agent\ninteractions, especially when robots need to coordinate with humans. However,\napplying these models requires inferring their specifications from observed\nbehaviors -- a challenging task known as the inverse game problem. Existing\ninverse game approaches often struggle to account for behavioral uncertainty\nand measurement noise, and leverage both offline and online data. To address\nthese limitations, we propose an inverse game method that integrates a\ngenerative trajectory model into a differentiable mixed-strategy game\nframework. By representing the mixed strategy with a conditional variational\nautoencoder (CVAE), our method can infer high-dimensional, multi-modal behavior\ndistributions from noisy measurements while adapting in real-time to new\nobservations. We extensively evaluate our method in a simulated navigation\nbenchmark, where the observations are generated by an unknown game model.\nDespite the model mismatch, our method can infer Nash-optimal actions\ncomparable to those of the ground-truth model and the oracle inverse game\nbaseline, even in the presence of uncertain agent objectives and noisy\nmeasurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Game-theoretic models are effective tools for modeling multi-agent\ninteractions, especially when robots need to coordinate with humans. However,\napplying these models requires inferring their specifications from observed\nbehaviors -- a challenging task known as the inverse game problem. Existing\ninverse game approaches often struggle to account for behavioral uncertainty\nand measurement noise, and leverage both offline and online data. To address\nthese limitations, we propose an inverse game method that integrates a\ngenerative trajectory model into a differentiable mixed-strategy game\nframework. By representing the mixed strategy with a conditional variational\nautoencoder (CVAE), our method can infer high-dimensional, multi-modal behavior\ndistributions from noisy measurements while adapting in real-time to new\nobservations. We extensively evaluate our method in a simulated navigation\nbenchmark, where the observations are generated by an unknown game model.\nDespite the model mismatch, our method can infer Nash-optimal actions\ncomparable to those of the ground-truth model and the oracle inverse game\nbaseline, even in the presence of uncertain agent objectives and noisy\nmeasurements."
                },
                "authors": [
                    {
                        "name": "Max Muchen Sun"
                    },
                    {
                        "name": "Pete Trautman"
                    },
                    {
                        "name": "Todd Murphey"
                    }
                ],
                "author_detail": {
                    "name": "Todd Murphey"
                },
                "author": "Todd Murphey",
                "arxiv_comment": "Accepted to ICRA 2025. 8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03353v1",
                "updated": "2025-02-05T16:48:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    48,
                    42,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:48:42Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    48,
                    42,
                    2,
                    36,
                    0
                ],
                "title": "Constraints on Ultra-light Axion Dark Matter through Galaxy Cluster\n  Number Counts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraints on Ultra-light Axion Dark Matter through Galaxy Cluster\n  Number Counts"
                },
                "summary": "Ultra-light axions are hypothetical scalar particles that influence the\nevolution of large-scale structures of the Universe. Depending on their mass,\nthey can potentially be part of the dark matter component of the Universe, as\ncandidates commonly referred to as fuzzy dark matter. While strong constraints\nhave been established for pure fuzzy dark matter models, the more general\nscenario where ultra-light axions constitute only a fraction of the dark matter\nhas been limited to a few observational probes. In this work, we use the galaxy\ncluster number counts obtained from the first All-Sky Survey (eRASS1) of the\nSRG/eROSITA mission together with gravitational weak lensing data from the Dark\nEnergy Survey, the Kilo-Degree Survey, and the Hyper Suprime-Cam, to constrain\nthe fraction of ultra-light axions in the mass range $10^{-32}$ eV to\n$10^{-24}$ eV. We put upper bounds on the ultra-light axion relic density in\nindependent logarithmic axion mass bins by performing a full cosmological\nparameter inference. We find an exclusion region in the intermediate\nultra-light axion mass regime with the tightest bounds reported so far in the\nmass bins around $m_\\mathrm{a}=10^{-27}$ eV with $\\Omega_\\mathrm{a} < 0.0036$\nand $m_\\mathrm{a}=10^{-26}$ eV with $\\Omega_\\mathrm{a} < 0.0084$, both at 95%\nconfidence level. When combining with CMB probes, these bounds are tightened to\n$\\Omega_\\mathrm{a} < 0.0030$ in the $m_\\mathrm{a}=10^{27}$ eV mass bin and\n$\\Omega_\\mathrm{a} < 0.0058$ in the $m_\\mathrm{a}=10^{-26}$ eV mass bin, both\nat 95% confidence level. This is the first time that constraints on ultra-light\naxions have been obtained using the growth of structure measured by galaxy\ncluster number counts. These results pave the way for large surveys, which can\nbe utilized to obtain tight constraints on the mass and relic density of\nultra-light axions with better theoretical modeling of the abundance of halos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-light axions are hypothetical scalar particles that influence the\nevolution of large-scale structures of the Universe. Depending on their mass,\nthey can potentially be part of the dark matter component of the Universe, as\ncandidates commonly referred to as fuzzy dark matter. While strong constraints\nhave been established for pure fuzzy dark matter models, the more general\nscenario where ultra-light axions constitute only a fraction of the dark matter\nhas been limited to a few observational probes. In this work, we use the galaxy\ncluster number counts obtained from the first All-Sky Survey (eRASS1) of the\nSRG/eROSITA mission together with gravitational weak lensing data from the Dark\nEnergy Survey, the Kilo-Degree Survey, and the Hyper Suprime-Cam, to constrain\nthe fraction of ultra-light axions in the mass range $10^{-32}$ eV to\n$10^{-24}$ eV. We put upper bounds on the ultra-light axion relic density in\nindependent logarithmic axion mass bins by performing a full cosmological\nparameter inference. We find an exclusion region in the intermediate\nultra-light axion mass regime with the tightest bounds reported so far in the\nmass bins around $m_\\mathrm{a}=10^{-27}$ eV with $\\Omega_\\mathrm{a} < 0.0036$\nand $m_\\mathrm{a}=10^{-26}$ eV with $\\Omega_\\mathrm{a} < 0.0084$, both at 95%\nconfidence level. When combining with CMB probes, these bounds are tightened to\n$\\Omega_\\mathrm{a} < 0.0030$ in the $m_\\mathrm{a}=10^{27}$ eV mass bin and\n$\\Omega_\\mathrm{a} < 0.0058$ in the $m_\\mathrm{a}=10^{-26}$ eV mass bin, both\nat 95% confidence level. This is the first time that constraints on ultra-light\naxions have been obtained using the growth of structure measured by galaxy\ncluster number counts. These results pave the way for large surveys, which can\nbe utilized to obtain tight constraints on the mass and relic density of\nultra-light axions with better theoretical modeling of the abundance of halos."
                },
                "authors": [
                    {
                        "name": "S. Zelmer"
                    },
                    {
                        "name": "E. Artis"
                    },
                    {
                        "name": "E. Bulbul"
                    },
                    {
                        "name": "S. Grandis"
                    },
                    {
                        "name": "V. Ghirardini"
                    },
                    {
                        "name": "A. von der Linden"
                    },
                    {
                        "name": "Y. E. Bahar"
                    },
                    {
                        "name": "F. Balzer"
                    },
                    {
                        "name": "M. Brggen"
                    },
                    {
                        "name": "I. Chiu"
                    },
                    {
                        "name": "N. Clerc"
                    },
                    {
                        "name": "J. Comparat"
                    },
                    {
                        "name": "F. Kleinebreil"
                    },
                    {
                        "name": "M. Kluge"
                    },
                    {
                        "name": "S. Krippendorf"
                    },
                    {
                        "name": "A. Liu"
                    },
                    {
                        "name": "N. Malavasi"
                    },
                    {
                        "name": "A. Merloni"
                    },
                    {
                        "name": "H. Miyatake"
                    },
                    {
                        "name": "S. Miyazaki"
                    },
                    {
                        "name": "K. Nandra"
                    },
                    {
                        "name": "N. Okabe"
                    },
                    {
                        "name": "M. E. Ramos-Ceja"
                    },
                    {
                        "name": "J. S. Sanders"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "R. Seppi"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "X. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "X. Zhang"
                },
                "author": "X. Zhang",
                "arxiv_comment": "16 pages, 11 figures, submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15745v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15745v3",
                "updated": "2025-02-05T16:43:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    43,
                    21,
                    2,
                    36,
                    0
                ],
                "published": "2024-04-24T09:04:13Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    9,
                    4,
                    13,
                    2,
                    115,
                    0
                ],
                "title": "Reconstructing the Magnetic Field in an Arbitrary Domain via Data-driven\n  Bayesian Methods and Numerical Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing the Magnetic Field in an Arbitrary Domain via Data-driven\n  Bayesian Methods and Numerical Simulations"
                },
                "summary": "Inverse problems are prevalent in numerous scientific and engineering\ndisciplines, where the objective is to determine unknown parameters within a\nphysical system using indirect measurements or observations. The inherent\nchallenge lies in deducing the most probable parameter values that align with\nthe collected data. This study introduces an algorithm for reconstructing\nparameters by addressing an inverse problem formulated through differential\nequations underpinned by uncertain boundary conditions or variant parameters.\nWe adopt a Bayesian approach for parameter inference, delineating the\nestablishment of prior, likelihood, and posterior distributions, and the\nsubsequent resolution of the maximum a posteriori problem via numerical\noptimization techniques. The proposed algorithm is applied to the task of\nmagnetic field reconstruction within a conical domain, demonstrating precise\nrecovery of the true parameter values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse problems are prevalent in numerous scientific and engineering\ndisciplines, where the objective is to determine unknown parameters within a\nphysical system using indirect measurements or observations. The inherent\nchallenge lies in deducing the most probable parameter values that align with\nthe collected data. This study introduces an algorithm for reconstructing\nparameters by addressing an inverse problem formulated through differential\nequations underpinned by uncertain boundary conditions or variant parameters.\nWe adopt a Bayesian approach for parameter inference, delineating the\nestablishment of prior, likelihood, and posterior distributions, and the\nsubsequent resolution of the maximum a posteriori problem via numerical\noptimization techniques. The proposed algorithm is applied to the task of\nmagnetic field reconstruction within a conical domain, demonstrating precise\nrecovery of the true parameter values."
                },
                "authors": [
                    {
                        "name": "Georgios E. Pavlou"
                    },
                    {
                        "name": "Vasiliki Pavlidou"
                    },
                    {
                        "name": "Vagelis Harmandaris"
                    }
                ],
                "author_detail": {
                    "name": "Vagelis Harmandaris"
                },
                "author": "Vagelis Harmandaris",
                "arxiv_doi": "10.3390/computation13020037",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/computation13020037",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.15745v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15745v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "28 pages, 11 figures",
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02370v2",
                "updated": "2025-02-05T16:40:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    40,
                    51,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-04T20:14:16Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    14,
                    16,
                    5,
                    4,
                    0
                ],
                "title": "Prepending or Cross-Attention for Speech-to-Text? An Empirical\n  Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prepending or Cross-Attention for Speech-to-Text? An Empirical\n  Comparison"
                },
                "summary": "Following the remarkable success of Large Language Models (LLMs) in NLP\ntasks, there is increasing interest in extending their capabilities to speech\n-- the most common form of communication. The most widespread approach to\nintegrating speech into LLMs is dense feature prepending (DFP), which prepends\nthe projected speech representations to the textual representations, allowing\nend-to-end training with a speech encoder. This raises questions about the need\nfor a sophisticated speech encoder for DFP and how its performance compares\nwith a standard encoder-decoder (i.e., cross-attention) architecture. We\ncompare DFP and cross-attention under a variety of configurations, such as CTC\ncompression, sequence-level knowledge distillation, on monolingual, bilingual,\nand multilingual models. To perform a controlled architectural comparison, we\ntrain all models from scratch rather than using large pretrained models and use\ncomparable data and parameter settings, testing speech-to-text recognition\n(ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. Despite the\nwide adoption of DFP, our results do not indicate a clear advantage of DFP over\ncross-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the remarkable success of Large Language Models (LLMs) in NLP\ntasks, there is increasing interest in extending their capabilities to speech\n-- the most common form of communication. The most widespread approach to\nintegrating speech into LLMs is dense feature prepending (DFP), which prepends\nthe projected speech representations to the textual representations, allowing\nend-to-end training with a speech encoder. This raises questions about the need\nfor a sophisticated speech encoder for DFP and how its performance compares\nwith a standard encoder-decoder (i.e., cross-attention) architecture. We\ncompare DFP and cross-attention under a variety of configurations, such as CTC\ncompression, sequence-level knowledge distillation, on monolingual, bilingual,\nand multilingual models. To perform a controlled architectural comparison, we\ntrain all models from scratch rather than using large pretrained models and use\ncomparable data and parameter settings, testing speech-to-text recognition\n(ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. Despite the\nwide adoption of DFP, our results do not indicate a clear advantage of DFP over\ncross-attention."
                },
                "authors": [
                    {
                        "name": "Tsz Kin Lam"
                    },
                    {
                        "name": "Marco Gaido"
                    },
                    {
                        "name": "Sara Papi"
                    },
                    {
                        "name": "Luisa Bentivogli"
                    },
                    {
                        "name": "Barry Haddow"
                    }
                ],
                "author_detail": {
                    "name": "Barry Haddow"
                },
                "author": "Barry Haddow",
                "arxiv_comment": "Accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03346v1",
                "updated": "2025-02-05T16:39:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    39,
                    26,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:39:26Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    39,
                    26,
                    2,
                    36,
                    0
                ],
                "title": "Implicit Communication in Human-Robot Collaborative Transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Communication in Human-Robot Collaborative Transport"
                },
                "summary": "We focus on human-robot collaborative transport, in which a robot and a user\ncollaboratively move an object to a goal pose. In the absence of explicit\ncommunication, this problem is challenging because it demands tight implicit\ncoordination between two heterogeneous agents, who have very different sensing,\nactuation, and reasoning capabilities. Our key insight is that the two agents\ncan coordinate fluently by encoding subtle, communicative signals into actions\nthat affect the state of the transported object. To this end, we design an\ninference mechanism that probabilistically maps observations of joint actions\nexecuted by the two agents to a set of joint strategies of workspace traversal.\nBased on this mechanism, we define a cost representing the human's uncertainty\nover the unfolding traversal strategy and introduce it into a model predictive\ncontroller that balances between uncertainty minimization and efficiency\nmaximization. We deploy our framework on a mobile manipulator (Hello Robot\nStretch) and evaluate it in a within-subjects lab study (N=24). We show that\nour framework enables greater team performance and empowers the robot to be\nperceived as a significantly more fluent and competent partner compared to\nbaselines lacking a communicative mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We focus on human-robot collaborative transport, in which a robot and a user\ncollaboratively move an object to a goal pose. In the absence of explicit\ncommunication, this problem is challenging because it demands tight implicit\ncoordination between two heterogeneous agents, who have very different sensing,\nactuation, and reasoning capabilities. Our key insight is that the two agents\ncan coordinate fluently by encoding subtle, communicative signals into actions\nthat affect the state of the transported object. To this end, we design an\ninference mechanism that probabilistically maps observations of joint actions\nexecuted by the two agents to a set of joint strategies of workspace traversal.\nBased on this mechanism, we define a cost representing the human's uncertainty\nover the unfolding traversal strategy and introduce it into a model predictive\ncontroller that balances between uncertainty minimization and efficiency\nmaximization. We deploy our framework on a mobile manipulator (Hello Robot\nStretch) and evaluate it in a within-subjects lab study (N=24). We show that\nour framework enables greater team performance and empowers the robot to be\nperceived as a significantly more fluent and competent partner compared to\nbaselines lacking a communicative mechanism."
                },
                "authors": [
                    {
                        "name": "Elvin Yang"
                    },
                    {
                        "name": "Christoforos Mavrogiannis"
                    }
                ],
                "author_detail": {
                    "name": "Christoforos Mavrogiannis"
                },
                "author": "Christoforos Mavrogiannis",
                "arxiv_comment": "Preprint. Accepted to HRI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03341v1",
                "updated": "2025-02-05T16:33:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    33,
                    59,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:33:59Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    33,
                    59,
                    2,
                    36,
                    0
                ],
                "title": "Adaptive Variational Inference in Probabilistic Graphical Models: Beyond\n  Bethe, Tree-Reweighted, and Convex Free Energies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Variational Inference in Probabilistic Graphical Models: Beyond\n  Bethe, Tree-Reweighted, and Convex Free Energies"
                },
                "summary": "Variational inference in probabilistic graphical models aims to approximate\nfundamental quantities such as marginal distributions and the partition\nfunction. Popular approaches are the Bethe approximation, tree-reweighted, and\nother types of convex free energies. These approximations are efficient but can\nfail if the model is complex and highly interactive. In this work, we analyze\ntwo classes of approximations that include the above methods as special cases:\nfirst, if the model parameters are changed; and second, if the entropy\napproximation is changed. We discuss benefits and drawbacks of either approach,\nand deduce from this analysis how a free energy approximation should ideally be\nconstructed. Based on our observations, we propose approximations that\nautomatically adapt to a given model and demonstrate their effectiveness for a\nrange of difficult problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational inference in probabilistic graphical models aims to approximate\nfundamental quantities such as marginal distributions and the partition\nfunction. Popular approaches are the Bethe approximation, tree-reweighted, and\nother types of convex free energies. These approximations are efficient but can\nfail if the model is complex and highly interactive. In this work, we analyze\ntwo classes of approximations that include the above methods as special cases:\nfirst, if the model parameters are changed; and second, if the entropy\napproximation is changed. We discuss benefits and drawbacks of either approach,\nand deduce from this analysis how a free energy approximation should ideally be\nconstructed. Based on our observations, we propose approximations that\nautomatically adapt to a given model and demonstrate their effectiveness for a\nrange of difficult problems."
                },
                "authors": [
                    {
                        "name": "Harald Leisenberger"
                    },
                    {
                        "name": "Franz Pernkopf"
                    }
                ],
                "author_detail": {
                    "name": "Franz Pernkopf"
                },
                "author": "Franz Pernkopf",
                "arxiv_comment": "This work has been submitted to the Conference on Uncertainty in\n  Artificial Intelligence (UAI) 2025 for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03332v1",
                "updated": "2025-02-05T16:26:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    26,
                    6,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:26:06Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    26,
                    6,
                    2,
                    36,
                    0
                ],
                "title": "A Mixture-Based Framework for Guiding Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Mixture-Based Framework for Guiding Diffusion Models"
                },
                "summary": "Denoising diffusion models have driven significant progress in the field of\nBayesian inverse problems. Recent approaches use pre-trained diffusion models\nas priors to solve a wide range of such problems, only leveraging\ninference-time compute and thereby eliminating the need to retrain\ntask-specific models on the same dataset. To approximate the posterior of a\nBayesian inverse problem, a diffusion model samples from a sequence of\nintermediate posterior distributions, each with an intractable likelihood\nfunction. This work proposes a novel mixture approximation of these\nintermediate distributions. Since direct gradient-based sampling of these\nmixtures is infeasible due to intractable terms, we propose a practical method\nbased on Gibbs sampling. We validate our approach through extensive experiments\non image inverse problems, utilizing both pixel- and latent-space diffusion\npriors, as well as on source separation with an audio diffusion model. The code\nis available at https://www.github.com/badr-moufad/mgdm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising diffusion models have driven significant progress in the field of\nBayesian inverse problems. Recent approaches use pre-trained diffusion models\nas priors to solve a wide range of such problems, only leveraging\ninference-time compute and thereby eliminating the need to retrain\ntask-specific models on the same dataset. To approximate the posterior of a\nBayesian inverse problem, a diffusion model samples from a sequence of\nintermediate posterior distributions, each with an intractable likelihood\nfunction. This work proposes a novel mixture approximation of these\nintermediate distributions. Since direct gradient-based sampling of these\nmixtures is infeasible due to intractable terms, we propose a practical method\nbased on Gibbs sampling. We validate our approach through extensive experiments\non image inverse problems, utilizing both pixel- and latent-space diffusion\npriors, as well as on source separation with an audio diffusion model. The code\nis available at https://www.github.com/badr-moufad/mgdm"
                },
                "authors": [
                    {
                        "name": "Yazid Janati"
                    },
                    {
                        "name": "Badr Moufad"
                    },
                    {
                        "name": "Mehdi Abou El Qassime"
                    },
                    {
                        "name": "Alain Durmus"
                    },
                    {
                        "name": "Eric Moulines"
                    },
                    {
                        "name": "Jimmy Olsson"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Olsson"
                },
                "author": "Jimmy Olsson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03325v1",
                "updated": "2025-02-05T16:22:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    22,
                    33,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:22:33Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    22,
                    33,
                    2,
                    36,
                    0
                ],
                "title": "ECM: A Unified Electronic Circuit Model for Explaining the Emergence of\n  In-Context Learning and Chain-of-Thought in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECM: A Unified Electronic Circuit Model for Explaining the Emergence of\n  In-Context Learning and Chain-of-Thought in Large Language Model"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to significant\nsuccesses across various applications, where the most noticeable is to a series\nof emerging capabilities, particularly in the areas of In-Context Learning\n(ICL) and Chain-of-Thought (CoT). To better understand and control model\nperformance, many studies have begun investigating the underlying causes of\nthese phenomena and their impact on task outcomes. However, existing\nexplanatory frameworks predominantly focus on isolating and explaining ICL and\nCoT independently, leading to an incomplete understanding of their combined\ninfluence on model performance. To address this gap, we propose the Electronic\nCircuit Model (ECM), which provides a foundation for developing scalable,\nlearnable policies and improving the management of AI-generated content.\nSpecifically, ECM conceptualizes model behavior as an electronic circuit: ICL\nis represented as semantic magnetic field to providing an additional voltage\nfollowing Faraday's Law, while CoT is modeled as series resistors to constrain\nthe model output performance following Ohm's Law. Experimental results\ndemonstrate that the ECM effectively predicts and explains LLM performance\nacross a variety of prompting strategies. Furthermore, we apply ECM to advanced\nreasoning strategy optimization on a series of tasks, such as the International\nOlympiad in Informatics (IOI) and the International Mathematical Olympiad\n(IMO), achieving competitive performance that surpasses nearly 80% of top human\ncompetitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to significant\nsuccesses across various applications, where the most noticeable is to a series\nof emerging capabilities, particularly in the areas of In-Context Learning\n(ICL) and Chain-of-Thought (CoT). To better understand and control model\nperformance, many studies have begun investigating the underlying causes of\nthese phenomena and their impact on task outcomes. However, existing\nexplanatory frameworks predominantly focus on isolating and explaining ICL and\nCoT independently, leading to an incomplete understanding of their combined\ninfluence on model performance. To address this gap, we propose the Electronic\nCircuit Model (ECM), which provides a foundation for developing scalable,\nlearnable policies and improving the management of AI-generated content.\nSpecifically, ECM conceptualizes model behavior as an electronic circuit: ICL\nis represented as semantic magnetic field to providing an additional voltage\nfollowing Faraday's Law, while CoT is modeled as series resistors to constrain\nthe model output performance following Ohm's Law. Experimental results\ndemonstrate that the ECM effectively predicts and explains LLM performance\nacross a variety of prompting strategies. Furthermore, we apply ECM to advanced\nreasoning strategy optimization on a series of tasks, such as the International\nOlympiad in Informatics (IOI) and the International Mathematical Olympiad\n(IMO), achieving competitive performance that surpasses nearly 80% of top human\ncompetitors."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "Manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03323v1",
                "updated": "2025-02-05T16:22:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    22,
                    9,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:22:09Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    22,
                    9,
                    2,
                    36,
                    0
                ],
                "title": "Out-of-Distribution Detection using Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Distribution Detection using Synthetic Data Generation"
                },
                "summary": "Distinguishing in- and out-of-distribution (OOD) inputs is crucial for\nreliable deployment of classification systems. However, OOD data is typically\nunavailable or difficult to collect, posing a significant challenge for\naccurate OOD detection. In this work, we present a method that harnesses the\ngenerative capabilities of Large Language Models (LLMs) to create high-quality\nsynthetic OOD proxies, eliminating the dependency on any external OOD data\nsource. We study the efficacy of our method on classical text classification\ntasks such as toxicity detection and sentiment classification as well as\nclassification tasks arising in LLM development and deployment, such as\ntraining a reward model for RLHF and detecting misaligned generations.\nExtensive experiments on nine InD-OOD dataset pairs and various model sizes\nshow that our approach dramatically lowers false positive rates (achieving a\nperfect zero in some cases) while maintaining high accuracy on in-distribution\ntasks, outperforming baseline methods by a significant margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distinguishing in- and out-of-distribution (OOD) inputs is crucial for\nreliable deployment of classification systems. However, OOD data is typically\nunavailable or difficult to collect, posing a significant challenge for\naccurate OOD detection. In this work, we present a method that harnesses the\ngenerative capabilities of Large Language Models (LLMs) to create high-quality\nsynthetic OOD proxies, eliminating the dependency on any external OOD data\nsource. We study the efficacy of our method on classical text classification\ntasks such as toxicity detection and sentiment classification as well as\nclassification tasks arising in LLM development and deployment, such as\ntraining a reward model for RLHF and detecting misaligned generations.\nExtensive experiments on nine InD-OOD dataset pairs and various model sizes\nshow that our approach dramatically lowers false positive rates (achieving a\nperfect zero in some cases) while maintaining high accuracy on in-distribution\ntasks, outperforming baseline methods by a significant margin."
                },
                "authors": [
                    {
                        "name": "Momin Abbas"
                    },
                    {
                        "name": "Muneeza Azmat"
                    },
                    {
                        "name": "Raya Horesh"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Yurochkin"
                },
                "author": "Mikhail Yurochkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11613v3",
                "updated": "2025-02-05T16:21:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    21,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-20T17:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    19,
                    2,
                    0,
                    20,
                    0
                ],
                "title": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems"
                },
                "summary": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications."
                },
                "authors": [
                    {
                        "name": "Giorgio Robino"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Robino"
                },
                "author": "Giorgio Robino",
                "arxiv_comment": "In version 3, we added Subsection 1.2, \"Single-Agent vs. Multi-Agent\n  Architectures,\" and Figure 1 to clarify CAS prompt composition. We also\n  refined code block and appendix log formatting for improved readability, with\n  minor formatting corrections throughout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02283v2",
                "updated": "2025-02-05T16:09:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    9,
                    26,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-04T12:50:16Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    12,
                    50,
                    16,
                    1,
                    35,
                    0
                ],
                "title": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting"
                },
                "summary": "3D Gaussian Splatting has emerged as an efficient photorealistic novel view\nsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)\npoint clouds consistently compromises the scene reconstruction quality. To\naddress these limitations, this paper proposes a novel 3D reconstruction\nframework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output\nGaussian Process model is developed to achieve adaptive and uncertainty-guided\ndensification of sparse SfM point clouds. Specifically, we propose a dynamic\nsampling and filtering pipeline that adaptively expands the SfM point clouds by\nleveraging GP-based predictions to infer new candidate points from the input 2D\npixels and depth maps. The pipeline utilizes uncertainty estimates to guide the\npruning of high-variance predictions, ensuring geometric consistency and\nenabling the generation of dense point clouds. The densified point clouds\nprovide high-quality initial 3D Gaussians to enhance reconstruction\nperformance. Extensive experiments conducted on synthetic and real-world\ndatasets across various scales validate the effectiveness and practicality of\nthe proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting has emerged as an efficient photorealistic novel view\nsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)\npoint clouds consistently compromises the scene reconstruction quality. To\naddress these limitations, this paper proposes a novel 3D reconstruction\nframework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output\nGaussian Process model is developed to achieve adaptive and uncertainty-guided\ndensification of sparse SfM point clouds. Specifically, we propose a dynamic\nsampling and filtering pipeline that adaptively expands the SfM point clouds by\nleveraging GP-based predictions to infer new candidate points from the input 2D\npixels and depth maps. The pipeline utilizes uncertainty estimates to guide the\npruning of high-variance predictions, ensuring geometric consistency and\nenabling the generation of dense point clouds. The densified point clouds\nprovide high-quality initial 3D Gaussians to enhance reconstruction\nperformance. Extensive experiments conducted on synthetic and real-world\ndatasets across various scales validate the effectiveness and practicality of\nthe proposed framework."
                },
                "authors": [
                    {
                        "name": "Zhihao Guo"
                    },
                    {
                        "name": "Jingxuan Su"
                    },
                    {
                        "name": "Shenglin Wang"
                    },
                    {
                        "name": "Jinlong Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Liangxiu Han"
                    },
                    {
                        "name": "Peng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Wang"
                },
                "author": "Peng Wang",
                "arxiv_comment": "14 pages,11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03307v1",
                "updated": "2025-02-05T16:08:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    8,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:08:05Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    8,
                    5,
                    2,
                    36,
                    0
                ],
                "title": "Intent Representation Learning with Large Language Model for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent Representation Learning with Large Language Model for\n  Recommendation"
                },
                "summary": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines. The implementation is available at\nhttps://github.com/wangyu0627/IRLLRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines. The implementation is available at\nhttps://github.com/wangyu0627/IRLLRec."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Lei Sang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Yiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Zhang"
                },
                "author": "Yiwen Zhang",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03304v1",
                "updated": "2025-02-05T16:03:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    3,
                    17,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:03:17Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    3,
                    17,
                    2,
                    36,
                    0
                ],
                "title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning"
                },
                "summary": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose \\textbf{Di}vergence-driven\n\\textbf{Z}eroth-\\textbf{O}rder (\\textbf{DiZO}) optimization. DiZO conducts\ndivergence-driven layer adaptation by incorporating projections to ZO updates,\ngenerating diverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48\\% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose \\textbf{Di}vergence-driven\n\\textbf{Z}eroth-\\textbf{O}rder (\\textbf{DiZO}) optimization. DiZO conducts\ndivergence-driven layer adaptation by incorporating projections to ZO updates,\ngenerating diverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48\\% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning."
                },
                "authors": [
                    {
                        "name": "Qitao Tan"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zheng Zhan"
                    },
                    {
                        "name": "Caiwei Ding"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Geng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Geng Yuan"
                },
                "author": "Geng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03300v1",
                "updated": "2025-02-05T15:58:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    58,
                    44,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:58:44Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    58,
                    44,
                    2,
                    36,
                    0
                ],
                "title": "ScNeuGM: Scalable Neural Graph Modeling for Coloring-Based Contention\n  and Interference Management in Wi-Fi 7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScNeuGM: Scalable Neural Graph Modeling for Coloring-Based Contention\n  and Interference Management in Wi-Fi 7"
                },
                "summary": "Carrier-sense multiple access with collision avoidance in Wi-Fi often leads\nto contention and interference, thereby increasing packet losses. These\nchallenges have traditionally been modeled as a graph, with stations (STAs)\nrepresented as vertices and contention or interference as edges. Graph coloring\nassigns orthogonal transmission slots to STAs, managing contention and\ninterference, e.g., using the restricted target wake time (RTWT) mechanism\nintroduced in Wi-Fi 7 standards. However, legacy graph models lack flexibility\nin optimizing these assignments, often failing to minimize slot usage while\nmaintaining reliable transmissions. To address this issue, we propose ScNeuGM,\na neural graph modeling (NGM) framework that flexibly trains a neural network\n(NN) to construct optimal graph models whose coloring corresponds to optimal\nslot assignments. ScNeuGM is highly scalable to large Wi-Fi networks with\nmassive STA pairs: 1) it utilizes an evolution strategy (ES) to directly\noptimize the NN parameters based on one network-wise reward signal, avoiding\nexhaustive edge-wise feedback estimations in all STA pairs; 2) ScNeuGM also\nleverages a deep hashing function (DHF) to group contending or interfering STA\npairs and restricts NGM NN training and inference to pairs within these groups,\nsignificantly reducing complexity. Simulations show that the ES-trained NN in\nScNeuGM returns near-optimal graphs 4-10 times more often than algorithms\nrequiring edge-wise feedback and reduces 25\\% slots than legacy graph\nconstructions. Furthermore, the DHF in ScNeuGM reduces the training and the\ninference time of NGM by 4 and 8 times, respectively, and the online slot\nassignment time by 3 times in large networks, and up to 30\\% fewer packet\nlosses in dynamic scenarios due to the timely assignments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carrier-sense multiple access with collision avoidance in Wi-Fi often leads\nto contention and interference, thereby increasing packet losses. These\nchallenges have traditionally been modeled as a graph, with stations (STAs)\nrepresented as vertices and contention or interference as edges. Graph coloring\nassigns orthogonal transmission slots to STAs, managing contention and\ninterference, e.g., using the restricted target wake time (RTWT) mechanism\nintroduced in Wi-Fi 7 standards. However, legacy graph models lack flexibility\nin optimizing these assignments, often failing to minimize slot usage while\nmaintaining reliable transmissions. To address this issue, we propose ScNeuGM,\na neural graph modeling (NGM) framework that flexibly trains a neural network\n(NN) to construct optimal graph models whose coloring corresponds to optimal\nslot assignments. ScNeuGM is highly scalable to large Wi-Fi networks with\nmassive STA pairs: 1) it utilizes an evolution strategy (ES) to directly\noptimize the NN parameters based on one network-wise reward signal, avoiding\nexhaustive edge-wise feedback estimations in all STA pairs; 2) ScNeuGM also\nleverages a deep hashing function (DHF) to group contending or interfering STA\npairs and restricts NGM NN training and inference to pairs within these groups,\nsignificantly reducing complexity. Simulations show that the ES-trained NN in\nScNeuGM returns near-optimal graphs 4-10 times more often than algorithms\nrequiring edge-wise feedback and reduces 25\\% slots than legacy graph\nconstructions. Furthermore, the DHF in ScNeuGM reduces the training and the\ninference time of NGM by 4 and 8 times, respectively, and the online slot\nassignment time by 3 times in large networks, and up to 30\\% fewer packet\nlosses in dynamic scenarios due to the timely assignments."
                },
                "authors": [
                    {
                        "name": "Zhouyou Gu"
                    },
                    {
                        "name": "Jihong Park"
                    },
                    {
                        "name": "Jinho Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Choi"
                },
                "author": "Jinho Choi",
                "arxiv_comment": "This work has been submitted to an IEEE journal for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03298v1",
                "updated": "2025-02-05T15:56:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    56,
                    37,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:56:37Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    56,
                    37,
                    2,
                    36,
                    0
                ],
                "title": "MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge\n  Letters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge\n  Letters"
                },
                "summary": "While increasing patients' access to medical documents improves medical care,\nthis benefit is limited by varying health literacy levels and complex medical\nterminology. Large language models (LLMs) offer solutions by simplifying\nmedical information. However, evaluating LLMs for safe and patient-friendly\ntext generation is difficult due to the lack of standardized evaluation\nresources. To fill this gap, we developed MeDiSumQA. MeDiSumQA is a dataset\ncreated from MIMIC-IV discharge summaries through an automated pipeline\ncombining LLM-based question-answer generation with manual quality checks. We\nuse this dataset to evaluate various LLMs on patient-oriented\nquestion-answering. Our findings reveal that general-purpose LLMs frequently\nsurpass biomedical-adapted models, while automated metrics correlate with human\njudgment. By releasing MeDiSumQA on PhysioNet, we aim to advance the\ndevelopment of LLMs to enhance patient understanding and ultimately improve\ncare outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While increasing patients' access to medical documents improves medical care,\nthis benefit is limited by varying health literacy levels and complex medical\nterminology. Large language models (LLMs) offer solutions by simplifying\nmedical information. However, evaluating LLMs for safe and patient-friendly\ntext generation is difficult due to the lack of standardized evaluation\nresources. To fill this gap, we developed MeDiSumQA. MeDiSumQA is a dataset\ncreated from MIMIC-IV discharge summaries through an automated pipeline\ncombining LLM-based question-answer generation with manual quality checks. We\nuse this dataset to evaluate various LLMs on patient-oriented\nquestion-answering. Our findings reveal that general-purpose LLMs frequently\nsurpass biomedical-adapted models, while automated metrics correlate with human\njudgment. By releasing MeDiSumQA on PhysioNet, we aim to advance the\ndevelopment of LLMs to enhance patient understanding and ultimately improve\ncare outcomes."
                },
                "authors": [
                    {
                        "name": "Amin Dada"
                    },
                    {
                        "name": "Osman Alperen Koras"
                    },
                    {
                        "name": "Marie Bauer"
                    },
                    {
                        "name": "Amanda Butler"
                    },
                    {
                        "name": "Kaleb E. Smith"
                    },
                    {
                        "name": "Jens Kleesiek"
                    },
                    {
                        "name": "Julian Friedrich"
                    }
                ],
                "author_detail": {
                    "name": "Julian Friedrich"
                },
                "author": "Julian Friedrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03283v1",
                "updated": "2025-02-05T15:37:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    37,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:37:05Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    37,
                    5,
                    2,
                    36,
                    0
                ],
                "title": "SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex\n  Reasoning over Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex\n  Reasoning over Knowledge Graphs"
                },
                "summary": "Recent advancements have highlighted that Large Language Models (LLMs) are\nprone to hallucinations when solving complex reasoning problems, leading to\nerroneous results. To tackle this issue, researchers incorporate Knowledge\nGraphs (KGs) to improve the reasoning ability of LLMs. However, existing\nmethods face two limitations: 1) they typically assume that all answers to the\nquestions are contained in KGs, neglecting the incompleteness issue of KGs, and\n2) they treat the KG as a static repository and overlook the implicit logical\nreasoning structures inherent in KGs. In this paper, we introduce SymAgent, an\ninnovative neural-symbolic agent framework that achieves collaborative\naugmentation between KGs and LLMs. We conceptualize KGs as dynamic environments\nand transform complex reasoning tasks into a multi-step interactive process,\nenabling KGs to participate deeply in the reasoning process. SymAgent consists\nof two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages\nLLM's inductive reasoning capability to extract symbolic rules from KGs,\nguiding efficient question decomposition. The Agent-Executor autonomously\ninvokes predefined action tools to integrate information from KGs and external\ndocuments, addressing the issues of KG incompleteness. Furthermore, we design a\nself-learning framework comprising online exploration and offline iterative\npolicy updating phases, enabling the agent to automatically synthesize\nreasoning trajectories and improve performance. Experimental results\ndemonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields\nbetter or comparable performance compared to various strong baselines. Further\nanalysis reveals that our agent can identify missing triples, facilitating\nautomatic KG updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have highlighted that Large Language Models (LLMs) are\nprone to hallucinations when solving complex reasoning problems, leading to\nerroneous results. To tackle this issue, researchers incorporate Knowledge\nGraphs (KGs) to improve the reasoning ability of LLMs. However, existing\nmethods face two limitations: 1) they typically assume that all answers to the\nquestions are contained in KGs, neglecting the incompleteness issue of KGs, and\n2) they treat the KG as a static repository and overlook the implicit logical\nreasoning structures inherent in KGs. In this paper, we introduce SymAgent, an\ninnovative neural-symbolic agent framework that achieves collaborative\naugmentation between KGs and LLMs. We conceptualize KGs as dynamic environments\nand transform complex reasoning tasks into a multi-step interactive process,\nenabling KGs to participate deeply in the reasoning process. SymAgent consists\nof two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages\nLLM's inductive reasoning capability to extract symbolic rules from KGs,\nguiding efficient question decomposition. The Agent-Executor autonomously\ninvokes predefined action tools to integrate information from KGs and external\ndocuments, addressing the issues of KG incompleteness. Furthermore, we design a\nself-learning framework comprising online exploration and offline iterative\npolicy updating phases, enabling the agent to automatically synthesize\nreasoning trajectories and improve performance. Experimental results\ndemonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields\nbetter or comparable performance compared to various strong baselines. Further\nanalysis reveals that our agent can identify missing triples, facilitating\nautomatic KG updates."
                },
                "authors": [
                    {
                        "name": "Ben Liu"
                    },
                    {
                        "name": "Jihai Zhang"
                    },
                    {
                        "name": "Fangquan Lin"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Min Peng"
                    },
                    {
                        "name": "Wotao Yin"
                    }
                ],
                "author_detail": {
                    "name": "Wotao Yin"
                },
                "author": "Wotao Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03279v1",
                "updated": "2025-02-05T15:35:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    35,
                    6,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:35:06Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    35,
                    6,
                    2,
                    36,
                    0
                ],
                "title": "Posterior SBC: Simulation-Based Calibration Checking Conditional on Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Posterior SBC: Simulation-Based Calibration Checking Conditional on Data"
                },
                "summary": "Simulation-based calibration checking (SBC) refers to the validation of an\ninference algorithm and model implementation through repeated inference on data\nsimulated from a generative model. In the original and commonly used approach,\nthe generative model uses parameters drawn from the prior, and thus the\napproach is testing whether the inference works for simulated data generated\nwith parameter values plausible under that prior. This approach is natural and\ndesirable when we want to test whether the inference works for a wide range of\ndatasets we might observe. However, after observing data, we are interested in\nanswering whether the inference works conditional on that particular data. In\nthis paper, we propose posterior SBC and demonstrate how it can be used to\nvalidate the inference conditionally on observed data. We illustrate the\nutility of posterior SBC in three case studies: (1) A simple multilevel model;\n(2) a model that is governed by differential equations; and (3) a joint\nintegrative neuroscience model which is approximated via amortized Bayesian\ninference with neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based calibration checking (SBC) refers to the validation of an\ninference algorithm and model implementation through repeated inference on data\nsimulated from a generative model. In the original and commonly used approach,\nthe generative model uses parameters drawn from the prior, and thus the\napproach is testing whether the inference works for simulated data generated\nwith parameter values plausible under that prior. This approach is natural and\ndesirable when we want to test whether the inference works for a wide range of\ndatasets we might observe. However, after observing data, we are interested in\nanswering whether the inference works conditional on that particular data. In\nthis paper, we propose posterior SBC and demonstrate how it can be used to\nvalidate the inference conditionally on observed data. We illustrate the\nutility of posterior SBC in three case studies: (1) A simple multilevel model;\n(2) a model that is governed by differential equations; and (3) a joint\nintegrative neuroscience model which is approximated via amortized Bayesian\ninference with neural networks."
                },
                "authors": [
                    {
                        "name": "Teemu Silynoja"
                    },
                    {
                        "name": "Marvin Schmitt"
                    },
                    {
                        "name": "Paul Brkner"
                    },
                    {
                        "name": "Aki Vehtari"
                    }
                ],
                "author_detail": {
                    "name": "Aki Vehtari"
                },
                "author": "Aki Vehtari",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03275v1",
                "updated": "2025-02-05T15:33:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    33,
                    0,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:33:00Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    33,
                    0,
                    2,
                    36,
                    0
                ],
                "title": "Token Assorted: Mixing Latent and Text Tokens for Improved Language\n  Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Assorted: Mixing Latent and Text Tokens for Improved Language\n  Model Reasoning"
                },
                "summary": "Large Language Models (LLMs) excel at reasoning and planning when trained on\nchainof-thought (CoT) data, where the step-by-step thought process is\nexplicitly outlined by text tokens. However, this results in lengthy inputs\nwhere many words support textual coherence rather than core reasoning\ninformation, and processing these inputs consumes substantial computation\nresources. In this work, we propose a hybrid representation of the reasoning\nprocess, where we partially abstract away the initial reasoning steps using\nlatent discrete tokens generated by VQ-VAE, significantly reducing the length\nof reasoning traces. We explore the use of latent trace abstractions in two\nscenarios: 1) training the model from scratch for the Keys-Finding Maze\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\nincluding unseen latent tokens, for both logical and mathematical reasoning\nproblems. To facilitate effective learning, we introduce a simple training\nprocedure that randomly mixes latent and text tokens, which enables fast\nadaptation to new latent tokens. Our approach consistently outperforms the\nbaselines methods in various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at reasoning and planning when trained on\nchainof-thought (CoT) data, where the step-by-step thought process is\nexplicitly outlined by text tokens. However, this results in lengthy inputs\nwhere many words support textual coherence rather than core reasoning\ninformation, and processing these inputs consumes substantial computation\nresources. In this work, we propose a hybrid representation of the reasoning\nprocess, where we partially abstract away the initial reasoning steps using\nlatent discrete tokens generated by VQ-VAE, significantly reducing the length\nof reasoning traces. We explore the use of latent trace abstractions in two\nscenarios: 1) training the model from scratch for the Keys-Finding Maze\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\nincluding unseen latent tokens, for both logical and mathematical reasoning\nproblems. To facilitate effective learning, we introduce a simple training\nprocedure that randomly mixes latent and text tokens, which enables fast\nadaptation to new latent tokens. Our approach consistently outperforms the\nbaselines methods in various benchmarks."
                },
                "authors": [
                    {
                        "name": "DiJia Su"
                    },
                    {
                        "name": "Hanlin Zhu"
                    },
                    {
                        "name": "Yingchen Xu"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Qinqing Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Qinqing Zheng"
                },
                "author": "Qinqing Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08958v2",
                "updated": "2025-02-05T15:26:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    26,
                    49,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-15T17:09:07Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    17,
                    9,
                    7,
                    2,
                    15,
                    0
                ],
                "title": "Kolmogorov-Arnold Networks for Time Series Granger Causality Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kolmogorov-Arnold Networks for Time Series Granger Causality Inference"
                },
                "summary": "We propose the Granger causality inference Kolmogorov-Arnold Networks\n(KANGCI), a novel architecture that extends the recently proposed\nKolmogorov-Arnold Networks (KAN) to the domain of causal inference. By\nextracting base weights from KAN layers and incorporating the sparsity-inducing\npenalty and ridge regularization, KANGCI effectively infers the Granger\ncausality from time series. Additionally, we propose an algorithm based on\ntime-reversed Granger causality that automatically selects causal relationships\nwith better inference performance from the original or time-reversed time\nseries or integrates the results to mitigate spurious connectivities.\nComprehensive experiments conducted on Lorenz-96, Gene regulatory networks,\nfMRI BOLD signals, VAR, and real-world EEG datasets demonstrate that the\nproposed model achieves competitive performance to state-of-the-art methods in\ninferring Granger causality from nonlinear, high-dimensional, and\nlimited-sample time series.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose the Granger causality inference Kolmogorov-Arnold Networks\n(KANGCI), a novel architecture that extends the recently proposed\nKolmogorov-Arnold Networks (KAN) to the domain of causal inference. By\nextracting base weights from KAN layers and incorporating the sparsity-inducing\npenalty and ridge regularization, KANGCI effectively infers the Granger\ncausality from time series. Additionally, we propose an algorithm based on\ntime-reversed Granger causality that automatically selects causal relationships\nwith better inference performance from the original or time-reversed time\nseries or integrates the results to mitigate spurious connectivities.\nComprehensive experiments conducted on Lorenz-96, Gene regulatory networks,\nfMRI BOLD signals, VAR, and real-world EEG datasets demonstrate that the\nproposed model achieves competitive performance to state-of-the-art methods in\ninferring Granger causality from nonlinear, high-dimensional, and\nlimited-sample time series."
                },
                "authors": [
                    {
                        "name": "Meiliang Liu"
                    },
                    {
                        "name": "Yunfang Xu"
                    },
                    {
                        "name": "Zijin Li"
                    },
                    {
                        "name": "Zhengye Si"
                    },
                    {
                        "name": "Xiaoxiao Yang"
                    },
                    {
                        "name": "Xinyue Yang"
                    },
                    {
                        "name": "Zhiwen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwen Zhao"
                },
                "author": "Zhiwen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10970v2",
                "updated": "2025-02-05T15:24:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    24,
                    26,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-19T07:09:11Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    7,
                    9,
                    11,
                    6,
                    19,
                    0
                ],
                "title": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically\n  Justify Replacing Human Annotators with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically\n  Justify Replacing Human Annotators with LLMs"
                },
                "summary": "The \"LLM-as-a-judge\" paradigm employs Large Language Models (LLMs) as\nannotators and evaluators in tasks traditionally performed by humans. LLM\nannotations are widely used, not only in NLP research but also in fields like\nmedicine, psychology, and social science. Despite their role in shaping study\nresults and insights, there is no standard or rigorous procedure to determine\nwhether LLMs can replace human annotators. In this paper, we propose a novel\nstatistical procedure -- the Alternative Annotator Test (alt-test) -- that\nrequires only a modest subset of annotated examples to justify using LLM\nannotations. Additionally, we introduce a versatile and interpretable measure\nfor comparing LLM judges. To demonstrate our procedure, we curated a diverse\ncollection of ten datasets, consisting of language and vision-language tasks,\nand conducted experiments with six LLMs and four prompting techniques. Our\nresults show that LLMs can sometimes replace humans with closed-source LLMs\n(such as GPT-4o), outperforming open-source LLMs, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"LLM-as-a-judge\" paradigm employs Large Language Models (LLMs) as\nannotators and evaluators in tasks traditionally performed by humans. LLM\nannotations are widely used, not only in NLP research but also in fields like\nmedicine, psychology, and social science. Despite their role in shaping study\nresults and insights, there is no standard or rigorous procedure to determine\nwhether LLMs can replace human annotators. In this paper, we propose a novel\nstatistical procedure -- the Alternative Annotator Test (alt-test) -- that\nrequires only a modest subset of annotated examples to justify using LLM\nannotations. Additionally, we introduce a versatile and interpretable measure\nfor comparing LLM judges. To demonstrate our procedure, we curated a diverse\ncollection of ten datasets, consisting of language and vision-language tasks,\nand conducted experiments with six LLMs and four prompting techniques. Our\nresults show that LLMs can sometimes replace humans with closed-source LLMs\n(such as GPT-4o), outperforming open-source LLMs, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices."
                },
                "authors": [
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Rotem Dror"
                    }
                ],
                "author_detail": {
                    "name": "Rotem Dror"
                },
                "author": "Rotem Dror",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03261v1",
                "updated": "2025-02-05T15:17:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    17,
                    25,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:17:25Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    17,
                    25,
                    2,
                    36,
                    0
                ],
                "title": "CARROT: A Cost Aware Rate Optimal Router",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARROT: A Cost Aware Rate Optimal Router"
                },
                "summary": "With the rapid growth in the number of Large Language Models (LLMs), there\nhas been a recent interest in LLM routing, or directing queries to the cheapest\nLLM that can deliver a suitable response. Following this line of work, we\nintroduce CARROT, a Cost AwaRe Rate Optimal rouTer that can select models based\non any desired trade-off between performance and cost. Given a query, CARROT\nselects a model based on estimates of models' cost and performance. Its\nsimplicity lends CARROT computational efficiency, while our theoretical\nanalysis demonstrates minimax rate-optimality in its routing performance.\nAlongside CARROT, we also introduce the Smart Price-aware Routing (SPROUT)\ndataset to facilitate routing on a wide spectrum of queries with the latest\nstate-of-the-art LLMs. Using SPROUT and prior benchmarks such as Routerbench\nand open-LLM-leaderboard-v2 we empirically validate CARROT's performance\nagainst several alternative routers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth in the number of Large Language Models (LLMs), there\nhas been a recent interest in LLM routing, or directing queries to the cheapest\nLLM that can deliver a suitable response. Following this line of work, we\nintroduce CARROT, a Cost AwaRe Rate Optimal rouTer that can select models based\non any desired trade-off between performance and cost. Given a query, CARROT\nselects a model based on estimates of models' cost and performance. Its\nsimplicity lends CARROT computational efficiency, while our theoretical\nanalysis demonstrates minimax rate-optimality in its routing performance.\nAlongside CARROT, we also introduce the Smart Price-aware Routing (SPROUT)\ndataset to facilitate routing on a wide spectrum of queries with the latest\nstate-of-the-art LLMs. Using SPROUT and prior benchmarks such as Routerbench\nand open-LLM-leaderboard-v2 we empirically validate CARROT's performance\nagainst several alternative routers."
                },
                "authors": [
                    {
                        "name": "Seamus Somerstep"
                    },
                    {
                        "name": "Felipe Maia Polo"
                    },
                    {
                        "name": "Allysson Flavio Melo de Oliveira"
                    },
                    {
                        "name": "Prattyush Mangal"
                    },
                    {
                        "name": "Mrian Silva"
                    },
                    {
                        "name": "Onkar Bhardwaj"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    },
                    {
                        "name": "Subha Maity"
                    }
                ],
                "author_detail": {
                    "name": "Subha Maity"
                },
                "author": "Subha Maity",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03260v1",
                "updated": "2025-02-05T15:16:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    16,
                    52,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:16:52Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    16,
                    52,
                    2,
                    36,
                    0
                ],
                "title": "Should Audio Front-ends be Adaptive? Comparing Learnable and Adaptive\n  Front-ends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Should Audio Front-ends be Adaptive? Comparing Learnable and Adaptive\n  Front-ends"
                },
                "summary": "Hand-crafted features, such as Mel-filterbanks, have traditionally been the\nchoice for many audio processing applications. Recently, there has been a\ngrowing interest in learnable front-ends that extract representations directly\nfrom the raw audio waveform. \\textcolor{black}{However, both hand-crafted\nfilterbanks and current learnable front-ends lead to fixed computation graphs\nat inference time, failing to dynamically adapt to varying acoustic\nenvironments, a key feature of human auditory systems.} To this end, we explore\nthe question of whether audio front-ends should be adaptive by comparing the\nAda-FE front-end (a recently developed adaptive front-end that employs a neural\nadaptive feedback controller to dynamically adjust the Q-factors of its\nspectral decomposition filters) to established learnable front-ends.\nSpecifically, we systematically investigate learnable front-ends and Ada-FE\nacross two commonly used back-end backbones and a wide range of audio\nbenchmarks including speech, sound event, and music. The comprehensive results\nshow that our Ada-FE outperforms advanced learnable front-ends, and more\nimportantly, it exhibits impressive stability or robustness on test samples\nover various training epochs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hand-crafted features, such as Mel-filterbanks, have traditionally been the\nchoice for many audio processing applications. Recently, there has been a\ngrowing interest in learnable front-ends that extract representations directly\nfrom the raw audio waveform. \\textcolor{black}{However, both hand-crafted\nfilterbanks and current learnable front-ends lead to fixed computation graphs\nat inference time, failing to dynamically adapt to varying acoustic\nenvironments, a key feature of human auditory systems.} To this end, we explore\nthe question of whether audio front-ends should be adaptive by comparing the\nAda-FE front-end (a recently developed adaptive front-end that employs a neural\nadaptive feedback controller to dynamically adjust the Q-factors of its\nspectral decomposition filters) to established learnable front-ends.\nSpecifically, we systematically investigate learnable front-ends and Ada-FE\nacross two commonly used back-end backbones and a wide range of audio\nbenchmarks including speech, sound event, and music. The comprehensive results\nshow that our Ada-FE outperforms advanced learnable front-ends, and more\nimportantly, it exhibits impressive stability or robustness on test samples\nover various training epochs."
                },
                "authors": [
                    {
                        "name": "Qiquan Zhang"
                    },
                    {
                        "name": "Buddhi Wickramasinghe"
                    },
                    {
                        "name": "Eliathamby Ambikairajah"
                    },
                    {
                        "name": "Vidhyasaharan Sethu"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "Accepted by IEEE TASLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20959v2",
                "updated": "2025-02-05T15:16:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    16,
                    8,
                    2,
                    36,
                    0
                ],
                "published": "2024-07-30T16:36:15Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    16,
                    36,
                    15,
                    1,
                    212,
                    0
                ],
                "title": "Learning Ordinality in Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Ordinality in Semantic Segmentation"
                },
                "summary": "Semantic segmentation consists of predicting a semantic label for each image\npixel. While existing deep learning approaches achieve high accuracy, they\noften overlook the ordinal relationships between classes, which can provide\ncritical domain knowledge (e.g., the pupil lies within the iris, and lane\nmarkings are part of the road). This paper introduces novel methods for spatial\nordinal segmentation that explicitly incorporate these inter-class\ndependencies. By treating each pixel as part of a structured image space rather\nthan as an independent observation, we propose two regularization terms and a\nnew metric to enforce ordinal consistency between neighboring pixels. Two loss\nregularization terms and one metric are proposed for structural ordinal\nsegmentation, which penalizes predictions of non-ordinal adjacent classes. Five\nbiomedical datasets and multiple configurations of autonomous driving datasets\ndemonstrate the efficacy of the proposed methods. Our approach achieves\nimprovements in ordinal metrics and enhances generalization, with up to a 15.7%\nrelative increase in the Dice coefficient. Importantly, these benefits come\nwithout additional inference time costs. This work highlights the significance\nof spatial ordinal relationships in semantic segmentation and provides a\nfoundation for further exploration in structured image representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic segmentation consists of predicting a semantic label for each image\npixel. While existing deep learning approaches achieve high accuracy, they\noften overlook the ordinal relationships between classes, which can provide\ncritical domain knowledge (e.g., the pupil lies within the iris, and lane\nmarkings are part of the road). This paper introduces novel methods for spatial\nordinal segmentation that explicitly incorporate these inter-class\ndependencies. By treating each pixel as part of a structured image space rather\nthan as an independent observation, we propose two regularization terms and a\nnew metric to enforce ordinal consistency between neighboring pixels. Two loss\nregularization terms and one metric are proposed for structural ordinal\nsegmentation, which penalizes predictions of non-ordinal adjacent classes. Five\nbiomedical datasets and multiple configurations of autonomous driving datasets\ndemonstrate the efficacy of the proposed methods. Our approach achieves\nimprovements in ordinal metrics and enhances generalization, with up to a 15.7%\nrelative increase in the Dice coefficient. Importantly, these benefits come\nwithout additional inference time costs. This work highlights the significance\nof spatial ordinal relationships in semantic segmentation and provides a\nfoundation for further exploration in structured image representations."
                },
                "authors": [
                    {
                        "name": "Ricardo P. M. Cruz"
                    },
                    {
                        "name": "Rafael Cristino"
                    },
                    {
                        "name": "Jaime S. Cardoso"
                    }
                ],
                "author_detail": {
                    "name": "Jaime S. Cardoso"
                },
                "author": "Jaime S. Cardoso",
                "arxiv_doi": "10.1109/ACCESS.2025.3537601",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3537601",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.20959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages",
                "arxiv_journal_ref": "IEEE Access (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15230v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15230v3",
                "updated": "2025-02-05T15:10:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    10,
                    23,
                    2,
                    36,
                    0
                ],
                "published": "2023-12-23T11:45:22Z",
                "published_parsed": [
                    2023,
                    12,
                    23,
                    11,
                    45,
                    22,
                    5,
                    357,
                    0
                ],
                "title": "PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs"
                },
                "summary": "Neural Networks can be effectively compressed through pruning, significantly\nreducing storage and compute demands while maintaining predictive performance.\nSimple yet effective methods like magnitude pruning remove less important\nparameters and typically require a costly retraining procedure to restore\nperformance. However, with the rise of LLMs, full retraining has become\ninfeasible due to memory and compute constraints. This study challenges the\npractice of retraining all parameters by showing that updating a small subset\nof highly expressive parameters can suffice to recover or even enhance\nperformance after pruning. Surprisingly, retraining just 0.01%-0.05% of the\nparameters in GPT-architectures can match the performance of full retraining\nacross various sparsity levels, significantly reducing compute and memory\nrequirements, and enabling retraining of models with up to 30 billion\nparameters on a single GPU in minutes. To bridge the gap to full retraining in\nthe high sparsity regime, we introduce two novel LoRA variants that, unlike\nstandard LoRA, allow merging adapters back without compromising sparsity. Going\na step further, we show that these methods can be applied for memory-efficient\nlayer-wise reconstruction, significantly enhancing state-of-the-art\nretraining-free methods like Wanda (Sun et al., 2023) and SparseGPT (Frantar &\nAlistarh, 2023). Our findings present a promising alternative to avoiding\nretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Networks can be effectively compressed through pruning, significantly\nreducing storage and compute demands while maintaining predictive performance.\nSimple yet effective methods like magnitude pruning remove less important\nparameters and typically require a costly retraining procedure to restore\nperformance. However, with the rise of LLMs, full retraining has become\ninfeasible due to memory and compute constraints. This study challenges the\npractice of retraining all parameters by showing that updating a small subset\nof highly expressive parameters can suffice to recover or even enhance\nperformance after pruning. Surprisingly, retraining just 0.01%-0.05% of the\nparameters in GPT-architectures can match the performance of full retraining\nacross various sparsity levels, significantly reducing compute and memory\nrequirements, and enabling retraining of models with up to 30 billion\nparameters on a single GPU in minutes. To bridge the gap to full retraining in\nthe high sparsity regime, we introduce two novel LoRA variants that, unlike\nstandard LoRA, allow merging adapters back without compromising sparsity. Going\na step further, we show that these methods can be applied for memory-efficient\nlayer-wise reconstruction, significantly enhancing state-of-the-art\nretraining-free methods like Wanda (Sun et al., 2023) and SparseGPT (Frantar &\nAlistarh, 2023). Our findings present a promising alternative to avoiding\nretraining."
                },
                "authors": [
                    {
                        "name": "Max Zimmer"
                    },
                    {
                        "name": "Megi Andoni"
                    },
                    {
                        "name": "Christoph Spiegel"
                    },
                    {
                        "name": "Sebastian Pokutta"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pokutta"
                },
                "author": "Sebastian Pokutta",
                "arxiv_comment": "32 pages, 7 figures, 24 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15230v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15230v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03253v1",
                "updated": "2025-02-05T15:08:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    8,
                    43,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:08:43Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    8,
                    43,
                    2,
                    36,
                    0
                ],
                "title": "How do Humans and Language Models Reason About Creativity? A Comparative\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do Humans and Language Models Reason About Creativity? A Comparative\n  Analysis"
                },
                "summary": "Creativity assessment in science and engineering is increasingly based on\nboth human and AI judgment, but the cognitive processes and biases behind these\nevaluations remain poorly understood. We conducted two experiments examining\nhow including example solutions with ratings impact creativity evaluation,\nusing a finegrained annotation protocol where raters were tasked with\nexplaining their originality scores and rating for the facets of remoteness\n(whether the response is \"far\" from everyday ideas), uncommonness (whether the\nresponse is rare), and cleverness. In Study 1, we analyzed creativity ratings\nfrom 72 experts with formal science or engineering training, comparing those\nwho received example solutions with ratings (example) to those who did not (no\nexample). Computational text analysis revealed that, compared to experts with\nexamples, no-example experts used more comparative language (e.g.,\n\"better/worse\") and emphasized solution uncommonness, suggesting they may have\nrelied more on memory retrieval for comparisons. In Study 2, parallel analyses\nwith state-of-the-art LLMs revealed that models prioritized uncommonness and\nremoteness of ideas when rating originality, suggesting an evaluative process\nrooted around the semantic similarity of ideas. In the example condition, while\nLLM accuracy in predicting the true originality scores improved, the\ncorrelations of remoteness, uncommonness, and cleverness with originality also\nincreased substantially - to upwards of 0.99 - suggesting a homogenization in\nthe LLMs evaluation of the individual facets. These findings highlight\nimportant implications for how humans and AI reason about creativity and\nsuggest diverging preferences for what different populations prioritize when\nrating.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creativity assessment in science and engineering is increasingly based on\nboth human and AI judgment, but the cognitive processes and biases behind these\nevaluations remain poorly understood. We conducted two experiments examining\nhow including example solutions with ratings impact creativity evaluation,\nusing a finegrained annotation protocol where raters were tasked with\nexplaining their originality scores and rating for the facets of remoteness\n(whether the response is \"far\" from everyday ideas), uncommonness (whether the\nresponse is rare), and cleverness. In Study 1, we analyzed creativity ratings\nfrom 72 experts with formal science or engineering training, comparing those\nwho received example solutions with ratings (example) to those who did not (no\nexample). Computational text analysis revealed that, compared to experts with\nexamples, no-example experts used more comparative language (e.g.,\n\"better/worse\") and emphasized solution uncommonness, suggesting they may have\nrelied more on memory retrieval for comparisons. In Study 2, parallel analyses\nwith state-of-the-art LLMs revealed that models prioritized uncommonness and\nremoteness of ideas when rating originality, suggesting an evaluative process\nrooted around the semantic similarity of ideas. In the example condition, while\nLLM accuracy in predicting the true originality scores improved, the\ncorrelations of remoteness, uncommonness, and cleverness with originality also\nincreased substantially - to upwards of 0.99 - suggesting a homogenization in\nthe LLMs evaluation of the individual facets. These findings highlight\nimportant implications for how humans and AI reason about creativity and\nsuggest diverging preferences for what different populations prioritize when\nrating."
                },
                "authors": [
                    {
                        "name": "Antonio Laverghetta Jr."
                    },
                    {
                        "name": "Tuhin Chakrabarty"
                    },
                    {
                        "name": "Tom Hope"
                    },
                    {
                        "name": "Jimmy Pronchick"
                    },
                    {
                        "name": "Krupa Bhawsar"
                    },
                    {
                        "name": "Roger E. Beaty"
                    }
                ],
                "author_detail": {
                    "name": "Roger E. Beaty"
                },
                "author": "Roger E. Beaty",
                "arxiv_comment": "CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03233v1",
                "updated": "2025-02-05T14:49:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    14,
                    49,
                    12,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T14:49:12Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    14,
                    49,
                    12,
                    2,
                    36,
                    0
                ],
                "title": "Exploring the Security Threats of Knowledge Base Poisoning in\n  Retrieval-Augmented Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Security Threats of Knowledge Base Poisoning in\n  Retrieval-Augmented Code Generation"
                },
                "summary": "The integration of Large Language Models (LLMs) into software development has\nrevolutionized the field, particularly through the use of Retrieval-Augmented\nCode Generation (RACG) systems that enhance code generation with information\nfrom external knowledge bases. However, the security implications of RACG\nsystems, particularly the risks posed by vulnerable code examples in the\nknowledge base, remain largely unexplored. This risk is particularly concerning\ngiven that public code repositories, which often serve as the sources for\nknowledge base collection in RACG systems, are usually accessible to anyone in\nthe community. Malicious attackers can exploit this accessibility to inject\nvulnerable code into the knowledge base, making it toxic. Once these poisoned\nsamples are retrieved and incorporated into the generated code, they can\npropagate security vulnerabilities into the final product. This paper presents\nthe first comprehensive study on the security risks associated with RACG\nsystems, focusing on how vulnerable code in the knowledge base compromises the\nsecurity of generated code. We investigate the LLM-generated code security\nacross different settings through extensive experiments using four major LLMs,\ntwo retrievers, and two poisoning scenarios. Our findings highlight the\nsignificant threat of knowledge base poisoning, where even a single poisoned\ncode example can compromise up to 48% of generated code. Our findings provide\ncrucial insights into vulnerability introduction in RACG systems and offer\npractical mitigation recommendations, thereby helping improve the security of\nLLM-generated code in future works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into software development has\nrevolutionized the field, particularly through the use of Retrieval-Augmented\nCode Generation (RACG) systems that enhance code generation with information\nfrom external knowledge bases. However, the security implications of RACG\nsystems, particularly the risks posed by vulnerable code examples in the\nknowledge base, remain largely unexplored. This risk is particularly concerning\ngiven that public code repositories, which often serve as the sources for\nknowledge base collection in RACG systems, are usually accessible to anyone in\nthe community. Malicious attackers can exploit this accessibility to inject\nvulnerable code into the knowledge base, making it toxic. Once these poisoned\nsamples are retrieved and incorporated into the generated code, they can\npropagate security vulnerabilities into the final product. This paper presents\nthe first comprehensive study on the security risks associated with RACG\nsystems, focusing on how vulnerable code in the knowledge base compromises the\nsecurity of generated code. We investigate the LLM-generated code security\nacross different settings through extensive experiments using four major LLMs,\ntwo retrievers, and two poisoning scenarios. Our findings highlight the\nsignificant threat of knowledge base poisoning, where even a single poisoned\ncode example can compromise up to 48% of generated code. Our findings provide\ncrucial insights into vulnerability introduction in RACG systems and offer\npractical mitigation recommendations, thereby helping improve the security of\nLLM-generated code in future works."
                },
                "authors": [
                    {
                        "name": "Bo Lin"
                    },
                    {
                        "name": "Shangwen Wang"
                    },
                    {
                        "name": "Liqian Chen"
                    },
                    {
                        "name": "Xiaoguang Mao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoguang Mao"
                },
                "author": "Xiaoguang Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09896v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09896v3",
                "updated": "2025-02-05T14:33:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    14,
                    33,
                    13,
                    2,
                    36,
                    0
                ],
                "published": "2024-07-13T14:24:22Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    14,
                    24,
                    22,
                    5,
                    195,
                    0
                ],
                "title": "PSC: Posterior Sampling-Based Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSC: Posterior Sampling-Based Compression"
                },
                "summary": "Diffusion models have transformed the landscape of image generation and now\nshow remarkable potential for image compression. Most of the recent\ndiffusion-based compression methods require training and are tailored for a\nspecific bit-rate. In this work, we propose Posterior Sampling-based\nCompression (PSC) - a zero-shot compression method that leverages a pre-trained\ndiffusion model as its sole neural network component, thus enabling the use of\ndiverse, publicly available models without additional training. Our approach is\ninspired by transform coding methods, which encode the image in some pre-chosen\ntransform domain. However, PSC constructs a transform that is adaptive to the\nimage. This is done by employing a zero-shot diffusion-based posterior sampler\nso as to progressively construct the rows of the transform matrix. Each new\nchunk of rows is chosen to reduce the uncertainty about the image given the\nquantized measurements collected thus far. Importantly, the same adaptive\nscheme can be replicated at the decoder, thus avoiding the need to encode the\ntransform itself. We demonstrate that even with basic quantization and entropy\ncoding, PSC's performance is comparable to established training-based methods\nin terms of rate, distortion, and perceptual quality. This is while providing\ngreater flexibility, allowing to choose at inference time any desired rate or\ndistortion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have transformed the landscape of image generation and now\nshow remarkable potential for image compression. Most of the recent\ndiffusion-based compression methods require training and are tailored for a\nspecific bit-rate. In this work, we propose Posterior Sampling-based\nCompression (PSC) - a zero-shot compression method that leverages a pre-trained\ndiffusion model as its sole neural network component, thus enabling the use of\ndiverse, publicly available models without additional training. Our approach is\ninspired by transform coding methods, which encode the image in some pre-chosen\ntransform domain. However, PSC constructs a transform that is adaptive to the\nimage. This is done by employing a zero-shot diffusion-based posterior sampler\nso as to progressively construct the rows of the transform matrix. Each new\nchunk of rows is chosen to reduce the uncertainty about the image given the\nquantized measurements collected thus far. Importantly, the same adaptive\nscheme can be replicated at the decoder, thus avoiding the need to encode the\ntransform itself. We demonstrate that even with basic quantization and entropy\ncoding, PSC's performance is comparable to established training-based methods\nin terms of rate, distortion, and perceptual quality. This is while providing\ngreater flexibility, allowing to choose at inference time any desired rate or\ndistortion."
                },
                "authors": [
                    {
                        "name": "Noam Elata"
                    },
                    {
                        "name": "Tomer Michaeli"
                    },
                    {
                        "name": "Michael Elad"
                    }
                ],
                "author_detail": {
                    "name": "Michael Elad"
                },
                "author": "Michael Elad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09896v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09896v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03199v1",
                "updated": "2025-02-05T14:19:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    14,
                    19,
                    52,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T14:19:52Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    14,
                    19,
                    52,
                    2,
                    36,
                    0
                ],
                "title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large\n  Language Models"
                },
                "summary": "Despite their impressive capacities, Large language models (LLMs) often\nstruggle with the hallucination issue of generating inaccurate or fabricated\ncontent even when they possess correct knowledge. In this paper, we extend the\nexploration of the correlation between hidden-state prediction changes and\noutput factuality into a deeper, token-wise level. Based on the insights , we\npropose cross-layer Entropy eNhanced Decoding (END), a decoding method that\nmitigates hallucinations without requiring extra training. END leverages inner\nprobability changes across layers to individually quantify the factual\nknowledge required for each candidate token, and adjusts the final predicting\ndistribution to prioritize tokens with higher factuality. Experiments on both\nhallucination and QA benchmarks demonstrate that END significantly enhances the\ntruthfulness and informativeness of generated content while maintaining robust\nQA accuracy. Moreover, our work provides a deeper perspective on understanding\nthe correlations between inherent knowledge and output factuality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capacities, Large language models (LLMs) often\nstruggle with the hallucination issue of generating inaccurate or fabricated\ncontent even when they possess correct knowledge. In this paper, we extend the\nexploration of the correlation between hidden-state prediction changes and\noutput factuality into a deeper, token-wise level. Based on the insights , we\npropose cross-layer Entropy eNhanced Decoding (END), a decoding method that\nmitigates hallucinations without requiring extra training. END leverages inner\nprobability changes across layers to individually quantify the factual\nknowledge required for each candidate token, and adjusts the final predicting\ndistribution to prioritize tokens with higher factuality. Experiments on both\nhallucination and QA benchmarks demonstrate that END significantly enhances the\ntruthfulness and informativeness of generated content while maintaining robust\nQA accuracy. Moreover, our work provides a deeper perspective on understanding\nthe correlations between inherent knowledge and output factuality."
                },
                "authors": [
                    {
                        "name": "Jialiang Wu"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Sijia Liu"
                    },
                    {
                        "name": "Yi Tang"
                    },
                    {
                        "name": "Sen Song"
                    },
                    {
                        "name": "Xiaoyi Wang"
                    },
                    {
                        "name": "Longjun Cai"
                    }
                ],
                "author_detail": {
                    "name": "Longjun Cai"
                },
                "author": "Longjun Cai",
                "arxiv_comment": "NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.17618v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.17618v2",
                "updated": "2025-02-05T14:09:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    14,
                    9,
                    33,
                    2,
                    36,
                    0
                ],
                "published": "2023-10-26T17:39:15Z",
                "published_parsed": [
                    2023,
                    10,
                    26,
                    17,
                    39,
                    15,
                    3,
                    299,
                    0
                ],
                "title": "Applications of emulation and Bayesian methods in heavy-ion physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications of emulation and Bayesian methods in heavy-ion physics"
                },
                "summary": "Heavy-ion collisions provide a window into the properties of many-body\nsystems of deconfined quarks and gluons. Understanding the collective\nproperties of quarks and gluons is possible by comparing models of heavy-ion\ncollisions to measurements of the distribution of particles produced at the end\nof the collisions. These model-to-data comparisons are extremely challenging,\nhowever, because of the complexity of the models, the large amount of\nexperimental data, and their uncertainties. Bayesian inference provides a\nrigorous statistical framework to constrain the properties of nuclear matter by\nsystematically comparing models and measurements.\n  This review covers model emulation and Bayesian methods as applied to\nmodel-to-data comparisons in heavy-ion collisions. Replacing the model outputs\n(observables) with Gaussian process emulators is key to the Bayesian approach\ncurrently used in the field, and both current uses of emulators and related\nrecent developments are reviewed. The general principles of Bayesian inference\nare then discussed along with other Bayesian methods, followed by a systematic\ncomparison of seven recent Bayesian analyses that studied quark-gluon plasma\nproperties, such as the shear and bulk viscosities. The latter comparison is\nused to illustrate sources of differences in analyses, and what it can teach us\nfor future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heavy-ion collisions provide a window into the properties of many-body\nsystems of deconfined quarks and gluons. Understanding the collective\nproperties of quarks and gluons is possible by comparing models of heavy-ion\ncollisions to measurements of the distribution of particles produced at the end\nof the collisions. These model-to-data comparisons are extremely challenging,\nhowever, because of the complexity of the models, the large amount of\nexperimental data, and their uncertainties. Bayesian inference provides a\nrigorous statistical framework to constrain the properties of nuclear matter by\nsystematically comparing models and measurements.\n  This review covers model emulation and Bayesian methods as applied to\nmodel-to-data comparisons in heavy-ion collisions. Replacing the model outputs\n(observables) with Gaussian process emulators is key to the Bayesian approach\ncurrently used in the field, and both current uses of emulators and related\nrecent developments are reviewed. The general principles of Bayesian inference\nare then discussed along with other Bayesian methods, followed by a systematic\ncomparison of seven recent Bayesian analyses that studied quark-gluon plasma\nproperties, such as the shear and bulk viscosities. The latter comparison is\nused to illustrate sources of differences in analyses, and what it can teach us\nfor future studies."
                },
                "authors": [
                    {
                        "name": "Jean-Franois Paquet"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Franois Paquet"
                },
                "author": "Jean-Franois Paquet",
                "arxiv_doi": "10.1088/1361-6471/ad6a2b",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1361-6471/ad6a2b",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.17618v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.17618v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "54 pages; fixes and added discussions; matches published version",
                "arxiv_journal_ref": "J. Phys. G: Nucl. Part. Phys. 51 103001 (2024)",
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17424v2",
                "updated": "2025-02-05T14:06:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    14,
                    6,
                    21,
                    2,
                    36,
                    0
                ],
                "published": "2024-05-27T17:59:32Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    17,
                    59,
                    32,
                    0,
                    148,
                    0
                ],
                "title": "LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence"
                },
                "summary": "Recent embodied agents are primarily built based on reinforcement learning\n(RL) or large language models (LLMs). Among them, RL agents are efficient for\ndeployment but only perform very few tasks. By contrast, giant LLM agents\n(often more than 1000B parameters) present strong generalization while\ndemanding enormous computing resources. In this work, we combine their\nadvantages while avoiding the drawbacks by conducting the proposed referee RL\non our developed large auto-regressive model (LARM). Specifically, LARM is\nbuilt upon a lightweight LLM (fewer than 5B parameters) and directly outputs\nthe next action to execute rather than text. We mathematically reveal that\nclassic RL feedbacks vanish in long-horizon embodied exploration and introduce\na giant LLM based referee to handle this reward vanishment during training\nLARM. In this way, LARM learns to complete diverse open-world tasks without\nhuman intervention. Especially, LARM successfully harvests enchanted diamond\nequipment in Minecraft, which demands significantly longer decision-making\nchains than the highest achievements of prior best methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent embodied agents are primarily built based on reinforcement learning\n(RL) or large language models (LLMs). Among them, RL agents are efficient for\ndeployment but only perform very few tasks. By contrast, giant LLM agents\n(often more than 1000B parameters) present strong generalization while\ndemanding enormous computing resources. In this work, we combine their\nadvantages while avoiding the drawbacks by conducting the proposed referee RL\non our developed large auto-regressive model (LARM). Specifically, LARM is\nbuilt upon a lightweight LLM (fewer than 5B parameters) and directly outputs\nthe next action to execute rather than text. We mathematically reveal that\nclassic RL feedbacks vanish in long-horizon embodied exploration and introduce\na giant LLM based referee to handle this reward vanishment during training\nLARM. In this way, LARM learns to complete diverse open-world tasks without\nhuman intervention. Especially, LARM successfully harvests enchanted diamond\nequipment in Minecraft, which demands significantly longer decision-making\nchains than the highest achievements of prior best methods."
                },
                "authors": [
                    {
                        "name": "Zhuoling Li"
                    },
                    {
                        "name": "Xiaogang Xu"
                    },
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "SerNam Lim"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hengshuang Zhao"
                },
                "author": "Hengshuang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09008v2",
                "updated": "2025-02-05T13:47:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    47,
                    11,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-11T17:25:52Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    25,
                    52,
                    4,
                    285,
                    0
                ],
                "title": "SuperCorrect: Supervising and Correcting Language Models with\n  Error-Driven Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperCorrect: Supervising and Correcting Language Models with\n  Error-Driven Insights"
                },
                "summary": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown\nsignificant improvements in various reasoning tasks. However, smaller models\nsuch as Llama-3-8B and DeepSeekMath-Base still struggle with complex\nmathematical reasoning because they fail to effectively identify and correct\nreasoning errors. Recent reflection-based methods aim to address these issues\nby enabling self-reflection and self-correction, but they still face challenges\nin independently detecting errors in their reasoning steps. To overcome these\nlimitations, we propose SuperCorrect, a novel two-stage framework that uses a\nlarge teacher model to supervise and correct both the reasoning and reflection\nprocesses of a smaller student model. In the first stage, we extract\nhierarchical high-level and detailed thought templates from the teacher model\nto guide the student model in eliciting more fine-grained reasoning thoughts.\nIn the second stage, we introduce cross-model collaborative direct preference\noptimization (DPO) to enhance the self-correction abilities of the student\nmodel by following the teacher's correction traces during training. This\ncross-model DPO approach teaches the student model to effectively locate and\nresolve erroneous thoughts with error-driven insights from the teacher model,\nbreaking the bottleneck of its thoughts and acquiring new skills and knowledge\nto tackle challenging problems. Extensive experiments consistently demonstrate\nour superiority over previous methods. Notably, our SuperCorrect-7B model\nsignificantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and\nQwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA\nperformance among all 7B models. Code:\nhttps://github.com/YangLing0818/SuperCorrect-llm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown\nsignificant improvements in various reasoning tasks. However, smaller models\nsuch as Llama-3-8B and DeepSeekMath-Base still struggle with complex\nmathematical reasoning because they fail to effectively identify and correct\nreasoning errors. Recent reflection-based methods aim to address these issues\nby enabling self-reflection and self-correction, but they still face challenges\nin independently detecting errors in their reasoning steps. To overcome these\nlimitations, we propose SuperCorrect, a novel two-stage framework that uses a\nlarge teacher model to supervise and correct both the reasoning and reflection\nprocesses of a smaller student model. In the first stage, we extract\nhierarchical high-level and detailed thought templates from the teacher model\nto guide the student model in eliciting more fine-grained reasoning thoughts.\nIn the second stage, we introduce cross-model collaborative direct preference\noptimization (DPO) to enhance the self-correction abilities of the student\nmodel by following the teacher's correction traces during training. This\ncross-model DPO approach teaches the student model to effectively locate and\nresolve erroneous thoughts with error-driven insights from the teacher model,\nbreaking the bottleneck of its thoughts and acquiring new skills and knowledge\nto tackle challenging problems. Extensive experiments consistently demonstrate\nour superiority over previous methods. Notably, our SuperCorrect-7B model\nsignificantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and\nQwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA\nperformance among all 7B models. Code:\nhttps://github.com/YangLing0818/SuperCorrect-llm"
                },
                "authors": [
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Zhaochen Yu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Minkai Xu"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "arxiv_comment": "ICLR 2025. Project: https://github.com/YangLing0818/SuperCorrect-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03166v1",
                "updated": "2025-02-05T13:39:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    39,
                    25,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T13:39:25Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    39,
                    25,
                    2,
                    36,
                    0
                ],
                "title": "Thermal spin wave noise as a probe for the Dzyaloshinkii-Moriya\n  interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermal spin wave noise as a probe for the Dzyaloshinkii-Moriya\n  interaction"
                },
                "summary": "Interfacial Dzyaloshinkii-Moriya interaction (DMI) is a key ingredient in the\nstabilization of chiral magnetic states in thin films. Its sign and strength\noften determine crucial properties of magnetic objects, like their topology or\nhow they can be manipulated with currents. A few experimental techniques are\ncurrently available to measure DMI quantitatively, based on the study of domain\nwalls, spin waves, or spin-orbit torques. In this work, we propose a\nqualitative variant of spin wave methods. We rely on magnetic noise from\nconfined thermal spin waves in domain walls and skyrmions in perpendicularly\nmagnetized thin films, which we probe with scanning NV center relaxometry. We\nshow both numerically and experimentally that the sign of the DMI can be\ninferred from the amplitude of the detected noise, which is affected by the\nnon-reciprocity in the spin wave dispersion. Furthermore, we also demonstrate\nthat the noise distribution around the contour of magnetic skyrmions reveals\ntheir N\\'eel/Bloch nature, giving therefore also insight into the strength of\nDMI involved in their stabilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interfacial Dzyaloshinkii-Moriya interaction (DMI) is a key ingredient in the\nstabilization of chiral magnetic states in thin films. Its sign and strength\noften determine crucial properties of magnetic objects, like their topology or\nhow they can be manipulated with currents. A few experimental techniques are\ncurrently available to measure DMI quantitatively, based on the study of domain\nwalls, spin waves, or spin-orbit torques. In this work, we propose a\nqualitative variant of spin wave methods. We rely on magnetic noise from\nconfined thermal spin waves in domain walls and skyrmions in perpendicularly\nmagnetized thin films, which we probe with scanning NV center relaxometry. We\nshow both numerically and experimentally that the sign of the DMI can be\ninferred from the amplitude of the detected noise, which is affected by the\nnon-reciprocity in the spin wave dispersion. Furthermore, we also demonstrate\nthat the noise distribution around the contour of magnetic skyrmions reveals\ntheir N\\'eel/Bloch nature, giving therefore also insight into the strength of\nDMI involved in their stabilization."
                },
                "authors": [
                    {
                        "name": "Aurore Finco"
                    },
                    {
                        "name": "Pawan Kumar"
                    },
                    {
                        "name": "Van Tuong Pham"
                    },
                    {
                        "name": "Joseba Urrestarazu-Larraaga"
                    },
                    {
                        "name": "Rodrigo Guedas Garcia"
                    },
                    {
                        "name": "Maxime Rollo"
                    },
                    {
                        "name": "Olivier Boulle"
                    },
                    {
                        "name": "Joo-Von Kim"
                    },
                    {
                        "name": "Vincent Jacques"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Jacques"
                },
                "author": "Vincent Jacques",
                "arxiv_comment": "6 pages, 4 figures, supplemental material available as ancillary file",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03159v1",
                "updated": "2025-02-05T13:32:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    32,
                    29,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T13:32:29Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    32,
                    29,
                    2,
                    36,
                    0
                ],
                "title": "PICBench: Benchmarking LLMs for Photonic Integrated Circuits Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PICBench: Benchmarking LLMs for Photonic Integrated Circuits Design"
                },
                "summary": "While large language models (LLMs) have shown remarkable potential in\nautomating various tasks in digital chip design, the field of Photonic\nIntegrated Circuits (PICs)-a promising solution to advanced chip\ndesigns-remains relatively unexplored in this context. The design of PICs is\ntime-consuming and prone to errors due to the extensive and repetitive nature\nof code involved in photonic chip design. In this paper, we introduce PICBench,\nthe first benchmarking and evaluation framework specifically designed to\nautomate PIC design generation using LLMs, where the generated output takes the\nform of a netlist. Our benchmark consists of dozens of meticulously crafted PIC\ndesign problems, spanning from fundamental device designs to more complex\ncircuit-level designs. It automatically evaluates both the syntax and\nfunctionality of generated PIC designs by comparing simulation outputs with\nexpert-written solutions, leveraging an open-source simulator. We evaluate a\nrange of existing LLMs, while also conducting comparative tests on various\nprompt engineering techniques to enhance LLM performance in automated PIC\ndesign. The results reveal the challenges and potential of LLMs in the PIC\ndesign domain, offering insights into the key areas that require further\nresearch and development to optimize automation in this field. Our benchmark\nand evaluation code is available at https://github.com/PICDA/PICBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown remarkable potential in\nautomating various tasks in digital chip design, the field of Photonic\nIntegrated Circuits (PICs)-a promising solution to advanced chip\ndesigns-remains relatively unexplored in this context. The design of PICs is\ntime-consuming and prone to errors due to the extensive and repetitive nature\nof code involved in photonic chip design. In this paper, we introduce PICBench,\nthe first benchmarking and evaluation framework specifically designed to\nautomate PIC design generation using LLMs, where the generated output takes the\nform of a netlist. Our benchmark consists of dozens of meticulously crafted PIC\ndesign problems, spanning from fundamental device designs to more complex\ncircuit-level designs. It automatically evaluates both the syntax and\nfunctionality of generated PIC designs by comparing simulation outputs with\nexpert-written solutions, leveraging an open-source simulator. We evaluate a\nrange of existing LLMs, while also conducting comparative tests on various\nprompt engineering techniques to enhance LLM performance in automated PIC\ndesign. The results reveal the challenges and potential of LLMs in the PIC\ndesign domain, offering insights into the key areas that require further\nresearch and development to optimize automation in this field. Our benchmark\nand evaluation code is available at https://github.com/PICDA/PICBench."
                },
                "authors": [
                    {
                        "name": "Yuchao Wu"
                    },
                    {
                        "name": "Xiaofei Yu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yang Luo"
                    },
                    {
                        "name": "Yeyu Tong"
                    },
                    {
                        "name": "Yuzhe Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhe Ma"
                },
                "author": "Yuzhe Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03158v1",
                "updated": "2025-02-05T13:31:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    31,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T13:31:38Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    31,
                    38,
                    2,
                    36,
                    0
                ],
                "title": "Strategizing with AI: Insights from a Beauty Contest Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategizing with AI: Insights from a Beauty Contest Experiment"
                },
                "summary": "A Keynesian beauty contest is a wide class of games of guessing the most\npopular strategy among other players. In particular, guessing a fraction of a\nmean of numbers chosen by all players is a classic behavioral experiment\ndesigned to test iterative reasoning patterns among various groups of people.\nThe previous literature reveals that the level of sophistication of the\nopponents is an important factor affecting the outcome of the game. Smarter\ndecision makers choose strategies that are closer to theoretical Nash\nequilibrium and demonstrate faster convergence to equilibrium in iterated\ncontests with information revelation. We replicate a series of classic\nexperiments by running virtual experiments with modern large language models\n(LLMs) who play against various groups of virtual players. We test how advanced\nthe LLMs' behavior is compared to the behavior of human players. We show that\nLLMs typically take into account the opponents' level of sophistication and\nadapt by changing the strategy. In various settings, most LLMs (with the\nexception of Llama) are more sophisticated and play lower numbers compared to\nhuman players. Our results suggest that LLMs (except Llama) are rather\nsuccessful in identifying the underlying strategic environment and adopting the\nstrategies to the changing set of parameters of the game in the same way that\nhuman players do. All LLMs still fail to play dominant strategies in a\ntwo-player game. Our results contribute to the discussion on the accuracy of\nmodeling human economic agents by artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Keynesian beauty contest is a wide class of games of guessing the most\npopular strategy among other players. In particular, guessing a fraction of a\nmean of numbers chosen by all players is a classic behavioral experiment\ndesigned to test iterative reasoning patterns among various groups of people.\nThe previous literature reveals that the level of sophistication of the\nopponents is an important factor affecting the outcome of the game. Smarter\ndecision makers choose strategies that are closer to theoretical Nash\nequilibrium and demonstrate faster convergence to equilibrium in iterated\ncontests with information revelation. We replicate a series of classic\nexperiments by running virtual experiments with modern large language models\n(LLMs) who play against various groups of virtual players. We test how advanced\nthe LLMs' behavior is compared to the behavior of human players. We show that\nLLMs typically take into account the opponents' level of sophistication and\nadapt by changing the strategy. In various settings, most LLMs (with the\nexception of Llama) are more sophisticated and play lower numbers compared to\nhuman players. Our results suggest that LLMs (except Llama) are rather\nsuccessful in identifying the underlying strategic environment and adopting the\nstrategies to the changing set of parameters of the game in the same way that\nhuman players do. All LLMs still fail to play dominant strategies in a\ntwo-player game. Our results contribute to the discussion on the accuracy of\nmodeling human economic agents by artificial intelligence."
                },
                "authors": [
                    {
                        "name": "Iuliia Alekseenko"
                    },
                    {
                        "name": "Dmitry Dagaev"
                    },
                    {
                        "name": "Sofia Paklina"
                    },
                    {
                        "name": "Petr Parshakov"
                    }
                ],
                "author_detail": {
                    "name": "Petr Parshakov"
                },
                "author": "Petr Parshakov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18094v2",
                "updated": "2025-02-05T13:23:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    23,
                    18,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-30T02:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    2,
                    10,
                    23,
                    3,
                    30,
                    0
                ],
                "title": "AlphaAdam:Asynchronous Masked Optimization with Dynamic Alpha for\n  Selective Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaAdam:Asynchronous Masked Optimization with Dynamic Alpha for\n  Selective Updates"
                },
                "summary": "In the training of large language models (LLMs), updating parameters more\nefficiently and stably has always been an important challenge. To achieve\nefficient parameter updates, existing methods usually achieve performance\ncomparable to full parameter updates through methods such as low-dimensional\ndecomposition or layer-wise selective updates. In this work, we propose\nAlphaAdam, an optimization framework for LLM from the perspective of\nintra-layer parameter updates. By decoupling parameter updates and dynamically\nadjusting their strength, AlphaAdam accelerates convergence and improves\ntraining stability. We construct parameter masks based on the consistency of\nhistorical momentum and gradient direction and combine them with an adaptive\nmask strength strategy to ensure efficient optimization and theoretical\nconvergence guarantees, which is also applicable to most momentum-based\noptimizers. Extensive experiments show that AlphaAdam outperforms\nstate-of-the-art methods such as AdamW in terms of convergence speed and\ncomputational efficiency across tasks, including GPT-2 pre-trained and\nfine-tuned RoBERTa and Llama-7B. Our AlphaAdam implements an optimizer\nenhancement framework for LLMs through intra-layer asynchronous masked adaptive\nupdates. Our code is available in this https://github.com/MaeChd/AlphaAdam.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the training of large language models (LLMs), updating parameters more\nefficiently and stably has always been an important challenge. To achieve\nefficient parameter updates, existing methods usually achieve performance\ncomparable to full parameter updates through methods such as low-dimensional\ndecomposition or layer-wise selective updates. In this work, we propose\nAlphaAdam, an optimization framework for LLM from the perspective of\nintra-layer parameter updates. By decoupling parameter updates and dynamically\nadjusting their strength, AlphaAdam accelerates convergence and improves\ntraining stability. We construct parameter masks based on the consistency of\nhistorical momentum and gradient direction and combine them with an adaptive\nmask strength strategy to ensure efficient optimization and theoretical\nconvergence guarantees, which is also applicable to most momentum-based\noptimizers. Extensive experiments show that AlphaAdam outperforms\nstate-of-the-art methods such as AdamW in terms of convergence speed and\ncomputational efficiency across tasks, including GPT-2 pre-trained and\nfine-tuned RoBERTa and Llama-7B. Our AlphaAdam implements an optimizer\nenhancement framework for LLMs through intra-layer asynchronous masked adaptive\nupdates. Our code is available in this https://github.com/MaeChd/AlphaAdam."
                },
                "authors": [
                    {
                        "name": "Da Chang"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Ganzhao Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Ganzhao Yuan"
                },
                "author": "Ganzhao Yuan",
                "arxiv_comment": "Theorem 3.5 has issues of insufficient rigor. The content \"Let\n  $E[g_i^2] = \\sigma_i^2$ ... $E[g_im_{t-1,i}] = \\rho_i \\sigma_i^2$ be the\n  correlation between gradients and historical momentum ....\" is a non-standard\n  assumption and may mislead readers. In the spirit of rigor and\n  responsibility, we temporarily withdraw this version of the content",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02910v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02910v3",
                "updated": "2025-02-05T13:20:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    20,
                    24,
                    2,
                    36,
                    0
                ],
                "published": "2024-03-05T12:21:57Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    12,
                    21,
                    57,
                    1,
                    65,
                    0
                ],
                "title": "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image"
                },
                "summary": "There has been an increasing interest in the alignment of large language\nmodels (LLMs) with human values. However, the safety issues of their\nintegration with a vision module, or vision language models (VLMs), remain\nrelatively underexplored. In this paper, we propose a novel jailbreaking attack\nagainst VLMs, aiming to bypass their safety barrier when a user inputs harmful\ninstructions. A scenario where our poisoned (image, text) data pairs are\nincluded in the training data is assumed. By replacing the original textual\ncaptions with malicious jailbreak prompts, our method can perform jailbreak\nattacks with the poisoned images. Moreover, we analyze the effect of poison\nratios and positions of trainable parameters on our attack's success rate. For\nevaluation, we design two metrics to quantify the success rate and the\nstealthiness of our attack. Together with a list of curated harmful\ninstructions, a benchmark for measuring attack efficacy is provided. We\ndemonstrate the efficacy of our attack by comparing it with baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been an increasing interest in the alignment of large language\nmodels (LLMs) with human values. However, the safety issues of their\nintegration with a vision module, or vision language models (VLMs), remain\nrelatively underexplored. In this paper, we propose a novel jailbreaking attack\nagainst VLMs, aiming to bypass their safety barrier when a user inputs harmful\ninstructions. A scenario where our poisoned (image, text) data pairs are\nincluded in the training data is assumed. By replacing the original textual\ncaptions with malicious jailbreak prompts, our method can perform jailbreak\nattacks with the poisoned images. Moreover, we analyze the effect of poison\nratios and positions of trainable parameters on our attack's success rate. For\nevaluation, we design two metrics to quantify the success rate and the\nstealthiness of our attack. Together with a list of curated harmful\ninstructions, a benchmark for measuring attack efficacy is provided. We\ndemonstrate the efficacy of our attack by comparing it with baseline methods."
                },
                "authors": [
                    {
                        "name": "Xijia Tao"
                    },
                    {
                        "name": "Shuai Zhong"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02910v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02910v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01506v2",
                "updated": "2025-02-05T13:18:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    18,
                    13,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-03T16:39:48Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    16,
                    39,
                    48,
                    0,
                    34,
                    0
                ],
                "title": "TwinMarket: A Scalable Behavioral and Social Simulation for Financial\n  Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TwinMarket: A Scalable Behavioral and Social Simulation for Financial\n  Markets"
                },
                "summary": "The study of social emergence has long been a central focus in social\nscience. Traditional modeling approaches, such as rule-based Agent-Based Models\n(ABMs), struggle to capture the diversity and complexity of human behavior,\nparticularly the irrational factors emphasized in behavioral economics.\nRecently, large language model (LLM) agents have gained traction as simulation\ntools for modeling human behavior in social science and role-playing\napplications. Studies suggest that LLMs can account for cognitive biases,\nemotional fluctuations, and other non-rational influences, enabling more\nrealistic simulations of socio-economic dynamics. In this work, we introduce\nTwinMarket, a novel multi-agent framework that leverages LLMs to simulate\nsocio-economic systems. Specifically, we examine how individual behaviors,\nthrough interactions and feedback mechanisms, give rise to collective dynamics\nand emergent phenomena. Through experiments in a simulated stock market\nenvironment, we demonstrate how individual actions can trigger group behaviors,\nleading to emergent outcomes such as financial bubbles and recessions. Our\napproach provides valuable insights into the complex interplay between\nindividual decision-making and collective socio-economic patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of social emergence has long been a central focus in social\nscience. Traditional modeling approaches, such as rule-based Agent-Based Models\n(ABMs), struggle to capture the diversity and complexity of human behavior,\nparticularly the irrational factors emphasized in behavioral economics.\nRecently, large language model (LLM) agents have gained traction as simulation\ntools for modeling human behavior in social science and role-playing\napplications. Studies suggest that LLMs can account for cognitive biases,\nemotional fluctuations, and other non-rational influences, enabling more\nrealistic simulations of socio-economic dynamics. In this work, we introduce\nTwinMarket, a novel multi-agent framework that leverages LLMs to simulate\nsocio-economic systems. Specifically, we examine how individual behaviors,\nthrough interactions and feedback mechanisms, give rise to collective dynamics\nand emergent phenomena. Through experiments in a simulated stock market\nenvironment, we demonstrate how individual actions can trigger group behaviors,\nleading to emergent outcomes such as financial bubbles and recessions. Our\napproach provides valuable insights into the complex interplay between\nindividual decision-making and collective socio-economic patterns."
                },
                "authors": [
                    {
                        "name": "Yuzhe Yang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Kaidi Zhang"
                    },
                    {
                        "name": "Yunmiao Zhang"
                    },
                    {
                        "name": "Honghai Yu"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03147v1",
                "updated": "2025-02-05T13:16:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    16,
                    41,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T13:16:41Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    16,
                    41,
                    2,
                    36,
                    0
                ],
                "title": "Scalable In-Context Learning on Tabular Data via Retrieval-Augmented\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable In-Context Learning on Tabular Data via Retrieval-Augmented\n  Large Language Models"
                },
                "summary": "Recent studies have shown that large language models (LLMs), when customized\nwith post-training on tabular data, can acquire general tabular in-context\nlearning (TabICL) capabilities. These models are able to transfer effectively\nacross diverse data schemas and different task domains. However, existing\nLLM-based TabICL approaches are constrained to few-shot scenarios due to the\nsequence length limitations of LLMs, as tabular instances represented in plain\ntext consume substantial tokens. To address this limitation and enable scalable\nTabICL for any data size, we propose retrieval-augmented LLMs tailored to\ntabular data. Our approach incorporates a customized retrieval module, combined\nwith retrieval-guided instruction-tuning for LLMs. This enables LLMs to\neffectively leverage larger datasets, achieving significantly improved\nperformance across 69 widely recognized datasets and demonstrating promising\nscaling behavior. Extensive comparisons with state-of-the-art tabular models\nreveal that, while LLM-based TabICL still lags behind well-tuned numeric models\nin overall performance, it uncovers powerful algorithms under limited contexts,\nenhances ensemble diversity, and excels on specific datasets. These unique\nproperties underscore the potential of language as a universal and accessible\ninterface for scalable tabular data learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that large language models (LLMs), when customized\nwith post-training on tabular data, can acquire general tabular in-context\nlearning (TabICL) capabilities. These models are able to transfer effectively\nacross diverse data schemas and different task domains. However, existing\nLLM-based TabICL approaches are constrained to few-shot scenarios due to the\nsequence length limitations of LLMs, as tabular instances represented in plain\ntext consume substantial tokens. To address this limitation and enable scalable\nTabICL for any data size, we propose retrieval-augmented LLMs tailored to\ntabular data. Our approach incorporates a customized retrieval module, combined\nwith retrieval-guided instruction-tuning for LLMs. This enables LLMs to\neffectively leverage larger datasets, achieving significantly improved\nperformance across 69 widely recognized datasets and demonstrating promising\nscaling behavior. Extensive comparisons with state-of-the-art tabular models\nreveal that, while LLM-based TabICL still lags behind well-tuned numeric models\nin overall performance, it uncovers powerful algorithms under limited contexts,\nenhances ensemble diversity, and excels on specific datasets. These unique\nproperties underscore the potential of language as a universal and accessible\ninterface for scalable tabular data learning."
                },
                "authors": [
                    {
                        "name": "Xumeng Wen"
                    },
                    {
                        "name": "Shun Zheng"
                    },
                    {
                        "name": "Zhen Xu"
                    },
                    {
                        "name": "Yiming Sun"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03661v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03661v3",
                "updated": "2025-02-05T13:06:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    6,
                    7,
                    2,
                    36,
                    0
                ],
                "published": "2024-09-05T16:14:07Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    14,
                    7,
                    3,
                    249,
                    0
                ],
                "title": "Ensemble noise properties of the European Pulsar Timing Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensemble noise properties of the European Pulsar Timing Array"
                },
                "summary": "The null hypothesis in Pulsar Timing Array (PTA) analyses includes\nassumptions about ensemble properties of pulsar time-correlated noise. These\nproperties are encoded in prior probabilities for the amplitude and the\nspectral index of the power-law power spectral density of temporal correlations\nof the noise. Because multiple realizations of time-correlated noise processes\nare found in pulsars, these ensemble noise properties could and should be\nmodelled in the full-PTA observations by parameterising the respective prior\ndistributions using the so-called hyperparameters. This approach is known as\nthe hierarchical Bayesian inference. In this work, we introduce a new procedure\nfor numerical marginalisation over hyperparameters. The procedure may be used\nin searches for nanohertz gravitational waves and other PTA analyses to resolve\nprior misspecification at negligible computational cost. Furthermore, we infer\nthe distribution of amplitudes and spectral indices of the power spectral\ndensity of spin noise and dispersion measure variation noise based on the\nobservation of 25 millisecond pulsars by the European Pulsar Timing Array\n(EPTA). Our results may be used for the simulation of realistic noise in PTAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The null hypothesis in Pulsar Timing Array (PTA) analyses includes\nassumptions about ensemble properties of pulsar time-correlated noise. These\nproperties are encoded in prior probabilities for the amplitude and the\nspectral index of the power-law power spectral density of temporal correlations\nof the noise. Because multiple realizations of time-correlated noise processes\nare found in pulsars, these ensemble noise properties could and should be\nmodelled in the full-PTA observations by parameterising the respective prior\ndistributions using the so-called hyperparameters. This approach is known as\nthe hierarchical Bayesian inference. In this work, we introduce a new procedure\nfor numerical marginalisation over hyperparameters. The procedure may be used\nin searches for nanohertz gravitational waves and other PTA analyses to resolve\nprior misspecification at negligible computational cost. Furthermore, we infer\nthe distribution of amplitudes and spectral indices of the power spectral\ndensity of spin noise and dispersion measure variation noise based on the\nobservation of 25 millisecond pulsars by the European Pulsar Timing Array\n(EPTA). Our results may be used for the simulation of realistic noise in PTAs."
                },
                "authors": [
                    {
                        "name": "Boris Goncharov"
                    },
                    {
                        "name": "Shubhit Sardana"
                    }
                ],
                "author_detail": {
                    "name": "Shubhit Sardana"
                },
                "author": "Shubhit Sardana",
                "arxiv_doi": "10.1093/mnras/staf190",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf190",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03661v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03661v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 6 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03139v1",
                "updated": "2025-02-05T13:02:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    2,
                    14,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T13:02:14Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    2,
                    14,
                    2,
                    36,
                    0
                ],
                "title": "Fast Sampling of Cosmological Initial Conditions with Gaussian Neural\n  Posterior Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Sampling of Cosmological Initial Conditions with Gaussian Neural\n  Posterior Estimation"
                },
                "summary": "Knowledge of the primordial matter density field from which the large-scale\nstructure of the Universe emerged over cosmic time is of fundamental importance\nfor cosmology. However, reconstructing these cosmological initial conditions\nfrom late-time observations is a notoriously difficult task, which requires\nadvanced cosmological simulators and sophisticated statistical methods to\nexplore a multi-million-dimensional parameter space. We show how\nsimulation-based inference (SBI) can be used to tackle this problem and to\nobtain data-constrained realisations of the primordial dark matter density\nfield in a simulation-efficient way with general non-differentiable simulators.\nOur method is applicable to full high-resolution dark matter $N$-body\nsimulations and is based on modelling the posterior distribution of the\nconstrained initial conditions to be Gaussian with a diagonal covariance matrix\nin Fourier space. As a result, we can generate thousands of posterior samples\nwithin seconds on a single GPU, orders of magnitude faster than existing\nmethods, paving the way for sequential SBI for cosmological fields.\nFurthermore, we perform an analytical fit of the estimated dependence of the\ncovariance on the wavenumber, effectively transforming any point-estimator of\ninitial conditions into a fast sampler. We test the validity of our obtained\nsamples by comparing them to the true values with summary statistics and\nperforming a Bayesian consistency test.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge of the primordial matter density field from which the large-scale\nstructure of the Universe emerged over cosmic time is of fundamental importance\nfor cosmology. However, reconstructing these cosmological initial conditions\nfrom late-time observations is a notoriously difficult task, which requires\nadvanced cosmological simulators and sophisticated statistical methods to\nexplore a multi-million-dimensional parameter space. We show how\nsimulation-based inference (SBI) can be used to tackle this problem and to\nobtain data-constrained realisations of the primordial dark matter density\nfield in a simulation-efficient way with general non-differentiable simulators.\nOur method is applicable to full high-resolution dark matter $N$-body\nsimulations and is based on modelling the posterior distribution of the\nconstrained initial conditions to be Gaussian with a diagonal covariance matrix\nin Fourier space. As a result, we can generate thousands of posterior samples\nwithin seconds on a single GPU, orders of magnitude faster than existing\nmethods, paving the way for sequential SBI for cosmological fields.\nFurthermore, we perform an analytical fit of the estimated dependence of the\ncovariance on the wavenumber, effectively transforming any point-estimator of\ninitial conditions into a fast sampler. We test the validity of our obtained\nsamples by comparing them to the true values with summary statistics and\nperforming a Bayesian consistency test."
                },
                "authors": [
                    {
                        "name": "Oleg Savchenko"
                    },
                    {
                        "name": "Guillermo Franco Abelln"
                    },
                    {
                        "name": "Florian List"
                    },
                    {
                        "name": "Noemi Anau Montel"
                    },
                    {
                        "name": "Christoph Weniger"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Weniger"
                },
                "author": "Christoph Weniger",
                "arxiv_comment": "9 + 2 pages, 7 figures, 1 table. Comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06479v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06479v3",
                "updated": "2025-02-05T12:50:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    50,
                    46,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-09T02:14:39Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    14,
                    39,
                    2,
                    283,
                    0
                ],
                "title": "Compressing Large Language Models with Automated Sub-Network Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing Large Language Models with Automated Sub-Network Search"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional reasoning abilities,\nenabling strong generalization across diverse tasks such as commonsense\nreasoning and instruction following. However, as LLMs scale, inference costs\nbecome increasingly prohibitive, accumulating significantly over their life\ncycle. In this paper we consider model compression for LLMs to reduce model\nsize while improving downstream task performance. We phrase this as a neural\narchitecture search problem that automatically prunes structural components,\nsuch as attention heads, neurons, and layers by searching for the\nPareto-optimal set of sub-networks balancing between performance and on-device\nlatency. Compared to state-of-the-art structural pruning approaches and\nfine-tuned smaller sub-networks extracted from the pre-trained model, our\nmethod achieves upto 9.85% improvement on average on 11 diverse downstream\ntasks, while achieving up to 22% improvement of on-device latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional reasoning abilities,\nenabling strong generalization across diverse tasks such as commonsense\nreasoning and instruction following. However, as LLMs scale, inference costs\nbecome increasingly prohibitive, accumulating significantly over their life\ncycle. In this paper we consider model compression for LLMs to reduce model\nsize while improving downstream task performance. We phrase this as a neural\narchitecture search problem that automatically prunes structural components,\nsuch as attention heads, neurons, and layers by searching for the\nPareto-optimal set of sub-networks balancing between performance and on-device\nlatency. Compared to state-of-the-art structural pruning approaches and\nfine-tuned smaller sub-networks extracted from the pre-trained model, our\nmethod achieves upto 9.85% improvement on average on 11 diverse downstream\ntasks, while achieving up to 22% improvement of on-device latency."
                },
                "authors": [
                    {
                        "name": "Rhea Sanjay Sukthanker"
                    },
                    {
                        "name": "Benedikt Staffler"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Aaron Klein"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Klein"
                },
                "author": "Aaron Klein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06479v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06479v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03129v1",
                "updated": "2025-02-05T12:39:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    39,
                    7,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T12:39:07Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    39,
                    7,
                    2,
                    36,
                    0
                ],
                "title": "Teaching Large Language Models Number-Focused Headline Generation With\n  Key Element Rationales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Large Language Models Number-Focused Headline Generation With\n  Key Element Rationales"
                },
                "summary": "Number-focused headline generation is a summarization task requiring both\nhigh textual quality and precise numerical accuracy, which poses a unique\nchallenge for Large Language Models (LLMs). Existing studies in the literature\nfocus only on either textual quality or numerical reasoning and thus are\ninadequate to address this challenge. In this paper, we propose a novel\nchain-of-thought framework for using rationales comprising key elements of the\nTopic, Entities, and Numerical reasoning (TEN) in news articles to enhance the\ncapability for LLMs to generate topic-aligned high-quality texts with precise\nnumerical accuracy. Specifically, a teacher LLM is employed to generate TEN\nrationales as supervision data, which are then used to teach and fine-tune a\nstudent LLM. Our approach teaches the student LLM automatic generation of\nrationales with enhanced capability for numerical reasoning and topic-aligned\nnumerical headline generation. Experiments show that our approach achieves\nsuperior performance in both textual quality and numerical accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Number-focused headline generation is a summarization task requiring both\nhigh textual quality and precise numerical accuracy, which poses a unique\nchallenge for Large Language Models (LLMs). Existing studies in the literature\nfocus only on either textual quality or numerical reasoning and thus are\ninadequate to address this challenge. In this paper, we propose a novel\nchain-of-thought framework for using rationales comprising key elements of the\nTopic, Entities, and Numerical reasoning (TEN) in news articles to enhance the\ncapability for LLMs to generate topic-aligned high-quality texts with precise\nnumerical accuracy. Specifically, a teacher LLM is employed to generate TEN\nrationales as supervision data, which are then used to teach and fine-tune a\nstudent LLM. Our approach teaches the student LLM automatic generation of\nrationales with enhanced capability for numerical reasoning and topic-aligned\nnumerical headline generation. Experiments show that our approach achieves\nsuperior performance in both textual quality and numerical accuracy."
                },
                "authors": [
                    {
                        "name": "Zhen Qian"
                    },
                    {
                        "name": "Xiuzhen Zhang"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Feng Xia"
                    }
                ],
                "author_detail": {
                    "name": "Feng Xia"
                },
                "author": "Feng Xia",
                "arxiv_comment": "Pre-print for a paper accepted to findings of NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03735v2",
                "updated": "2025-02-05T12:31:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    31,
                    1,
                    2,
                    36,
                    0
                ],
                "published": "2024-09-05T17:50:31Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    50,
                    31,
                    3,
                    249,
                    0
                ],
                "title": "Investigating Privacy Bias in Training Data of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Privacy Bias in Training Data of Language Models"
                },
                "summary": "As LLMs are integrated into sociotechnical systems, it is crucial to examine\nthe privacy biases they exhibit. A privacy bias refers to the skew in the\nappropriateness of information flows within a given context that LLMs acquire\nfrom large amounts of non-publicly available training data. This skew may\neither align with existing expectations or signal a symptom of systemic issues\nreflected in the training datasets.\n  We formulate a novel research question: how can we examine privacy biases in\nthe training data of LLMs? We present a novel approach to assess the privacy\nbiases using a contextual integrity-based methodology to evaluate the responses\nfrom different LLMs. Our approach accounts for the sensitivity of responses\nacross prompt variations, which hinders the evaluation of privacy biases. We\ninvestigate how privacy biases are affected by model capacities and\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs are integrated into sociotechnical systems, it is crucial to examine\nthe privacy biases they exhibit. A privacy bias refers to the skew in the\nappropriateness of information flows within a given context that LLMs acquire\nfrom large amounts of non-publicly available training data. This skew may\neither align with existing expectations or signal a symptom of systemic issues\nreflected in the training datasets.\n  We formulate a novel research question: how can we examine privacy biases in\nthe training data of LLMs? We present a novel approach to assess the privacy\nbiases using a contextual integrity-based methodology to evaluate the responses\nfrom different LLMs. Our approach accounts for the sensitivity of responses\nacross prompt variations, which hinders the evaluation of privacy biases. We\ninvestigate how privacy biases are affected by model capacities and\noptimizations."
                },
                "authors": [
                    {
                        "name": "Yan Shvartzshnaider"
                    },
                    {
                        "name": "Vasisht Duddu"
                    }
                ],
                "author_detail": {
                    "name": "Vasisht Duddu"
                },
                "author": "Vasisht Duddu",
                "arxiv_comment": "16 pages, 4 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12502v2",
                "updated": "2025-02-05T12:29:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    29,
                    1,
                    2,
                    36,
                    0
                ],
                "published": "2024-06-18T11:05:37Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    11,
                    5,
                    37,
                    1,
                    170,
                    0
                ],
                "title": "Code-Optimise: Self-Generated Preference Data for Correctness and\n  Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-Optimise: Self-Generated Preference Data for Correctness and\n  Efficiency"
                },
                "summary": "Code Language Models have been trained to generate accurate solutions,\ntypically with no regard for runtime. On the other hand, previous works that\nexplored execution optimisation have observed corresponding drops in functional\ncorrectness. To that end, we introduce Code-Optimise, a framework that\nincorporates both correctness (passed, failed) and runtime (quick, slow) as\nlearning signals via self-generated preference data. Our framework is both\nlightweight and robust as it dynamically selects solutions to reduce\noverfitting while avoiding a reliance on larger models for learning signals.\nCode-Optimise achieves significant improvements in pass@k while decreasing the\ncompetitive baseline runtimes by an additional 6% for in-domain data and up to\n3% for out-of-domain data. As a by-product, the average length of the generated\nsolutions is reduced by up to 48% on MBPP and 23% on HumanEval, resulting in\nfaster and cheaper inference. The generated data and codebase is open-sourced\nat https://github.com/huawei-noah/HEBO/tree/Code_Optimise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Language Models have been trained to generate accurate solutions,\ntypically with no regard for runtime. On the other hand, previous works that\nexplored execution optimisation have observed corresponding drops in functional\ncorrectness. To that end, we introduce Code-Optimise, a framework that\nincorporates both correctness (passed, failed) and runtime (quick, slow) as\nlearning signals via self-generated preference data. Our framework is both\nlightweight and robust as it dynamically selects solutions to reduce\noverfitting while avoiding a reliance on larger models for learning signals.\nCode-Optimise achieves significant improvements in pass@k while decreasing the\ncompetitive baseline runtimes by an additional 6% for in-domain data and up to\n3% for out-of-domain data. As a by-product, the average length of the generated\nsolutions is reduced by up to 48% on MBPP and 23% on HumanEval, resulting in\nfaster and cheaper inference. The generated data and codebase is open-sourced\nat https://github.com/huawei-noah/HEBO/tree/Code_Optimise."
                },
                "authors": [
                    {
                        "name": "Leonidas Gee"
                    },
                    {
                        "name": "Milan Gritta"
                    },
                    {
                        "name": "Gerasimos Lampouras"
                    },
                    {
                        "name": "Ignacio Iacobacci"
                    }
                ],
                "author_detail": {
                    "name": "Ignacio Iacobacci"
                },
                "author": "Ignacio Iacobacci",
                "arxiv_comment": "NAACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09768v2",
                "updated": "2025-02-05T12:17:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    17,
                    36,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-15T11:32:35Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    32,
                    35,
                    2,
                    15,
                    0
                ],
                "title": "Can Large Language Models Predict the Outcome of Judicial Decisions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Predict the Outcome of Judicial Decisions?"
                },
                "summary": "Large Language Models (LLMs) have shown exceptional capabilities in Natural\nLanguage Processing (NLP) across diverse domains. However, their application in\nspecialized tasks such as Legal Judgment Prediction (LJP) for low-resource\nlanguages like Arabic remains underexplored. In this work, we address this gap\nby developing an Arabic LJP dataset, collected and preprocessed from Saudi\ncommercial court judgments. We benchmark state-of-the-art open-source LLMs,\nincluding LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as\nzero-shot, one-shot, and fine-tuning using QLoRA. Additionally, we used a\ncomprehensive evaluation framework combining quantitative metrics (BLEU and\nROUGE) and qualitative assessments (Coherence, legal language, clarity). Our\nresults demonstrate that fine-tuned smaller models achieve comparable\nperformance to larger models in task-specific contexts while offering\nsignificant resource efficiency. Furthermore, we investigate the effects of\nprompt engineering and fine-tuning on model outputs, providing insights into\nperformance variability and instruction sensitivity. By making the dataset,\nimplementation code, and models publicly available, we establish a robust\nfoundation for future research in Arabic legal NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown exceptional capabilities in Natural\nLanguage Processing (NLP) across diverse domains. However, their application in\nspecialized tasks such as Legal Judgment Prediction (LJP) for low-resource\nlanguages like Arabic remains underexplored. In this work, we address this gap\nby developing an Arabic LJP dataset, collected and preprocessed from Saudi\ncommercial court judgments. We benchmark state-of-the-art open-source LLMs,\nincluding LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as\nzero-shot, one-shot, and fine-tuning using QLoRA. Additionally, we used a\ncomprehensive evaluation framework combining quantitative metrics (BLEU and\nROUGE) and qualitative assessments (Coherence, legal language, clarity). Our\nresults demonstrate that fine-tuned smaller models achieve comparable\nperformance to larger models in task-specific contexts while offering\nsignificant resource efficiency. Furthermore, we investigate the effects of\nprompt engineering and fine-tuning on model outputs, providing insights into\nperformance variability and instruction sensitivity. By making the dataset,\nimplementation code, and models publicly available, we establish a robust\nfoundation for future research in Arabic legal NLP."
                },
                "authors": [
                    {
                        "name": "Mohamed Bayan Kmainasi"
                    },
                    {
                        "name": "Ali Ezzat Shahroor"
                    },
                    {
                        "name": "Amani Al-Ghraibah"
                    }
                ],
                "author_detail": {
                    "name": "Amani Al-Ghraibah"
                },
                "author": "Amani Al-Ghraibah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03102v1",
                "updated": "2025-02-05T11:59:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    59,
                    22,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T11:59:22Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    59,
                    22,
                    2,
                    36,
                    0
                ],
                "title": "Structured Token Retention and Computational Memory Paths in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Token Retention and Computational Memory Paths in Large\n  Language Models"
                },
                "summary": "Memory retention mechanisms play a central role in determining the efficiency\nof computational architectures designed for processing extended sequences.\nConventional methods for token management often impose fixed retention\nthresholds or rely on uniform attention weight distributions, leading to\ninefficient memory utilization and premature information loss in extended\nsequence modeling. Structured Token Retention (STR) introduces a probabilistic\nselection framework that dynamically adjusts token persistence based on\ncontextual significance, ensuring that computational resources are allocated to\nsemantically relevant elements. Computational Memory Paths (CMP) extend this\nframework through hierarchical memory allocation, refining retention efficiency\nthrough structured reallocation of token embeddings. Comparative assessments\nagainst baseline models demonstrate that STR and CMP improve token survival\nrates across long input sequences while reducing cumulative error propagation\nacross processing layers. Experimental results further indicate reductions in\ncomputational overhead, improving inference speed without degrading contextual\ncoherence. Token distribution analyses reveal that structured memory allocation\nprevents excessive redundancy in attention weight calculations, optimizing\ninformation retrieval efficiency in large-scale generative architectures. The\nintegration of STR and CMP into an open-source model illustrates the\nadaptability of structured memory retention methodologies, highlighting their\napplicability in generative text processing, long-context comprehension, and\nscalable sequence modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory retention mechanisms play a central role in determining the efficiency\nof computational architectures designed for processing extended sequences.\nConventional methods for token management often impose fixed retention\nthresholds or rely on uniform attention weight distributions, leading to\ninefficient memory utilization and premature information loss in extended\nsequence modeling. Structured Token Retention (STR) introduces a probabilistic\nselection framework that dynamically adjusts token persistence based on\ncontextual significance, ensuring that computational resources are allocated to\nsemantically relevant elements. Computational Memory Paths (CMP) extend this\nframework through hierarchical memory allocation, refining retention efficiency\nthrough structured reallocation of token embeddings. Comparative assessments\nagainst baseline models demonstrate that STR and CMP improve token survival\nrates across long input sequences while reducing cumulative error propagation\nacross processing layers. Experimental results further indicate reductions in\ncomputational overhead, improving inference speed without degrading contextual\ncoherence. Token distribution analyses reveal that structured memory allocation\nprevents excessive redundancy in attention weight calculations, optimizing\ninformation retrieval efficiency in large-scale generative architectures. The\nintegration of STR and CMP into an open-source model illustrates the\nadaptability of structured memory retention methodologies, highlighting their\napplicability in generative text processing, long-context comprehension, and\nscalable sequence modeling."
                },
                "authors": [
                    {
                        "name": "Jonathan Delena"
                    },
                    {
                        "name": "Augustin Moreau"
                    },
                    {
                        "name": "Dominic Ravensdale"
                    },
                    {
                        "name": "Frederick Chatterton"
                    }
                ],
                "author_detail": {
                    "name": "Frederick Chatterton"
                },
                "author": "Frederick Chatterton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11650v2",
                "updated": "2025-02-05T11:55:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    55,
                    55,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-20T18:29:12Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    18,
                    29,
                    12,
                    0,
                    20,
                    0
                ],
                "title": "Changes over time in the 100-year return value of climate model\n  variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Changes over time in the 100-year return value of climate model\n  variables"
                },
                "summary": "We assess evidence for changes in tail characteristics of wind, solar\nirradiance and temperature variables output from CMIP6 global climate models\n(GCMs) due to climate forcing. We estimate global and climate zone annual\nmaximum and annual means for period (2015, 2100) from daily output of seven\nGCMs for daily wind speed, maximum wind speed, solar irradiance and\nnear-surface temperature. We calculate corresponding annualised data for\nindividual locations within neighbourhoods of the North Atlantic and Celtic Sea\nregion. We consider output for three climate scenarios and multiple climate\nensembles. We estimate non-stationary extreme value models for annual extremes,\nand non-homogeneous Gaussian regressions for annual means, using Bayesian\ninference. We use estimated statistical models to quantify the distribution of\n(i) the change in 100-year return value for annual extremes, and (2) the change\nin annual mean, over the period (2025, 2125). To summarise results, we estimate\nlinear mixed effects models for observed variation of (i) and (ii). Evidence\nfor changes in the 100-year return value for annual maxima of solar irradiance\nand temperature is much stronger than for wind variables over time and with\nclimate scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We assess evidence for changes in tail characteristics of wind, solar\nirradiance and temperature variables output from CMIP6 global climate models\n(GCMs) due to climate forcing. We estimate global and climate zone annual\nmaximum and annual means for period (2015, 2100) from daily output of seven\nGCMs for daily wind speed, maximum wind speed, solar irradiance and\nnear-surface temperature. We calculate corresponding annualised data for\nindividual locations within neighbourhoods of the North Atlantic and Celtic Sea\nregion. We consider output for three climate scenarios and multiple climate\nensembles. We estimate non-stationary extreme value models for annual extremes,\nand non-homogeneous Gaussian regressions for annual means, using Bayesian\ninference. We use estimated statistical models to quantify the distribution of\n(i) the change in 100-year return value for annual extremes, and (2) the change\nin annual mean, over the period (2025, 2125). To summarise results, we estimate\nlinear mixed effects models for observed variation of (i) and (ii). Evidence\nfor changes in the 100-year return value for annual maxima of solar irradiance\nand temperature is much stronger than for wind variables over time and with\nclimate scenario."
                },
                "authors": [
                    {
                        "name": "Callum Leach"
                    },
                    {
                        "name": "Kevin Ewans"
                    },
                    {
                        "name": "Philip Jonathan"
                    }
                ],
                "author_detail": {
                    "name": "Philip Jonathan"
                },
                "author": "Philip Jonathan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03095v1",
                "updated": "2025-02-05T11:41:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    41,
                    43,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T11:41:43Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    41,
                    43,
                    2,
                    36,
                    0
                ],
                "title": "Reveal the Mystery of DPO: The Connection between DPO and RL Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reveal the Mystery of DPO: The Connection between DPO and RL Algorithms"
                },
                "summary": "With the rapid development of Large Language Models (LLMs), numerous\nReinforcement Learning from Human Feedback (RLHF) algorithms have been\nintroduced to improve model safety and alignment with human preferences. These\nalgorithms can be divided into two main frameworks based on whether they\nrequire an explicit reward (or value) function for training: actor-critic-based\nProximal Policy Optimization (PPO) and alignment-based Direct Preference\nOptimization (DPO). The mismatch between DPO and PPO, such as DPO's use of a\nclassification loss driven by human-preferred data, has raised confusion about\nwhether DPO should be classified as a Reinforcement Learning (RL) algorithm. To\naddress these ambiguities, we focus on three key aspects related to DPO, RL,\nand other RLHF algorithms: (1) the construction of the loss function; (2) the\ntarget distribution at which the algorithm converges; (3) the impact of key\ncomponents within the loss function. Specifically, we first establish a unified\nframework named UDRRA connecting these algorithms based on the construction of\ntheir loss functions. Next, we uncover their target policy distributions within\nthis framework. Finally, we investigate the critical components of DPO to\nunderstand their impact on the convergence rate. Our work provides a deeper\nunderstanding of the relationship between DPO, RL, and other RLHF algorithms,\noffering new insights for improving existing algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of Large Language Models (LLMs), numerous\nReinforcement Learning from Human Feedback (RLHF) algorithms have been\nintroduced to improve model safety and alignment with human preferences. These\nalgorithms can be divided into two main frameworks based on whether they\nrequire an explicit reward (or value) function for training: actor-critic-based\nProximal Policy Optimization (PPO) and alignment-based Direct Preference\nOptimization (DPO). The mismatch between DPO and PPO, such as DPO's use of a\nclassification loss driven by human-preferred data, has raised confusion about\nwhether DPO should be classified as a Reinforcement Learning (RL) algorithm. To\naddress these ambiguities, we focus on three key aspects related to DPO, RL,\nand other RLHF algorithms: (1) the construction of the loss function; (2) the\ntarget distribution at which the algorithm converges; (3) the impact of key\ncomponents within the loss function. Specifically, we first establish a unified\nframework named UDRRA connecting these algorithms based on the construction of\ntheir loss functions. Next, we uncover their target policy distributions within\nthis framework. Finally, we investigate the critical components of DPO to\nunderstand their impact on the convergence rate. Our work provides a deeper\nunderstanding of the relationship between DPO, RL, and other RLHF algorithms,\noffering new insights for improving existing algorithms."
                },
                "authors": [
                    {
                        "name": "Xuerui Su"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Jinhua Zhu"
                    },
                    {
                        "name": "Mingyang Yi"
                    },
                    {
                        "name": "Feng Xu"
                    },
                    {
                        "name": "Zhiming Ma"
                    },
                    {
                        "name": "Yuting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuting Liu"
                },
                "author": "Yuting Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00334v2",
                "updated": "2025-02-05T11:36:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    36,
                    53,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-01T06:42:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    6,
                    42,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning\n  with Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nsolving complex reasoning tasks, particularly in mathematics. However, the\ndomain of physics reasoning presents unique challenges that have received\nsignificantly less attention. Existing benchmarks often fall short in\nevaluating LLMs' abilities on the breadth and depth of undergraduate-level\nphysics, underscoring the need for a comprehensive evaluation. To fill this\ngap, we introduce UGPhysics, a large-scale and comprehensive benchmark\nspecifically designed to evaluate UnderGraduate-level Physics (UGPhysics)\nreasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics\nproblems in both English and Chinese, covering 13 subjects with seven different\nanswer types and four distinct physics reasoning skills, all rigorously\nscreened for data leakage. Additionally, we develop a Model-Assistant\nRule-based Judgment (MARJ) pipeline specifically tailored for assessing answer\ncorrectness of physics problems, ensuring accurate evaluation. Our evaluation\nof 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by\nOpenAI-o1-mini), emphasizes the necessity for models with stronger physics\nreasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ,\nwill drive future advancements in AI for physics reasoning. Codes and data are\navailable at https://github.com/YangLabHKUST/UGPhysics .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nsolving complex reasoning tasks, particularly in mathematics. However, the\ndomain of physics reasoning presents unique challenges that have received\nsignificantly less attention. Existing benchmarks often fall short in\nevaluating LLMs' abilities on the breadth and depth of undergraduate-level\nphysics, underscoring the need for a comprehensive evaluation. To fill this\ngap, we introduce UGPhysics, a large-scale and comprehensive benchmark\nspecifically designed to evaluate UnderGraduate-level Physics (UGPhysics)\nreasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics\nproblems in both English and Chinese, covering 13 subjects with seven different\nanswer types and four distinct physics reasoning skills, all rigorously\nscreened for data leakage. Additionally, we develop a Model-Assistant\nRule-based Judgment (MARJ) pipeline specifically tailored for assessing answer\ncorrectness of physics problems, ensuring accurate evaluation. Our evaluation\nof 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by\nOpenAI-o1-mini), emphasizes the necessity for models with stronger physics\nreasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ,\nwill drive future advancements in AI for physics reasoning. Codes and data are\navailable at https://github.com/YangLabHKUST/UGPhysics ."
                },
                "authors": [
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Qiyun Xu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Tianhao Chen"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Can Yang"
                    },
                    {
                        "name": "Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Wang"
                },
                "author": "Yang Wang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09342v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09342v3",
                "updated": "2025-02-05T11:33:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    33,
                    20,
                    2,
                    36,
                    0
                ],
                "published": "2024-02-14T17:44:29Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    17,
                    44,
                    29,
                    2,
                    45,
                    0
                ],
                "title": "Changing disc compositions via internal photoevaporation I: Solar-mass\n  stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Changing disc compositions via internal photoevaporation I: Solar-mass\n  stars"
                },
                "summary": "The chemical evolution of protoplanetary discs is not fully understood,\nseveral factors influence the final distribution of disc material. One such\nfactor are inward drifting and evaporating pebbles that enrich the inner disc\nwith vapour. In particular, it is first enriched with water vapour, resulting\nin a low C/O ratio, before carbon-rich gas from the outer disc is transported\ninwards elevating the C/O ratio again. However, it is unclear how internal\nphotoevaporation, which carries away gas and opens gaps that block inward\ndrifting pebbles, affects the chemical composition of the disc. We aim to study\nthese effects in discs around solar-like stars, where we especially focus on\nthe C/O ratio and the water content. The simulations are carried out using a\nsemi-analytical 1D disc model. Our code chemcomp includes viscous evolution and\nheating, pebble growth and drift, pebble evaporation and condensation, and a\nsimple chemical partitioning model. We show that internal photoevaporation\nplays a major role in the (chemical) evolution of protoplanetary discs: As it\nopens a gap, inward drifting pebbles are stopped and cannot contribute to the\nvolatile content any more. In addition, gas from the outer disc is carried away\nby photoevaporative winds. Consequently, the C/O ratio in the inner disc is\nlow. In contrast, gaps opened by giant planets allow the gas to pass, resulting\nin an elevated C/O ratio, similar to viscous discs without internal\nphotoevaporation. This will enable us to distinguish observationally between\nthese two scenarios when measuring the C/O ratio, implying that we can infer\nthe cause of gap structures in disc observations. In the case of a\nphotoevaporative disc, we additionally find an elevated water content in the\ninner disc as the water vapour and ice undergo a cycle of\nevaporation/re-condensation, preventing its inward accretion onto the star.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The chemical evolution of protoplanetary discs is not fully understood,\nseveral factors influence the final distribution of disc material. One such\nfactor are inward drifting and evaporating pebbles that enrich the inner disc\nwith vapour. In particular, it is first enriched with water vapour, resulting\nin a low C/O ratio, before carbon-rich gas from the outer disc is transported\ninwards elevating the C/O ratio again. However, it is unclear how internal\nphotoevaporation, which carries away gas and opens gaps that block inward\ndrifting pebbles, affects the chemical composition of the disc. We aim to study\nthese effects in discs around solar-like stars, where we especially focus on\nthe C/O ratio and the water content. The simulations are carried out using a\nsemi-analytical 1D disc model. Our code chemcomp includes viscous evolution and\nheating, pebble growth and drift, pebble evaporation and condensation, and a\nsimple chemical partitioning model. We show that internal photoevaporation\nplays a major role in the (chemical) evolution of protoplanetary discs: As it\nopens a gap, inward drifting pebbles are stopped and cannot contribute to the\nvolatile content any more. In addition, gas from the outer disc is carried away\nby photoevaporative winds. Consequently, the C/O ratio in the inner disc is\nlow. In contrast, gaps opened by giant planets allow the gas to pass, resulting\nin an elevated C/O ratio, similar to viscous discs without internal\nphotoevaporation. This will enable us to distinguish observationally between\nthese two scenarios when measuring the C/O ratio, implying that we can infer\nthe cause of gap structures in disc observations. In the case of a\nphotoevaporative disc, we additionally find an elevated water content in the\ninner disc as the water vapour and ice undergo a cycle of\nevaporation/re-condensation, preventing its inward accretion onto the star."
                },
                "authors": [
                    {
                        "name": "Julia Lena Lienert"
                    },
                    {
                        "name": "Bertram Bitsch"
                    },
                    {
                        "name": "Thomas Henning"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Henning"
                },
                "author": "Thomas Henning",
                "arxiv_doi": "10.1051/0004-6361/202348798",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202348798",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.09342v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09342v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "A&A 691, A72 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16040v3",
                "updated": "2025-02-05T11:32:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    32,
                    34,
                    2,
                    36,
                    0
                ],
                "published": "2024-09-24T12:42:18Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    12,
                    42,
                    18,
                    1,
                    268,
                    0
                ],
                "title": "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of\n  Experts"
                },
                "summary": "Deep learning for time series forecasting has seen significant advancements\nover the past decades. However, despite the success of large-scale pre-training\nin language and vision domains, pre-trained time series models remain limited\nin scale and operate at a high cost, hindering the development of larger\ncapable forecasting models in real-world applications. In response, we\nintroduce Time-MoE, a scalable and unified architecture designed to pre-train\nlarger, more capable forecasting foundation models while reducing inference\ncosts. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE\nenhances computational efficiency by activating only a subset of networks for\neach prediction, reducing computational load while maintaining high model\ncapacity. This allows Time-MoE to scale effectively without a corresponding\nincrease in inference costs. Time-MoE comprises a family of decoder-only\ntransformer models that operate in an auto-regressive manner and support\nflexible forecasting horizons with varying input context lengths. We\npre-trained these models on our newly introduced large-scale data Time-300B,\nwhich spans over 9 domains and encompassing over 300 billion time points. For\nthe first time, we scaled a time series foundation model up to 2.4 billion\nparameters, achieving significantly improved forecasting precision. Our results\nvalidate the applicability of scaling laws for training tokens and model size\nin the context of time series forecasting. Compared to dense models with the\nsame number of activated parameters or equivalent computation budgets, our\nmodels consistently outperform them by large margin. These advancements\nposition Time-MoE as a state-of-the-art solution for tackling real-world time\nseries forecasting challenges with superior capability, efficiency, and\nflexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning for time series forecasting has seen significant advancements\nover the past decades. However, despite the success of large-scale pre-training\nin language and vision domains, pre-trained time series models remain limited\nin scale and operate at a high cost, hindering the development of larger\ncapable forecasting models in real-world applications. In response, we\nintroduce Time-MoE, a scalable and unified architecture designed to pre-train\nlarger, more capable forecasting foundation models while reducing inference\ncosts. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE\nenhances computational efficiency by activating only a subset of networks for\neach prediction, reducing computational load while maintaining high model\ncapacity. This allows Time-MoE to scale effectively without a corresponding\nincrease in inference costs. Time-MoE comprises a family of decoder-only\ntransformer models that operate in an auto-regressive manner and support\nflexible forecasting horizons with varying input context lengths. We\npre-trained these models on our newly introduced large-scale data Time-300B,\nwhich spans over 9 domains and encompassing over 300 billion time points. For\nthe first time, we scaled a time series foundation model up to 2.4 billion\nparameters, achieving significantly improved forecasting precision. Our results\nvalidate the applicability of scaling laws for training tokens and model size\nin the context of time series forecasting. Compared to dense models with the\nsame number of activated parameters or equivalent computation budgets, our\nmodels consistently outperform them by large margin. These advancements\nposition Time-MoE as a state-of-the-art solution for tackling real-world time\nseries forecasting challenges with superior capability, efficiency, and\nflexibility."
                },
                "authors": [
                    {
                        "name": "Xiaoming Shi"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Yuqi Nie"
                    },
                    {
                        "name": "Dianqi Li"
                    },
                    {
                        "name": "Zhou Ye"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Ming Jin"
                    }
                ],
                "author_detail": {
                    "name": "Ming Jin"
                },
                "author": "Ming Jin",
                "arxiv_comment": "Accepted by the 13th International Conference on Learning\n  Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03084v1",
                "updated": "2025-02-05T11:21:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    21,
                    4,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T11:21:04Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    21,
                    4,
                    2,
                    36,
                    0
                ],
                "title": "Inference on varying coefficients in spatial autoregressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on varying coefficients in spatial autoregressions"
                },
                "summary": "We present simple to implement Wald-type statistics that deliver a general\nnonparametric inference theory for linear restrictions on varying coefficients\nin a range of spatial autoregressive models. Our theory covers error dependence\nof a general form, allows for a degree of misspecification robustness via\nnonparametric spatial weights and permits inference on both varying regression\nand spatial coefficients. One application of our method finds evidence for\nconstant returns to scale in the production function of the Chinese nonmetal\nmineral industry, while another finds a nonlinear impact of the distance to the\nemployment center on housing prices in Boston. A simulation study confirms that\nour tests perform well in finite-samples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present simple to implement Wald-type statistics that deliver a general\nnonparametric inference theory for linear restrictions on varying coefficients\nin a range of spatial autoregressive models. Our theory covers error dependence\nof a general form, allows for a degree of misspecification robustness via\nnonparametric spatial weights and permits inference on both varying regression\nand spatial coefficients. One application of our method finds evidence for\nconstant returns to scale in the production function of the Chinese nonmetal\nmineral industry, while another finds a nonlinear impact of the distance to the\nemployment center on housing prices in Boston. A simulation study confirms that\nour tests perform well in finite-samples."
                },
                "authors": [
                    {
                        "name": "Abhimanyu Gupta"
                    },
                    {
                        "name": "Xi Qu"
                    },
                    {
                        "name": "Sorawoot Srisuma"
                    },
                    {
                        "name": "Jiajun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Zhang"
                },
                "author": "Jiajun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03080v1",
                "updated": "2025-02-05T11:14:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    14,
                    20,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T11:14:20Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    14,
                    20,
                    2,
                    36,
                    0
                ],
                "title": "IAO Prompting: Making Knowledge Flow Explicit in LLMs through Structured\n  Reasoning Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAO Prompting: Making Knowledge Flow Explicit in LLMs through Structured\n  Reasoning Templates"
                },
                "summary": "While Large Language Models (LLMs) demonstrate impressive reasoning\ncapabilities, understanding and validating their knowledge utilization remains\nchallenging. Chain-of-thought (CoT) prompting partially addresses this by\nrevealing intermediate reasoning steps, but the knowledge flow and application\nremain implicit. We introduce IAO (Input-Action-Output) prompting, a structured\ntemplate-based method that explicitly models how LLMs access and apply their\nknowledge during complex reasoning tasks. IAO decomposes problems into\nsequential steps, each clearly identifying the input knowledge being used, the\naction being performed, and the resulting output. This structured decomposition\nenables us to trace knowledge flow, verify factual consistency, and identify\npotential knowledge gaps or misapplications. Through experiments across diverse\nreasoning tasks, we demonstrate that IAO not only improves zero-shot\nperformance but also provides transparency in how LLMs leverage their stored\nknowledge. Human evaluation confirms that this structured approach enhances our\nability to verify knowledge utilization and detect potential hallucinations or\nreasoning errors. Our findings provide insights into both knowledge\nrepresentation within LLMs and methods for more reliable knowledge application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) demonstrate impressive reasoning\ncapabilities, understanding and validating their knowledge utilization remains\nchallenging. Chain-of-thought (CoT) prompting partially addresses this by\nrevealing intermediate reasoning steps, but the knowledge flow and application\nremain implicit. We introduce IAO (Input-Action-Output) prompting, a structured\ntemplate-based method that explicitly models how LLMs access and apply their\nknowledge during complex reasoning tasks. IAO decomposes problems into\nsequential steps, each clearly identifying the input knowledge being used, the\naction being performed, and the resulting output. This structured decomposition\nenables us to trace knowledge flow, verify factual consistency, and identify\npotential knowledge gaps or misapplications. Through experiments across diverse\nreasoning tasks, we demonstrate that IAO not only improves zero-shot\nperformance but also provides transparency in how LLMs leverage their stored\nknowledge. Human evaluation confirms that this structured approach enhances our\nability to verify knowledge utilization and detect potential hallucinations or\nreasoning errors. Our findings provide insights into both knowledge\nrepresentation within LLMs and methods for more reliable knowledge application."
                },
                "authors": [
                    {
                        "name": "Aissatou Diallo"
                    },
                    {
                        "name": "Antonis Bikakis"
                    },
                    {
                        "name": "Luke Dickens"
                    },
                    {
                        "name": "Anthony Hunter"
                    },
                    {
                        "name": "Rob Miller"
                    }
                ],
                "author_detail": {
                    "name": "Rob Miller"
                },
                "author": "Rob Miller",
                "arxiv_comment": "Accepted as Oral at KnowFM @ AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03067v1",
                "updated": "2025-02-05T11:00:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    0,
                    51,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T11:00:51Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    0,
                    51,
                    2,
                    36,
                    0
                ],
                "title": "Optimizing Electric Vehicles Charging using Large Language Models and\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Electric Vehicles Charging using Large Language Models and\n  Graph Neural Networks"
                },
                "summary": "Maintaining grid stability amid widespread electric vehicle (EV) adoption is\nvital for sustainable transportation. Traditional optimization methods and\nReinforcement Learning (RL) approaches often struggle with the high\ndimensionality and dynamic nature of real-time EV charging, leading to\nsub-optimal solutions. To address these challenges, this study demonstrates\nthat combining Large Language Models (LLMs), for sequence modeling, with Graph\nNeural Networks (GNNs), for relational information extraction, not only\noutperforms conventional EV smart charging methods, but also paves the way for\nentirely new research directions and innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintaining grid stability amid widespread electric vehicle (EV) adoption is\nvital for sustainable transportation. Traditional optimization methods and\nReinforcement Learning (RL) approaches often struggle with the high\ndimensionality and dynamic nature of real-time EV charging, leading to\nsub-optimal solutions. To address these challenges, this study demonstrates\nthat combining Large Language Models (LLMs), for sequence modeling, with Graph\nNeural Networks (GNNs), for relational information extraction, not only\noutperforms conventional EV smart charging methods, but also paves the way for\nentirely new research directions and innovative solutions."
                },
                "authors": [
                    {
                        "name": "Stavros Orfanoudakis"
                    },
                    {
                        "name": "Peter Palensky"
                    },
                    {
                        "name": "Pedro P. Vergara"
                    }
                ],
                "author_detail": {
                    "name": "Pedro P. Vergara"
                },
                "author": "Pedro P. Vergara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03062v1",
                "updated": "2025-02-05T10:48:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    10,
                    48,
                    12,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T10:48:12Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    10,
                    48,
                    12,
                    2,
                    36,
                    0
                ],
                "title": "Time Series Anomaly Detection in the Frequency Domain with Statistical\n  Reliability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Anomaly Detection in the Frequency Domain with Statistical\n  Reliability"
                },
                "summary": "Effective anomaly detection in complex systems requires identifying change\npoints (CPs) in the frequency domain, as abnormalities often arise across\nmultiple frequencies. This paper extends recent advancements in statistically\nsignificant CP detection, based on Selective Inference (SI), to the frequency\ndomain. The proposed SI method quantifies the statistical significance of\ndetected CPs in the frequency domain using $p$-values, ensuring that the\ndetected changes reflect genuine structural shifts in the target system. We\naddress two major technical challenges to achieve this. First, we extend the\nexisting SI framework to the frequency domain by appropriately utilizing the\nproperties of discrete Fourier transform (DFT). Second, we develop an SI method\nthat provides valid $p$-values for CPs where changes occur across multiple\nfrequencies. Experimental results demonstrate that the proposed method reliably\nidentifies genuine CPs with strong statistical guarantees, enabling more\naccurate root-cause analysis in the frequency domain of complex systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective anomaly detection in complex systems requires identifying change\npoints (CPs) in the frequency domain, as abnormalities often arise across\nmultiple frequencies. This paper extends recent advancements in statistically\nsignificant CP detection, based on Selective Inference (SI), to the frequency\ndomain. The proposed SI method quantifies the statistical significance of\ndetected CPs in the frequency domain using $p$-values, ensuring that the\ndetected changes reflect genuine structural shifts in the target system. We\naddress two major technical challenges to achieve this. First, we extend the\nexisting SI framework to the frequency domain by appropriately utilizing the\nproperties of discrete Fourier transform (DFT). Second, we develop an SI method\nthat provides valid $p$-values for CPs where changes occur across multiple\nfrequencies. Experimental results demonstrate that the proposed method reliably\nidentifies genuine CPs with strong statistical guarantees, enabling more\naccurate root-cause analysis in the frequency domain of complex systems."
                },
                "authors": [
                    {
                        "name": "Akifumi Yamada"
                    },
                    {
                        "name": "Tomohiro Shiraishi"
                    },
                    {
                        "name": "Shuichi Nishino"
                    },
                    {
                        "name": "Teruyuki Katsuoka"
                    },
                    {
                        "name": "Kouichi Taji"
                    },
                    {
                        "name": "Ichiro Takeuchi"
                    }
                ],
                "author_detail": {
                    "name": "Ichiro Takeuchi"
                },
                "author": "Ichiro Takeuchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01208v2",
                "updated": "2025-02-05T10:47:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    10,
                    47,
                    19,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-03T09:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "title": "Almost Surely Safe Alignment of Large Language Models at Inference-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Almost Surely Safe Alignment of Large Language Models at Inference-Time"
                },
                "summary": "Even highly capable large language models (LLMs) can produce biased or unsafe\nresponses, and alignment techniques, such as RLHF, aimed at mitigating this\nissue, are expensive and prone to overfitting as they retrain the LLM. This\npaper introduces a novel inference-time alignment approach that ensures LLMs\ngenerate safe responses almost surely, i.e., with a probability approaching\none. We achieve this by framing the safe generation of inference-time responses\nas a constrained Markov decision process within the LLM's latent space.\nCrucially, we augment a safety state that tracks the evolution of safety\nconstraints and enables us to demonstrate formal safety guarantees upon solving\nthe MDP in the latent space. Building on this foundation, we propose\nInferenceGuard, a practical implementation that safely aligns LLMs without\nmodifying the model weights. Empirically, we demonstrate InferenceGuard\neffectively balances safety and task performance, outperforming existing\ninference-time alignment methods in generating safe and aligned responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even highly capable large language models (LLMs) can produce biased or unsafe\nresponses, and alignment techniques, such as RLHF, aimed at mitigating this\nissue, are expensive and prone to overfitting as they retrain the LLM. This\npaper introduces a novel inference-time alignment approach that ensures LLMs\ngenerate safe responses almost surely, i.e., with a probability approaching\none. We achieve this by framing the safe generation of inference-time responses\nas a constrained Markov decision process within the LLM's latent space.\nCrucially, we augment a safety state that tracks the evolution of safety\nconstraints and enables us to demonstrate formal safety guarantees upon solving\nthe MDP in the latent space. Building on this foundation, we propose\nInferenceGuard, a practical implementation that safely aligns LLMs without\nmodifying the model weights. Empirically, we demonstrate InferenceGuard\neffectively balances safety and task performance, outperforming existing\ninference-time alignment methods in generating safe and aligned responses."
                },
                "authors": [
                    {
                        "name": "Xiaotong Ji"
                    },
                    {
                        "name": "Shyam Sundhar Ramesh"
                    },
                    {
                        "name": "Matthieu Zimmer"
                    },
                    {
                        "name": "Ilija Bogunovic"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haitham Bou Ammar"
                    }
                ],
                "author_detail": {
                    "name": "Haitham Bou Ammar"
                },
                "author": "Haitham Bou Ammar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19054v2",
                "updated": "2025-02-05T10:43:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    10,
                    43,
                    26,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-31T11:28:16Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    28,
                    16,
                    4,
                    31,
                    0
                ],
                "title": "Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models"
                },
                "summary": "Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively."
                },
                "authors": [
                    {
                        "name": "Ruiyu Wang"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Shizhao Sun"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03052v1",
                "updated": "2025-02-05T10:29:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    10,
                    29,
                    54,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T10:29:54Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    10,
                    29,
                    54,
                    2,
                    36,
                    0
                ],
                "title": "Understanding and Enhancing the Transferability of Jailbreaking Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Enhancing the Transferability of Jailbreaking Attacks"
                },
                "summary": "Jailbreaking attacks can effectively manipulate open-source large language\nmodels (LLMs) to produce harmful responses. However, these attacks exhibit\nlimited transferability, failing to disrupt proprietary LLMs consistently. To\nreliably identify vulnerabilities in proprietary LLMs, this work investigates\nthe transferability of jailbreaking attacks by analysing their impact on the\nmodel's intent perception. By incorporating adversarial sequences, these\nattacks can redirect the source LLM's focus away from malicious-intent tokens\nin the original input, thereby obstructing the model's intent recognition and\neliciting harmful responses. Nevertheless, these adversarial sequences fail to\nmislead the target LLM's intent perception, allowing the target LLM to refocus\non malicious-intent tokens and abstain from responding. Our analysis further\nreveals the inherent distributional dependency within the generated adversarial\nsequences, whose effectiveness stems from overfitting the source LLM's\nparameters, resulting in limited transferability to target LLMs. To this end,\nwe propose the Perceived-importance Flatten (PiF) method, which uniformly\ndisperses the model's focus across neutral-intent tokens in the original input,\nthus obscuring malicious-intent tokens without relying on overfitted\nadversarial sequences. Extensive experiments demonstrate that PiF provides an\neffective and efficient red-teaming evaluation for proprietary LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking attacks can effectively manipulate open-source large language\nmodels (LLMs) to produce harmful responses. However, these attacks exhibit\nlimited transferability, failing to disrupt proprietary LLMs consistently. To\nreliably identify vulnerabilities in proprietary LLMs, this work investigates\nthe transferability of jailbreaking attacks by analysing their impact on the\nmodel's intent perception. By incorporating adversarial sequences, these\nattacks can redirect the source LLM's focus away from malicious-intent tokens\nin the original input, thereby obstructing the model's intent recognition and\neliciting harmful responses. Nevertheless, these adversarial sequences fail to\nmislead the target LLM's intent perception, allowing the target LLM to refocus\non malicious-intent tokens and abstain from responding. Our analysis further\nreveals the inherent distributional dependency within the generated adversarial\nsequences, whose effectiveness stems from overfitting the source LLM's\nparameters, resulting in limited transferability to target LLMs. To this end,\nwe propose the Perceived-importance Flatten (PiF) method, which uniformly\ndisperses the model's focus across neutral-intent tokens in the original input,\nthus obscuring malicious-intent tokens without relying on overfitted\nadversarial sequences. Extensive experiments demonstrate that PiF provides an\neffective and efficient red-teaming evaluation for proprietary LLMs."
                },
                "authors": [
                    {
                        "name": "Runqi Lin"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Fengwang Li"
                    },
                    {
                        "name": "Tongling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tongling Liu"
                },
                "author": "Tongling Liu",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05498v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05498v3",
                "updated": "2025-02-05T10:29:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    10,
                    29,
                    7,
                    2,
                    36,
                    0
                ],
                "published": "2024-06-08T15:45:31Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    15,
                    45,
                    31,
                    5,
                    160,
                    0
                ],
                "title": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a\n  Practical Manner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a\n  Practical Manner"
                },
                "summary": "Jailbreaking is an emerging adversarial attack that bypasses the safety\nalignment deployed in off-the-shelf large language models (LLMs) and has\nevolved into multiple categories: human-based, optimization-based,\ngeneration-based, and the recent indirect and multilingual jailbreaks. However,\ndelivering a practical jailbreak defense is challenging because it needs to not\nonly handle all the above jailbreak attacks but also incur negligible delays to\nuser prompts, as well as be compatible with both open-source and closed-source\nLLMs. Inspired by how the traditional security concept of shadow stacks defends\nagainst memory overflow attacks, this paper introduces a generic LLM jailbreak\ndefense framework called SelfDefend, which establishes a shadow LLM as a\ndefense instance (in detection state) to concurrently protect the target LLM\ninstance (in normal answering state) in the normal stack and collaborate with\nit for checkpoint-based access control. The effectiveness of SelfDefend builds\nupon our observation that existing LLMs can identify harmful prompts or\nintentions in user queries, which we empirically validate using mainstream\nGPT-3.5/4 models against major jailbreak attacks. To further improve the\ndefense's robustness and minimize costs, we employ a data distillation approach\nto tune dedicated open-source defense models. When deployed to protect\nGPT-3.5/4, Claude, Llama-2-7b/13b, and Mistral, these models outperform seven\nstate-of-the-art defenses and match the performance of GPT-4-based SelfDefend,\nwith significantly lower extra delays. Further experiments show that the tuned\nmodels are robust to adaptive jailbreaks and prompt injections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking is an emerging adversarial attack that bypasses the safety\nalignment deployed in off-the-shelf large language models (LLMs) and has\nevolved into multiple categories: human-based, optimization-based,\ngeneration-based, and the recent indirect and multilingual jailbreaks. However,\ndelivering a practical jailbreak defense is challenging because it needs to not\nonly handle all the above jailbreak attacks but also incur negligible delays to\nuser prompts, as well as be compatible with both open-source and closed-source\nLLMs. Inspired by how the traditional security concept of shadow stacks defends\nagainst memory overflow attacks, this paper introduces a generic LLM jailbreak\ndefense framework called SelfDefend, which establishes a shadow LLM as a\ndefense instance (in detection state) to concurrently protect the target LLM\ninstance (in normal answering state) in the normal stack and collaborate with\nit for checkpoint-based access control. The effectiveness of SelfDefend builds\nupon our observation that existing LLMs can identify harmful prompts or\nintentions in user queries, which we empirically validate using mainstream\nGPT-3.5/4 models against major jailbreak attacks. To further improve the\ndefense's robustness and minimize costs, we employ a data distillation approach\nto tune dedicated open-source defense models. When deployed to protect\nGPT-3.5/4, Claude, Llama-2-7b/13b, and Mistral, these models outperform seven\nstate-of-the-art defenses and match the performance of GPT-4-based SelfDefend,\nwith significantly lower extra delays. Further experiments show that the tuned\nmodels are robust to adaptive jailbreaks and prompt injections."
                },
                "authors": [
                    {
                        "name": "Xunguang Wang"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Zhenlan Ji"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yingjiu Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Juergen Rahmel"
                    }
                ],
                "author_detail": {
                    "name": "Juergen Rahmel"
                },
                "author": "Juergen Rahmel",
                "arxiv_comment": "Accepted by USENIX Security Symposium 2025. Please cite the\n  conference version of this paper, i.e., \"Xunguang Wang, Daoyuan Wu, Zhenlan\n  Ji, Zongjie Li, Pingchuan Ma, Shuai Wang, Yingjiu Li, Yang Liu, Ning Liu, and\n  Juergen Rahmel. SelfDefend: LLMs Can Defend Themselves against Jailbreaking\n  in a Practical Manner. In Proc. USENIX Security, 2025.\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05498v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05498v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03042v1",
                "updated": "2025-02-05T09:59:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    59,
                    28,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T09:59:28Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    59,
                    28,
                    2,
                    36,
                    0
                ],
                "title": "Energy Diffusion and Advection Coefficients in Kinetic Simulations of\n  Relativistic Plasma Turbulence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Diffusion and Advection Coefficients in Kinetic Simulations of\n  Relativistic Plasma Turbulence"
                },
                "summary": "Turbulent, relativistic nonthermal plasmas are ubiquitous in high-energy\nastrophysical systems, as inferred from broadband nonthermal emission spectra.\nThe underlying turbulent nonthermal particle acceleration (NTPA) processes have\ntraditionally been modelled with a Fokker-Planck (FP) diffusion-advection\nequation for the particle energy distribution. We test FP-type NTPA theories by\nperforming and analysing particle-in-cell (PIC) simulations of turbulence in\ncollisionless relativistic pair plasma. By tracking large numbers of particles\nin simulations with different initial magnetisation and system size, we first\ntest and confirm the applicability of the FP framework. We then measure the FP\nenergy diffusion ($D$) and advection ($A$) coefficients as functions of\nparticle energy $\\gamma m c^2$, and compare their dependence to theoretical\npredictions. At high energies, we robustly find $D \\sim \\gamma^2$ for all\ncases. Hence, we fit $D = D_0 \\gamma^2$ and find a scaling consistent with $D_0\n\\sim \\sigma^{3/2}$ at low instantaneous magnetisation $\\sigma(t)$, flattening\nto $D_0 \\sim \\sigma$ at higher $\\sigma \\sim 1$. We also find that the power-law\nindex $\\alpha(t)$ of the particle energy distribution converges exponentially\nin time. We build and test an analytic model connecting the FP coefficients and\n$\\alpha(t)$, predicting $A(\\gamma) \\sim \\gamma \\log \\gamma$. We confirm this\nfunctional form in our measurements of $A(\\gamma,t)$, which allows us to\npredict $\\alpha(t)$ through the model relations. Our results suggest that the\nbasic second-order Fermi acceleration model, which predicts $D_0 \\sim \\sigma$,\nmay not be a complete description of NTPA in turbulent plasmas. These findings\nencourage further application of tracked particles and FP coefficients as a\ndiagnostic in kinetic simulations of various astrophysically relevant plasma\nprocesses like collisionless shocks and magnetic reconnection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turbulent, relativistic nonthermal plasmas are ubiquitous in high-energy\nastrophysical systems, as inferred from broadband nonthermal emission spectra.\nThe underlying turbulent nonthermal particle acceleration (NTPA) processes have\ntraditionally been modelled with a Fokker-Planck (FP) diffusion-advection\nequation for the particle energy distribution. We test FP-type NTPA theories by\nperforming and analysing particle-in-cell (PIC) simulations of turbulence in\ncollisionless relativistic pair plasma. By tracking large numbers of particles\nin simulations with different initial magnetisation and system size, we first\ntest and confirm the applicability of the FP framework. We then measure the FP\nenergy diffusion ($D$) and advection ($A$) coefficients as functions of\nparticle energy $\\gamma m c^2$, and compare their dependence to theoretical\npredictions. At high energies, we robustly find $D \\sim \\gamma^2$ for all\ncases. Hence, we fit $D = D_0 \\gamma^2$ and find a scaling consistent with $D_0\n\\sim \\sigma^{3/2}$ at low instantaneous magnetisation $\\sigma(t)$, flattening\nto $D_0 \\sim \\sigma$ at higher $\\sigma \\sim 1$. We also find that the power-law\nindex $\\alpha(t)$ of the particle energy distribution converges exponentially\nin time. We build and test an analytic model connecting the FP coefficients and\n$\\alpha(t)$, predicting $A(\\gamma) \\sim \\gamma \\log \\gamma$. We confirm this\nfunctional form in our measurements of $A(\\gamma,t)$, which allows us to\npredict $\\alpha(t)$ through the model relations. Our results suggest that the\nbasic second-order Fermi acceleration model, which predicts $D_0 \\sim \\sigma$,\nmay not be a complete description of NTPA in turbulent plasmas. These findings\nencourage further application of tracked particles and FP coefficients as a\ndiagnostic in kinetic simulations of various astrophysically relevant plasma\nprocesses like collisionless shocks and magnetic reconnection."
                },
                "authors": [
                    {
                        "name": "Kai W. Wong"
                    },
                    {
                        "name": "Vladimir Zhdankin"
                    },
                    {
                        "name": "Dmitri A. Uzdensky"
                    },
                    {
                        "name": "Gregory R. Werner"
                    },
                    {
                        "name": "Mitchell C. Begelman"
                    }
                ],
                "author_detail": {
                    "name": "Mitchell C. Begelman"
                },
                "arxiv_affiliation": "JILA, Univ. Colorado & NIST",
                "author": "Mitchell C. Begelman",
                "arxiv_comment": "22 pages, 24 figures, submitted for publication. Comments are\n  welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03041v1",
                "updated": "2025-02-05T09:56:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    56,
                    52,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T09:56:52Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    56,
                    52,
                    2,
                    36,
                    0
                ],
                "title": "Large Language Models Are Universal Recommendation Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Universal Recommendation Learners"
                },
                "summary": "In real-world recommender systems, different tasks are typically addressed\nusing supervised learning on task-specific datasets with carefully designed\nmodel architectures. We demonstrate that large language models (LLMs) can\nfunction as universal recommendation learners, capable of handling multiple\ntasks within a unified input-output framework, eliminating the need for\nspecialized model designs. To improve the recommendation performance of LLMs,\nwe introduce a multimodal fusion module for item representation and a\nsequence-in-set-out approach for efficient candidate generation. When applied\nto industrial-scale data, our LLM achieves competitive results with expert\nmodels elaborately designed for different recommendation tasks. Furthermore,\nour analysis reveals that recommendation outcomes are highly sensitive to text\ninput, highlighting the potential of prompt engineering in optimizing\nindustrial-scale recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world recommender systems, different tasks are typically addressed\nusing supervised learning on task-specific datasets with carefully designed\nmodel architectures. We demonstrate that large language models (LLMs) can\nfunction as universal recommendation learners, capable of handling multiple\ntasks within a unified input-output framework, eliminating the need for\nspecialized model designs. To improve the recommendation performance of LLMs,\nwe introduce a multimodal fusion module for item representation and a\nsequence-in-set-out approach for efficient candidate generation. When applied\nto industrial-scale data, our LLM achieves competitive results with expert\nmodels elaborately designed for different recommendation tasks. Furthermore,\nour analysis reveals that recommendation outcomes are highly sensitive to text\ninput, highlighting the potential of prompt engineering in optimizing\nindustrial-scale recommender systems."
                },
                "authors": [
                    {
                        "name": "Junguang Jiang"
                    },
                    {
                        "name": "Yanwen Huang"
                    },
                    {
                        "name": "Bin Liu"
                    },
                    {
                        "name": "Xiaoyu Kong"
                    },
                    {
                        "name": "Ziru Xu"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2110.06250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2110.06250v2",
                "updated": "2025-02-05T09:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    53,
                    31,
                    2,
                    36,
                    0
                ],
                "published": "2021-10-12T18:04:48Z",
                "published_parsed": [
                    2021,
                    10,
                    12,
                    18,
                    4,
                    48,
                    1,
                    285,
                    0
                ],
                "title": "On the Minimum Attainable Risk in Permutation Invariant Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Minimum Attainable Risk in Permutation Invariant Problems"
                },
                "summary": "We consider a broad class of permutation invariant statistical problems by\nextending the standard decision theoretic definition to allow also selective\ninference tasks, where the target is specified only after seeing the data. For\nany such problem we show that, among all permutation invariant procedures, the\nminimizer of the risk at $\\boldsymbol{\\theta}$ is precisely the rule that\nminimizes the Bayes risk under a (postulated) discrete prior assigning equal\nprobability to every permutation of $\\boldsymbol{\\theta}$. This gives an\nexplicit characterization of the greatest lower bound on the risk of every\nsensible procedure in a wide range of problems. Furthermore, in a permutation\ninvariant problem of estimating the parameter of a selected population under\nsquared loss, we prove that this lower bound coincides asymptotically with a\nsimpler lower bound, attained by the Bayes solution that replaces the\naforementioned uniform prior on all permutations of $\\boldsymbol{\\theta}$ by\nthe i.i.d. prior with the same marginals. This has important algorithmic\nimplications because it suggests that our greatest lower bound is\nasymptotically attainable uniformly in $\\boldsymbol{\\theta}$ by an empirical\nBayes procedure. Altogether, the above extends theory that has been established\nin the existing literature only for the very special case of compound decision\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a broad class of permutation invariant statistical problems by\nextending the standard decision theoretic definition to allow also selective\ninference tasks, where the target is specified only after seeing the data. For\nany such problem we show that, among all permutation invariant procedures, the\nminimizer of the risk at $\\boldsymbol{\\theta}$ is precisely the rule that\nminimizes the Bayes risk under a (postulated) discrete prior assigning equal\nprobability to every permutation of $\\boldsymbol{\\theta}$. This gives an\nexplicit characterization of the greatest lower bound on the risk of every\nsensible procedure in a wide range of problems. Furthermore, in a permutation\ninvariant problem of estimating the parameter of a selected population under\nsquared loss, we prove that this lower bound coincides asymptotically with a\nsimpler lower bound, attained by the Bayes solution that replaces the\naforementioned uniform prior on all permutations of $\\boldsymbol{\\theta}$ by\nthe i.i.d. prior with the same marginals. This has important algorithmic\nimplications because it suggests that our greatest lower bound is\nasymptotically attainable uniformly in $\\boldsymbol{\\theta}$ by an empirical\nBayes procedure. Altogether, the above extends theory that has been established\nin the existing literature only for the very special case of compound decision\nproblems."
                },
                "authors": [
                    {
                        "name": "Asaf Weinstein"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Weinstein"
                },
                "author": "Asaf Weinstein",
                "arxiv_comment": "1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2110.06250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2110.06250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62C05, 62C12, 62C25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03034v1",
                "updated": "2025-02-05T09:43:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    43,
                    14,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T09:43:14Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    43,
                    14,
                    2,
                    36,
                    0
                ],
                "title": "Knowledge Distillation from Large Language Models for Household Energy\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation from Large Language Models for Household Energy\n  Modeling"
                },
                "summary": "Machine learning (ML) is increasingly vital for smart-grid research, yet\nrestricted access to realistic, diverse data - often due to privacy concerns -\nslows progress and fuels doubts within the energy sector about adopting\nML-based strategies. We propose integrating Large Language Models (LLMs) in\nenergy modeling to generate realistic, culturally sensitive, and\nbehavior-specific data for household energy usage across diverse geographies.\nIn this study, we employ and compare five different LLMs to systematically\nproduce family structures, weather patterns, and daily consumption profiles for\nhouseholds in six distinct countries. A four-stage methodology synthesizes\ncontextual daily data, including culturally nuanced activities, realistic\nweather ranges, HVAC operations, and distinct `energy signatures' that capture\nunique consumption footprints. Additionally, we explore an alternative strategy\nwhere external weather datasets can be directly integrated, bypassing\nintermediate weather modeling stages while ensuring physically consistent data\ninputs. The resulting dataset provides insights into how cultural, climatic,\nand behavioral factors converge to shape carbon emissions, offering a\ncost-effective avenue for scenario-based energy optimization. This approach\nunderscores how prompt engineering, combined with knowledge distillation, can\nadvance sustainable energy research and climate mitigation efforts. Source code\nis available at\nhttps://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) is increasingly vital for smart-grid research, yet\nrestricted access to realistic, diverse data - often due to privacy concerns -\nslows progress and fuels doubts within the energy sector about adopting\nML-based strategies. We propose integrating Large Language Models (LLMs) in\nenergy modeling to generate realistic, culturally sensitive, and\nbehavior-specific data for household energy usage across diverse geographies.\nIn this study, we employ and compare five different LLMs to systematically\nproduce family structures, weather patterns, and daily consumption profiles for\nhouseholds in six distinct countries. A four-stage methodology synthesizes\ncontextual daily data, including culturally nuanced activities, realistic\nweather ranges, HVAC operations, and distinct `energy signatures' that capture\nunique consumption footprints. Additionally, we explore an alternative strategy\nwhere external weather datasets can be directly integrated, bypassing\nintermediate weather modeling stages while ensuring physically consistent data\ninputs. The resulting dataset provides insights into how cultural, climatic,\nand behavioral factors converge to shape carbon emissions, offering a\ncost-effective avenue for scenario-based energy optimization. This approach\nunderscores how prompt engineering, combined with knowledge distillation, can\nadvance sustainable energy research and climate mitigation efforts. Source code\nis available at\nhttps://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation ."
                },
                "authors": [
                    {
                        "name": "Mohannad Takrouri"
                    },
                    {
                        "name": "Nicols M. Cuadrado"
                    },
                    {
                        "name": "Martin Tak"
                    }
                ],
                "author_detail": {
                    "name": "Martin Tak"
                },
                "author": "Martin Tak",
                "arxiv_comment": "Source code is available at\n  https://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10038v2",
                "updated": "2025-02-05T09:40:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    40,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2024-12-13T10:59:28Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    10,
                    59,
                    28,
                    4,
                    348,
                    0
                ],
                "title": "Stochastic Variational Inference for Structured Additive Distributional\n  Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Variational Inference for Structured Additive Distributional\n  Regression"
                },
                "summary": "In structured additive distributional regression, the conditional\ndistribution of the response variables given the covariate information and the\nvector of model parameters is modelled using a P-parametric probability density\nfunction where each parameter is modelled through a linear predictor and a\nbijective response function that maps the domain of the predictor into the\ndomain of the parameter. We present a method to perform inference in structured\nadditive distributional regression using stochastic variational inference. We\npropose two strategies for constructing a multivariate Gaussian variational\ndistribution to estimate the posterior distribution of the regression\ncoefficients. The first strategy leverages covariate information and\nhyperparameters to learn both the location vector and the precision matrix. The\nsecond strategy tackles the complexity challenges of the first by initially\nassuming independence among all smooth terms and then introducing correlations\nthrough an additional set of variational parameters. Furthermore, we present\ntwo approaches for estimating the smoothing parameters. The first treats them\nas free parameters and provides point estimates, while the second accounts for\nuncertainty by applying a variational approximation to the posterior\ndistribution. Our model was benchmarked against state-of-the-art competitors in\nlogistic and gamma regression simulation studies. Finally, we validated our\napproach by comparing its posterior estimates to those obtained using Markov\nChain Monte Carlo on a dataset of patents from the biotechnology/pharmaceutics\nand semiconductor/computer sectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In structured additive distributional regression, the conditional\ndistribution of the response variables given the covariate information and the\nvector of model parameters is modelled using a P-parametric probability density\nfunction where each parameter is modelled through a linear predictor and a\nbijective response function that maps the domain of the predictor into the\ndomain of the parameter. We present a method to perform inference in structured\nadditive distributional regression using stochastic variational inference. We\npropose two strategies for constructing a multivariate Gaussian variational\ndistribution to estimate the posterior distribution of the regression\ncoefficients. The first strategy leverages covariate information and\nhyperparameters to learn both the location vector and the precision matrix. The\nsecond strategy tackles the complexity challenges of the first by initially\nassuming independence among all smooth terms and then introducing correlations\nthrough an additional set of variational parameters. Furthermore, we present\ntwo approaches for estimating the smoothing parameters. The first treats them\nas free parameters and provides point estimates, while the second accounts for\nuncertainty by applying a variational approximation to the posterior\ndistribution. Our model was benchmarked against state-of-the-art competitors in\nlogistic and gamma regression simulation studies. Finally, we validated our\napproach by comparing its posterior estimates to those obtained using Markov\nChain Monte Carlo on a dataset of patents from the biotechnology/pharmaceutics\nand semiconductor/computer sectors."
                },
                "authors": [
                    {
                        "name": "Gianmarco Callegher"
                    },
                    {
                        "name": "Thomas Kneib"
                    },
                    {
                        "name": "Johannes Sding"
                    },
                    {
                        "name": "Paul Wiemann"
                    }
                ],
                "author_detail": {
                    "name": "Paul Wiemann"
                },
                "author": "Paul Wiemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v2",
                "updated": "2025-02-05T09:35:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    35,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03030v1",
                "updated": "2025-02-05T09:33:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    33,
                    42,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T09:33:42Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    33,
                    42,
                    2,
                    36,
                    0
                ],
                "title": "Rank-Based Identification of High-dimensional Surrogate Markers:\n  Application to Vaccinology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rank-Based Identification of High-dimensional Surrogate Markers:\n  Application to Vaccinology"
                },
                "summary": "In vaccine trials with long-term participant follow-up, it is of great\nimportance to identify surrogate markers that accurately infer long-term immune\nresponses. These markers offer practical advantages such as providing early,\nindirect evidence of vaccine efficacy, and can accelerate vaccine development\nwhile identifying potential biomarkers. High-throughput technologies like\nRNA-sequencing have emerged as promising tools for understanding complex\nbiological systems and informing new treatment strategies. However, these data\nare high-dimensional, presenting unique statistical challenges for existing\nsurrogate marker identification methods. We introduce Rank-based Identification\nof high-dimensional SurrogatE Markers (RISE), a novel approach designed for\nsmall sample, high-dimensional settings typical in modern vaccine experiments.\nRISE employs a non-parametric univariate test to screen variables for promising\ncandidates, followed by surrogate evaluation on independent data. Our\nsimulation studies demonstrate RISE's desirable properties, including type one\nerror rate control and empirical power under various conditions. Applying RISE\nto a clinical trial for inactivated influenza vaccination, we sought to\nidentify genes whose post-vaccination expression could serve as a surrogate for\nthe induced immune response. This analysis revealed a signature of genes whose\ncombined expression at 1 day post-injection appears to be a reasonable\nsurrogate for the neutralising antibody titres at 28 days after vaccination.\nPathways related to innate antiviral signalling and interferon stimulation were\nstrongly represented in this derived surrogate, providing a clear immunological\ninterpretation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In vaccine trials with long-term participant follow-up, it is of great\nimportance to identify surrogate markers that accurately infer long-term immune\nresponses. These markers offer practical advantages such as providing early,\nindirect evidence of vaccine efficacy, and can accelerate vaccine development\nwhile identifying potential biomarkers. High-throughput technologies like\nRNA-sequencing have emerged as promising tools for understanding complex\nbiological systems and informing new treatment strategies. However, these data\nare high-dimensional, presenting unique statistical challenges for existing\nsurrogate marker identification methods. We introduce Rank-based Identification\nof high-dimensional SurrogatE Markers (RISE), a novel approach designed for\nsmall sample, high-dimensional settings typical in modern vaccine experiments.\nRISE employs a non-parametric univariate test to screen variables for promising\ncandidates, followed by surrogate evaluation on independent data. Our\nsimulation studies demonstrate RISE's desirable properties, including type one\nerror rate control and empirical power under various conditions. Applying RISE\nto a clinical trial for inactivated influenza vaccination, we sought to\nidentify genes whose post-vaccination expression could serve as a surrogate for\nthe induced immune response. This analysis revealed a signature of genes whose\ncombined expression at 1 day post-injection appears to be a reasonable\nsurrogate for the neutralising antibody titres at 28 days after vaccination.\nPathways related to innate antiviral signalling and interferon stimulation were\nstrongly represented in this derived surrogate, providing a clear immunological\ninterpretation."
                },
                "authors": [
                    {
                        "name": "Arthur Hughes"
                    },
                    {
                        "name": "Layla Parast"
                    },
                    {
                        "name": "Rodolphe Thibaut"
                    },
                    {
                        "name": "Boris P. Hejblum"
                    }
                ],
                "author_detail": {
                    "name": "Boris P. Hejblum"
                },
                "author": "Boris P. Hejblum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03029v1",
                "updated": "2025-02-05T09:31:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    31,
                    27,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T09:31:27Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    31,
                    27,
                    2,
                    36,
                    0
                ],
                "title": "On Zero-Initialized Attention: Optimal Prompt and Gating Factor\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Zero-Initialized Attention: Optimal Prompt and Gating Factor\n  Estimation"
                },
                "summary": "The LLaMA-Adapter has recently emerged as an efficient fine-tuning technique\nfor LLaMA models, leveraging zero-initialized attention to stabilize training\nand enhance performance. However, despite its empirical success, the\ntheoretical foundations of zero-initialized attention remain largely\nunexplored. In this paper, we provide a rigorous theoretical analysis,\nestablishing a connection between zero-initialized attention and\nmixture-of-expert models. We prove that both linear and non-linear prompts,\nalong with gating functions, can be optimally estimated, with non-linear\nprompts offering greater flexibility for future applications. Empirically, we\nvalidate our findings on the open LLM benchmarks, demonstrating that non-linear\nprompts outperform linear ones. Notably, even with limited training data, both\nprompt types consistently surpass vanilla attention, highlighting the\nrobustness and adaptability of zero-initialized attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLaMA-Adapter has recently emerged as an efficient fine-tuning technique\nfor LLaMA models, leveraging zero-initialized attention to stabilize training\nand enhance performance. However, despite its empirical success, the\ntheoretical foundations of zero-initialized attention remain largely\nunexplored. In this paper, we provide a rigorous theoretical analysis,\nestablishing a connection between zero-initialized attention and\nmixture-of-expert models. We prove that both linear and non-linear prompts,\nalong with gating functions, can be optimally estimated, with non-linear\nprompts offering greater flexibility for future applications. Empirically, we\nvalidate our findings on the open LLM benchmarks, demonstrating that non-linear\nprompts outperform linear ones. Notably, even with limited training data, both\nprompt types consistently surpass vanilla attention, highlighting the\nrobustness and adaptability of zero-initialized attention."
                },
                "authors": [
                    {
                        "name": "Nghiem T. Diep"
                    },
                    {
                        "name": "Huy Nguyen"
                    },
                    {
                        "name": "Chau Nguyen"
                    },
                    {
                        "name": "Minh Le"
                    },
                    {
                        "name": "Duy M. H. Nguyen"
                    },
                    {
                        "name": "Daniel Sonntag"
                    },
                    {
                        "name": "Mathias Niepert"
                    },
                    {
                        "name": "Nhat Ho"
                    }
                ],
                "author_detail": {
                    "name": "Nhat Ho"
                },
                "author": "Nhat Ho",
                "arxiv_comment": "43 pages, 5 tables, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03021v1",
                "updated": "2025-02-05T09:24:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    24,
                    53,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T09:24:53Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    24,
                    53,
                    2,
                    36,
                    0
                ],
                "title": "More is better: Strong constraints on the stellar properties of LEGA-C z\n  ~ 1 galaxies with Prospector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More is better: Strong constraints on the stellar properties of LEGA-C z\n  ~ 1 galaxies with Prospector"
                },
                "summary": "We present the stellar properties of 2908 galaxies at 0.6 < z < 1.0 from the\nLEGA-C survey. We emphasize the importance of high signal-to-noise, high\nspectral resolution spectroscopy in the inference of stellar population\nproperties of galaxies. We estimate the galaxy properties with the SED fitting\ncode Prospector, by fitting spectroscopy and broadband photometry together,\ndrawn from the LEGA-C DR3 and UltraVISTA catalogs respectively. We report a\npositive correlation between light-weighted ages and stellar velocity\ndispersion ($\\sigma_\\star$). The trend with $\\sigma_\\star$ is weaker for the\nmass-weighted ages and stellar metallicity ($Z_\\star$). On average, quiescent\ngalaxies are characterized by high $Z_\\star$, they are \\sim 1.1 Gyr older, less\ndusty, with steeper dust attenuation slopes compared to star-forming galaxies.\nConversely, star-forming galaxies are characterized by significantly higher\ndust optical depths and shallower (grayer) attenuation slopes. Low mass (high\nmass) star-forming galaxies have lower (higher) $Z_\\star$, while their stellar\npopulations are on average younger (older). A key pragmatic result of our study\nis that a linear-space metallicity prior is preferable to a logarithmic-space\none when using photometry alone, as the latter biases the posteriors downward.\nSpectroscopy greatly improves stellar population measurements and is required\nto provide meaningful constraints on age, metallicity, and other properties.\nPairing spectroscopy with photometry helps resolving the dust-age-metallicity\ndegeneracy, yielding more accurate mass- and light-weighted ages, with ages\ninferred from photometry alone suffering such large uncertainties. Stellar\nmetallicities are constrained by our spectroscopy, but precise measurements\nremain challenging (and impossible with photometry alone), particularly in the\nabsence of Mg and Fe lines redward of 5000 $\\AA$ in the observed spectrum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the stellar properties of 2908 galaxies at 0.6 < z < 1.0 from the\nLEGA-C survey. We emphasize the importance of high signal-to-noise, high\nspectral resolution spectroscopy in the inference of stellar population\nproperties of galaxies. We estimate the galaxy properties with the SED fitting\ncode Prospector, by fitting spectroscopy and broadband photometry together,\ndrawn from the LEGA-C DR3 and UltraVISTA catalogs respectively. We report a\npositive correlation between light-weighted ages and stellar velocity\ndispersion ($\\sigma_\\star$). The trend with $\\sigma_\\star$ is weaker for the\nmass-weighted ages and stellar metallicity ($Z_\\star$). On average, quiescent\ngalaxies are characterized by high $Z_\\star$, they are \\sim 1.1 Gyr older, less\ndusty, with steeper dust attenuation slopes compared to star-forming galaxies.\nConversely, star-forming galaxies are characterized by significantly higher\ndust optical depths and shallower (grayer) attenuation slopes. Low mass (high\nmass) star-forming galaxies have lower (higher) $Z_\\star$, while their stellar\npopulations are on average younger (older). A key pragmatic result of our study\nis that a linear-space metallicity prior is preferable to a logarithmic-space\none when using photometry alone, as the latter biases the posteriors downward.\nSpectroscopy greatly improves stellar population measurements and is required\nto provide meaningful constraints on age, metallicity, and other properties.\nPairing spectroscopy with photometry helps resolving the dust-age-metallicity\ndegeneracy, yielding more accurate mass- and light-weighted ages, with ages\ninferred from photometry alone suffering such large uncertainties. Stellar\nmetallicities are constrained by our spectroscopy, but precise measurements\nremain challenging (and impossible with photometry alone), particularly in the\nabsence of Mg and Fe lines redward of 5000 $\\AA$ in the observed spectrum."
                },
                "authors": [
                    {
                        "name": "Angelos Nersesian"
                    },
                    {
                        "name": "Arjen van der Wel"
                    },
                    {
                        "name": "Anna R. Gallazzi"
                    },
                    {
                        "name": "Yasha Kaushal"
                    },
                    {
                        "name": "Rachel Bezanson"
                    },
                    {
                        "name": "Stefano Zibetti"
                    },
                    {
                        "name": "Eric F. Bell"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Joel Leja"
                    },
                    {
                        "name": "Marco Martorano"
                    },
                    {
                        "name": "Po-Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Po-Feng Wu"
                },
                "author": "Po-Feng Wu",
                "arxiv_comment": "Accepted, 25 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03019v1",
                "updated": "2025-02-05T09:22:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    22,
                    33,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T09:22:33Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    22,
                    33,
                    2,
                    36,
                    0
                ],
                "title": "Panel Data Estimation and Inference: Homogeneity versus Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panel Data Estimation and Inference: Homogeneity versus Heterogeneity"
                },
                "summary": "In this paper, we define an underlying data generating process that allows\nfor different magnitudes of cross-sectional dependence, along with time series\nautocorrelation. This is achieved via high-dimensional moving average processes\nof infinite order (HDMA($\\infty$)). Our setup and investigation integrates and\nenhances homogenous and heterogeneous panel data estimation and testing in a\nunified way. To study HDMA($\\infty$), we extend the Beveridge-Nelson\ndecomposition to a high-dimensional time series setting, and derive a complete\ntoolkit set. We exam homogeneity versus heterogeneity using Gaussian\napproximation, a prevalent technique for establishing uniform inference. For\npost-testing inference, we derive central limit theorems through Edgeworth\nexpansions for both homogenous and heterogeneous settings. Additionally, we\nshowcase the practical relevance of the established asymptotic properties by\nrevisiting the common correlated effects (CCE) estimators, and a classic\nnonstationary panel data process. Finally, we verify our theoretical findings\nvia extensive numerical studies using both simulated and real datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we define an underlying data generating process that allows\nfor different magnitudes of cross-sectional dependence, along with time series\nautocorrelation. This is achieved via high-dimensional moving average processes\nof infinite order (HDMA($\\infty$)). Our setup and investigation integrates and\nenhances homogenous and heterogeneous panel data estimation and testing in a\nunified way. To study HDMA($\\infty$), we extend the Beveridge-Nelson\ndecomposition to a high-dimensional time series setting, and derive a complete\ntoolkit set. We exam homogeneity versus heterogeneity using Gaussian\napproximation, a prevalent technique for establishing uniform inference. For\npost-testing inference, we derive central limit theorems through Edgeworth\nexpansions for both homogenous and heterogeneous settings. Additionally, we\nshowcase the practical relevance of the established asymptotic properties by\nrevisiting the common correlated effects (CCE) estimators, and a classic\nnonstationary panel data process. Finally, we verify our theoretical findings\nvia extensive numerical studies using both simulated and real datasets."
                },
                "authors": [
                    {
                        "name": "Jiti Gao"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Bin Peng"
                    },
                    {
                        "name": "Yayi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yayi Yan"
                },
                "author": "Yayi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.03461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03461v1",
                "updated": "2025-02-05T18:58:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    58,
                    19,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:58:19Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    58,
                    19,
                    2,
                    36,
                    0
                ],
                "title": "Do Large Language Model Benchmarks Test Reliability?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Model Benchmarks Test Reliability?"
                },
                "summary": "When deploying large language models (LLMs), it is important to ensure that\nthese models are not only capable, but also reliable. Many benchmarks have been\ncreated to track LLMs' growing capabilities, however there has been no similar\nfocus on measuring their reliability. To understand the potential ramifications\nof this gap, we investigate how well current benchmarks quantify model\nreliability. We find that pervasive label errors can compromise these\nevaluations, obscuring lingering model failures and hiding unreliable behavior.\n  Motivated by this gap in the evaluation of reliability, we then propose the\nconcept of so-called platinum benchmarks, i.e., benchmarks carefully curated to\nminimize label errors and ambiguity. As a first attempt at constructing such\nbenchmarks, we revise examples from fifteen existing popular benchmarks. We\nevaluate a wide range of models on these platinum benchmarks and find that,\nindeed, frontier LLMs still exhibit failures on simple tasks such as\nelementary-level math word problems. Analyzing these failures further reveals\npreviously unidentified patterns of problems on which frontier models\nconsistently struggle. We provide code at\nhttps://github.com/MadryLab/platinum-benchmarks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When deploying large language models (LLMs), it is important to ensure that\nthese models are not only capable, but also reliable. Many benchmarks have been\ncreated to track LLMs' growing capabilities, however there has been no similar\nfocus on measuring their reliability. To understand the potential ramifications\nof this gap, we investigate how well current benchmarks quantify model\nreliability. We find that pervasive label errors can compromise these\nevaluations, obscuring lingering model failures and hiding unreliable behavior.\n  Motivated by this gap in the evaluation of reliability, we then propose the\nconcept of so-called platinum benchmarks, i.e., benchmarks carefully curated to\nminimize label errors and ambiguity. As a first attempt at constructing such\nbenchmarks, we revise examples from fifteen existing popular benchmarks. We\nevaluate a wide range of models on these platinum benchmarks and find that,\nindeed, frontier LLMs still exhibit failures on simple tasks such as\nelementary-level math word problems. Analyzing these failures further reveals\npreviously unidentified patterns of problems on which frontier models\nconsistently struggle. We provide code at\nhttps://github.com/MadryLab/platinum-benchmarks"
                },
                "authors": [
                    {
                        "name": "Joshua Vendrow"
                    },
                    {
                        "name": "Edward Vendrow"
                    },
                    {
                        "name": "Sara Beery"
                    },
                    {
                        "name": "Aleksander Madry"
                    }
                ],
                "author_detail": {
                    "name": "Aleksander Madry"
                },
                "author": "Aleksander Madry",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03460v1",
                "updated": "2025-02-05T18:57:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    57,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:57:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    57,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language\n  Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language\n  Model Training"
                },
                "summary": "Small language models (SLMs) have attracted considerable attention from both\nacademia and industry due to their broad range of applications in edge devices.\nTo obtain SLMs with strong performance, conventional approaches either\npre-train the models from scratch, which incurs substantial computational\ncosts, or compress/prune existing large language models (LLMs), which results\nin performance drops and falls short in comparison to pre-training. In this\npaper, we investigate the family of acceleration methods that involve both\nstructured pruning and model training. We found 1) layer-wise adaptive pruning\n(Adapt-Pruner) is extremely effective in LLMs and yields significant\nimprovements over existing pruning techniques, 2) adaptive pruning equipped\nwith further training leads to models comparable to those pre-training from\nscratch, 3) incremental pruning brings non-trivial performance gain by\ninterleaving pruning with training and only removing a small portion of neurons\n($\\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that\nAdapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner,\nFLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense\nbenchmarks. Additionally, Adapt-Pruner restores the performance of\nMobileLLM-125M to 600M on the MMLU benchmark with 200$\\times$ fewer tokens via\npruning from its larger counterparts, and discovers a new 1B model that\nsurpasses LLaMA-3.2-1B in multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs) have attracted considerable attention from both\nacademia and industry due to their broad range of applications in edge devices.\nTo obtain SLMs with strong performance, conventional approaches either\npre-train the models from scratch, which incurs substantial computational\ncosts, or compress/prune existing large language models (LLMs), which results\nin performance drops and falls short in comparison to pre-training. In this\npaper, we investigate the family of acceleration methods that involve both\nstructured pruning and model training. We found 1) layer-wise adaptive pruning\n(Adapt-Pruner) is extremely effective in LLMs and yields significant\nimprovements over existing pruning techniques, 2) adaptive pruning equipped\nwith further training leads to models comparable to those pre-training from\nscratch, 3) incremental pruning brings non-trivial performance gain by\ninterleaving pruning with training and only removing a small portion of neurons\n($\\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that\nAdapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner,\nFLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense\nbenchmarks. Additionally, Adapt-Pruner restores the performance of\nMobileLLM-125M to 600M on the MMLU benchmark with 200$\\times$ fewer tokens via\npruning from its larger counterparts, and discovers a new 1B model that\nsurpasses LLaMA-3.2-1B in multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Boyao Wang"
                    },
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Xingyuan Pan"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Renjie Pi"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03450v1",
                "updated": "2025-02-05T18:50:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    50,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:50:38Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    50,
                    38,
                    2,
                    36,
                    0
                ],
                "title": "A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene\n  Graphs with Large-Language-Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene\n  Graphs with Large-Language-Models (LLMs)"
                },
                "summary": "Scene graphs have emerged as a structured and serializable environment\nrepresentation for grounded spatial reasoning with Large Language Models\n(LLMs). In this work, we propose SG-RwR, a Schema-Guided Retrieve-while-Reason\nframework for reasoning and planning with scene graphs. Our approach employs\ntwo cooperative, code-writing LLM agents: a (1) Reasoner for task planning and\ninformation queries generation, and a (2) Retriever for extracting\ncorresponding graph information following the queries. Two agents collaborate\niteratively, enabling sequential reasoning and adaptive attention to graph\ninformation. Unlike prior works, both agents are prompted only with the scene\ngraph schema rather than the full graph data, which reduces the hallucination\nby limiting input tokens, and drives the Reasoner to generate reasoning trace\nabstractly.Following the trace, the Retriever programmatically query the scene\ngraph data based on the schema understanding, allowing dynamic and global\nattention on the graph that enhances alignment between reasoning and retrieval.\nThrough experiments in multiple simulation environments, we show that our\nframework surpasses existing LLM-based approaches in numerical Q\\&A and\nplanning tasks, and can benefit from task-level few-shot examples, even in the\nabsence of agent-level demonstrations. Project code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene graphs have emerged as a structured and serializable environment\nrepresentation for grounded spatial reasoning with Large Language Models\n(LLMs). In this work, we propose SG-RwR, a Schema-Guided Retrieve-while-Reason\nframework for reasoning and planning with scene graphs. Our approach employs\ntwo cooperative, code-writing LLM agents: a (1) Reasoner for task planning and\ninformation queries generation, and a (2) Retriever for extracting\ncorresponding graph information following the queries. Two agents collaborate\niteratively, enabling sequential reasoning and adaptive attention to graph\ninformation. Unlike prior works, both agents are prompted only with the scene\ngraph schema rather than the full graph data, which reduces the hallucination\nby limiting input tokens, and drives the Reasoner to generate reasoning trace\nabstractly.Following the trace, the Retriever programmatically query the scene\ngraph data based on the schema understanding, allowing dynamic and global\nattention on the graph that enhances alignment between reasoning and retrieval.\nThrough experiments in multiple simulation environments, we show that our\nframework surpasses existing LLM-based approaches in numerical Q\\&A and\nplanning tasks, and can benefit from task-level few-shot examples, even in the\nabsence of agent-level demonstrations. Project code will be released."
                },
                "authors": [
                    {
                        "name": "Yiye Chen"
                    },
                    {
                        "name": "Harpreet Sawhney"
                    },
                    {
                        "name": "Nicholas Gyd"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Jack Saunders"
                    },
                    {
                        "name": "Patricio Vela"
                    },
                    {
                        "name": "Ben Lundell"
                    }
                ],
                "author_detail": {
                    "name": "Ben Lundell"
                },
                "author": "Ben Lundell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03447v1",
                "updated": "2025-02-05T18:45:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    45,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:45:38Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    45,
                    38,
                    2,
                    36,
                    0
                ],
                "title": "Designing LLM-simulated Immersive Spaces to Enhance Autistic Children's\n  Social Affordances Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing LLM-simulated Immersive Spaces to Enhance Autistic Children's\n  Social Affordances Understanding"
                },
                "summary": "One of the key challenges faced by autistic children is understanding social\naffordances in complex environments, which further impacts their ability to\nrespond appropriately to social signals. In traffic scenarios, this impairment\ncan even lead to safety concerns. In this paper, we introduce an LLM-simulated\nimmersive projection environment designed to improve this ability in autistic\nchildren while ensuring their safety. We first propose 17 design considerations\nacross four major categories, derived from a comprehensive review of previous\nresearch. Next, we developed a system called AIroad, which leverages LLMs to\nsimulate drivers with varying social intents, expressed through explicit\nmultimodal social signals. AIroad helps autistic children bridge the gap in\nrecognizing the intentions behind behaviors and learning appropriate responses\nthrough various stimuli. A user study involving 14 participants demonstrated\nthat this technology effectively engages autistic children and leads to\nsignificant improvements in their comprehension of social affordances in\ntraffic scenarios. Additionally, parents reported high perceived usability of\nthe system. These findings highlight the potential of combining LLM technology\nwith immersive environments for the functional rehabilitation of autistic\nchildren in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the key challenges faced by autistic children is understanding social\naffordances in complex environments, which further impacts their ability to\nrespond appropriately to social signals. In traffic scenarios, this impairment\ncan even lead to safety concerns. In this paper, we introduce an LLM-simulated\nimmersive projection environment designed to improve this ability in autistic\nchildren while ensuring their safety. We first propose 17 design considerations\nacross four major categories, derived from a comprehensive review of previous\nresearch. Next, we developed a system called AIroad, which leverages LLMs to\nsimulate drivers with varying social intents, expressed through explicit\nmultimodal social signals. AIroad helps autistic children bridge the gap in\nrecognizing the intentions behind behaviors and learning appropriate responses\nthrough various stimuli. A user study involving 14 participants demonstrated\nthat this technology effectively engages autistic children and leads to\nsignificant improvements in their comprehension of social affordances in\ntraffic scenarios. Additionally, parents reported high perceived usability of\nthe system. These findings highlight the potential of combining LLM technology\nwith immersive environments for the functional rehabilitation of autistic\nchildren in the future."
                },
                "authors": [
                    {
                        "name": "Yancheng Cao"
                    },
                    {
                        "name": "Yangyang HE"
                    },
                    {
                        "name": "Yonglin Chen"
                    },
                    {
                        "name": "Menghan Chen"
                    },
                    {
                        "name": "Shanhe You"
                    },
                    {
                        "name": "Yulin Qiu"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Chuan Luo"
                    },
                    {
                        "name": "Chen Zheng"
                    },
                    {
                        "name": "Xin Tong"
                    },
                    {
                        "name": "Jing Liang"
                    },
                    {
                        "name": "Jiangtao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiangtao Gong"
                },
                "author": "Jiangtao Gong",
                "arxiv_doi": "10.1145/3708359.3712142.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712142.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.03447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "iui2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v7",
                "updated": "2025-02-05T18:39:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    39,
                    43,
                    2,
                    36,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads"
                },
                "summary": "Sparse attention, which selectively attends to a subset of tokens in the\ncontext was supposed to be efficient. However, its theoretical reduction in\nFLOPs has rarely translated into wall-clock speed-up over its dense attention\ncounterparts due to the lack of hardware-aware optimizations like\nFlashAttention. Meanwhile, it remains unclear whether sparse attention can\nmaintain the model's quality at a scale of today's large language models (LLMs)\nand how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library\nthat provides kernel optimization for sparse attention customizable at both\nper-head and per-context-range levels. S2-Attention enables the exploration of\nnovel and high-performance sparse attention techniques, which we demonstrate\nthrough extensive ablations across a wide range of sparse attention designs at\nvarious model scales. From these insights, we present several basic guidelines\nto design sparse attention that can achieve not only practical efficiency\nimprovements, but also strong downstream performance. To achieve high\nparallelization and optimized memory IO, sparse attention should shard the\ncontext heterogeneously across attention heads, where each head attends to a\ndifferent subset of tokens while collectively covering the full context.\nMeanwhile, we find hybrid architectures combining sparse and dense attention\nparticularly beneficial in practice. S2-Attention achieves wall-clock speedup\nof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with\nstrong downstream performance on-par with full attention and perfect retrieval\nperformance at a 128k context length. At inference, for 7B models, our model,\nwith the help of our S2-Attention kernel, achieves 4.5x speed-up compared to\ndense counterparts. S2-Attention is released with easy-to-customize APIs for\ndirect usage in Megatron and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse attention, which selectively attends to a subset of tokens in the\ncontext was supposed to be efficient. However, its theoretical reduction in\nFLOPs has rarely translated into wall-clock speed-up over its dense attention\ncounterparts due to the lack of hardware-aware optimizations like\nFlashAttention. Meanwhile, it remains unclear whether sparse attention can\nmaintain the model's quality at a scale of today's large language models (LLMs)\nand how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library\nthat provides kernel optimization for sparse attention customizable at both\nper-head and per-context-range levels. S2-Attention enables the exploration of\nnovel and high-performance sparse attention techniques, which we demonstrate\nthrough extensive ablations across a wide range of sparse attention designs at\nvarious model scales. From these insights, we present several basic guidelines\nto design sparse attention that can achieve not only practical efficiency\nimprovements, but also strong downstream performance. To achieve high\nparallelization and optimized memory IO, sparse attention should shard the\ncontext heterogeneously across attention heads, where each head attends to a\ndifferent subset of tokens while collectively covering the full context.\nMeanwhile, we find hybrid architectures combining sparse and dense attention\nparticularly beneficial in practice. S2-Attention achieves wall-clock speedup\nof 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with\nstrong downstream performance on-par with full attention and perfect retrieval\nperformance at a 128k context length. At inference, for 7B models, our model,\nwith the help of our S2-Attention kernel, achieves 4.5x speed-up compared to\ndense counterparts. S2-Attention is released with easy-to-customize APIs for\ndirect usage in Megatron and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Liliang Ren"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03438v1",
                "updated": "2025-02-05T18:33:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    33,
                    36,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:33:36Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    33,
                    36,
                    2,
                    36,
                    0
                ],
                "title": "BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic\n  Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic\n  Theorem Proving"
                },
                "summary": "Recent advancements in large language models (LLMs) have spurred growing\ninterest in automatic theorem proving using Lean4, where effective tree search\nmethods are crucial for navigating proof search spaces. While the existing\napproaches primarily rely on value functions and Monte Carlo Tree Search\n(MCTS), the potential of simpler methods like Best-First Search (BFS) remains\nunderexplored. This paper investigates whether BFS can achieve competitive\nperformance in large-scale theorem proving tasks. We present\n\\texttt{BFS-Prover}, a scalable expert iteration framework, featuring three key\ninnovations. First, we implement strategic data filtering at each expert\niteration round, excluding problems solvable via beam search node expansion to\nfocus on harder cases. Second, we improve the sample efficiency of BFS through\nDirect Preference Optimization (DPO) applied to state-tactic pairs\nautomatically annotated with compiler error feedback, refining the LLM's policy\nto prioritize productive expansions. Third, we employ length normalization in\nBFS to encourage exploration of deeper proof paths. \\texttt{BFS-Prover}\nachieves a score of $71.31$ on the MiniF2F test set and therefore challenges\nthe perceived necessity of complex tree search methods, demonstrating that BFS\ncan achieve competitive performance when properly scaled.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have spurred growing\ninterest in automatic theorem proving using Lean4, where effective tree search\nmethods are crucial for navigating proof search spaces. While the existing\napproaches primarily rely on value functions and Monte Carlo Tree Search\n(MCTS), the potential of simpler methods like Best-First Search (BFS) remains\nunderexplored. This paper investigates whether BFS can achieve competitive\nperformance in large-scale theorem proving tasks. We present\n\\texttt{BFS-Prover}, a scalable expert iteration framework, featuring three key\ninnovations. First, we implement strategic data filtering at each expert\niteration round, excluding problems solvable via beam search node expansion to\nfocus on harder cases. Second, we improve the sample efficiency of BFS through\nDirect Preference Optimization (DPO) applied to state-tactic pairs\nautomatically annotated with compiler error feedback, refining the LLM's policy\nto prioritize productive expansions. Third, we employ length normalization in\nBFS to encourage exploration of deeper proof paths. \\texttt{BFS-Prover}\nachieves a score of $71.31$ on the MiniF2F test set and therefore challenges\nthe perceived necessity of complex tree search methods, demonstrating that BFS\ncan achieve competitive performance when properly scaled."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Chenguang Xi"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Xia Xiao"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Shen Zheng"
                    },
                    {
                        "name": "Kai Shen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shen"
                },
                "author": "Kai Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03425v1",
                "updated": "2025-02-05T18:15:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    15,
                    9,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:15:09Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    15,
                    9,
                    2,
                    36,
                    0
                ],
                "title": "Harnessing Large Language Models for Curated Code Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language Models for Curated Code Reviews"
                },
                "summary": "In code review, generating structured and relevant comments is crucial for\nidentifying code issues and facilitating accurate code changes that ensure an\nefficient code review process. Well-crafted comments not only streamline the\ncode review itself but are also essential for subsequent tasks like code\nrefinement, where the code is modified to satisfy the input review comment.\nAlthough various AI-based approaches aimed to automate comment generation,\ntheir effectiveness remains limited by the quality of the training data.\nExisting code review datasets are often noisy and unrefined, posing limitations\nto the learning potential of AI models and hindering the automation process.\n  To address these challenges, we propose a curation pipeline designed to\nenhance the quality of the largest publicly available code review dataset. We\nbegin by establishing an evaluation framework, incorporating specific criteria\nand categories to empirically study the initial quality of the dataset. Using a\nlarge language model (LLM)-driven approach, we then apply our curation pipeline\nto refine the dataset. A comparative analysis of the newly curated dataset,\nbased on the same evaluation framework, demonstrates substantial improvements\nin the clarity and conciseness of the comments. Additionally, we assess the\nimpact of the curated dataset on automating downstream tasks, specifically\ncomment generation and code refinement. Our findings show that the curated\ndataset leads to enhanced model performance in generating more accurate\ncomments. Curated comments are also more useful as they lead to more accurate\ncode refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In code review, generating structured and relevant comments is crucial for\nidentifying code issues and facilitating accurate code changes that ensure an\nefficient code review process. Well-crafted comments not only streamline the\ncode review itself but are also essential for subsequent tasks like code\nrefinement, where the code is modified to satisfy the input review comment.\nAlthough various AI-based approaches aimed to automate comment generation,\ntheir effectiveness remains limited by the quality of the training data.\nExisting code review datasets are often noisy and unrefined, posing limitations\nto the learning potential of AI models and hindering the automation process.\n  To address these challenges, we propose a curation pipeline designed to\nenhance the quality of the largest publicly available code review dataset. We\nbegin by establishing an evaluation framework, incorporating specific criteria\nand categories to empirically study the initial quality of the dataset. Using a\nlarge language model (LLM)-driven approach, we then apply our curation pipeline\nto refine the dataset. A comparative analysis of the newly curated dataset,\nbased on the same evaluation framework, demonstrates substantial improvements\nin the clarity and conciseness of the comments. Additionally, we assess the\nimpact of the curated dataset on automating downstream tasks, specifically\ncomment generation and code refinement. Our findings show that the curated\ndataset leads to enhanced model performance in generating more accurate\ncomments. Curated comments are also more useful as they lead to more accurate\ncode refinement."
                },
                "authors": [
                    {
                        "name": "Oussama Ben Sghaier"
                    },
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Houari Sahraoui"
                    }
                ],
                "author_detail": {
                    "name": "Houari Sahraoui"
                },
                "author": "Houari Sahraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14566v3",
                "updated": "2025-02-05T18:12:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    12,
                    52,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-24T15:19:04Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    19,
                    4,
                    4,
                    24,
                    0
                ],
                "title": "Calibrating Wireless AI via Meta-Learned Context-Dependent Conformal\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating Wireless AI via Meta-Learned Context-Dependent Conformal\n  Prediction"
                },
                "summary": "Modern software-defined networks, such as Open Radio Access Network (O-RAN)\nsystems, rely on artificial intelligence (AI)-powered applications running on\ncontrollers interfaced with the radio access network. To ensure that these AI\napplications operate reliably at runtime, they must be properly calibrated\nbefore deployment. A promising and theoretically grounded approach to\ncalibration is conformal prediction (CP), which enhances any AI model by\ntransforming it into a provably reliable set predictor that provides error bars\nfor estimates and decisions. CP requires calibration data that matches the\ndistribution of the environment encountered during runtime. However, in\npractical scenarios, network controllers often have access only to data\ncollected under different contexts -- such as varying traffic patterns and\nnetwork conditions -- leading to a mismatch between the calibration and runtime\ndistributions. This paper introduces a novel methodology to address this\ncalibration-test distribution shift. The approach leverages meta-learning to\ndevelop a zero-shot estimator of distribution shifts, relying solely on\ncontextual information. The proposed method, called meta-learned\ncontext-dependent weighted conformal prediction (ML-WCP), enables effective\ncalibration of AI applications without requiring data from the current context.\nAdditionally, it can incorporate data from multiple contexts to further enhance\ncalibration reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software-defined networks, such as Open Radio Access Network (O-RAN)\nsystems, rely on artificial intelligence (AI)-powered applications running on\ncontrollers interfaced with the radio access network. To ensure that these AI\napplications operate reliably at runtime, they must be properly calibrated\nbefore deployment. A promising and theoretically grounded approach to\ncalibration is conformal prediction (CP), which enhances any AI model by\ntransforming it into a provably reliable set predictor that provides error bars\nfor estimates and decisions. CP requires calibration data that matches the\ndistribution of the environment encountered during runtime. However, in\npractical scenarios, network controllers often have access only to data\ncollected under different contexts -- such as varying traffic patterns and\nnetwork conditions -- leading to a mismatch between the calibration and runtime\ndistributions. This paper introduces a novel methodology to address this\ncalibration-test distribution shift. The approach leverages meta-learning to\ndevelop a zero-shot estimator of distribution shifts, relying solely on\ncontextual information. The proposed method, called meta-learned\ncontext-dependent weighted conformal prediction (ML-WCP), enables effective\ncalibration of AI applications without requiring data from the current context.\nAdditionally, it can incorporate data from multiple contexts to further enhance\ncalibration reliability."
                },
                "authors": [
                    {
                        "name": "Seonghoon Yoo"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Petar Popovski"
                    },
                    {
                        "name": "Joonhyuk Kang"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03418v1",
                "updated": "2025-02-05T18:04:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    4,
                    29,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T18:04:29Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    4,
                    29,
                    2,
                    36,
                    0
                ],
                "title": "Think or Step-by-Step? UnZIPping the Black Box in Zero-Shot Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think or Step-by-Step? UnZIPping the Black Box in Zero-Shot Prompts"
                },
                "summary": "Zero-shot prompting techniques have significantly improved the performance of\nLarge Language Models (LLMs). However, we lack a clear understanding of why\nzero-shot prompts are so effective. For example, in the prompt \"Let's think\nstep-by-step,\" is \"think\" or \"step-by-step\" more crucial to its success?\nExisting interpretability methods, such as gradient-based and attention-based\napproaches, are computationally intensive and restricted to open-source models.\nWe introduce the ZIP score (Zero-shot Importance of Perturbation score), a\nversatile metric applicable to both open and closed-source models, based on\nsystematic input word perturbations. Our experiments across four recent LLMs,\nseven widely-used prompts, and several tasks, reveal interesting patterns in\nword importance. For instance, while both 'step-by-step' and 'think' show high\nZIP scores, which one is more influential depends on the model and task. We\nvalidate our method using controlled experiments and compare our results with\nhuman judgments, finding that proprietary models align more closely with human\nintuition regarding word significance. These findings enhance our understanding\nof LLM behavior and contribute to developing more effective zero-shot prompts\nand improved model analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot prompting techniques have significantly improved the performance of\nLarge Language Models (LLMs). However, we lack a clear understanding of why\nzero-shot prompts are so effective. For example, in the prompt \"Let's think\nstep-by-step,\" is \"think\" or \"step-by-step\" more crucial to its success?\nExisting interpretability methods, such as gradient-based and attention-based\napproaches, are computationally intensive and restricted to open-source models.\nWe introduce the ZIP score (Zero-shot Importance of Perturbation score), a\nversatile metric applicable to both open and closed-source models, based on\nsystematic input word perturbations. Our experiments across four recent LLMs,\nseven widely-used prompts, and several tasks, reveal interesting patterns in\nword importance. For instance, while both 'step-by-step' and 'think' show high\nZIP scores, which one is more influential depends on the model and task. We\nvalidate our method using controlled experiments and compare our results with\nhuman judgments, finding that proprietary models align more closely with human\nintuition regarding word significance. These findings enhance our understanding\nof LLM behavior and contribute to developing more effective zero-shot prompts\nand improved model analysis."
                },
                "authors": [
                    {
                        "name": "Nikta Gohari Sadr"
                    },
                    {
                        "name": "Sangmitra Madhusudan"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "arxiv_comment": "8 pages (excluding references)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02542v2",
                "updated": "2025-02-05T17:58:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    58,
                    46,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-04T18:12:41Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    12,
                    41,
                    1,
                    35,
                    0
                ],
                "title": "OverThink: Slowdown Attacks on Reasoning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OverThink: Slowdown Attacks on Reasoning LLMs"
                },
                "summary": "We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 18x slowdown on FreshQA dataset and 46x\nslowdown on SQuAD dataset. The attack also shows high transferability across\nmodels. To protect applications, we discuss and implement defenses leveraging\nLLM-based and system design approaches. Finally, we discuss societal,\nfinancial, and energy impacts of OVERTHINK attack which could amplify the costs\nfor third-party applications operating reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 18x slowdown on FreshQA dataset and 46x\nslowdown on SQuAD dataset. The attack also shows high transferability across\nmodels. To protect applications, we discuss and implement defenses leveraging\nLLM-based and system design approaches. Finally, we discuss societal,\nfinancial, and energy impacts of OVERTHINK attack which could amplify the costs\nfor third-party applications operating reasoning models."
                },
                "authors": [
                    {
                        "name": "Abhinav Kumar"
                    },
                    {
                        "name": "Jaechul Roh"
                    },
                    {
                        "name": "Ali Naseh"
                    },
                    {
                        "name": "Marzena Karpinska"
                    },
                    {
                        "name": "Mohit Iyyer"
                    },
                    {
                        "name": "Amir Houmansadr"
                    },
                    {
                        "name": "Eugene Bagdasarian"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Bagdasarian"
                },
                "author": "Eugene Bagdasarian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20724v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20724v4",
                "updated": "2025-02-05T17:45:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    45,
                    24,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-28T04:39:32Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    4,
                    39,
                    32,
                    0,
                    302,
                    0
                ],
                "title": "Simple Is Effective: The Roles of Graphs and Large Language Models in\n  Knowledge-Graph-Based Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple Is Effective: The Roles of Graphs and Large Language Models in\n  Knowledge-Graph-Based Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) demonstrate strong reasoning abilities but face\nlimitations such as hallucinations and outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by\ngrounding LLM outputs in structured external knowledge from KGs. However,\ncurrent KG-based RAG frameworks still struggle to optimize the trade-off\nbetween retrieval effectiveness and efficiency in identifying a suitable amount\nof relevant graph information for the LLM to digest. We introduce SubgraphRAG,\nextending the KG-based RAG framework that retrieves subgraphs and leverages\nLLMs for reasoning and answer prediction. Our approach innovatively integrates\na lightweight multilayer perceptron with a parallel triple-scoring mechanism\nfor efficient and flexible subgraph retrieval while encoding directional\nstructural distances to enhance retrieval effectiveness. The size of retrieved\nsubgraphs can be flexibly adjusted to match the query's need and the downstream\nLLM's capabilities. This design strikes a balance between model complexity and\nreasoning power, enabling scalable and generalizable retrieval processes.\nNotably, based on our retrieved subgraphs, smaller LLMs like\nLlama3.1-8B-Instruct deliver competitive results with explainable reasoning,\nwhile larger models like GPT-4o achieve state-of-the-art accuracy compared with\nprevious baselines -- all without fine-tuning. Extensive evaluations on the\nWebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,\naccuracy, and reliability by reducing hallucinations and improving response\ngrounding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate strong reasoning abilities but face\nlimitations such as hallucinations and outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by\ngrounding LLM outputs in structured external knowledge from KGs. However,\ncurrent KG-based RAG frameworks still struggle to optimize the trade-off\nbetween retrieval effectiveness and efficiency in identifying a suitable amount\nof relevant graph information for the LLM to digest. We introduce SubgraphRAG,\nextending the KG-based RAG framework that retrieves subgraphs and leverages\nLLMs for reasoning and answer prediction. Our approach innovatively integrates\na lightweight multilayer perceptron with a parallel triple-scoring mechanism\nfor efficient and flexible subgraph retrieval while encoding directional\nstructural distances to enhance retrieval effectiveness. The size of retrieved\nsubgraphs can be flexibly adjusted to match the query's need and the downstream\nLLM's capabilities. This design strikes a balance between model complexity and\nreasoning power, enabling scalable and generalizable retrieval processes.\nNotably, based on our retrieved subgraphs, smaller LLMs like\nLlama3.1-8B-Instruct deliver competitive results with explainable reasoning,\nwhile larger models like GPT-4o achieve state-of-the-art accuracy compared with\nprevious baselines -- all without fine-tuning. Extensive evaluations on the\nWebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,\naccuracy, and reliability by reducing hallucinations and improving response\ngrounding."
                },
                "authors": [
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Siqi Miao"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "arxiv_comment": "Accepted by ICLR 2025; Code available at\n  https://github.com/Graph-COM/SubgraphRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20724v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20724v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09662v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09662v3",
                "updated": "2025-02-05T17:41:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    41,
                    42,
                    2,
                    36,
                    0
                ],
                "published": "2024-09-15T08:25:24Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    8,
                    25,
                    24,
                    6,
                    259,
                    0
                ],
                "title": "ExploreSelf: Fostering User-driven Exploration and Reflection on\n  Personal Challenges with Adaptive Guidance by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExploreSelf: Fostering User-driven Exploration and Reflection on\n  Personal Challenges with Adaptive Guidance by Large Language Models"
                },
                "summary": "Expressing stressful experiences in words is proven to improve mental and\nphysical health, but individuals often disengage with writing interventions as\nthey struggle to organize their thoughts and emotions. Reflective prompts have\nbeen used to provide direction, and large language models (LLMs) have\ndemonstrated the potential to provide tailored guidance. However, current\nsystems often limit users' flexibility to direct their reflections. We thus\npresent ExploreSelf, an LLM-driven application designed to empower users to\ncontrol their reflective journey, providing adaptive support through\ndynamically generated questions. Through an exploratory study with 19\nparticipants, we examine how participants explore and reflect on personal\nchallenges using ExploreSelf. Our findings demonstrate that participants valued\nthe flexible navigation of adaptive guidance to control their reflective\njourney, leading to deeper engagement and insight. Building on our findings, we\ndiscuss the implications of designing LLM-driven tools that facilitate\nuser-driven and effective reflection of personal challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expressing stressful experiences in words is proven to improve mental and\nphysical health, but individuals often disengage with writing interventions as\nthey struggle to organize their thoughts and emotions. Reflective prompts have\nbeen used to provide direction, and large language models (LLMs) have\ndemonstrated the potential to provide tailored guidance. However, current\nsystems often limit users' flexibility to direct their reflections. We thus\npresent ExploreSelf, an LLM-driven application designed to empower users to\ncontrol their reflective journey, providing adaptive support through\ndynamically generated questions. Through an exploratory study with 19\nparticipants, we examine how participants explore and reflect on personal\nchallenges using ExploreSelf. Our findings demonstrate that participants valued\nthe flexible navigation of adaptive guidance to control their reflective\njourney, leading to deeper engagement and insight. Building on our findings, we\ndiscuss the implications of designing LLM-driven tools that facilitate\nuser-driven and effective reflection of personal challenges."
                },
                "authors": [
                    {
                        "name": "Inhwa Song"
                    },
                    {
                        "name": "SoHyun Park"
                    },
                    {
                        "name": "Sachin R. Pendse"
                    },
                    {
                        "name": "Jessica Lee Schleider"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    },
                    {
                        "name": "Young-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young-Ho Kim"
                },
                "author": "Young-Ho Kim",
                "arxiv_comment": "17 pages excluding reference and appendix. Accepted at ACM CHI 2025.\n  https://naver-ai.github.io/exploreself",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09662v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09662v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03397v1",
                "updated": "2025-02-05T17:32:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    32,
                    29,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T17:32:29Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    32,
                    29,
                    2,
                    36,
                    0
                ],
                "title": "SPRI: Aligning Large Language Models with Context-Situated Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPRI: Aligning Large Language Models with Context-Situated Principles"
                },
                "summary": "Aligning Large Language Models to integrate and reflect human values,\nespecially for tasks that demand intricate human oversight, is arduous since it\nis resource-intensive and time-consuming to depend on human expertise for\ncontext-specific guidance. Prior work has utilized predefined sets of rules or\nprinciples to steer the behavior of models (Bai et al., 2022; Sun et al.,\n2023). However, these principles tend to be generic, making it challenging to\nadapt them to each individual input query or context. In this work, we present\nSituated-PRInciples (SPRI), a framework requiring minimal or no human effort\nthat is designed to automatically generate guiding principles in real-time for\neach input query and utilize them to align each response. We evaluate SPRI on\nthree tasks, and show that 1) SPRI can derive principles in a complex\ndomain-specific task that leads to on-par performance as expert-crafted ones;\n2) SPRI-generated principles lead to instance-specific rubrics that outperform\nprior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data\nleads to substantial improvement on truthfulness. We release our code and model\ngenerations at https://github.com/honglizhan/SPRI-public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models to integrate and reflect human values,\nespecially for tasks that demand intricate human oversight, is arduous since it\nis resource-intensive and time-consuming to depend on human expertise for\ncontext-specific guidance. Prior work has utilized predefined sets of rules or\nprinciples to steer the behavior of models (Bai et al., 2022; Sun et al.,\n2023). However, these principles tend to be generic, making it challenging to\nadapt them to each individual input query or context. In this work, we present\nSituated-PRInciples (SPRI), a framework requiring minimal or no human effort\nthat is designed to automatically generate guiding principles in real-time for\neach input query and utilize them to align each response. We evaluate SPRI on\nthree tasks, and show that 1) SPRI can derive principles in a complex\ndomain-specific task that leads to on-par performance as expert-crafted ones;\n2) SPRI-generated principles lead to instance-specific rubrics that outperform\nprior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data\nleads to substantial improvement on truthfulness. We release our code and model\ngenerations at https://github.com/honglizhan/SPRI-public."
                },
                "authors": [
                    {
                        "name": "Hongli Zhan"
                    },
                    {
                        "name": "Muneeza Azmat"
                    },
                    {
                        "name": "Raya Horesh"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Yurochkin"
                },
                "author": "Mikhail Yurochkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03395v1",
                "updated": "2025-02-05T17:30:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    30,
                    31,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T17:30:31Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    30,
                    31,
                    2,
                    36,
                    0
                ],
                "title": "Benchmarking Time Series Forecasting Models: From Statistical Techniques\n  to Foundation Models in Real-World Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Time Series Forecasting Models: From Statistical Techniques\n  to Foundation Models in Real-World Applications"
                },
                "summary": "Time series forecasting is essential for operational intelligence in the\nhospitality industry, and particularly challenging in large-scale, distributed\nsystems. This study evaluates the performance of statistical, machine learning\n(ML), deep learning, and foundation models in forecasting hourly sales over a\n14-day horizon using real-world data from a network of thousands of restaurants\nacross Germany. The forecasting solution includes features such as weather\nconditions, calendar events, and time-of-day patterns. Results demonstrate the\nstrong performance of ML-based meta-models and highlight the emerging potential\nof foundation models like Chronos and TimesFM, which deliver competitive\nperformance with minimal feature engineering, leveraging only the pre-trained\nmodel (zero-shot inference). Additionally, a hybrid PySpark-Pandas approach\nproves to be a robust solution for achieving horizontal scalability in\nlarge-scale deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting is essential for operational intelligence in the\nhospitality industry, and particularly challenging in large-scale, distributed\nsystems. This study evaluates the performance of statistical, machine learning\n(ML), deep learning, and foundation models in forecasting hourly sales over a\n14-day horizon using real-world data from a network of thousands of restaurants\nacross Germany. The forecasting solution includes features such as weather\nconditions, calendar events, and time-of-day patterns. Results demonstrate the\nstrong performance of ML-based meta-models and highlight the emerging potential\nof foundation models like Chronos and TimesFM, which deliver competitive\nperformance with minimal feature engineering, leveraging only the pre-trained\nmodel (zero-shot inference). Additionally, a hybrid PySpark-Pandas approach\nproves to be a robust solution for achieving horizontal scalability in\nlarge-scale deployments."
                },
                "authors": [
                    {
                        "name": "Issar Arab"
                    },
                    {
                        "name": "Rodrigo Benitez"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Benitez"
                },
                "author": "Rodrigo Benitez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01976v2",
                "updated": "2025-02-05T17:26:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    26,
                    35,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-04T03:36:44Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    36,
                    44,
                    1,
                    35,
                    0
                ],
                "title": "CITER: Collaborative Inference for Efficient Large Language Model\n  Decoding with Token-Level Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITER: Collaborative Inference for Efficient Large Language Model\n  Decoding with Token-Level Routing"
                },
                "summary": "Large language models have achieved remarkable success in various tasks but\nsuffer from high computational costs during inference, limiting their\ndeployment in resource-constrained applications. To address this issue, we\npropose a novel CITER (\\textbf{C}ollaborative \\textbf{I}nference with\n\\textbf{T}oken-l\\textbf{E}vel \\textbf{R}outing) framework that enables\nefficient collaboration between small and large language models (SLMs & LLMs)\nthrough a token-level routing strategy. Specifically, CITER routes non-critical\ntokens to an SLM for efficiency and routes critical tokens to an LLM for\ngeneralization quality. We formulate router training as a policy optimization,\nwhere the router receives rewards based on both the quality of predictions and\nthe inference costs of generation. This allows the router to learn to predict\ntoken-level routing scores and make routing decisions based on both the current\ntoken and the future impact of its decisions. To further accelerate the reward\nevaluation process, we introduce a shortcut which significantly reduces the\ncosts of the reward estimation and improving the practicality of our approach.\nExtensive experiments on five benchmark datasets demonstrate that CITER reduces\nthe inference costs while preserving high-quality generation, offering a\npromising solution for real-time and resource-constrained applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved remarkable success in various tasks but\nsuffer from high computational costs during inference, limiting their\ndeployment in resource-constrained applications. To address this issue, we\npropose a novel CITER (\\textbf{C}ollaborative \\textbf{I}nference with\n\\textbf{T}oken-l\\textbf{E}vel \\textbf{R}outing) framework that enables\nefficient collaboration between small and large language models (SLMs & LLMs)\nthrough a token-level routing strategy. Specifically, CITER routes non-critical\ntokens to an SLM for efficiency and routes critical tokens to an LLM for\ngeneralization quality. We formulate router training as a policy optimization,\nwhere the router receives rewards based on both the quality of predictions and\nthe inference costs of generation. This allows the router to learn to predict\ntoken-level routing scores and make routing decisions based on both the current\ntoken and the future impact of its decisions. To further accelerate the reward\nevaluation process, we introduce a shortcut which significantly reduces the\ncosts of the reward estimation and improving the practicality of our approach.\nExtensive experiments on five benchmark datasets demonstrate that CITER reduces\nthe inference costs while preserving high-quality generation, offering a\npromising solution for real-time and resource-constrained applications."
                },
                "authors": [
                    {
                        "name": "Wenhao Zheng"
                    },
                    {
                        "name": "Yixiao Chen"
                    },
                    {
                        "name": "Weitong Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Yun Li"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric P. Xing"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Huaxiu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Huaxiu Yao"
                },
                "author": "Huaxiu Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03382v1",
                "updated": "2025-02-05T17:18:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    18,
                    55,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T17:18:55Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    18,
                    55,
                    2,
                    36,
                    0
                ],
                "title": "High-Fidelity Simultaneous Speech-To-Speech Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Fidelity Simultaneous Speech-To-Speech Translation"
                },
                "summary": "We introduce Hibiki, a decoder-only model for simultaneous speech\ntranslation. Hibiki leverages a multistream language model to synchronously\nprocess source and target speech, and jointly produces text and audio tokens to\nperform speech-to-text and speech-to-speech translation. We furthermore address\nthe fundamental challenge of simultaneous interpretation, which unlike its\nconsecutive counterpart, where one waits for the end of the source utterance to\nstart translating, adapts its flow to accumulate just enough context to produce\na correct translation in real-time, chunk by chunk. To do so, we introduce a\nweakly-supervised method that leverages the perplexity of an off-the-shelf text\ntranslation system to identify optimal delays on a per-word basis and create\naligned synthetic data. After supervised training, Hibiki performs adaptive,\nsimultaneous speech translation with vanilla temperature sampling. On a\nFrench-English simultaneous speech translation task, Hibiki demonstrates\nstate-of-the-art performance in translation quality, speaker fidelity and\nnaturalness. Moreover, the simplicity of its inference process makes it\ncompatible with batched translation and even real-time on-device deployment. We\nprovide examples as well as models and inference code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Hibiki, a decoder-only model for simultaneous speech\ntranslation. Hibiki leverages a multistream language model to synchronously\nprocess source and target speech, and jointly produces text and audio tokens to\nperform speech-to-text and speech-to-speech translation. We furthermore address\nthe fundamental challenge of simultaneous interpretation, which unlike its\nconsecutive counterpart, where one waits for the end of the source utterance to\nstart translating, adapts its flow to accumulate just enough context to produce\na correct translation in real-time, chunk by chunk. To do so, we introduce a\nweakly-supervised method that leverages the perplexity of an off-the-shelf text\ntranslation system to identify optimal delays on a per-word basis and create\naligned synthetic data. After supervised training, Hibiki performs adaptive,\nsimultaneous speech translation with vanilla temperature sampling. On a\nFrench-English simultaneous speech translation task, Hibiki demonstrates\nstate-of-the-art performance in translation quality, speaker fidelity and\nnaturalness. Moreover, the simplicity of its inference process makes it\ncompatible with batched translation and even real-time on-device deployment. We\nprovide examples as well as models and inference code."
                },
                "authors": [
                    {
                        "name": "Tom Labiausse"
                    },
                    {
                        "name": "Laurent Mazar"
                    },
                    {
                        "name": "Edouard Grave"
                    },
                    {
                        "name": "Patrick Prez"
                    },
                    {
                        "name": "Alexandre Dfossez"
                    },
                    {
                        "name": "Neil Zeghidour"
                    }
                ],
                "author_detail": {
                    "name": "Neil Zeghidour"
                },
                "author": "Neil Zeghidour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03378v1",
                "updated": "2025-02-05T17:16:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    16,
                    44,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T17:16:44Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    16,
                    44,
                    2,
                    36,
                    0
                ],
                "title": "Learning to Identify Conflicts in RPKI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Identify Conflicts in RPKI"
                },
                "summary": "The long history of misconfigurations and errors in RPKI indicates that they\ncannot be easily avoided and will most probably persist also in the future.\nThese errors create conflicts between BGP announcements and their covering\nROAs, causing the RPKI validation to result in status invalid. Networks that\nenforce RPKI filtering with Route Origin Validation (ROV) would block such\nconflicting BGP announcements and as a result lose traffic from the\ncorresponding origins. Since the business incentives of networks are tightly\ncoupled with the traffic they relay, filtering legitimate traffic leads to a\nloss of revenue, reducing the motivation to filter invalid announcements with\nROV.\n  In this work, we introduce a new mechanism, LOV, designed for whitelisting\nbenign conflicts on an Internet scale. The resulting whitelist is made\navailable to RPKI supporting ASes to avoid filtering RPKI-invalid but benign\nroutes. Saving legitimate traffic resolves one main obstacle towards RPKI\ndeployment. We measure live BGP updates using LOV during a period of half a\nyear and whitelist 52,846 routes with benign origin errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The long history of misconfigurations and errors in RPKI indicates that they\ncannot be easily avoided and will most probably persist also in the future.\nThese errors create conflicts between BGP announcements and their covering\nROAs, causing the RPKI validation to result in status invalid. Networks that\nenforce RPKI filtering with Route Origin Validation (ROV) would block such\nconflicting BGP announcements and as a result lose traffic from the\ncorresponding origins. Since the business incentives of networks are tightly\ncoupled with the traffic they relay, filtering legitimate traffic leads to a\nloss of revenue, reducing the motivation to filter invalid announcements with\nROV.\n  In this work, we introduce a new mechanism, LOV, designed for whitelisting\nbenign conflicts on an Internet scale. The resulting whitelist is made\navailable to RPKI supporting ASes to avoid filtering RPKI-invalid but benign\nroutes. Saving legitimate traffic resolves one main obstacle towards RPKI\ndeployment. We measure live BGP updates using LOV during a period of half a\nyear and whitelist 52,846 routes with benign origin errors."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Shujie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Shujie Zhao"
                },
                "author": "Shujie Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03377v1",
                "updated": "2025-02-05T17:16:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    16,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T17:16:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    16,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement\n  Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement\n  Learning Approach"
                },
                "summary": "With the rapid development of next-generation Internet of Things (NG-IoT)\nnetworks, the increasing number of connected devices has led to a surge in\npower consumption. This rise in energy demand poses significant challenges to\nresource availability and raises sustainability concerns for large-scale IoT\ndeployments. Efficient energy utilization in communication networks,\nparticularly for power-constrained IoT devices, has thus become a critical area\nof research. In this paper, we deployed flying LoRa gateways (GWs) mounted on\nunmanned aerial vehicles (UAVs) to collect data from LoRa end devices (EDs) and\ntransmit it to a central server. Our primary objective is to maximize the\nglobal system energy efficiency (EE) of wireless LoRa networks by joint\noptimization of transmission power (TP), spreading factor (SF), bandwidth (W),\nand ED association. To solve this challenging problem, we model the problem as\na partially observable Markov decision process (POMDP), where each flying LoRa\nGW acts as a learning agent using a cooperative Multi-Agent Reinforcement\nLearning (MARL) approach under centralized training and decentralized execution\n(CTDE). Simulation results demonstrate that our proposed method, based on the\nmulti-agent proximal policy optimization (MAPPO) algorithm, significantly\nimproves the global system EE and surpasses the conventional MARL schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of next-generation Internet of Things (NG-IoT)\nnetworks, the increasing number of connected devices has led to a surge in\npower consumption. This rise in energy demand poses significant challenges to\nresource availability and raises sustainability concerns for large-scale IoT\ndeployments. Efficient energy utilization in communication networks,\nparticularly for power-constrained IoT devices, has thus become a critical area\nof research. In this paper, we deployed flying LoRa gateways (GWs) mounted on\nunmanned aerial vehicles (UAVs) to collect data from LoRa end devices (EDs) and\ntransmit it to a central server. Our primary objective is to maximize the\nglobal system energy efficiency (EE) of wireless LoRa networks by joint\noptimization of transmission power (TP), spreading factor (SF), bandwidth (W),\nand ED association. To solve this challenging problem, we model the problem as\na partially observable Markov decision process (POMDP), where each flying LoRa\nGW acts as a learning agent using a cooperative Multi-Agent Reinforcement\nLearning (MARL) approach under centralized training and decentralized execution\n(CTDE). Simulation results demonstrate that our proposed method, based on the\nmulti-agent proximal policy optimization (MAPPO) algorithm, significantly\nimproves the global system EE and surpasses the conventional MARL schemes."
                },
                "authors": [
                    {
                        "name": "Abdullahi Isa Ahmed"
                    },
                    {
                        "name": "El Mehdi Amhoud"
                    }
                ],
                "author_detail": {
                    "name": "El Mehdi Amhoud"
                },
                "author": "El Mehdi Amhoud",
                "arxiv_comment": "6 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03373v1",
                "updated": "2025-02-05T17:13:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    13,
                    32,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T17:13:32Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    13,
                    32,
                    2,
                    36,
                    0
                ],
                "title": "Demystifying Long Chain-of-Thought Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Long Chain-of-Thought Reasoning in LLMs"
                },
                "summary": "Scaling inference compute enhances reasoning in large language models (LLMs),\nwith long chains-of-thought (CoTs) enabling strategies like backtracking and\nerror correction. Reinforcement learning (RL) has emerged as a crucial method\nfor developing these capabilities, yet the conditions under which long CoTs\nemerge remain unclear, and RL training requires careful design choices. In this\nstudy, we systematically investigate the mechanics of long CoT reasoning,\nidentifying the key factors that enable models to generate long CoT\ntrajectories. Through extensive supervised fine-tuning (SFT) and RL\nexperiments, we present four main findings: (1) While SFT is not strictly\nnecessary, it simplifies training and improves efficiency; (2) Reasoning\ncapabilities tend to emerge with increased training compute, but their\ndevelopment is not guaranteed, making reward shaping crucial for stabilizing\nCoT length growth; (3) Scaling verifiable reward signals is critical for RL. We\nfind that leveraging noisy, web-extracted solutions with filtering mechanisms\nshows strong potential, particularly for out-of-distribution (OOD) tasks such\nas STEM reasoning; and (4) Core abilities like error correction are inherently\npresent in base models, but incentivizing these skills effectively for complex\ntasks via RL demands significant compute, and measuring their emergence\nrequires a nuanced approach. These insights provide practical guidance for\noptimizing training strategies to enhance long CoT reasoning in LLMs. Our code\nis available at: https://github.com/eddycmu/demystify-long-cot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling inference compute enhances reasoning in large language models (LLMs),\nwith long chains-of-thought (CoTs) enabling strategies like backtracking and\nerror correction. Reinforcement learning (RL) has emerged as a crucial method\nfor developing these capabilities, yet the conditions under which long CoTs\nemerge remain unclear, and RL training requires careful design choices. In this\nstudy, we systematically investigate the mechanics of long CoT reasoning,\nidentifying the key factors that enable models to generate long CoT\ntrajectories. Through extensive supervised fine-tuning (SFT) and RL\nexperiments, we present four main findings: (1) While SFT is not strictly\nnecessary, it simplifies training and improves efficiency; (2) Reasoning\ncapabilities tend to emerge with increased training compute, but their\ndevelopment is not guaranteed, making reward shaping crucial for stabilizing\nCoT length growth; (3) Scaling verifiable reward signals is critical for RL. We\nfind that leveraging noisy, web-extracted solutions with filtering mechanisms\nshows strong potential, particularly for out-of-distribution (OOD) tasks such\nas STEM reasoning; and (4) Core abilities like error correction are inherently\npresent in base models, but incentivizing these skills effectively for complex\ntasks via RL demands significant compute, and measuring their emergence\nrequires a nuanced approach. These insights provide practical guidance for\noptimizing training strategies to enhance long CoT reasoning in LLMs. Our code\nis available at: https://github.com/eddycmu/demystify-long-cot."
                },
                "authors": [
                    {
                        "name": "Edward Yeo"
                    },
                    {
                        "name": "Yuxuan Tong"
                    },
                    {
                        "name": "Morry Niu"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Xiang Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Yue"
                },
                "author": "Xiang Yue",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00326v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00326v8",
                "updated": "2025-02-05T17:08:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    8,
                    17,
                    2,
                    36,
                    0
                ],
                "published": "2023-12-01T03:44:54Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    3,
                    44,
                    54,
                    4,
                    335,
                    0
                ],
                "title": "Agent-OM: Leveraging LLM Agents for Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-OM: Leveraging LLM Agents for Ontology Matching"
                },
                "summary": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Kerry Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Kerry Taylor"
                },
                "author": "Kerry Taylor",
                "arxiv_comment": "19 pages, 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00326v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00326v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03368v1",
                "updated": "2025-02-05T17:06:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    6,
                    59,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T17:06:59Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    6,
                    59,
                    2,
                    36,
                    0
                ],
                "title": "PalimpChat: Declarative and Interactive AI analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PalimpChat: Declarative and Interactive AI analytics"
                },
                "summary": "Thanks to the advances in generative architectures and large language models,\ndata scientists can now code pipelines of machine-learning operations to\nprocess large collections of unstructured data. Recent progress has seen the\nrise of declarative AI frameworks (e.g., Palimpzest, Lotus, and DocETL) to\nbuild optimized and increasingly complex pipelines, but these systems often\nremain accessible only to expert programmers. In this demonstration, we present\nPalimpChat, a chat-based interface to Palimpzest that bridges this gap by\nletting users create and run sophisticated AI pipelines through natural\nlanguage alone. By integrating Archytas, a ReAct-based reasoning agent, and\nPalimpzest's suite of relational and LLM-based operators, PalimpChat provides a\npractical illustration of how a chat interface can make declarative AI\nframeworks truly accessible to non-experts.\n  Our demo system is publicly available online. At SIGMOD'25, participants can\nexplore three real-world scenarios--scientific discovery, legal discovery, and\nreal estate search--or apply PalimpChat to their own datasets. In this paper,\nwe focus on how PalimpChat, supported by the Palimpzest optimizer, simplifies\ncomplex AI workflows such as extracting and analyzing biomedical data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thanks to the advances in generative architectures and large language models,\ndata scientists can now code pipelines of machine-learning operations to\nprocess large collections of unstructured data. Recent progress has seen the\nrise of declarative AI frameworks (e.g., Palimpzest, Lotus, and DocETL) to\nbuild optimized and increasingly complex pipelines, but these systems often\nremain accessible only to expert programmers. In this demonstration, we present\nPalimpChat, a chat-based interface to Palimpzest that bridges this gap by\nletting users create and run sophisticated AI pipelines through natural\nlanguage alone. By integrating Archytas, a ReAct-based reasoning agent, and\nPalimpzest's suite of relational and LLM-based operators, PalimpChat provides a\npractical illustration of how a chat interface can make declarative AI\nframeworks truly accessible to non-experts.\n  Our demo system is publicly available online. At SIGMOD'25, participants can\nexplore three real-world scenarios--scientific discovery, legal discovery, and\nreal estate search--or apply PalimpChat to their own datasets. In this paper,\nwe focus on how PalimpChat, supported by the Palimpzest optimizer, simplifies\ncomplex AI workflows such as extracting and analyzing biomedical data."
                },
                "authors": [
                    {
                        "name": "Chunwei Liu"
                    },
                    {
                        "name": "Gerardo Vitagliano"
                    },
                    {
                        "name": "Brandon Rose"
                    },
                    {
                        "name": "Matt Prinz"
                    },
                    {
                        "name": "David Andrew Samson"
                    },
                    {
                        "name": "Michael Cafarella"
                    }
                ],
                "author_detail": {
                    "name": "Michael Cafarella"
                },
                "author": "Michael Cafarella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10121v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10121v2",
                "updated": "2025-02-05T16:54:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    54,
                    42,
                    2,
                    36,
                    0
                ],
                "published": "2024-05-16T14:21:33Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    14,
                    21,
                    33,
                    3,
                    137,
                    0
                ],
                "title": "Distilling Implicit Multimodal Knowledge into Large Language Models for\n  Zero-Resource Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Implicit Multimodal Knowledge into Large Language Models for\n  Zero-Resource Dialogue Generation"
                },
                "summary": "Integrating multimodal knowledge into large language models (LLMs) represents\na significant advancement in dialogue generation capabilities. However, the\neffective incorporation of such knowledge in zero-resource scenarios remains a\nsubstantial challenge due to the scarcity of diverse, high-quality dialogue\ndatasets. To address this, we propose the Visual Implicit Knowledge\nDistillation Framework (VIKDF), an innovative approach aimed at enhancing LLMs\nfor enriched dialogue generation in zero-resource contexts by leveraging\nimplicit multimodal knowledge. VIKDF comprises two main stages: knowledge\ndistillation, using an Implicit Query Transformer to extract and encode visual\nimplicit knowledge from image-text pairs into knowledge vectors; and knowledge\nintegration, employing a novel Bidirectional Variational Information Fusion\ntechnique to seamlessly integrate these distilled vectors into LLMs. This\nenables the LLMs to generate dialogues that are not only coherent and engaging\nbut also exhibit a deep understanding of the context through implicit\nmultimodal cues, effectively overcoming the limitations of zero-resource\nscenarios. Our extensive experimentation across two dialogue datasets shows\nthat VIKDF outperforms existing state-of-the-art models in generating\nhigh-quality dialogues. The code is available at\nhttps://github.com/zhangbo-nlp/VIKDF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating multimodal knowledge into large language models (LLMs) represents\na significant advancement in dialogue generation capabilities. However, the\neffective incorporation of such knowledge in zero-resource scenarios remains a\nsubstantial challenge due to the scarcity of diverse, high-quality dialogue\ndatasets. To address this, we propose the Visual Implicit Knowledge\nDistillation Framework (VIKDF), an innovative approach aimed at enhancing LLMs\nfor enriched dialogue generation in zero-resource contexts by leveraging\nimplicit multimodal knowledge. VIKDF comprises two main stages: knowledge\ndistillation, using an Implicit Query Transformer to extract and encode visual\nimplicit knowledge from image-text pairs into knowledge vectors; and knowledge\nintegration, employing a novel Bidirectional Variational Information Fusion\ntechnique to seamlessly integrate these distilled vectors into LLMs. This\nenables the LLMs to generate dialogues that are not only coherent and engaging\nbut also exhibit a deep understanding of the context through implicit\nmultimodal cues, effectively overcoming the limitations of zero-resource\nscenarios. Our extensive experimentation across two dialogue datasets shows\nthat VIKDF outperforms existing state-of-the-art models in generating\nhigh-quality dialogues. The code is available at\nhttps://github.com/zhangbo-nlp/VIKDF."
                },
                "authors": [
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Hui Ma"
                    },
                    {
                        "name": "Jian Ding"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Hongfei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Hongfei Lin"
                },
                "author": "Hongfei Lin",
                "arxiv_doi": "10.1016/j.inffus.2025.102985",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.inffus.2025.102985",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.10121v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10121v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by Information Fusion. The code is available at\n  https://github.com/zhangbo-nlp/VIKDF",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03358v1",
                "updated": "2025-02-05T16:53:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    53,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:53:45Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    53,
                    45,
                    2,
                    36,
                    0
                ],
                "title": "Minerva: A Programmable Memory Test Benchmark for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minerva: A Programmable Memory Test Benchmark for Language Models"
                },
                "summary": "How effectively can LLM-based AI assistants utilize their memory (context) to\nperform various tasks? Traditional data benchmarks, which are often manually\ncrafted, suffer from several limitations: they are static, susceptible to\noverfitting, difficult to interpret, and lack actionable insights--failing to\npinpoint the specific capabilities a model lacks when it does not pass a test.\nIn this paper, we present a framework for automatically generating a\ncomprehensive set of tests to evaluate models' abilities to use their memory\neffectively. Our framework extends the range of capability tests beyond the\ncommonly explored (passkey, key-value, needle in the haystack) search, a\ndominant focus in the literature. Specifically, we evaluate models on atomic\ntasks such as searching, recalling, editing, matching, comparing information in\ncontext memory, and performing basic operations when inputs are structured into\ndistinct blocks, simulating real-world data. Additionally, we design composite\ntests to investigate the models' ability to maintain state while operating on\nmemory. Our benchmark enables an interpretable, detailed assessment of memory\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How effectively can LLM-based AI assistants utilize their memory (context) to\nperform various tasks? Traditional data benchmarks, which are often manually\ncrafted, suffer from several limitations: they are static, susceptible to\noverfitting, difficult to interpret, and lack actionable insights--failing to\npinpoint the specific capabilities a model lacks when it does not pass a test.\nIn this paper, we present a framework for automatically generating a\ncomprehensive set of tests to evaluate models' abilities to use their memory\neffectively. Our framework extends the range of capability tests beyond the\ncommonly explored (passkey, key-value, needle in the haystack) search, a\ndominant focus in the literature. Specifically, we evaluate models on atomic\ntasks such as searching, recalling, editing, matching, comparing information in\ncontext memory, and performing basic operations when inputs are structured into\ndistinct blocks, simulating real-world data. Additionally, we design composite\ntests to investigate the models' ability to maintain state while operating on\nmemory. Our benchmark enables an interpretable, detailed assessment of memory\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Menglin Xia"
                    },
                    {
                        "name": "Victor Ruehle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Reza Shokri"
                    }
                ],
                "author_detail": {
                    "name": "Reza Shokri"
                },
                "author": "Reza Shokri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02370v2",
                "updated": "2025-02-05T16:40:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    40,
                    51,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-04T20:14:16Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    14,
                    16,
                    5,
                    4,
                    0
                ],
                "title": "Prepending or Cross-Attention for Speech-to-Text? An Empirical\n  Comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prepending or Cross-Attention for Speech-to-Text? An Empirical\n  Comparison"
                },
                "summary": "Following the remarkable success of Large Language Models (LLMs) in NLP\ntasks, there is increasing interest in extending their capabilities to speech\n-- the most common form of communication. The most widespread approach to\nintegrating speech into LLMs is dense feature prepending (DFP), which prepends\nthe projected speech representations to the textual representations, allowing\nend-to-end training with a speech encoder. This raises questions about the need\nfor a sophisticated speech encoder for DFP and how its performance compares\nwith a standard encoder-decoder (i.e., cross-attention) architecture. We\ncompare DFP and cross-attention under a variety of configurations, such as CTC\ncompression, sequence-level knowledge distillation, on monolingual, bilingual,\nand multilingual models. To perform a controlled architectural comparison, we\ntrain all models from scratch rather than using large pretrained models and use\ncomparable data and parameter settings, testing speech-to-text recognition\n(ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. Despite the\nwide adoption of DFP, our results do not indicate a clear advantage of DFP over\ncross-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the remarkable success of Large Language Models (LLMs) in NLP\ntasks, there is increasing interest in extending their capabilities to speech\n-- the most common form of communication. The most widespread approach to\nintegrating speech into LLMs is dense feature prepending (DFP), which prepends\nthe projected speech representations to the textual representations, allowing\nend-to-end training with a speech encoder. This raises questions about the need\nfor a sophisticated speech encoder for DFP and how its performance compares\nwith a standard encoder-decoder (i.e., cross-attention) architecture. We\ncompare DFP and cross-attention under a variety of configurations, such as CTC\ncompression, sequence-level knowledge distillation, on monolingual, bilingual,\nand multilingual models. To perform a controlled architectural comparison, we\ntrain all models from scratch rather than using large pretrained models and use\ncomparable data and parameter settings, testing speech-to-text recognition\n(ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. Despite the\nwide adoption of DFP, our results do not indicate a clear advantage of DFP over\ncross-attention."
                },
                "authors": [
                    {
                        "name": "Tsz Kin Lam"
                    },
                    {
                        "name": "Marco Gaido"
                    },
                    {
                        "name": "Sara Papi"
                    },
                    {
                        "name": "Luisa Bentivogli"
                    },
                    {
                        "name": "Barry Haddow"
                    }
                ],
                "author_detail": {
                    "name": "Barry Haddow"
                },
                "author": "Barry Haddow",
                "arxiv_comment": "Accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03325v1",
                "updated": "2025-02-05T16:22:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    22,
                    33,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:22:33Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    22,
                    33,
                    2,
                    36,
                    0
                ],
                "title": "ECM: A Unified Electronic Circuit Model for Explaining the Emergence of\n  In-Context Learning and Chain-of-Thought in Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECM: A Unified Electronic Circuit Model for Explaining the Emergence of\n  In-Context Learning and Chain-of-Thought in Large Language Model"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to significant\nsuccesses across various applications, where the most noticeable is to a series\nof emerging capabilities, particularly in the areas of In-Context Learning\n(ICL) and Chain-of-Thought (CoT). To better understand and control model\nperformance, many studies have begun investigating the underlying causes of\nthese phenomena and their impact on task outcomes. However, existing\nexplanatory frameworks predominantly focus on isolating and explaining ICL and\nCoT independently, leading to an incomplete understanding of their combined\ninfluence on model performance. To address this gap, we propose the Electronic\nCircuit Model (ECM), which provides a foundation for developing scalable,\nlearnable policies and improving the management of AI-generated content.\nSpecifically, ECM conceptualizes model behavior as an electronic circuit: ICL\nis represented as semantic magnetic field to providing an additional voltage\nfollowing Faraday's Law, while CoT is modeled as series resistors to constrain\nthe model output performance following Ohm's Law. Experimental results\ndemonstrate that the ECM effectively predicts and explains LLM performance\nacross a variety of prompting strategies. Furthermore, we apply ECM to advanced\nreasoning strategy optimization on a series of tasks, such as the International\nOlympiad in Informatics (IOI) and the International Mathematical Olympiad\n(IMO), achieving competitive performance that surpasses nearly 80% of top human\ncompetitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to significant\nsuccesses across various applications, where the most noticeable is to a series\nof emerging capabilities, particularly in the areas of In-Context Learning\n(ICL) and Chain-of-Thought (CoT). To better understand and control model\nperformance, many studies have begun investigating the underlying causes of\nthese phenomena and their impact on task outcomes. However, existing\nexplanatory frameworks predominantly focus on isolating and explaining ICL and\nCoT independently, leading to an incomplete understanding of their combined\ninfluence on model performance. To address this gap, we propose the Electronic\nCircuit Model (ECM), which provides a foundation for developing scalable,\nlearnable policies and improving the management of AI-generated content.\nSpecifically, ECM conceptualizes model behavior as an electronic circuit: ICL\nis represented as semantic magnetic field to providing an additional voltage\nfollowing Faraday's Law, while CoT is modeled as series resistors to constrain\nthe model output performance following Ohm's Law. Experimental results\ndemonstrate that the ECM effectively predicts and explains LLM performance\nacross a variety of prompting strategies. Furthermore, we apply ECM to advanced\nreasoning strategy optimization on a series of tasks, such as the International\nOlympiad in Informatics (IOI) and the International Mathematical Olympiad\n(IMO), achieving competitive performance that surpasses nearly 80% of top human\ncompetitors."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "Manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03323v1",
                "updated": "2025-02-05T16:22:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    22,
                    9,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:22:09Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    22,
                    9,
                    2,
                    36,
                    0
                ],
                "title": "Out-of-Distribution Detection using Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Distribution Detection using Synthetic Data Generation"
                },
                "summary": "Distinguishing in- and out-of-distribution (OOD) inputs is crucial for\nreliable deployment of classification systems. However, OOD data is typically\nunavailable or difficult to collect, posing a significant challenge for\naccurate OOD detection. In this work, we present a method that harnesses the\ngenerative capabilities of Large Language Models (LLMs) to create high-quality\nsynthetic OOD proxies, eliminating the dependency on any external OOD data\nsource. We study the efficacy of our method on classical text classification\ntasks such as toxicity detection and sentiment classification as well as\nclassification tasks arising in LLM development and deployment, such as\ntraining a reward model for RLHF and detecting misaligned generations.\nExtensive experiments on nine InD-OOD dataset pairs and various model sizes\nshow that our approach dramatically lowers false positive rates (achieving a\nperfect zero in some cases) while maintaining high accuracy on in-distribution\ntasks, outperforming baseline methods by a significant margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distinguishing in- and out-of-distribution (OOD) inputs is crucial for\nreliable deployment of classification systems. However, OOD data is typically\nunavailable or difficult to collect, posing a significant challenge for\naccurate OOD detection. In this work, we present a method that harnesses the\ngenerative capabilities of Large Language Models (LLMs) to create high-quality\nsynthetic OOD proxies, eliminating the dependency on any external OOD data\nsource. We study the efficacy of our method on classical text classification\ntasks such as toxicity detection and sentiment classification as well as\nclassification tasks arising in LLM development and deployment, such as\ntraining a reward model for RLHF and detecting misaligned generations.\nExtensive experiments on nine InD-OOD dataset pairs and various model sizes\nshow that our approach dramatically lowers false positive rates (achieving a\nperfect zero in some cases) while maintaining high accuracy on in-distribution\ntasks, outperforming baseline methods by a significant margin."
                },
                "authors": [
                    {
                        "name": "Momin Abbas"
                    },
                    {
                        "name": "Muneeza Azmat"
                    },
                    {
                        "name": "Raya Horesh"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Yurochkin"
                },
                "author": "Mikhail Yurochkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11613v3",
                "updated": "2025-02-05T16:21:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    21,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-20T17:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    19,
                    2,
                    0,
                    20,
                    0
                ],
                "title": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems"
                },
                "summary": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof-of-concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nfunctions (tools) developed by software engineers, creating an efficient\ndivision of responsibilities where developers focus on core API implementation\nand domain experts handle conversation design. While the framework shows\npromise in accessibility and adaptability, we identify key challenges including\ncomputational overhead, non-deterministic behavior, and domain-specific logic\noptimization. Future research directions include CR evaluation methods based on\nprompt engineering frameworks driven by goal-oriented grading criteria,\nimproving scalability for complex multi-agent interactions, and enhancing\nsystem robustness to address the identified limitations across diverse business\napplications."
                },
                "authors": [
                    {
                        "name": "Giorgio Robino"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Robino"
                },
                "author": "Giorgio Robino",
                "arxiv_comment": "In version 3, we added Subsection 1.2, \"Single-Agent vs. Multi-Agent\n  Architectures,\" and Figure 1 to clarify CAS prompt composition. We also\n  refined code block and appendix log formatting for improved readability, with\n  minor formatting corrections throughout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14617v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14617v2",
                "updated": "2025-02-05T16:10:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    10,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2024-01-26T03:20:40Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    3,
                    20,
                    40,
                    4,
                    26,
                    0
                ],
                "title": "A Systematic Literature Review on Explainability for Machine/Deep\n  Learning-based Software Engineering Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Literature Review on Explainability for Machine/Deep\n  Learning-based Software Engineering Research"
                },
                "summary": "The remarkable achievements of Artificial Intelligence (AI) algorithms,\nparticularly in Machine Learning (ML) and Deep Learning (DL), have fueled their\nextensive deployment across multiple sectors, including Software Engineering\n(SE). However, due to their black-box nature, these promising AI-driven SE\nmodels are still far from being deployed in practice. This lack of\nexplainability poses unwanted risks for their applications in critical tasks,\nsuch as vulnerability detection, where decision-making transparency is of\nparamount importance. This paper endeavors to elucidate this interdisciplinary\ndomain by presenting a systematic literature review of approaches that aim to\nimprove the explainability of AI models within the context of SE. The review\ncanvasses work appearing in the most prominent SE & AI conferences and\njournals, and spans 108 papers across 23 unique SE tasks. Based on three key\nResearch Questions (RQs), we aim to (1) summarize the SE tasks where XAI\ntechniques have shown success to date; (2) classify and analyze different XAI\ntechniques; and (3) investigate existing evaluation approaches. Based on our\nfindings, we identified a set of challenges remaining to be addressed in\nexisting studies, together with a set of guidelines highlighting potential\nopportunities we deemed appropriate and important for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable achievements of Artificial Intelligence (AI) algorithms,\nparticularly in Machine Learning (ML) and Deep Learning (DL), have fueled their\nextensive deployment across multiple sectors, including Software Engineering\n(SE). However, due to their black-box nature, these promising AI-driven SE\nmodels are still far from being deployed in practice. This lack of\nexplainability poses unwanted risks for their applications in critical tasks,\nsuch as vulnerability detection, where decision-making transparency is of\nparamount importance. This paper endeavors to elucidate this interdisciplinary\ndomain by presenting a systematic literature review of approaches that aim to\nimprove the explainability of AI models within the context of SE. The review\ncanvasses work appearing in the most prominent SE & AI conferences and\njournals, and spans 108 papers across 23 unique SE tasks. Based on three key\nResearch Questions (RQs), we aim to (1) summarize the SE tasks where XAI\ntechniques have shown success to date; (2) classify and analyze different XAI\ntechniques; and (3) investigate existing evaluation approaches. Based on our\nfindings, we identified a set of challenges remaining to be addressed in\nexisting studies, together with a set of guidelines highlighting potential\nopportunities we deemed appropriate and important for future work."
                },
                "authors": [
                    {
                        "name": "Sicong Cao"
                    },
                    {
                        "name": "Xiaobing Sun"
                    },
                    {
                        "name": "Ratnadira Widyasari"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Xiaoxue Wu"
                    },
                    {
                        "name": "Lili Bo"
                    },
                    {
                        "name": "Jiale Zhang"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Yixin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Chen"
                },
                "author": "Yixin Chen",
                "arxiv_comment": "Under Review in ACM Computing Surveys (Major Revision)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14617v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14617v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03307v1",
                "updated": "2025-02-05T16:08:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    8,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:08:05Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    8,
                    5,
                    2,
                    36,
                    0
                ],
                "title": "Intent Representation Learning with Large Language Model for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent Representation Learning with Large Language Model for\n  Recommendation"
                },
                "summary": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines. The implementation is available at\nhttps://github.com/wangyu0627/IRLLRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent-based recommender systems have garnered significant attention for\nuncovering latent fine-grained preferences. Intents, as underlying factors of\ninteractions, are crucial for improving recommendation interpretability. Most\nmethods define intents as learnable parameters updated alongside interactions.\nHowever, existing frameworks often overlook textual information (e.g., user\nreviews, item descriptions), which is crucial for alleviating the sparsity of\ninteraction intents. Exploring these multimodal intents, especially the\ninherent differences in representation spaces, poses two key challenges: i) How\nto align multimodal intents and effectively mitigate noise issues; ii) How to\nextract and match latent key intents across modalities. To tackle these\nchallenges, we propose a model-agnostic framework, Intent Representation\nLearning with Large Language Model (IRLLRec), which leverages large language\nmodels (LLMs) to construct multimodal intents and enhance recommendations.\nSpecifically, IRLLRec employs a dual-tower architecture to learn multimodal\nintent representations. Next, we propose pairwise and translation alignment to\neliminate inter-modal differences and enhance robustness against noisy input\nfeatures. Finally, to better match textual and interaction-based intents, we\nemploy momentum distillation to perform teacher-student learning on fused\nintent representations. Empirical evaluations on three datasets show that our\nIRLLRec framework outperforms baselines. The implementation is available at\nhttps://github.com/wangyu0627/IRLLRec."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Lei Sang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Yiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Zhang"
                },
                "author": "Yiwen Zhang",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03304v1",
                "updated": "2025-02-05T16:03:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    3,
                    17,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T16:03:17Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    16,
                    3,
                    17,
                    2,
                    36,
                    0
                ],
                "title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning"
                },
                "summary": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose \\textbf{Di}vergence-driven\n\\textbf{Z}eroth-\\textbf{O}rder (\\textbf{DiZO}) optimization. DiZO conducts\ndivergence-driven layer adaptation by incorporating projections to ZO updates,\ngenerating diverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48\\% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose \\textbf{Di}vergence-driven\n\\textbf{Z}eroth-\\textbf{O}rder (\\textbf{DiZO}) optimization. DiZO conducts\ndivergence-driven layer adaptation by incorporating projections to ZO updates,\ngenerating diverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48\\% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning."
                },
                "authors": [
                    {
                        "name": "Qitao Tan"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zheng Zhan"
                    },
                    {
                        "name": "Caiwei Ding"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Geng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Geng Yuan"
                },
                "author": "Geng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03298v1",
                "updated": "2025-02-05T15:56:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    56,
                    37,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:56:37Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    56,
                    37,
                    2,
                    36,
                    0
                ],
                "title": "MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge\n  Letters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge\n  Letters"
                },
                "summary": "While increasing patients' access to medical documents improves medical care,\nthis benefit is limited by varying health literacy levels and complex medical\nterminology. Large language models (LLMs) offer solutions by simplifying\nmedical information. However, evaluating LLMs for safe and patient-friendly\ntext generation is difficult due to the lack of standardized evaluation\nresources. To fill this gap, we developed MeDiSumQA. MeDiSumQA is a dataset\ncreated from MIMIC-IV discharge summaries through an automated pipeline\ncombining LLM-based question-answer generation with manual quality checks. We\nuse this dataset to evaluate various LLMs on patient-oriented\nquestion-answering. Our findings reveal that general-purpose LLMs frequently\nsurpass biomedical-adapted models, while automated metrics correlate with human\njudgment. By releasing MeDiSumQA on PhysioNet, we aim to advance the\ndevelopment of LLMs to enhance patient understanding and ultimately improve\ncare outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While increasing patients' access to medical documents improves medical care,\nthis benefit is limited by varying health literacy levels and complex medical\nterminology. Large language models (LLMs) offer solutions by simplifying\nmedical information. However, evaluating LLMs for safe and patient-friendly\ntext generation is difficult due to the lack of standardized evaluation\nresources. To fill this gap, we developed MeDiSumQA. MeDiSumQA is a dataset\ncreated from MIMIC-IV discharge summaries through an automated pipeline\ncombining LLM-based question-answer generation with manual quality checks. We\nuse this dataset to evaluate various LLMs on patient-oriented\nquestion-answering. Our findings reveal that general-purpose LLMs frequently\nsurpass biomedical-adapted models, while automated metrics correlate with human\njudgment. By releasing MeDiSumQA on PhysioNet, we aim to advance the\ndevelopment of LLMs to enhance patient understanding and ultimately improve\ncare outcomes."
                },
                "authors": [
                    {
                        "name": "Amin Dada"
                    },
                    {
                        "name": "Osman Alperen Koras"
                    },
                    {
                        "name": "Marie Bauer"
                    },
                    {
                        "name": "Amanda Butler"
                    },
                    {
                        "name": "Kaleb E. Smith"
                    },
                    {
                        "name": "Jens Kleesiek"
                    },
                    {
                        "name": "Julian Friedrich"
                    }
                ],
                "author_detail": {
                    "name": "Julian Friedrich"
                },
                "author": "Julian Friedrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04933v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04933v3",
                "updated": "2025-02-05T15:41:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    41,
                    15,
                    2,
                    36,
                    0
                ],
                "published": "2024-02-07T15:11:37Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    15,
                    11,
                    37,
                    2,
                    38,
                    0
                ],
                "title": "Context in Public Health for Underserved Communities: A Bayesian\n  Approach to Online Restless Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context in Public Health for Underserved Communities: A Bayesian\n  Approach to Online Restless Bandits"
                },
                "summary": "Public health programs often provide interventions to encourage program\nadherence, and effectively allocating interventions is vital for producing the\ngreatest overall health outcomes, especially in underserved communities where\nresources are limited. Such resource allocation problems are often modeled as\nrestless multi-armed bandits (RMABs) with unknown underlying transition\ndynamics, hence requiring online reinforcement learning (RL). We present\nBayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs\nthat novelly combines techniques in Bayesian modeling with Thompson sampling to\nflexibly model the complex RMAB settings present in public health program\nadherence problems, namely context and non-stationarity. BCoR's key strength is\nthe ability to leverage shared information within and between arms to learn the\nunknown RMAB transition dynamics quickly in intervention-scarce settings with\nrelatively short time horizons, which is common in public health applications.\nEmpirically, BCoR achieves substantially higher finite-sample performance over\na range of experimental settings, including a setting using real-world\nadherence data that was developed in collaboration with ARMMAN, an NGO in India\nwhich runs a large-scale maternal mHealth program, showcasing BCoR practical\nutility and potential for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public health programs often provide interventions to encourage program\nadherence, and effectively allocating interventions is vital for producing the\ngreatest overall health outcomes, especially in underserved communities where\nresources are limited. Such resource allocation problems are often modeled as\nrestless multi-armed bandits (RMABs) with unknown underlying transition\ndynamics, hence requiring online reinforcement learning (RL). We present\nBayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs\nthat novelly combines techniques in Bayesian modeling with Thompson sampling to\nflexibly model the complex RMAB settings present in public health program\nadherence problems, namely context and non-stationarity. BCoR's key strength is\nthe ability to leverage shared information within and between arms to learn the\nunknown RMAB transition dynamics quickly in intervention-scarce settings with\nrelatively short time horizons, which is common in public health applications.\nEmpirically, BCoR achieves substantially higher finite-sample performance over\na range of experimental settings, including a setting using real-world\nadherence data that was developed in collaboration with ARMMAN, an NGO in India\nwhich runs a large-scale maternal mHealth program, showcasing BCoR practical\nutility and potential for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Biyonka Liang"
                    },
                    {
                        "name": "Lily Xu"
                    },
                    {
                        "name": "Aparna Taneja"
                    },
                    {
                        "name": "Milind Tambe"
                    },
                    {
                        "name": "Lucas Janson"
                    }
                ],
                "author_detail": {
                    "name": "Lucas Janson"
                },
                "author": "Lucas Janson",
                "arxiv_comment": "29 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04933v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04933v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03283v1",
                "updated": "2025-02-05T15:37:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    37,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:37:05Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    37,
                    5,
                    2,
                    36,
                    0
                ],
                "title": "SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex\n  Reasoning over Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex\n  Reasoning over Knowledge Graphs"
                },
                "summary": "Recent advancements have highlighted that Large Language Models (LLMs) are\nprone to hallucinations when solving complex reasoning problems, leading to\nerroneous results. To tackle this issue, researchers incorporate Knowledge\nGraphs (KGs) to improve the reasoning ability of LLMs. However, existing\nmethods face two limitations: 1) they typically assume that all answers to the\nquestions are contained in KGs, neglecting the incompleteness issue of KGs, and\n2) they treat the KG as a static repository and overlook the implicit logical\nreasoning structures inherent in KGs. In this paper, we introduce SymAgent, an\ninnovative neural-symbolic agent framework that achieves collaborative\naugmentation between KGs and LLMs. We conceptualize KGs as dynamic environments\nand transform complex reasoning tasks into a multi-step interactive process,\nenabling KGs to participate deeply in the reasoning process. SymAgent consists\nof two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages\nLLM's inductive reasoning capability to extract symbolic rules from KGs,\nguiding efficient question decomposition. The Agent-Executor autonomously\ninvokes predefined action tools to integrate information from KGs and external\ndocuments, addressing the issues of KG incompleteness. Furthermore, we design a\nself-learning framework comprising online exploration and offline iterative\npolicy updating phases, enabling the agent to automatically synthesize\nreasoning trajectories and improve performance. Experimental results\ndemonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields\nbetter or comparable performance compared to various strong baselines. Further\nanalysis reveals that our agent can identify missing triples, facilitating\nautomatic KG updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements have highlighted that Large Language Models (LLMs) are\nprone to hallucinations when solving complex reasoning problems, leading to\nerroneous results. To tackle this issue, researchers incorporate Knowledge\nGraphs (KGs) to improve the reasoning ability of LLMs. However, existing\nmethods face two limitations: 1) they typically assume that all answers to the\nquestions are contained in KGs, neglecting the incompleteness issue of KGs, and\n2) they treat the KG as a static repository and overlook the implicit logical\nreasoning structures inherent in KGs. In this paper, we introduce SymAgent, an\ninnovative neural-symbolic agent framework that achieves collaborative\naugmentation between KGs and LLMs. We conceptualize KGs as dynamic environments\nand transform complex reasoning tasks into a multi-step interactive process,\nenabling KGs to participate deeply in the reasoning process. SymAgent consists\nof two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages\nLLM's inductive reasoning capability to extract symbolic rules from KGs,\nguiding efficient question decomposition. The Agent-Executor autonomously\ninvokes predefined action tools to integrate information from KGs and external\ndocuments, addressing the issues of KG incompleteness. Furthermore, we design a\nself-learning framework comprising online exploration and offline iterative\npolicy updating phases, enabling the agent to automatically synthesize\nreasoning trajectories and improve performance. Experimental results\ndemonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields\nbetter or comparable performance compared to various strong baselines. Further\nanalysis reveals that our agent can identify missing triples, facilitating\nautomatic KG updates."
                },
                "authors": [
                    {
                        "name": "Ben Liu"
                    },
                    {
                        "name": "Jihai Zhang"
                    },
                    {
                        "name": "Fangquan Lin"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Min Peng"
                    },
                    {
                        "name": "Wotao Yin"
                    }
                ],
                "author_detail": {
                    "name": "Wotao Yin"
                },
                "author": "Wotao Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03275v1",
                "updated": "2025-02-05T15:33:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    33,
                    0,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:33:00Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    33,
                    0,
                    2,
                    36,
                    0
                ],
                "title": "Token Assorted: Mixing Latent and Text Tokens for Improved Language\n  Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Assorted: Mixing Latent and Text Tokens for Improved Language\n  Model Reasoning"
                },
                "summary": "Large Language Models (LLMs) excel at reasoning and planning when trained on\nchainof-thought (CoT) data, where the step-by-step thought process is\nexplicitly outlined by text tokens. However, this results in lengthy inputs\nwhere many words support textual coherence rather than core reasoning\ninformation, and processing these inputs consumes substantial computation\nresources. In this work, we propose a hybrid representation of the reasoning\nprocess, where we partially abstract away the initial reasoning steps using\nlatent discrete tokens generated by VQ-VAE, significantly reducing the length\nof reasoning traces. We explore the use of latent trace abstractions in two\nscenarios: 1) training the model from scratch for the Keys-Finding Maze\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\nincluding unseen latent tokens, for both logical and mathematical reasoning\nproblems. To facilitate effective learning, we introduce a simple training\nprocedure that randomly mixes latent and text tokens, which enables fast\nadaptation to new latent tokens. Our approach consistently outperforms the\nbaselines methods in various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at reasoning and planning when trained on\nchainof-thought (CoT) data, where the step-by-step thought process is\nexplicitly outlined by text tokens. However, this results in lengthy inputs\nwhere many words support textual coherence rather than core reasoning\ninformation, and processing these inputs consumes substantial computation\nresources. In this work, we propose a hybrid representation of the reasoning\nprocess, where we partially abstract away the initial reasoning steps using\nlatent discrete tokens generated by VQ-VAE, significantly reducing the length\nof reasoning traces. We explore the use of latent trace abstractions in two\nscenarios: 1) training the model from scratch for the Keys-Finding Maze\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\nincluding unseen latent tokens, for both logical and mathematical reasoning\nproblems. To facilitate effective learning, we introduce a simple training\nprocedure that randomly mixes latent and text tokens, which enables fast\nadaptation to new latent tokens. Our approach consistently outperforms the\nbaselines methods in various benchmarks."
                },
                "authors": [
                    {
                        "name": "DiJia Su"
                    },
                    {
                        "name": "Hanlin Zhu"
                    },
                    {
                        "name": "Yingchen Xu"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Qinqing Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Qinqing Zheng"
                },
                "author": "Qinqing Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03274v1",
                "updated": "2025-02-05T15:29:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    29,
                    41,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:29:41Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    29,
                    41,
                    2,
                    36,
                    0
                ],
                "title": "A Scalable Approach to Probabilistic Neuro-Symbolic Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Approach to Probabilistic Neuro-Symbolic Verification"
                },
                "summary": "Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising\ndirection for integrating neural learning with symbolic reasoning. In the\nprobabilistic variant of such systems, a neural network first extracts a set of\nsymbols from sub-symbolic input, which are then used by a symbolic component to\nreason in a probabilistic manner towards answering a query. In this work, we\naddress the problem of formally verifying the robustness of such NeSy\nprobabilistic reasoning systems, therefore paving the way for their safe\ndeployment in critical domains. We analyze the complexity of solving this\nproblem exactly, and show that it is $\\mathrm{NP}^{\\# \\mathrm{P}}$-hard. To\novercome this issue, we propose the first approach for approximate,\nrelaxation-based verification of probabilistic NeSy systems. We demonstrate\nexperimentally that the proposed method scales exponentially better than\nsolver-based solutions and apply our technique to a real-world autonomous\ndriving dataset, where we verify a safety property under large input\ndimensionalities and network sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising\ndirection for integrating neural learning with symbolic reasoning. In the\nprobabilistic variant of such systems, a neural network first extracts a set of\nsymbols from sub-symbolic input, which are then used by a symbolic component to\nreason in a probabilistic manner towards answering a query. In this work, we\naddress the problem of formally verifying the robustness of such NeSy\nprobabilistic reasoning systems, therefore paving the way for their safe\ndeployment in critical domains. We analyze the complexity of solving this\nproblem exactly, and show that it is $\\mathrm{NP}^{\\# \\mathrm{P}}$-hard. To\novercome this issue, we propose the first approach for approximate,\nrelaxation-based verification of probabilistic NeSy systems. We demonstrate\nexperimentally that the proposed method scales exponentially better than\nsolver-based solutions and apply our technique to a real-world autonomous\ndriving dataset, where we verify a safety property under large input\ndimensionalities and network sizes."
                },
                "authors": [
                    {
                        "name": "Vasileios Manginas"
                    },
                    {
                        "name": "Nikolaos Manginas"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Sherwin Varghese"
                    },
                    {
                        "name": "Nikos Katzouris"
                    },
                    {
                        "name": "Georgios Paliouras"
                    },
                    {
                        "name": "Alessio Lomuscio"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Lomuscio"
                },
                "author": "Alessio Lomuscio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10970v2",
                "updated": "2025-02-05T15:24:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    24,
                    26,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-19T07:09:11Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    7,
                    9,
                    11,
                    6,
                    19,
                    0
                ],
                "title": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically\n  Justify Replacing Human Annotators with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically\n  Justify Replacing Human Annotators with LLMs"
                },
                "summary": "The \"LLM-as-a-judge\" paradigm employs Large Language Models (LLMs) as\nannotators and evaluators in tasks traditionally performed by humans. LLM\nannotations are widely used, not only in NLP research but also in fields like\nmedicine, psychology, and social science. Despite their role in shaping study\nresults and insights, there is no standard or rigorous procedure to determine\nwhether LLMs can replace human annotators. In this paper, we propose a novel\nstatistical procedure -- the Alternative Annotator Test (alt-test) -- that\nrequires only a modest subset of annotated examples to justify using LLM\nannotations. Additionally, we introduce a versatile and interpretable measure\nfor comparing LLM judges. To demonstrate our procedure, we curated a diverse\ncollection of ten datasets, consisting of language and vision-language tasks,\nand conducted experiments with six LLMs and four prompting techniques. Our\nresults show that LLMs can sometimes replace humans with closed-source LLMs\n(such as GPT-4o), outperforming open-source LLMs, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"LLM-as-a-judge\" paradigm employs Large Language Models (LLMs) as\nannotators and evaluators in tasks traditionally performed by humans. LLM\nannotations are widely used, not only in NLP research but also in fields like\nmedicine, psychology, and social science. Despite their role in shaping study\nresults and insights, there is no standard or rigorous procedure to determine\nwhether LLMs can replace human annotators. In this paper, we propose a novel\nstatistical procedure -- the Alternative Annotator Test (alt-test) -- that\nrequires only a modest subset of annotated examples to justify using LLM\nannotations. Additionally, we introduce a versatile and interpretable measure\nfor comparing LLM judges. To demonstrate our procedure, we curated a diverse\ncollection of ten datasets, consisting of language and vision-language tasks,\nand conducted experiments with six LLMs and four prompting techniques. Our\nresults show that LLMs can sometimes replace humans with closed-source LLMs\n(such as GPT-4o), outperforming open-source LLMs, and that prompting techniques\nyield judges of varying quality. We hope this study encourages more rigorous\nand reliable practices."
                },
                "authors": [
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Rotem Dror"
                    }
                ],
                "author_detail": {
                    "name": "Rotem Dror"
                },
                "author": "Rotem Dror",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03261v1",
                "updated": "2025-02-05T15:17:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    17,
                    25,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:17:25Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    17,
                    25,
                    2,
                    36,
                    0
                ],
                "title": "CARROT: A Cost Aware Rate Optimal Router",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARROT: A Cost Aware Rate Optimal Router"
                },
                "summary": "With the rapid growth in the number of Large Language Models (LLMs), there\nhas been a recent interest in LLM routing, or directing queries to the cheapest\nLLM that can deliver a suitable response. Following this line of work, we\nintroduce CARROT, a Cost AwaRe Rate Optimal rouTer that can select models based\non any desired trade-off between performance and cost. Given a query, CARROT\nselects a model based on estimates of models' cost and performance. Its\nsimplicity lends CARROT computational efficiency, while our theoretical\nanalysis demonstrates minimax rate-optimality in its routing performance.\nAlongside CARROT, we also introduce the Smart Price-aware Routing (SPROUT)\ndataset to facilitate routing on a wide spectrum of queries with the latest\nstate-of-the-art LLMs. Using SPROUT and prior benchmarks such as Routerbench\nand open-LLM-leaderboard-v2 we empirically validate CARROT's performance\nagainst several alternative routers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth in the number of Large Language Models (LLMs), there\nhas been a recent interest in LLM routing, or directing queries to the cheapest\nLLM that can deliver a suitable response. Following this line of work, we\nintroduce CARROT, a Cost AwaRe Rate Optimal rouTer that can select models based\non any desired trade-off between performance and cost. Given a query, CARROT\nselects a model based on estimates of models' cost and performance. Its\nsimplicity lends CARROT computational efficiency, while our theoretical\nanalysis demonstrates minimax rate-optimality in its routing performance.\nAlongside CARROT, we also introduce the Smart Price-aware Routing (SPROUT)\ndataset to facilitate routing on a wide spectrum of queries with the latest\nstate-of-the-art LLMs. Using SPROUT and prior benchmarks such as Routerbench\nand open-LLM-leaderboard-v2 we empirically validate CARROT's performance\nagainst several alternative routers."
                },
                "authors": [
                    {
                        "name": "Seamus Somerstep"
                    },
                    {
                        "name": "Felipe Maia Polo"
                    },
                    {
                        "name": "Allysson Flavio Melo de Oliveira"
                    },
                    {
                        "name": "Prattyush Mangal"
                    },
                    {
                        "name": "Mrian Silva"
                    },
                    {
                        "name": "Onkar Bhardwaj"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    },
                    {
                        "name": "Subha Maity"
                    }
                ],
                "author_detail": {
                    "name": "Subha Maity"
                },
                "author": "Subha Maity",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15230v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15230v3",
                "updated": "2025-02-05T15:10:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    10,
                    23,
                    2,
                    36,
                    0
                ],
                "published": "2023-12-23T11:45:22Z",
                "published_parsed": [
                    2023,
                    12,
                    23,
                    11,
                    45,
                    22,
                    5,
                    357,
                    0
                ],
                "title": "PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs"
                },
                "summary": "Neural Networks can be effectively compressed through pruning, significantly\nreducing storage and compute demands while maintaining predictive performance.\nSimple yet effective methods like magnitude pruning remove less important\nparameters and typically require a costly retraining procedure to restore\nperformance. However, with the rise of LLMs, full retraining has become\ninfeasible due to memory and compute constraints. This study challenges the\npractice of retraining all parameters by showing that updating a small subset\nof highly expressive parameters can suffice to recover or even enhance\nperformance after pruning. Surprisingly, retraining just 0.01%-0.05% of the\nparameters in GPT-architectures can match the performance of full retraining\nacross various sparsity levels, significantly reducing compute and memory\nrequirements, and enabling retraining of models with up to 30 billion\nparameters on a single GPU in minutes. To bridge the gap to full retraining in\nthe high sparsity regime, we introduce two novel LoRA variants that, unlike\nstandard LoRA, allow merging adapters back without compromising sparsity. Going\na step further, we show that these methods can be applied for memory-efficient\nlayer-wise reconstruction, significantly enhancing state-of-the-art\nretraining-free methods like Wanda (Sun et al., 2023) and SparseGPT (Frantar &\nAlistarh, 2023). Our findings present a promising alternative to avoiding\nretraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Networks can be effectively compressed through pruning, significantly\nreducing storage and compute demands while maintaining predictive performance.\nSimple yet effective methods like magnitude pruning remove less important\nparameters and typically require a costly retraining procedure to restore\nperformance. However, with the rise of LLMs, full retraining has become\ninfeasible due to memory and compute constraints. This study challenges the\npractice of retraining all parameters by showing that updating a small subset\nof highly expressive parameters can suffice to recover or even enhance\nperformance after pruning. Surprisingly, retraining just 0.01%-0.05% of the\nparameters in GPT-architectures can match the performance of full retraining\nacross various sparsity levels, significantly reducing compute and memory\nrequirements, and enabling retraining of models with up to 30 billion\nparameters on a single GPU in minutes. To bridge the gap to full retraining in\nthe high sparsity regime, we introduce two novel LoRA variants that, unlike\nstandard LoRA, allow merging adapters back without compromising sparsity. Going\na step further, we show that these methods can be applied for memory-efficient\nlayer-wise reconstruction, significantly enhancing state-of-the-art\nretraining-free methods like Wanda (Sun et al., 2023) and SparseGPT (Frantar &\nAlistarh, 2023). Our findings present a promising alternative to avoiding\nretraining."
                },
                "authors": [
                    {
                        "name": "Max Zimmer"
                    },
                    {
                        "name": "Megi Andoni"
                    },
                    {
                        "name": "Christoph Spiegel"
                    },
                    {
                        "name": "Sebastian Pokutta"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pokutta"
                },
                "author": "Sebastian Pokutta",
                "arxiv_comment": "32 pages, 7 figures, 24 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15230v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15230v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03253v1",
                "updated": "2025-02-05T15:08:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    8,
                    43,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:08:43Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    8,
                    43,
                    2,
                    36,
                    0
                ],
                "title": "How do Humans and Language Models Reason About Creativity? A Comparative\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do Humans and Language Models Reason About Creativity? A Comparative\n  Analysis"
                },
                "summary": "Creativity assessment in science and engineering is increasingly based on\nboth human and AI judgment, but the cognitive processes and biases behind these\nevaluations remain poorly understood. We conducted two experiments examining\nhow including example solutions with ratings impact creativity evaluation,\nusing a finegrained annotation protocol where raters were tasked with\nexplaining their originality scores and rating for the facets of remoteness\n(whether the response is \"far\" from everyday ideas), uncommonness (whether the\nresponse is rare), and cleverness. In Study 1, we analyzed creativity ratings\nfrom 72 experts with formal science or engineering training, comparing those\nwho received example solutions with ratings (example) to those who did not (no\nexample). Computational text analysis revealed that, compared to experts with\nexamples, no-example experts used more comparative language (e.g.,\n\"better/worse\") and emphasized solution uncommonness, suggesting they may have\nrelied more on memory retrieval for comparisons. In Study 2, parallel analyses\nwith state-of-the-art LLMs revealed that models prioritized uncommonness and\nremoteness of ideas when rating originality, suggesting an evaluative process\nrooted around the semantic similarity of ideas. In the example condition, while\nLLM accuracy in predicting the true originality scores improved, the\ncorrelations of remoteness, uncommonness, and cleverness with originality also\nincreased substantially - to upwards of 0.99 - suggesting a homogenization in\nthe LLMs evaluation of the individual facets. These findings highlight\nimportant implications for how humans and AI reason about creativity and\nsuggest diverging preferences for what different populations prioritize when\nrating.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creativity assessment in science and engineering is increasingly based on\nboth human and AI judgment, but the cognitive processes and biases behind these\nevaluations remain poorly understood. We conducted two experiments examining\nhow including example solutions with ratings impact creativity evaluation,\nusing a finegrained annotation protocol where raters were tasked with\nexplaining their originality scores and rating for the facets of remoteness\n(whether the response is \"far\" from everyday ideas), uncommonness (whether the\nresponse is rare), and cleverness. In Study 1, we analyzed creativity ratings\nfrom 72 experts with formal science or engineering training, comparing those\nwho received example solutions with ratings (example) to those who did not (no\nexample). Computational text analysis revealed that, compared to experts with\nexamples, no-example experts used more comparative language (e.g.,\n\"better/worse\") and emphasized solution uncommonness, suggesting they may have\nrelied more on memory retrieval for comparisons. In Study 2, parallel analyses\nwith state-of-the-art LLMs revealed that models prioritized uncommonness and\nremoteness of ideas when rating originality, suggesting an evaluative process\nrooted around the semantic similarity of ideas. In the example condition, while\nLLM accuracy in predicting the true originality scores improved, the\ncorrelations of remoteness, uncommonness, and cleverness with originality also\nincreased substantially - to upwards of 0.99 - suggesting a homogenization in\nthe LLMs evaluation of the individual facets. These findings highlight\nimportant implications for how humans and AI reason about creativity and\nsuggest diverging preferences for what different populations prioritize when\nrating."
                },
                "authors": [
                    {
                        "name": "Antonio Laverghetta Jr."
                    },
                    {
                        "name": "Tuhin Chakrabarty"
                    },
                    {
                        "name": "Tom Hope"
                    },
                    {
                        "name": "Jimmy Pronchick"
                    },
                    {
                        "name": "Krupa Bhawsar"
                    },
                    {
                        "name": "Roger E. Beaty"
                    }
                ],
                "author_detail": {
                    "name": "Roger E. Beaty"
                },
                "author": "Roger E. Beaty",
                "arxiv_comment": "CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03250v1",
                "updated": "2025-02-05T15:05:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    5,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T15:05:05Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    15,
                    5,
                    5,
                    2,
                    36,
                    0
                ],
                "title": "SkyOctopus: Enabling Low-Latency Mobile Satellite Network through\n  Multiple Anchors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyOctopus: Enabling Low-Latency Mobile Satellite Network through\n  Multiple Anchors"
                },
                "summary": "The rapid deployment of low earth orbit (LEO) satellite constellations has\ndrawn attention to the potential of nonterrestrial networks (NTN) in providing\nglobal communication services. Telecom operators are attempting to collaborate\nwith satellite network providers to develop mobile satellite networks, which\nserve as an effective supplement to terrestrial networks. However, current\nmobile satellite network architectures still employ the single-anchor design of\nterrestrial mobile networks, leading to severely circuitous routing for users\nand significantly impacting their service experience. To reduce unnecessary\nlatency caused by circuitous routing and provide users with low-latency global\ninternet services, this paper presents SkyOctopus, an advanced multi-anchor\nmobile satellite network architecture. SkyOctopus innovatively deploys traffic\nclassifiers on satellites to enable connections between users and multiple\nanchor points distributed globally. It guarantees optimal anchor point\nselection for each user's target server by monitoring multiple end-to-end\npaths. We build a prototype of SkyOctopus using enhanced Open5GS and UERANSIM,\nwhich is driven by actual LEO satellite constellations such as Starlink,\nKuiper, and OneWeb. We conducted extensive experiments, and the results\ndemonstrate that, compared to standard 5G NTN and two other existing schemes,\nSkyOctopus can reduce end-to-end latency by up to 53\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid deployment of low earth orbit (LEO) satellite constellations has\ndrawn attention to the potential of nonterrestrial networks (NTN) in providing\nglobal communication services. Telecom operators are attempting to collaborate\nwith satellite network providers to develop mobile satellite networks, which\nserve as an effective supplement to terrestrial networks. However, current\nmobile satellite network architectures still employ the single-anchor design of\nterrestrial mobile networks, leading to severely circuitous routing for users\nand significantly impacting their service experience. To reduce unnecessary\nlatency caused by circuitous routing and provide users with low-latency global\ninternet services, this paper presents SkyOctopus, an advanced multi-anchor\nmobile satellite network architecture. SkyOctopus innovatively deploys traffic\nclassifiers on satellites to enable connections between users and multiple\nanchor points distributed globally. It guarantees optimal anchor point\nselection for each user's target server by monitoring multiple end-to-end\npaths. We build a prototype of SkyOctopus using enhanced Open5GS and UERANSIM,\nwhich is driven by actual LEO satellite constellations such as Starlink,\nKuiper, and OneWeb. We conducted extensive experiments, and the results\ndemonstrate that, compared to standard 5G NTN and two other existing schemes,\nSkyOctopus can reduce end-to-end latency by up to 53\\%."
                },
                "authors": [
                    {
                        "name": "Shaojie Su"
                    },
                    {
                        "name": "Jiasheng Wu"
                    },
                    {
                        "name": "Zijie Ying"
                    },
                    {
                        "name": "Zhiyuan Zhao"
                    },
                    {
                        "name": "Xiangyu Jia"
                    },
                    {
                        "name": "Wenjun Zhu"
                    },
                    {
                        "name": "Yue Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Gao"
                },
                "author": "Yue Gao",
                "arxiv_comment": "11 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03233v1",
                "updated": "2025-02-05T14:49:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    14,
                    49,
                    12,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T14:49:12Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    14,
                    49,
                    12,
                    2,
                    36,
                    0
                ],
                "title": "Exploring the Security Threats of Knowledge Base Poisoning in\n  Retrieval-Augmented Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Security Threats of Knowledge Base Poisoning in\n  Retrieval-Augmented Code Generation"
                },
                "summary": "The integration of Large Language Models (LLMs) into software development has\nrevolutionized the field, particularly through the use of Retrieval-Augmented\nCode Generation (RACG) systems that enhance code generation with information\nfrom external knowledge bases. However, the security implications of RACG\nsystems, particularly the risks posed by vulnerable code examples in the\nknowledge base, remain largely unexplored. This risk is particularly concerning\ngiven that public code repositories, which often serve as the sources for\nknowledge base collection in RACG systems, are usually accessible to anyone in\nthe community. Malicious attackers can exploit this accessibility to inject\nvulnerable code into the knowledge base, making it toxic. Once these poisoned\nsamples are retrieved and incorporated into the generated code, they can\npropagate security vulnerabilities into the final product. This paper presents\nthe first comprehensive study on the security risks associated with RACG\nsystems, focusing on how vulnerable code in the knowledge base compromises the\nsecurity of generated code. We investigate the LLM-generated code security\nacross different settings through extensive experiments using four major LLMs,\ntwo retrievers, and two poisoning scenarios. Our findings highlight the\nsignificant threat of knowledge base poisoning, where even a single poisoned\ncode example can compromise up to 48% of generated code. Our findings provide\ncrucial insights into vulnerability introduction in RACG systems and offer\npractical mitigation recommendations, thereby helping improve the security of\nLLM-generated code in future works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into software development has\nrevolutionized the field, particularly through the use of Retrieval-Augmented\nCode Generation (RACG) systems that enhance code generation with information\nfrom external knowledge bases. However, the security implications of RACG\nsystems, particularly the risks posed by vulnerable code examples in the\nknowledge base, remain largely unexplored. This risk is particularly concerning\ngiven that public code repositories, which often serve as the sources for\nknowledge base collection in RACG systems, are usually accessible to anyone in\nthe community. Malicious attackers can exploit this accessibility to inject\nvulnerable code into the knowledge base, making it toxic. Once these poisoned\nsamples are retrieved and incorporated into the generated code, they can\npropagate security vulnerabilities into the final product. This paper presents\nthe first comprehensive study on the security risks associated with RACG\nsystems, focusing on how vulnerable code in the knowledge base compromises the\nsecurity of generated code. We investigate the LLM-generated code security\nacross different settings through extensive experiments using four major LLMs,\ntwo retrievers, and two poisoning scenarios. Our findings highlight the\nsignificant threat of knowledge base poisoning, where even a single poisoned\ncode example can compromise up to 48% of generated code. Our findings provide\ncrucial insights into vulnerability introduction in RACG systems and offer\npractical mitigation recommendations, thereby helping improve the security of\nLLM-generated code in future works."
                },
                "authors": [
                    {
                        "name": "Bo Lin"
                    },
                    {
                        "name": "Shangwen Wang"
                    },
                    {
                        "name": "Liqian Chen"
                    },
                    {
                        "name": "Xiaoguang Mao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoguang Mao"
                },
                "author": "Xiaoguang Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03199v1",
                "updated": "2025-02-05T14:19:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    14,
                    19,
                    52,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T14:19:52Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    14,
                    19,
                    52,
                    2,
                    36,
                    0
                ],
                "title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large\n  Language Models"
                },
                "summary": "Despite their impressive capacities, Large language models (LLMs) often\nstruggle with the hallucination issue of generating inaccurate or fabricated\ncontent even when they possess correct knowledge. In this paper, we extend the\nexploration of the correlation between hidden-state prediction changes and\noutput factuality into a deeper, token-wise level. Based on the insights , we\npropose cross-layer Entropy eNhanced Decoding (END), a decoding method that\nmitigates hallucinations without requiring extra training. END leverages inner\nprobability changes across layers to individually quantify the factual\nknowledge required for each candidate token, and adjusts the final predicting\ndistribution to prioritize tokens with higher factuality. Experiments on both\nhallucination and QA benchmarks demonstrate that END significantly enhances the\ntruthfulness and informativeness of generated content while maintaining robust\nQA accuracy. Moreover, our work provides a deeper perspective on understanding\nthe correlations between inherent knowledge and output factuality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capacities, Large language models (LLMs) often\nstruggle with the hallucination issue of generating inaccurate or fabricated\ncontent even when they possess correct knowledge. In this paper, we extend the\nexploration of the correlation between hidden-state prediction changes and\noutput factuality into a deeper, token-wise level. Based on the insights , we\npropose cross-layer Entropy eNhanced Decoding (END), a decoding method that\nmitigates hallucinations without requiring extra training. END leverages inner\nprobability changes across layers to individually quantify the factual\nknowledge required for each candidate token, and adjusts the final predicting\ndistribution to prioritize tokens with higher factuality. Experiments on both\nhallucination and QA benchmarks demonstrate that END significantly enhances the\ntruthfulness and informativeness of generated content while maintaining robust\nQA accuracy. Moreover, our work provides a deeper perspective on understanding\nthe correlations between inherent knowledge and output factuality."
                },
                "authors": [
                    {
                        "name": "Jialiang Wu"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Sijia Liu"
                    },
                    {
                        "name": "Yi Tang"
                    },
                    {
                        "name": "Sen Song"
                    },
                    {
                        "name": "Xiaoyi Wang"
                    },
                    {
                        "name": "Longjun Cai"
                    }
                ],
                "author_detail": {
                    "name": "Longjun Cai"
                },
                "author": "Longjun Cai",
                "arxiv_comment": "NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17424v2",
                "updated": "2025-02-05T14:06:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    14,
                    6,
                    21,
                    2,
                    36,
                    0
                ],
                "published": "2024-05-27T17:59:32Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    17,
                    59,
                    32,
                    0,
                    148,
                    0
                ],
                "title": "LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence"
                },
                "summary": "Recent embodied agents are primarily built based on reinforcement learning\n(RL) or large language models (LLMs). Among them, RL agents are efficient for\ndeployment but only perform very few tasks. By contrast, giant LLM agents\n(often more than 1000B parameters) present strong generalization while\ndemanding enormous computing resources. In this work, we combine their\nadvantages while avoiding the drawbacks by conducting the proposed referee RL\non our developed large auto-regressive model (LARM). Specifically, LARM is\nbuilt upon a lightweight LLM (fewer than 5B parameters) and directly outputs\nthe next action to execute rather than text. We mathematically reveal that\nclassic RL feedbacks vanish in long-horizon embodied exploration and introduce\na giant LLM based referee to handle this reward vanishment during training\nLARM. In this way, LARM learns to complete diverse open-world tasks without\nhuman intervention. Especially, LARM successfully harvests enchanted diamond\nequipment in Minecraft, which demands significantly longer decision-making\nchains than the highest achievements of prior best methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent embodied agents are primarily built based on reinforcement learning\n(RL) or large language models (LLMs). Among them, RL agents are efficient for\ndeployment but only perform very few tasks. By contrast, giant LLM agents\n(often more than 1000B parameters) present strong generalization while\ndemanding enormous computing resources. In this work, we combine their\nadvantages while avoiding the drawbacks by conducting the proposed referee RL\non our developed large auto-regressive model (LARM). Specifically, LARM is\nbuilt upon a lightweight LLM (fewer than 5B parameters) and directly outputs\nthe next action to execute rather than text. We mathematically reveal that\nclassic RL feedbacks vanish in long-horizon embodied exploration and introduce\na giant LLM based referee to handle this reward vanishment during training\nLARM. In this way, LARM learns to complete diverse open-world tasks without\nhuman intervention. Especially, LARM successfully harvests enchanted diamond\nequipment in Minecraft, which demands significantly longer decision-making\nchains than the highest achievements of prior best methods."
                },
                "authors": [
                    {
                        "name": "Zhuoling Li"
                    },
                    {
                        "name": "Xiaogang Xu"
                    },
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "SerNam Lim"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hengshuang Zhao"
                },
                "author": "Hengshuang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09008v2",
                "updated": "2025-02-05T13:47:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    47,
                    11,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-11T17:25:52Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    25,
                    52,
                    4,
                    285,
                    0
                ],
                "title": "SuperCorrect: Supervising and Correcting Language Models with\n  Error-Driven Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperCorrect: Supervising and Correcting Language Models with\n  Error-Driven Insights"
                },
                "summary": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown\nsignificant improvements in various reasoning tasks. However, smaller models\nsuch as Llama-3-8B and DeepSeekMath-Base still struggle with complex\nmathematical reasoning because they fail to effectively identify and correct\nreasoning errors. Recent reflection-based methods aim to address these issues\nby enabling self-reflection and self-correction, but they still face challenges\nin independently detecting errors in their reasoning steps. To overcome these\nlimitations, we propose SuperCorrect, a novel two-stage framework that uses a\nlarge teacher model to supervise and correct both the reasoning and reflection\nprocesses of a smaller student model. In the first stage, we extract\nhierarchical high-level and detailed thought templates from the teacher model\nto guide the student model in eliciting more fine-grained reasoning thoughts.\nIn the second stage, we introduce cross-model collaborative direct preference\noptimization (DPO) to enhance the self-correction abilities of the student\nmodel by following the teacher's correction traces during training. This\ncross-model DPO approach teaches the student model to effectively locate and\nresolve erroneous thoughts with error-driven insights from the teacher model,\nbreaking the bottleneck of its thoughts and acquiring new skills and knowledge\nto tackle challenging problems. Extensive experiments consistently demonstrate\nour superiority over previous methods. Notably, our SuperCorrect-7B model\nsignificantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and\nQwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA\nperformance among all 7B models. Code:\nhttps://github.com/YangLing0818/SuperCorrect-llm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown\nsignificant improvements in various reasoning tasks. However, smaller models\nsuch as Llama-3-8B and DeepSeekMath-Base still struggle with complex\nmathematical reasoning because they fail to effectively identify and correct\nreasoning errors. Recent reflection-based methods aim to address these issues\nby enabling self-reflection and self-correction, but they still face challenges\nin independently detecting errors in their reasoning steps. To overcome these\nlimitations, we propose SuperCorrect, a novel two-stage framework that uses a\nlarge teacher model to supervise and correct both the reasoning and reflection\nprocesses of a smaller student model. In the first stage, we extract\nhierarchical high-level and detailed thought templates from the teacher model\nto guide the student model in eliciting more fine-grained reasoning thoughts.\nIn the second stage, we introduce cross-model collaborative direct preference\noptimization (DPO) to enhance the self-correction abilities of the student\nmodel by following the teacher's correction traces during training. This\ncross-model DPO approach teaches the student model to effectively locate and\nresolve erroneous thoughts with error-driven insights from the teacher model,\nbreaking the bottleneck of its thoughts and acquiring new skills and knowledge\nto tackle challenging problems. Extensive experiments consistently demonstrate\nour superiority over previous methods. Notably, our SuperCorrect-7B model\nsignificantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and\nQwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA\nperformance among all 7B models. Code:\nhttps://github.com/YangLing0818/SuperCorrect-llm"
                },
                "authors": [
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Zhaochen Yu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Minkai Xu"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "arxiv_comment": "ICLR 2025. Project: https://github.com/YangLing0818/SuperCorrect-llm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03159v1",
                "updated": "2025-02-05T13:32:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    32,
                    29,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T13:32:29Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    32,
                    29,
                    2,
                    36,
                    0
                ],
                "title": "PICBench: Benchmarking LLMs for Photonic Integrated Circuits Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PICBench: Benchmarking LLMs for Photonic Integrated Circuits Design"
                },
                "summary": "While large language models (LLMs) have shown remarkable potential in\nautomating various tasks in digital chip design, the field of Photonic\nIntegrated Circuits (PICs)-a promising solution to advanced chip\ndesigns-remains relatively unexplored in this context. The design of PICs is\ntime-consuming and prone to errors due to the extensive and repetitive nature\nof code involved in photonic chip design. In this paper, we introduce PICBench,\nthe first benchmarking and evaluation framework specifically designed to\nautomate PIC design generation using LLMs, where the generated output takes the\nform of a netlist. Our benchmark consists of dozens of meticulously crafted PIC\ndesign problems, spanning from fundamental device designs to more complex\ncircuit-level designs. It automatically evaluates both the syntax and\nfunctionality of generated PIC designs by comparing simulation outputs with\nexpert-written solutions, leveraging an open-source simulator. We evaluate a\nrange of existing LLMs, while also conducting comparative tests on various\nprompt engineering techniques to enhance LLM performance in automated PIC\ndesign. The results reveal the challenges and potential of LLMs in the PIC\ndesign domain, offering insights into the key areas that require further\nresearch and development to optimize automation in this field. Our benchmark\nand evaluation code is available at https://github.com/PICDA/PICBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown remarkable potential in\nautomating various tasks in digital chip design, the field of Photonic\nIntegrated Circuits (PICs)-a promising solution to advanced chip\ndesigns-remains relatively unexplored in this context. The design of PICs is\ntime-consuming and prone to errors due to the extensive and repetitive nature\nof code involved in photonic chip design. In this paper, we introduce PICBench,\nthe first benchmarking and evaluation framework specifically designed to\nautomate PIC design generation using LLMs, where the generated output takes the\nform of a netlist. Our benchmark consists of dozens of meticulously crafted PIC\ndesign problems, spanning from fundamental device designs to more complex\ncircuit-level designs. It automatically evaluates both the syntax and\nfunctionality of generated PIC designs by comparing simulation outputs with\nexpert-written solutions, leveraging an open-source simulator. We evaluate a\nrange of existing LLMs, while also conducting comparative tests on various\nprompt engineering techniques to enhance LLM performance in automated PIC\ndesign. The results reveal the challenges and potential of LLMs in the PIC\ndesign domain, offering insights into the key areas that require further\nresearch and development to optimize automation in this field. Our benchmark\nand evaluation code is available at https://github.com/PICDA/PICBench."
                },
                "authors": [
                    {
                        "name": "Yuchao Wu"
                    },
                    {
                        "name": "Xiaofei Yu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yang Luo"
                    },
                    {
                        "name": "Yeyu Tong"
                    },
                    {
                        "name": "Yuzhe Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhe Ma"
                },
                "author": "Yuzhe Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03158v1",
                "updated": "2025-02-05T13:31:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    31,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T13:31:38Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    31,
                    38,
                    2,
                    36,
                    0
                ],
                "title": "Strategizing with AI: Insights from a Beauty Contest Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategizing with AI: Insights from a Beauty Contest Experiment"
                },
                "summary": "A Keynesian beauty contest is a wide class of games of guessing the most\npopular strategy among other players. In particular, guessing a fraction of a\nmean of numbers chosen by all players is a classic behavioral experiment\ndesigned to test iterative reasoning patterns among various groups of people.\nThe previous literature reveals that the level of sophistication of the\nopponents is an important factor affecting the outcome of the game. Smarter\ndecision makers choose strategies that are closer to theoretical Nash\nequilibrium and demonstrate faster convergence to equilibrium in iterated\ncontests with information revelation. We replicate a series of classic\nexperiments by running virtual experiments with modern large language models\n(LLMs) who play against various groups of virtual players. We test how advanced\nthe LLMs' behavior is compared to the behavior of human players. We show that\nLLMs typically take into account the opponents' level of sophistication and\nadapt by changing the strategy. In various settings, most LLMs (with the\nexception of Llama) are more sophisticated and play lower numbers compared to\nhuman players. Our results suggest that LLMs (except Llama) are rather\nsuccessful in identifying the underlying strategic environment and adopting the\nstrategies to the changing set of parameters of the game in the same way that\nhuman players do. All LLMs still fail to play dominant strategies in a\ntwo-player game. Our results contribute to the discussion on the accuracy of\nmodeling human economic agents by artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Keynesian beauty contest is a wide class of games of guessing the most\npopular strategy among other players. In particular, guessing a fraction of a\nmean of numbers chosen by all players is a classic behavioral experiment\ndesigned to test iterative reasoning patterns among various groups of people.\nThe previous literature reveals that the level of sophistication of the\nopponents is an important factor affecting the outcome of the game. Smarter\ndecision makers choose strategies that are closer to theoretical Nash\nequilibrium and demonstrate faster convergence to equilibrium in iterated\ncontests with information revelation. We replicate a series of classic\nexperiments by running virtual experiments with modern large language models\n(LLMs) who play against various groups of virtual players. We test how advanced\nthe LLMs' behavior is compared to the behavior of human players. We show that\nLLMs typically take into account the opponents' level of sophistication and\nadapt by changing the strategy. In various settings, most LLMs (with the\nexception of Llama) are more sophisticated and play lower numbers compared to\nhuman players. Our results suggest that LLMs (except Llama) are rather\nsuccessful in identifying the underlying strategic environment and adopting the\nstrategies to the changing set of parameters of the game in the same way that\nhuman players do. All LLMs still fail to play dominant strategies in a\ntwo-player game. Our results contribute to the discussion on the accuracy of\nmodeling human economic agents by artificial intelligence."
                },
                "authors": [
                    {
                        "name": "Iuliia Alekseenko"
                    },
                    {
                        "name": "Dmitry Dagaev"
                    },
                    {
                        "name": "Sofia Paklina"
                    },
                    {
                        "name": "Petr Parshakov"
                    }
                ],
                "author_detail": {
                    "name": "Petr Parshakov"
                },
                "author": "Petr Parshakov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18094v2",
                "updated": "2025-02-05T13:23:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    23,
                    18,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-30T02:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    2,
                    10,
                    23,
                    3,
                    30,
                    0
                ],
                "title": "AlphaAdam:Asynchronous Masked Optimization with Dynamic Alpha for\n  Selective Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaAdam:Asynchronous Masked Optimization with Dynamic Alpha for\n  Selective Updates"
                },
                "summary": "In the training of large language models (LLMs), updating parameters more\nefficiently and stably has always been an important challenge. To achieve\nefficient parameter updates, existing methods usually achieve performance\ncomparable to full parameter updates through methods such as low-dimensional\ndecomposition or layer-wise selective updates. In this work, we propose\nAlphaAdam, an optimization framework for LLM from the perspective of\nintra-layer parameter updates. By decoupling parameter updates and dynamically\nadjusting their strength, AlphaAdam accelerates convergence and improves\ntraining stability. We construct parameter masks based on the consistency of\nhistorical momentum and gradient direction and combine them with an adaptive\nmask strength strategy to ensure efficient optimization and theoretical\nconvergence guarantees, which is also applicable to most momentum-based\noptimizers. Extensive experiments show that AlphaAdam outperforms\nstate-of-the-art methods such as AdamW in terms of convergence speed and\ncomputational efficiency across tasks, including GPT-2 pre-trained and\nfine-tuned RoBERTa and Llama-7B. Our AlphaAdam implements an optimizer\nenhancement framework for LLMs through intra-layer asynchronous masked adaptive\nupdates. Our code is available in this https://github.com/MaeChd/AlphaAdam.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the training of large language models (LLMs), updating parameters more\nefficiently and stably has always been an important challenge. To achieve\nefficient parameter updates, existing methods usually achieve performance\ncomparable to full parameter updates through methods such as low-dimensional\ndecomposition or layer-wise selective updates. In this work, we propose\nAlphaAdam, an optimization framework for LLM from the perspective of\nintra-layer parameter updates. By decoupling parameter updates and dynamically\nadjusting their strength, AlphaAdam accelerates convergence and improves\ntraining stability. We construct parameter masks based on the consistency of\nhistorical momentum and gradient direction and combine them with an adaptive\nmask strength strategy to ensure efficient optimization and theoretical\nconvergence guarantees, which is also applicable to most momentum-based\noptimizers. Extensive experiments show that AlphaAdam outperforms\nstate-of-the-art methods such as AdamW in terms of convergence speed and\ncomputational efficiency across tasks, including GPT-2 pre-trained and\nfine-tuned RoBERTa and Llama-7B. Our AlphaAdam implements an optimizer\nenhancement framework for LLMs through intra-layer asynchronous masked adaptive\nupdates. Our code is available in this https://github.com/MaeChd/AlphaAdam."
                },
                "authors": [
                    {
                        "name": "Da Chang"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Ganzhao Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Ganzhao Yuan"
                },
                "author": "Ganzhao Yuan",
                "arxiv_comment": "Theorem 3.5 has issues of insufficient rigor. The content \"Let\n  $E[g_i^2] = \\sigma_i^2$ ... $E[g_im_{t-1,i}] = \\rho_i \\sigma_i^2$ be the\n  correlation between gradients and historical momentum ....\" is a non-standard\n  assumption and may mislead readers. In the spirit of rigor and\n  responsibility, we temporarily withdraw this version of the content",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02910v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02910v3",
                "updated": "2025-02-05T13:20:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    20,
                    24,
                    2,
                    36,
                    0
                ],
                "published": "2024-03-05T12:21:57Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    12,
                    21,
                    57,
                    1,
                    65,
                    0
                ],
                "title": "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image"
                },
                "summary": "There has been an increasing interest in the alignment of large language\nmodels (LLMs) with human values. However, the safety issues of their\nintegration with a vision module, or vision language models (VLMs), remain\nrelatively underexplored. In this paper, we propose a novel jailbreaking attack\nagainst VLMs, aiming to bypass their safety barrier when a user inputs harmful\ninstructions. A scenario where our poisoned (image, text) data pairs are\nincluded in the training data is assumed. By replacing the original textual\ncaptions with malicious jailbreak prompts, our method can perform jailbreak\nattacks with the poisoned images. Moreover, we analyze the effect of poison\nratios and positions of trainable parameters on our attack's success rate. For\nevaluation, we design two metrics to quantify the success rate and the\nstealthiness of our attack. Together with a list of curated harmful\ninstructions, a benchmark for measuring attack efficacy is provided. We\ndemonstrate the efficacy of our attack by comparing it with baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been an increasing interest in the alignment of large language\nmodels (LLMs) with human values. However, the safety issues of their\nintegration with a vision module, or vision language models (VLMs), remain\nrelatively underexplored. In this paper, we propose a novel jailbreaking attack\nagainst VLMs, aiming to bypass their safety barrier when a user inputs harmful\ninstructions. A scenario where our poisoned (image, text) data pairs are\nincluded in the training data is assumed. By replacing the original textual\ncaptions with malicious jailbreak prompts, our method can perform jailbreak\nattacks with the poisoned images. Moreover, we analyze the effect of poison\nratios and positions of trainable parameters on our attack's success rate. For\nevaluation, we design two metrics to quantify the success rate and the\nstealthiness of our attack. Together with a list of curated harmful\ninstructions, a benchmark for measuring attack efficacy is provided. We\ndemonstrate the efficacy of our attack by comparing it with baseline methods."
                },
                "authors": [
                    {
                        "name": "Xijia Tao"
                    },
                    {
                        "name": "Shuai Zhong"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02910v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02910v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01506v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01506v2",
                "updated": "2025-02-05T13:18:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    18,
                    13,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-03T16:39:48Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    16,
                    39,
                    48,
                    0,
                    34,
                    0
                ],
                "title": "TwinMarket: A Scalable Behavioral and Social Simulation for Financial\n  Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TwinMarket: A Scalable Behavioral and Social Simulation for Financial\n  Markets"
                },
                "summary": "The study of social emergence has long been a central focus in social\nscience. Traditional modeling approaches, such as rule-based Agent-Based Models\n(ABMs), struggle to capture the diversity and complexity of human behavior,\nparticularly the irrational factors emphasized in behavioral economics.\nRecently, large language model (LLM) agents have gained traction as simulation\ntools for modeling human behavior in social science and role-playing\napplications. Studies suggest that LLMs can account for cognitive biases,\nemotional fluctuations, and other non-rational influences, enabling more\nrealistic simulations of socio-economic dynamics. In this work, we introduce\nTwinMarket, a novel multi-agent framework that leverages LLMs to simulate\nsocio-economic systems. Specifically, we examine how individual behaviors,\nthrough interactions and feedback mechanisms, give rise to collective dynamics\nand emergent phenomena. Through experiments in a simulated stock market\nenvironment, we demonstrate how individual actions can trigger group behaviors,\nleading to emergent outcomes such as financial bubbles and recessions. Our\napproach provides valuable insights into the complex interplay between\nindividual decision-making and collective socio-economic patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of social emergence has long been a central focus in social\nscience. Traditional modeling approaches, such as rule-based Agent-Based Models\n(ABMs), struggle to capture the diversity and complexity of human behavior,\nparticularly the irrational factors emphasized in behavioral economics.\nRecently, large language model (LLM) agents have gained traction as simulation\ntools for modeling human behavior in social science and role-playing\napplications. Studies suggest that LLMs can account for cognitive biases,\nemotional fluctuations, and other non-rational influences, enabling more\nrealistic simulations of socio-economic dynamics. In this work, we introduce\nTwinMarket, a novel multi-agent framework that leverages LLMs to simulate\nsocio-economic systems. Specifically, we examine how individual behaviors,\nthrough interactions and feedback mechanisms, give rise to collective dynamics\nand emergent phenomena. Through experiments in a simulated stock market\nenvironment, we demonstrate how individual actions can trigger group behaviors,\nleading to emergent outcomes such as financial bubbles and recessions. Our\napproach provides valuable insights into the complex interplay between\nindividual decision-making and collective socio-economic patterns."
                },
                "authors": [
                    {
                        "name": "Yuzhe Yang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Kaidi Zhang"
                    },
                    {
                        "name": "Yunmiao Zhang"
                    },
                    {
                        "name": "Honghai Yu"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01506v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01506v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03147v1",
                "updated": "2025-02-05T13:16:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    16,
                    41,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T13:16:41Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    16,
                    41,
                    2,
                    36,
                    0
                ],
                "title": "Scalable In-Context Learning on Tabular Data via Retrieval-Augmented\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable In-Context Learning on Tabular Data via Retrieval-Augmented\n  Large Language Models"
                },
                "summary": "Recent studies have shown that large language models (LLMs), when customized\nwith post-training on tabular data, can acquire general tabular in-context\nlearning (TabICL) capabilities. These models are able to transfer effectively\nacross diverse data schemas and different task domains. However, existing\nLLM-based TabICL approaches are constrained to few-shot scenarios due to the\nsequence length limitations of LLMs, as tabular instances represented in plain\ntext consume substantial tokens. To address this limitation and enable scalable\nTabICL for any data size, we propose retrieval-augmented LLMs tailored to\ntabular data. Our approach incorporates a customized retrieval module, combined\nwith retrieval-guided instruction-tuning for LLMs. This enables LLMs to\neffectively leverage larger datasets, achieving significantly improved\nperformance across 69 widely recognized datasets and demonstrating promising\nscaling behavior. Extensive comparisons with state-of-the-art tabular models\nreveal that, while LLM-based TabICL still lags behind well-tuned numeric models\nin overall performance, it uncovers powerful algorithms under limited contexts,\nenhances ensemble diversity, and excels on specific datasets. These unique\nproperties underscore the potential of language as a universal and accessible\ninterface for scalable tabular data learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that large language models (LLMs), when customized\nwith post-training on tabular data, can acquire general tabular in-context\nlearning (TabICL) capabilities. These models are able to transfer effectively\nacross diverse data schemas and different task domains. However, existing\nLLM-based TabICL approaches are constrained to few-shot scenarios due to the\nsequence length limitations of LLMs, as tabular instances represented in plain\ntext consume substantial tokens. To address this limitation and enable scalable\nTabICL for any data size, we propose retrieval-augmented LLMs tailored to\ntabular data. Our approach incorporates a customized retrieval module, combined\nwith retrieval-guided instruction-tuning for LLMs. This enables LLMs to\neffectively leverage larger datasets, achieving significantly improved\nperformance across 69 widely recognized datasets and demonstrating promising\nscaling behavior. Extensive comparisons with state-of-the-art tabular models\nreveal that, while LLM-based TabICL still lags behind well-tuned numeric models\nin overall performance, it uncovers powerful algorithms under limited contexts,\nenhances ensemble diversity, and excels on specific datasets. These unique\nproperties underscore the potential of language as a universal and accessible\ninterface for scalable tabular data learning."
                },
                "authors": [
                    {
                        "name": "Xumeng Wen"
                    },
                    {
                        "name": "Shun Zheng"
                    },
                    {
                        "name": "Zhen Xu"
                    },
                    {
                        "name": "Yiming Sun"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06479v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06479v3",
                "updated": "2025-02-05T12:50:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    50,
                    46,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-09T02:14:39Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    14,
                    39,
                    2,
                    283,
                    0
                ],
                "title": "Compressing Large Language Models with Automated Sub-Network Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing Large Language Models with Automated Sub-Network Search"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional reasoning abilities,\nenabling strong generalization across diverse tasks such as commonsense\nreasoning and instruction following. However, as LLMs scale, inference costs\nbecome increasingly prohibitive, accumulating significantly over their life\ncycle. In this paper we consider model compression for LLMs to reduce model\nsize while improving downstream task performance. We phrase this as a neural\narchitecture search problem that automatically prunes structural components,\nsuch as attention heads, neurons, and layers by searching for the\nPareto-optimal set of sub-networks balancing between performance and on-device\nlatency. Compared to state-of-the-art structural pruning approaches and\nfine-tuned smaller sub-networks extracted from the pre-trained model, our\nmethod achieves upto 9.85% improvement on average on 11 diverse downstream\ntasks, while achieving up to 22% improvement of on-device latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional reasoning abilities,\nenabling strong generalization across diverse tasks such as commonsense\nreasoning and instruction following. However, as LLMs scale, inference costs\nbecome increasingly prohibitive, accumulating significantly over their life\ncycle. In this paper we consider model compression for LLMs to reduce model\nsize while improving downstream task performance. We phrase this as a neural\narchitecture search problem that automatically prunes structural components,\nsuch as attention heads, neurons, and layers by searching for the\nPareto-optimal set of sub-networks balancing between performance and on-device\nlatency. Compared to state-of-the-art structural pruning approaches and\nfine-tuned smaller sub-networks extracted from the pre-trained model, our\nmethod achieves upto 9.85% improvement on average on 11 diverse downstream\ntasks, while achieving up to 22% improvement of on-device latency."
                },
                "authors": [
                    {
                        "name": "Rhea Sanjay Sukthanker"
                    },
                    {
                        "name": "Benedikt Staffler"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Aaron Klein"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Klein"
                },
                "author": "Aaron Klein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06479v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06479v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03132v1",
                "updated": "2025-02-05T12:49:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    49,
                    26,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T12:49:26Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    49,
                    26,
                    2,
                    36,
                    0
                ],
                "title": "SPARK: A Modular Benchmark for Humanoid Robot Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPARK: A Modular Benchmark for Humanoid Robot Safety"
                },
                "summary": "This paper introduces the Safe Protective and Assistive Robot Kit (SPARK), a\ncomprehensive benchmark designed to ensure safety in humanoid autonomy and\nteleoperation. Humanoid robots pose significant safety risks due to their\nphysical capabilities of interacting with complex environments. The physical\nstructures of humanoid robots further add complexity to the design of general\nsafety solutions. To facilitate the safe deployment of complex robot systems,\nSPARK can be used as a toolbox that comes with state-of-the-art safe control\nalgorithms in a modular and composable robot control framework. Users can\neasily configure safety criteria and sensitivity levels to optimize the balance\nbetween safety and performance. To accelerate humanoid safety research and\ndevelopment, SPARK provides a simulation benchmark that compares safety\napproaches in a variety of environments, tasks, and robot models. Furthermore,\nSPARK allows quick deployment of synthesized safe controllers on real robots.\nFor hardware deployment, SPARK supports Apple Vision Pro (AVP) or a Motion\nCapture System as external sensors, while also offering interfaces for seamless\nintegration with alternative hardware setups. This paper demonstrates SPARK's\ncapability with both simulation experiments and case studies with a Unitree G1\nhumanoid robot. Leveraging these advantages of SPARK, users and researchers can\nsignificantly improve the safety of their humanoid systems as well as\naccelerate relevant research. The open-source code is available at\nhttps://github.com/intelligent-control-lab/spark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the Safe Protective and Assistive Robot Kit (SPARK), a\ncomprehensive benchmark designed to ensure safety in humanoid autonomy and\nteleoperation. Humanoid robots pose significant safety risks due to their\nphysical capabilities of interacting with complex environments. The physical\nstructures of humanoid robots further add complexity to the design of general\nsafety solutions. To facilitate the safe deployment of complex robot systems,\nSPARK can be used as a toolbox that comes with state-of-the-art safe control\nalgorithms in a modular and composable robot control framework. Users can\neasily configure safety criteria and sensitivity levels to optimize the balance\nbetween safety and performance. To accelerate humanoid safety research and\ndevelopment, SPARK provides a simulation benchmark that compares safety\napproaches in a variety of environments, tasks, and robot models. Furthermore,\nSPARK allows quick deployment of synthesized safe controllers on real robots.\nFor hardware deployment, SPARK supports Apple Vision Pro (AVP) or a Motion\nCapture System as external sensors, while also offering interfaces for seamless\nintegration with alternative hardware setups. This paper demonstrates SPARK's\ncapability with both simulation experiments and case studies with a Unitree G1\nhumanoid robot. Leveraging these advantages of SPARK, users and researchers can\nsignificantly improve the safety of their humanoid systems as well as\naccelerate relevant research. The open-source code is available at\nhttps://github.com/intelligent-control-lab/spark."
                },
                "authors": [
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Kai S. Yun"
                    },
                    {
                        "name": "Yikuan Fang"
                    },
                    {
                        "name": "Sebin Jung"
                    },
                    {
                        "name": "Feihan Li"
                    },
                    {
                        "name": "Bowei Li"
                    },
                    {
                        "name": "Weiye Zhao"
                    },
                    {
                        "name": "Changliu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Changliu Liu"
                },
                "author": "Changliu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03129v1",
                "updated": "2025-02-05T12:39:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    39,
                    7,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T12:39:07Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    39,
                    7,
                    2,
                    36,
                    0
                ],
                "title": "Teaching Large Language Models Number-Focused Headline Generation With\n  Key Element Rationales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Large Language Models Number-Focused Headline Generation With\n  Key Element Rationales"
                },
                "summary": "Number-focused headline generation is a summarization task requiring both\nhigh textual quality and precise numerical accuracy, which poses a unique\nchallenge for Large Language Models (LLMs). Existing studies in the literature\nfocus only on either textual quality or numerical reasoning and thus are\ninadequate to address this challenge. In this paper, we propose a novel\nchain-of-thought framework for using rationales comprising key elements of the\nTopic, Entities, and Numerical reasoning (TEN) in news articles to enhance the\ncapability for LLMs to generate topic-aligned high-quality texts with precise\nnumerical accuracy. Specifically, a teacher LLM is employed to generate TEN\nrationales as supervision data, which are then used to teach and fine-tune a\nstudent LLM. Our approach teaches the student LLM automatic generation of\nrationales with enhanced capability for numerical reasoning and topic-aligned\nnumerical headline generation. Experiments show that our approach achieves\nsuperior performance in both textual quality and numerical accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Number-focused headline generation is a summarization task requiring both\nhigh textual quality and precise numerical accuracy, which poses a unique\nchallenge for Large Language Models (LLMs). Existing studies in the literature\nfocus only on either textual quality or numerical reasoning and thus are\ninadequate to address this challenge. In this paper, we propose a novel\nchain-of-thought framework for using rationales comprising key elements of the\nTopic, Entities, and Numerical reasoning (TEN) in news articles to enhance the\ncapability for LLMs to generate topic-aligned high-quality texts with precise\nnumerical accuracy. Specifically, a teacher LLM is employed to generate TEN\nrationales as supervision data, which are then used to teach and fine-tune a\nstudent LLM. Our approach teaches the student LLM automatic generation of\nrationales with enhanced capability for numerical reasoning and topic-aligned\nnumerical headline generation. Experiments show that our approach achieves\nsuperior performance in both textual quality and numerical accuracy."
                },
                "authors": [
                    {
                        "name": "Zhen Qian"
                    },
                    {
                        "name": "Xiuzhen Zhang"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Feng Xia"
                    }
                ],
                "author_detail": {
                    "name": "Feng Xia"
                },
                "author": "Feng Xia",
                "arxiv_comment": "Pre-print for a paper accepted to findings of NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03735v2",
                "updated": "2025-02-05T12:31:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    31,
                    1,
                    2,
                    36,
                    0
                ],
                "published": "2024-09-05T17:50:31Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    50,
                    31,
                    3,
                    249,
                    0
                ],
                "title": "Investigating Privacy Bias in Training Data of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Privacy Bias in Training Data of Language Models"
                },
                "summary": "As LLMs are integrated into sociotechnical systems, it is crucial to examine\nthe privacy biases they exhibit. A privacy bias refers to the skew in the\nappropriateness of information flows within a given context that LLMs acquire\nfrom large amounts of non-publicly available training data. This skew may\neither align with existing expectations or signal a symptom of systemic issues\nreflected in the training datasets.\n  We formulate a novel research question: how can we examine privacy biases in\nthe training data of LLMs? We present a novel approach to assess the privacy\nbiases using a contextual integrity-based methodology to evaluate the responses\nfrom different LLMs. Our approach accounts for the sensitivity of responses\nacross prompt variations, which hinders the evaluation of privacy biases. We\ninvestigate how privacy biases are affected by model capacities and\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs are integrated into sociotechnical systems, it is crucial to examine\nthe privacy biases they exhibit. A privacy bias refers to the skew in the\nappropriateness of information flows within a given context that LLMs acquire\nfrom large amounts of non-publicly available training data. This skew may\neither align with existing expectations or signal a symptom of systemic issues\nreflected in the training datasets.\n  We formulate a novel research question: how can we examine privacy biases in\nthe training data of LLMs? We present a novel approach to assess the privacy\nbiases using a contextual integrity-based methodology to evaluate the responses\nfrom different LLMs. Our approach accounts for the sensitivity of responses\nacross prompt variations, which hinders the evaluation of privacy biases. We\ninvestigate how privacy biases are affected by model capacities and\noptimizations."
                },
                "authors": [
                    {
                        "name": "Yan Shvartzshnaider"
                    },
                    {
                        "name": "Vasisht Duddu"
                    }
                ],
                "author_detail": {
                    "name": "Vasisht Duddu"
                },
                "author": "Vasisht Duddu",
                "arxiv_comment": "16 pages, 4 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09768v2",
                "updated": "2025-02-05T12:17:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    17,
                    36,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-15T11:32:35Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    11,
                    32,
                    35,
                    2,
                    15,
                    0
                ],
                "title": "Can Large Language Models Predict the Outcome of Judicial Decisions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Predict the Outcome of Judicial Decisions?"
                },
                "summary": "Large Language Models (LLMs) have shown exceptional capabilities in Natural\nLanguage Processing (NLP) across diverse domains. However, their application in\nspecialized tasks such as Legal Judgment Prediction (LJP) for low-resource\nlanguages like Arabic remains underexplored. In this work, we address this gap\nby developing an Arabic LJP dataset, collected and preprocessed from Saudi\ncommercial court judgments. We benchmark state-of-the-art open-source LLMs,\nincluding LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as\nzero-shot, one-shot, and fine-tuning using QLoRA. Additionally, we used a\ncomprehensive evaluation framework combining quantitative metrics (BLEU and\nROUGE) and qualitative assessments (Coherence, legal language, clarity). Our\nresults demonstrate that fine-tuned smaller models achieve comparable\nperformance to larger models in task-specific contexts while offering\nsignificant resource efficiency. Furthermore, we investigate the effects of\nprompt engineering and fine-tuning on model outputs, providing insights into\nperformance variability and instruction sensitivity. By making the dataset,\nimplementation code, and models publicly available, we establish a robust\nfoundation for future research in Arabic legal NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown exceptional capabilities in Natural\nLanguage Processing (NLP) across diverse domains. However, their application in\nspecialized tasks such as Legal Judgment Prediction (LJP) for low-resource\nlanguages like Arabic remains underexplored. In this work, we address this gap\nby developing an Arabic LJP dataset, collected and preprocessed from Saudi\ncommercial court judgments. We benchmark state-of-the-art open-source LLMs,\nincluding LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as\nzero-shot, one-shot, and fine-tuning using QLoRA. Additionally, we used a\ncomprehensive evaluation framework combining quantitative metrics (BLEU and\nROUGE) and qualitative assessments (Coherence, legal language, clarity). Our\nresults demonstrate that fine-tuned smaller models achieve comparable\nperformance to larger models in task-specific contexts while offering\nsignificant resource efficiency. Furthermore, we investigate the effects of\nprompt engineering and fine-tuning on model outputs, providing insights into\nperformance variability and instruction sensitivity. By making the dataset,\nimplementation code, and models publicly available, we establish a robust\nfoundation for future research in Arabic legal NLP."
                },
                "authors": [
                    {
                        "name": "Mohamed Bayan Kmainasi"
                    },
                    {
                        "name": "Ali Ezzat Shahroor"
                    },
                    {
                        "name": "Amani Al-Ghraibah"
                    }
                ],
                "author_detail": {
                    "name": "Amani Al-Ghraibah"
                },
                "author": "Amani Al-Ghraibah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.02885v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.02885v3",
                "updated": "2025-02-05T12:16:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    12,
                    16,
                    0,
                    2,
                    36,
                    0
                ],
                "published": "2023-08-05T14:04:39Z",
                "published_parsed": [
                    2023,
                    8,
                    5,
                    14,
                    4,
                    39,
                    5,
                    217,
                    0
                ],
                "title": "REED: Chiplet-Based Accelerator for Fully Homomorphic Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REED: Chiplet-Based Accelerator for Fully Homomorphic Encryption"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables privacy-preserving computation and\nhas many applications. However, its practical implementation faces massive\ncomputation and memory overheads. To address this bottleneck, several\nApplication-Specific Integrated Circuit (ASIC) FHE accelerators have been\nproposed. All these prior works put every component needed for FHE onto one\nchip (monolithic), hence offering high performance. However, they suffer from\npractical problems associated with large-scale chip design, such as\ninflexibility, low yield, and high manufacturing cost.\n  In this paper, we present the first-of-its-kind multi-chiplet-based FHE\naccelerator `REED' for overcoming the limitations of prior monolithic designs.\nTo utilize the advantages of multi-chiplet structures while matching the\nperformance of larger monolithic systems, we propose and implement several\nnovel strategies in the context of FHE. These include a scalable chiplet design\napproach, an effective framework for workload distribution, a custom\ninter-chiplet communication strategy, and advanced pipelined Number Theoretic\nTransform and automorphism design to enhance performance.\n  Experimental results demonstrate that REED 2.5D microprocessor consumes 96.7\nmm$^2$ chip area, 49.4 W average power in 7nm technology. It could achieve a\nremarkable speedup of up to 2,991x compared to a CPU (24-core 2xIntel X5690)\nand offer 1.9x better performance, along with a 50% reduction in development\ncosts when compared to state-of-the-art ASIC FHE accelerators. Furthermore, our\nwork presents the first instance of benchmarking an encrypted deep neural\nnetwork (DNN) training. Overall, the REED architecture design offers a highly\neffective solution for accelerating FHE, thereby significantly advancing the\npracticality and deployability of FHE in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables privacy-preserving computation and\nhas many applications. However, its practical implementation faces massive\ncomputation and memory overheads. To address this bottleneck, several\nApplication-Specific Integrated Circuit (ASIC) FHE accelerators have been\nproposed. All these prior works put every component needed for FHE onto one\nchip (monolithic), hence offering high performance. However, they suffer from\npractical problems associated with large-scale chip design, such as\ninflexibility, low yield, and high manufacturing cost.\n  In this paper, we present the first-of-its-kind multi-chiplet-based FHE\naccelerator `REED' for overcoming the limitations of prior monolithic designs.\nTo utilize the advantages of multi-chiplet structures while matching the\nperformance of larger monolithic systems, we propose and implement several\nnovel strategies in the context of FHE. These include a scalable chiplet design\napproach, an effective framework for workload distribution, a custom\ninter-chiplet communication strategy, and advanced pipelined Number Theoretic\nTransform and automorphism design to enhance performance.\n  Experimental results demonstrate that REED 2.5D microprocessor consumes 96.7\nmm$^2$ chip area, 49.4 W average power in 7nm technology. It could achieve a\nremarkable speedup of up to 2,991x compared to a CPU (24-core 2xIntel X5690)\nand offer 1.9x better performance, along with a 50% reduction in development\ncosts when compared to state-of-the-art ASIC FHE accelerators. Furthermore, our\nwork presents the first instance of benchmarking an encrypted deep neural\nnetwork (DNN) training. Overall, the REED architecture design offers a highly\neffective solution for accelerating FHE, thereby significantly advancing the\npracticality and deployability of FHE in real-world applications."
                },
                "authors": [
                    {
                        "name": "Aikata Aikata"
                    },
                    {
                        "name": "Ahmet Can Mert"
                    },
                    {
                        "name": "Sunmin Kwon"
                    },
                    {
                        "name": "Maxim Deryabin"
                    },
                    {
                        "name": "Sujoy Sinha Roy"
                    }
                ],
                "author_detail": {
                    "name": "Sujoy Sinha Roy"
                },
                "author": "Sujoy Sinha Roy",
                "arxiv_journal_ref": "https://ches.iacr.org/2025/papersubmission.php",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.02885v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.02885v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03095v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03095v1",
                "updated": "2025-02-05T11:41:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    41,
                    43,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T11:41:43Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    41,
                    43,
                    2,
                    36,
                    0
                ],
                "title": "Reveal the Mystery of DPO: The Connection between DPO and RL Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reveal the Mystery of DPO: The Connection between DPO and RL Algorithms"
                },
                "summary": "With the rapid development of Large Language Models (LLMs), numerous\nReinforcement Learning from Human Feedback (RLHF) algorithms have been\nintroduced to improve model safety and alignment with human preferences. These\nalgorithms can be divided into two main frameworks based on whether they\nrequire an explicit reward (or value) function for training: actor-critic-based\nProximal Policy Optimization (PPO) and alignment-based Direct Preference\nOptimization (DPO). The mismatch between DPO and PPO, such as DPO's use of a\nclassification loss driven by human-preferred data, has raised confusion about\nwhether DPO should be classified as a Reinforcement Learning (RL) algorithm. To\naddress these ambiguities, we focus on three key aspects related to DPO, RL,\nand other RLHF algorithms: (1) the construction of the loss function; (2) the\ntarget distribution at which the algorithm converges; (3) the impact of key\ncomponents within the loss function. Specifically, we first establish a unified\nframework named UDRRA connecting these algorithms based on the construction of\ntheir loss functions. Next, we uncover their target policy distributions within\nthis framework. Finally, we investigate the critical components of DPO to\nunderstand their impact on the convergence rate. Our work provides a deeper\nunderstanding of the relationship between DPO, RL, and other RLHF algorithms,\noffering new insights for improving existing algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of Large Language Models (LLMs), numerous\nReinforcement Learning from Human Feedback (RLHF) algorithms have been\nintroduced to improve model safety and alignment with human preferences. These\nalgorithms can be divided into two main frameworks based on whether they\nrequire an explicit reward (or value) function for training: actor-critic-based\nProximal Policy Optimization (PPO) and alignment-based Direct Preference\nOptimization (DPO). The mismatch between DPO and PPO, such as DPO's use of a\nclassification loss driven by human-preferred data, has raised confusion about\nwhether DPO should be classified as a Reinforcement Learning (RL) algorithm. To\naddress these ambiguities, we focus on three key aspects related to DPO, RL,\nand other RLHF algorithms: (1) the construction of the loss function; (2) the\ntarget distribution at which the algorithm converges; (3) the impact of key\ncomponents within the loss function. Specifically, we first establish a unified\nframework named UDRRA connecting these algorithms based on the construction of\ntheir loss functions. Next, we uncover their target policy distributions within\nthis framework. Finally, we investigate the critical components of DPO to\nunderstand their impact on the convergence rate. Our work provides a deeper\nunderstanding of the relationship between DPO, RL, and other RLHF algorithms,\noffering new insights for improving existing algorithms."
                },
                "authors": [
                    {
                        "name": "Xuerui Su"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Jinhua Zhu"
                    },
                    {
                        "name": "Mingyang Yi"
                    },
                    {
                        "name": "Feng Xu"
                    },
                    {
                        "name": "Zhiming Ma"
                    },
                    {
                        "name": "Yuting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuting Liu"
                },
                "author": "Yuting Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03095v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03095v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00334v2",
                "updated": "2025-02-05T11:36:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    36,
                    53,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-01T06:42:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    6,
                    42,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning\n  with Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nsolving complex reasoning tasks, particularly in mathematics. However, the\ndomain of physics reasoning presents unique challenges that have received\nsignificantly less attention. Existing benchmarks often fall short in\nevaluating LLMs' abilities on the breadth and depth of undergraduate-level\nphysics, underscoring the need for a comprehensive evaluation. To fill this\ngap, we introduce UGPhysics, a large-scale and comprehensive benchmark\nspecifically designed to evaluate UnderGraduate-level Physics (UGPhysics)\nreasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics\nproblems in both English and Chinese, covering 13 subjects with seven different\nanswer types and four distinct physics reasoning skills, all rigorously\nscreened for data leakage. Additionally, we develop a Model-Assistant\nRule-based Judgment (MARJ) pipeline specifically tailored for assessing answer\ncorrectness of physics problems, ensuring accurate evaluation. Our evaluation\nof 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by\nOpenAI-o1-mini), emphasizes the necessity for models with stronger physics\nreasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ,\nwill drive future advancements in AI for physics reasoning. Codes and data are\navailable at https://github.com/YangLabHKUST/UGPhysics .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nsolving complex reasoning tasks, particularly in mathematics. However, the\ndomain of physics reasoning presents unique challenges that have received\nsignificantly less attention. Existing benchmarks often fall short in\nevaluating LLMs' abilities on the breadth and depth of undergraduate-level\nphysics, underscoring the need for a comprehensive evaluation. To fill this\ngap, we introduce UGPhysics, a large-scale and comprehensive benchmark\nspecifically designed to evaluate UnderGraduate-level Physics (UGPhysics)\nreasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics\nproblems in both English and Chinese, covering 13 subjects with seven different\nanswer types and four distinct physics reasoning skills, all rigorously\nscreened for data leakage. Additionally, we develop a Model-Assistant\nRule-based Judgment (MARJ) pipeline specifically tailored for assessing answer\ncorrectness of physics problems, ensuring accurate evaluation. Our evaluation\nof 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by\nOpenAI-o1-mini), emphasizes the necessity for models with stronger physics\nreasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ,\nwill drive future advancements in AI for physics reasoning. Codes and data are\navailable at https://github.com/YangLabHKUST/UGPhysics ."
                },
                "authors": [
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Qiyun Xu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Tianhao Chen"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Jiaxin Zhang"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Can Yang"
                    },
                    {
                        "name": "Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Wang"
                },
                "author": "Yang Wang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03080v1",
                "updated": "2025-02-05T11:14:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    14,
                    20,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T11:14:20Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    14,
                    20,
                    2,
                    36,
                    0
                ],
                "title": "IAO Prompting: Making Knowledge Flow Explicit in LLMs through Structured\n  Reasoning Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAO Prompting: Making Knowledge Flow Explicit in LLMs through Structured\n  Reasoning Templates"
                },
                "summary": "While Large Language Models (LLMs) demonstrate impressive reasoning\ncapabilities, understanding and validating their knowledge utilization remains\nchallenging. Chain-of-thought (CoT) prompting partially addresses this by\nrevealing intermediate reasoning steps, but the knowledge flow and application\nremain implicit. We introduce IAO (Input-Action-Output) prompting, a structured\ntemplate-based method that explicitly models how LLMs access and apply their\nknowledge during complex reasoning tasks. IAO decomposes problems into\nsequential steps, each clearly identifying the input knowledge being used, the\naction being performed, and the resulting output. This structured decomposition\nenables us to trace knowledge flow, verify factual consistency, and identify\npotential knowledge gaps or misapplications. Through experiments across diverse\nreasoning tasks, we demonstrate that IAO not only improves zero-shot\nperformance but also provides transparency in how LLMs leverage their stored\nknowledge. Human evaluation confirms that this structured approach enhances our\nability to verify knowledge utilization and detect potential hallucinations or\nreasoning errors. Our findings provide insights into both knowledge\nrepresentation within LLMs and methods for more reliable knowledge application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) demonstrate impressive reasoning\ncapabilities, understanding and validating their knowledge utilization remains\nchallenging. Chain-of-thought (CoT) prompting partially addresses this by\nrevealing intermediate reasoning steps, but the knowledge flow and application\nremain implicit. We introduce IAO (Input-Action-Output) prompting, a structured\ntemplate-based method that explicitly models how LLMs access and apply their\nknowledge during complex reasoning tasks. IAO decomposes problems into\nsequential steps, each clearly identifying the input knowledge being used, the\naction being performed, and the resulting output. This structured decomposition\nenables us to trace knowledge flow, verify factual consistency, and identify\npotential knowledge gaps or misapplications. Through experiments across diverse\nreasoning tasks, we demonstrate that IAO not only improves zero-shot\nperformance but also provides transparency in how LLMs leverage their stored\nknowledge. Human evaluation confirms that this structured approach enhances our\nability to verify knowledge utilization and detect potential hallucinations or\nreasoning errors. Our findings provide insights into both knowledge\nrepresentation within LLMs and methods for more reliable knowledge application."
                },
                "authors": [
                    {
                        "name": "Aissatou Diallo"
                    },
                    {
                        "name": "Antonis Bikakis"
                    },
                    {
                        "name": "Luke Dickens"
                    },
                    {
                        "name": "Anthony Hunter"
                    },
                    {
                        "name": "Rob Miller"
                    }
                ],
                "author_detail": {
                    "name": "Rob Miller"
                },
                "author": "Rob Miller",
                "arxiv_comment": "Accepted as Oral at KnowFM @ AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03067v1",
                "updated": "2025-02-05T11:00:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    0,
                    51,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T11:00:51Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    11,
                    0,
                    51,
                    2,
                    36,
                    0
                ],
                "title": "Optimizing Electric Vehicles Charging using Large Language Models and\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Electric Vehicles Charging using Large Language Models and\n  Graph Neural Networks"
                },
                "summary": "Maintaining grid stability amid widespread electric vehicle (EV) adoption is\nvital for sustainable transportation. Traditional optimization methods and\nReinforcement Learning (RL) approaches often struggle with the high\ndimensionality and dynamic nature of real-time EV charging, leading to\nsub-optimal solutions. To address these challenges, this study demonstrates\nthat combining Large Language Models (LLMs), for sequence modeling, with Graph\nNeural Networks (GNNs), for relational information extraction, not only\noutperforms conventional EV smart charging methods, but also paves the way for\nentirely new research directions and innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintaining grid stability amid widespread electric vehicle (EV) adoption is\nvital for sustainable transportation. Traditional optimization methods and\nReinforcement Learning (RL) approaches often struggle with the high\ndimensionality and dynamic nature of real-time EV charging, leading to\nsub-optimal solutions. To address these challenges, this study demonstrates\nthat combining Large Language Models (LLMs), for sequence modeling, with Graph\nNeural Networks (GNNs), for relational information extraction, not only\noutperforms conventional EV smart charging methods, but also paves the way for\nentirely new research directions and innovative solutions."
                },
                "authors": [
                    {
                        "name": "Stavros Orfanoudakis"
                    },
                    {
                        "name": "Peter Palensky"
                    },
                    {
                        "name": "Pedro P. Vergara"
                    }
                ],
                "author_detail": {
                    "name": "Pedro P. Vergara"
                },
                "author": "Pedro P. Vergara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01208v2",
                "updated": "2025-02-05T10:47:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    10,
                    47,
                    19,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-03T09:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "title": "Almost Surely Safe Alignment of Large Language Models at Inference-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Almost Surely Safe Alignment of Large Language Models at Inference-Time"
                },
                "summary": "Even highly capable large language models (LLMs) can produce biased or unsafe\nresponses, and alignment techniques, such as RLHF, aimed at mitigating this\nissue, are expensive and prone to overfitting as they retrain the LLM. This\npaper introduces a novel inference-time alignment approach that ensures LLMs\ngenerate safe responses almost surely, i.e., with a probability approaching\none. We achieve this by framing the safe generation of inference-time responses\nas a constrained Markov decision process within the LLM's latent space.\nCrucially, we augment a safety state that tracks the evolution of safety\nconstraints and enables us to demonstrate formal safety guarantees upon solving\nthe MDP in the latent space. Building on this foundation, we propose\nInferenceGuard, a practical implementation that safely aligns LLMs without\nmodifying the model weights. Empirically, we demonstrate InferenceGuard\neffectively balances safety and task performance, outperforming existing\ninference-time alignment methods in generating safe and aligned responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even highly capable large language models (LLMs) can produce biased or unsafe\nresponses, and alignment techniques, such as RLHF, aimed at mitigating this\nissue, are expensive and prone to overfitting as they retrain the LLM. This\npaper introduces a novel inference-time alignment approach that ensures LLMs\ngenerate safe responses almost surely, i.e., with a probability approaching\none. We achieve this by framing the safe generation of inference-time responses\nas a constrained Markov decision process within the LLM's latent space.\nCrucially, we augment a safety state that tracks the evolution of safety\nconstraints and enables us to demonstrate formal safety guarantees upon solving\nthe MDP in the latent space. Building on this foundation, we propose\nInferenceGuard, a practical implementation that safely aligns LLMs without\nmodifying the model weights. Empirically, we demonstrate InferenceGuard\neffectively balances safety and task performance, outperforming existing\ninference-time alignment methods in generating safe and aligned responses."
                },
                "authors": [
                    {
                        "name": "Xiaotong Ji"
                    },
                    {
                        "name": "Shyam Sundhar Ramesh"
                    },
                    {
                        "name": "Matthieu Zimmer"
                    },
                    {
                        "name": "Ilija Bogunovic"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Haitham Bou Ammar"
                    }
                ],
                "author_detail": {
                    "name": "Haitham Bou Ammar"
                },
                "author": "Haitham Bou Ammar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19054v2",
                "updated": "2025-02-05T10:43:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    10,
                    43,
                    26,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-31T11:28:16Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    28,
                    16,
                    4,
                    31,
                    0
                ],
                "title": "Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models"
                },
                "summary": "Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively."
                },
                "authors": [
                    {
                        "name": "Ruiyu Wang"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Shizhao Sun"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03052v1",
                "updated": "2025-02-05T10:29:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    10,
                    29,
                    54,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T10:29:54Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    10,
                    29,
                    54,
                    2,
                    36,
                    0
                ],
                "title": "Understanding and Enhancing the Transferability of Jailbreaking Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Enhancing the Transferability of Jailbreaking Attacks"
                },
                "summary": "Jailbreaking attacks can effectively manipulate open-source large language\nmodels (LLMs) to produce harmful responses. However, these attacks exhibit\nlimited transferability, failing to disrupt proprietary LLMs consistently. To\nreliably identify vulnerabilities in proprietary LLMs, this work investigates\nthe transferability of jailbreaking attacks by analysing their impact on the\nmodel's intent perception. By incorporating adversarial sequences, these\nattacks can redirect the source LLM's focus away from malicious-intent tokens\nin the original input, thereby obstructing the model's intent recognition and\neliciting harmful responses. Nevertheless, these adversarial sequences fail to\nmislead the target LLM's intent perception, allowing the target LLM to refocus\non malicious-intent tokens and abstain from responding. Our analysis further\nreveals the inherent distributional dependency within the generated adversarial\nsequences, whose effectiveness stems from overfitting the source LLM's\nparameters, resulting in limited transferability to target LLMs. To this end,\nwe propose the Perceived-importance Flatten (PiF) method, which uniformly\ndisperses the model's focus across neutral-intent tokens in the original input,\nthus obscuring malicious-intent tokens without relying on overfitted\nadversarial sequences. Extensive experiments demonstrate that PiF provides an\neffective and efficient red-teaming evaluation for proprietary LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking attacks can effectively manipulate open-source large language\nmodels (LLMs) to produce harmful responses. However, these attacks exhibit\nlimited transferability, failing to disrupt proprietary LLMs consistently. To\nreliably identify vulnerabilities in proprietary LLMs, this work investigates\nthe transferability of jailbreaking attacks by analysing their impact on the\nmodel's intent perception. By incorporating adversarial sequences, these\nattacks can redirect the source LLM's focus away from malicious-intent tokens\nin the original input, thereby obstructing the model's intent recognition and\neliciting harmful responses. Nevertheless, these adversarial sequences fail to\nmislead the target LLM's intent perception, allowing the target LLM to refocus\non malicious-intent tokens and abstain from responding. Our analysis further\nreveals the inherent distributional dependency within the generated adversarial\nsequences, whose effectiveness stems from overfitting the source LLM's\nparameters, resulting in limited transferability to target LLMs. To this end,\nwe propose the Perceived-importance Flatten (PiF) method, which uniformly\ndisperses the model's focus across neutral-intent tokens in the original input,\nthus obscuring malicious-intent tokens without relying on overfitted\nadversarial sequences. Extensive experiments demonstrate that PiF provides an\neffective and efficient red-teaming evaluation for proprietary LLMs."
                },
                "authors": [
                    {
                        "name": "Runqi Lin"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Fengwang Li"
                    },
                    {
                        "name": "Tongling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tongling Liu"
                },
                "author": "Tongling Liu",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05498v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05498v3",
                "updated": "2025-02-05T10:29:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    10,
                    29,
                    7,
                    2,
                    36,
                    0
                ],
                "published": "2024-06-08T15:45:31Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    15,
                    45,
                    31,
                    5,
                    160,
                    0
                ],
                "title": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a\n  Practical Manner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a\n  Practical Manner"
                },
                "summary": "Jailbreaking is an emerging adversarial attack that bypasses the safety\nalignment deployed in off-the-shelf large language models (LLMs) and has\nevolved into multiple categories: human-based, optimization-based,\ngeneration-based, and the recent indirect and multilingual jailbreaks. However,\ndelivering a practical jailbreak defense is challenging because it needs to not\nonly handle all the above jailbreak attacks but also incur negligible delays to\nuser prompts, as well as be compatible with both open-source and closed-source\nLLMs. Inspired by how the traditional security concept of shadow stacks defends\nagainst memory overflow attacks, this paper introduces a generic LLM jailbreak\ndefense framework called SelfDefend, which establishes a shadow LLM as a\ndefense instance (in detection state) to concurrently protect the target LLM\ninstance (in normal answering state) in the normal stack and collaborate with\nit for checkpoint-based access control. The effectiveness of SelfDefend builds\nupon our observation that existing LLMs can identify harmful prompts or\nintentions in user queries, which we empirically validate using mainstream\nGPT-3.5/4 models against major jailbreak attacks. To further improve the\ndefense's robustness and minimize costs, we employ a data distillation approach\nto tune dedicated open-source defense models. When deployed to protect\nGPT-3.5/4, Claude, Llama-2-7b/13b, and Mistral, these models outperform seven\nstate-of-the-art defenses and match the performance of GPT-4-based SelfDefend,\nwith significantly lower extra delays. Further experiments show that the tuned\nmodels are robust to adaptive jailbreaks and prompt injections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking is an emerging adversarial attack that bypasses the safety\nalignment deployed in off-the-shelf large language models (LLMs) and has\nevolved into multiple categories: human-based, optimization-based,\ngeneration-based, and the recent indirect and multilingual jailbreaks. However,\ndelivering a practical jailbreak defense is challenging because it needs to not\nonly handle all the above jailbreak attacks but also incur negligible delays to\nuser prompts, as well as be compatible with both open-source and closed-source\nLLMs. Inspired by how the traditional security concept of shadow stacks defends\nagainst memory overflow attacks, this paper introduces a generic LLM jailbreak\ndefense framework called SelfDefend, which establishes a shadow LLM as a\ndefense instance (in detection state) to concurrently protect the target LLM\ninstance (in normal answering state) in the normal stack and collaborate with\nit for checkpoint-based access control. The effectiveness of SelfDefend builds\nupon our observation that existing LLMs can identify harmful prompts or\nintentions in user queries, which we empirically validate using mainstream\nGPT-3.5/4 models against major jailbreak attacks. To further improve the\ndefense's robustness and minimize costs, we employ a data distillation approach\nto tune dedicated open-source defense models. When deployed to protect\nGPT-3.5/4, Claude, Llama-2-7b/13b, and Mistral, these models outperform seven\nstate-of-the-art defenses and match the performance of GPT-4-based SelfDefend,\nwith significantly lower extra delays. Further experiments show that the tuned\nmodels are robust to adaptive jailbreaks and prompt injections."
                },
                "authors": [
                    {
                        "name": "Xunguang Wang"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Zhenlan Ji"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Pingchuan Ma"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yingjiu Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Juergen Rahmel"
                    }
                ],
                "author_detail": {
                    "name": "Juergen Rahmel"
                },
                "author": "Juergen Rahmel",
                "arxiv_comment": "Accepted by USENIX Security Symposium 2025. Please cite the\n  conference version of this paper, i.e., \"Xunguang Wang, Daoyuan Wu, Zhenlan\n  Ji, Zongjie Li, Pingchuan Ma, Shuai Wang, Yingjiu Li, Yang Liu, Ning Liu, and\n  Juergen Rahmel. SelfDefend: LLMs Can Defend Themselves against Jailbreaking\n  in a Practical Manner. In Proc. USENIX Security, 2025.\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05498v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05498v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03041v1",
                "updated": "2025-02-05T09:56:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    56,
                    52,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T09:56:52Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    56,
                    52,
                    2,
                    36,
                    0
                ],
                "title": "Large Language Models Are Universal Recommendation Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Universal Recommendation Learners"
                },
                "summary": "In real-world recommender systems, different tasks are typically addressed\nusing supervised learning on task-specific datasets with carefully designed\nmodel architectures. We demonstrate that large language models (LLMs) can\nfunction as universal recommendation learners, capable of handling multiple\ntasks within a unified input-output framework, eliminating the need for\nspecialized model designs. To improve the recommendation performance of LLMs,\nwe introduce a multimodal fusion module for item representation and a\nsequence-in-set-out approach for efficient candidate generation. When applied\nto industrial-scale data, our LLM achieves competitive results with expert\nmodels elaborately designed for different recommendation tasks. Furthermore,\nour analysis reveals that recommendation outcomes are highly sensitive to text\ninput, highlighting the potential of prompt engineering in optimizing\nindustrial-scale recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world recommender systems, different tasks are typically addressed\nusing supervised learning on task-specific datasets with carefully designed\nmodel architectures. We demonstrate that large language models (LLMs) can\nfunction as universal recommendation learners, capable of handling multiple\ntasks within a unified input-output framework, eliminating the need for\nspecialized model designs. To improve the recommendation performance of LLMs,\nwe introduce a multimodal fusion module for item representation and a\nsequence-in-set-out approach for efficient candidate generation. When applied\nto industrial-scale data, our LLM achieves competitive results with expert\nmodels elaborately designed for different recommendation tasks. Furthermore,\nour analysis reveals that recommendation outcomes are highly sensitive to text\ninput, highlighting the potential of prompt engineering in optimizing\nindustrial-scale recommender systems."
                },
                "authors": [
                    {
                        "name": "Junguang Jiang"
                    },
                    {
                        "name": "Yanwen Huang"
                    },
                    {
                        "name": "Bin Liu"
                    },
                    {
                        "name": "Xiaoyu Kong"
                    },
                    {
                        "name": "Ziru Xu"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03040v1",
                "updated": "2025-02-05T09:54:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    54,
                    13,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T09:54:13Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    54,
                    13,
                    2,
                    36,
                    0
                ],
                "title": "A Framework for IoT-Enabled Smart Manufacturing for Energy and Resource\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for IoT-Enabled Smart Manufacturing for Energy and Resource\n  Optimization"
                },
                "summary": "The increasing demands for sustainable and efficient manufacturing systems\nhave driven the integration of Internet of Things (IoT) technologies into smart\nmanufacturing. This study investigates IoT-enabled systems designed to enhance\nenergy efficiency and resource optimization in the manufacturing sector,\nfocusing on a multi-layered architecture integrating sensors, edge computing,\nand cloud platforms. MATLAB Simulink was utilized for modeling and simulation,\nreplicating typical manufacturing conditions to evaluate energy consumption,\nmachine uptime, and resource usage. The results demonstrate an 18% reduction in\nenergy consumption, a 22% decrease in machine downtime, and a 15% improvement\nin resource utilization. Comparative analyses highlight the superiority of the\nproposed framework in addressing operational inefficiencies and aligning with\nsustainability goals. The study underscores the potential of IoT in\ntransforming traditional manufacturing into interconnected, intelligent\nsystems, offering practical implications for industrial stakeholders aiming to\noptimize operations while adhering to global sustainability standards. Future\nwork will focus on addressing identified challenges such as high deployment\ncosts and data security concerns, aiming to facilitate the broader adoption of\nIoT in industrial applications.\n  Keywords: IoT (Internet of Things), Smart Manufacturing, Energy Efficiency,\nResource Optimization, Manufacturing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demands for sustainable and efficient manufacturing systems\nhave driven the integration of Internet of Things (IoT) technologies into smart\nmanufacturing. This study investigates IoT-enabled systems designed to enhance\nenergy efficiency and resource optimization in the manufacturing sector,\nfocusing on a multi-layered architecture integrating sensors, edge computing,\nand cloud platforms. MATLAB Simulink was utilized for modeling and simulation,\nreplicating typical manufacturing conditions to evaluate energy consumption,\nmachine uptime, and resource usage. The results demonstrate an 18% reduction in\nenergy consumption, a 22% decrease in machine downtime, and a 15% improvement\nin resource utilization. Comparative analyses highlight the superiority of the\nproposed framework in addressing operational inefficiencies and aligning with\nsustainability goals. The study underscores the potential of IoT in\ntransforming traditional manufacturing into interconnected, intelligent\nsystems, offering practical implications for industrial stakeholders aiming to\noptimize operations while adhering to global sustainability standards. Future\nwork will focus on addressing identified challenges such as high deployment\ncosts and data security concerns, aiming to facilitate the broader adoption of\nIoT in industrial applications.\n  Keywords: IoT (Internet of Things), Smart Manufacturing, Energy Efficiency,\nResource Optimization, Manufacturing"
                },
                "authors": [
                    {
                        "name": "Bazigu Alex"
                    },
                    {
                        "name": "Mwebaze Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Mwebaze Johnson"
                },
                "author": "Mwebaze Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03034v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03034v1",
                "updated": "2025-02-05T09:43:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    43,
                    14,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T09:43:14Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    43,
                    14,
                    2,
                    36,
                    0
                ],
                "title": "Knowledge Distillation from Large Language Models for Household Energy\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation from Large Language Models for Household Energy\n  Modeling"
                },
                "summary": "Machine learning (ML) is increasingly vital for smart-grid research, yet\nrestricted access to realistic, diverse data - often due to privacy concerns -\nslows progress and fuels doubts within the energy sector about adopting\nML-based strategies. We propose integrating Large Language Models (LLMs) in\nenergy modeling to generate realistic, culturally sensitive, and\nbehavior-specific data for household energy usage across diverse geographies.\nIn this study, we employ and compare five different LLMs to systematically\nproduce family structures, weather patterns, and daily consumption profiles for\nhouseholds in six distinct countries. A four-stage methodology synthesizes\ncontextual daily data, including culturally nuanced activities, realistic\nweather ranges, HVAC operations, and distinct `energy signatures' that capture\nunique consumption footprints. Additionally, we explore an alternative strategy\nwhere external weather datasets can be directly integrated, bypassing\nintermediate weather modeling stages while ensuring physically consistent data\ninputs. The resulting dataset provides insights into how cultural, climatic,\nand behavioral factors converge to shape carbon emissions, offering a\ncost-effective avenue for scenario-based energy optimization. This approach\nunderscores how prompt engineering, combined with knowledge distillation, can\nadvance sustainable energy research and climate mitigation efforts. Source code\nis available at\nhttps://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) is increasingly vital for smart-grid research, yet\nrestricted access to realistic, diverse data - often due to privacy concerns -\nslows progress and fuels doubts within the energy sector about adopting\nML-based strategies. We propose integrating Large Language Models (LLMs) in\nenergy modeling to generate realistic, culturally sensitive, and\nbehavior-specific data for household energy usage across diverse geographies.\nIn this study, we employ and compare five different LLMs to systematically\nproduce family structures, weather patterns, and daily consumption profiles for\nhouseholds in six distinct countries. A four-stage methodology synthesizes\ncontextual daily data, including culturally nuanced activities, realistic\nweather ranges, HVAC operations, and distinct `energy signatures' that capture\nunique consumption footprints. Additionally, we explore an alternative strategy\nwhere external weather datasets can be directly integrated, bypassing\nintermediate weather modeling stages while ensuring physically consistent data\ninputs. The resulting dataset provides insights into how cultural, climatic,\nand behavioral factors converge to shape carbon emissions, offering a\ncost-effective avenue for scenario-based energy optimization. This approach\nunderscores how prompt engineering, combined with knowledge distillation, can\nadvance sustainable energy research and climate mitigation efforts. Source code\nis available at\nhttps://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation ."
                },
                "authors": [
                    {
                        "name": "Mohannad Takrouri"
                    },
                    {
                        "name": "Nicols M. Cuadrado"
                    },
                    {
                        "name": "Martin Tak"
                    }
                ],
                "author_detail": {
                    "name": "Martin Tak"
                },
                "author": "Martin Tak",
                "arxiv_comment": "Source code is available at\n  https://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03034v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03034v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v2",
                "updated": "2025-02-05T09:35:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    35,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03029v1",
                "updated": "2025-02-05T09:31:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    31,
                    27,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T09:31:27Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    31,
                    27,
                    2,
                    36,
                    0
                ],
                "title": "On Zero-Initialized Attention: Optimal Prompt and Gating Factor\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Zero-Initialized Attention: Optimal Prompt and Gating Factor\n  Estimation"
                },
                "summary": "The LLaMA-Adapter has recently emerged as an efficient fine-tuning technique\nfor LLaMA models, leveraging zero-initialized attention to stabilize training\nand enhance performance. However, despite its empirical success, the\ntheoretical foundations of zero-initialized attention remain largely\nunexplored. In this paper, we provide a rigorous theoretical analysis,\nestablishing a connection between zero-initialized attention and\nmixture-of-expert models. We prove that both linear and non-linear prompts,\nalong with gating functions, can be optimally estimated, with non-linear\nprompts offering greater flexibility for future applications. Empirically, we\nvalidate our findings on the open LLM benchmarks, demonstrating that non-linear\nprompts outperform linear ones. Notably, even with limited training data, both\nprompt types consistently surpass vanilla attention, highlighting the\nrobustness and adaptability of zero-initialized attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLaMA-Adapter has recently emerged as an efficient fine-tuning technique\nfor LLaMA models, leveraging zero-initialized attention to stabilize training\nand enhance performance. However, despite its empirical success, the\ntheoretical foundations of zero-initialized attention remain largely\nunexplored. In this paper, we provide a rigorous theoretical analysis,\nestablishing a connection between zero-initialized attention and\nmixture-of-expert models. We prove that both linear and non-linear prompts,\nalong with gating functions, can be optimally estimated, with non-linear\nprompts offering greater flexibility for future applications. Empirically, we\nvalidate our findings on the open LLM benchmarks, demonstrating that non-linear\nprompts outperform linear ones. Notably, even with limited training data, both\nprompt types consistently surpass vanilla attention, highlighting the\nrobustness and adaptability of zero-initialized attention."
                },
                "authors": [
                    {
                        "name": "Nghiem T. Diep"
                    },
                    {
                        "name": "Huy Nguyen"
                    },
                    {
                        "name": "Chau Nguyen"
                    },
                    {
                        "name": "Minh Le"
                    },
                    {
                        "name": "Duy M. H. Nguyen"
                    },
                    {
                        "name": "Daniel Sonntag"
                    },
                    {
                        "name": "Mathias Niepert"
                    },
                    {
                        "name": "Nhat Ho"
                    }
                ],
                "author_detail": {
                    "name": "Nhat Ho"
                },
                "author": "Nhat Ho",
                "arxiv_comment": "43 pages, 5 tables, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02451v2",
                "updated": "2025-02-05T09:17:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    17,
                    17,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-04T16:17:01Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    17,
                    1,
                    1,
                    35,
                    0
                ],
                "title": "Beyond English: Evaluating Automated Measurement of Moral Foundations in\n  Non-English Discourse with a Chinese Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond English: Evaluating Automated Measurement of Moral Foundations in\n  Non-English Discourse with a Chinese Case Study"
                },
                "summary": "This study explores computational approaches for measuring moral foundations\n(MFs) in non-English corpora. Since most resources are developed primarily for\nEnglish, cross-linguistic applications of moral foundation theory remain\nlimited. Using Chinese as a case study, this paper evaluates the effectiveness\nof applying English resources to machine translated text, local language\nlexicons, multilingual language models, and large language models (LLMs) in\nmeasuring MFs in non-English texts. The results indicate that machine\ntranslation and local lexicon approaches are insufficient for complex moral\nassessments, frequently resulting in a substantial loss of cultural\ninformation. In contrast, multilingual models and LLMs demonstrate reliable\ncross-language performance with transfer learning, with LLMs excelling in terms\nof data efficiency. Importantly, this study also underscores the need for\nhuman-in-the-loop validation of automated MF assessment, as the most advanced\nmodels may overlook cultural nuances in cross-language measurements. The\nfindings highlight the potential of LLMs for cross-language MF measurements and\nother complex multilingual deductive coding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores computational approaches for measuring moral foundations\n(MFs) in non-English corpora. Since most resources are developed primarily for\nEnglish, cross-linguistic applications of moral foundation theory remain\nlimited. Using Chinese as a case study, this paper evaluates the effectiveness\nof applying English resources to machine translated text, local language\nlexicons, multilingual language models, and large language models (LLMs) in\nmeasuring MFs in non-English texts. The results indicate that machine\ntranslation and local lexicon approaches are insufficient for complex moral\nassessments, frequently resulting in a substantial loss of cultural\ninformation. In contrast, multilingual models and LLMs demonstrate reliable\ncross-language performance with transfer learning, with LLMs excelling in terms\nof data efficiency. Importantly, this study also underscores the need for\nhuman-in-the-loop validation of automated MF assessment, as the most advanced\nmodels may overlook cultural nuances in cross-language measurements. The\nfindings highlight the potential of LLMs for cross-language MF measurements and\nother complex multilingual deductive coding tasks."
                },
                "authors": [
                    {
                        "name": "Calvin Yixiang Cheng"
                    },
                    {
                        "name": "Scott A Hale"
                    }
                ],
                "author_detail": {
                    "name": "Scott A Hale"
                },
                "author": "Scott A Hale",
                "arxiv_comment": "12 pages, 2 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01904v2",
                "updated": "2025-02-05T09:17:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    17,
                    1,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-03T17:14:16Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    14,
                    16,
                    4,
                    3,
                    0
                ],
                "title": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM"
                },
                "summary": "Recently, slow-thinking reasoning systems, built upon large language models\n(LLMs), have garnered widespread attention by scaling the thinking time during\ninference. There is also growing interest in adapting this capability to\nmultimodal large language models (MLLMs). Given that MLLMs handle more complex\ndata semantics across different modalities, it is intuitively more challenging\nto implement multimodal slow-thinking systems.\n  To address this issue, in this paper, we explore a straightforward approach\nby fine-tuning a capable MLLM with a small amount of textual long-form thought\ndata, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning\nwith long thought). We find that these long-form reasoning processes, expressed\nin natural language, can be effectively transferred to MLLMs. Moreover, it\nseems that such textual reasoning data can be even more effective than visual\nreasoning data in eliciting the slow-thinking capacities of MLLMs. While this\nwork is preliminary, it demonstrates that slow-thinking capacities are\nfundamentally associated with the language model component, which can be\ntransferred across modalities or domains. This finding can be leveraged to\nguide the development of more powerful slow-thinking reasoning systems. We\nrelease our resources at https://github.com/RUCAIBox/Virgo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, slow-thinking reasoning systems, built upon large language models\n(LLMs), have garnered widespread attention by scaling the thinking time during\ninference. There is also growing interest in adapting this capability to\nmultimodal large language models (MLLMs). Given that MLLMs handle more complex\ndata semantics across different modalities, it is intuitively more challenging\nto implement multimodal slow-thinking systems.\n  To address this issue, in this paper, we explore a straightforward approach\nby fine-tuning a capable MLLM with a small amount of textual long-form thought\ndata, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning\nwith long thought). We find that these long-form reasoning processes, expressed\nin natural language, can be effectively transferred to MLLMs. Moreover, it\nseems that such textual reasoning data can be even more effective than visual\nreasoning data in eliciting the slow-thinking capacities of MLLMs. While this\nwork is preliminary, it demonstrates that slow-thinking capacities are\nfundamentally associated with the language model component, which can be\ntransferred across modalities or domains. This finding can be leveraged to\nguide the development of more powerful slow-thinking reasoning systems. We\nrelease our resources at https://github.com/RUCAIBox/Virgo."
                },
                "authors": [
                    {
                        "name": "Yifan Du"
                    },
                    {
                        "name": "Zikang Liu"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Yuqi Huo"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Technical Report on Slow Thinking with LLMs: Visual Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13693v3",
                "updated": "2025-02-05T09:11:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    11,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2024-12-18T10:33:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    10,
                    33,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "UITrans: Seamless UI Translation from Android to HarmonyOS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UITrans: Seamless UI Translation from Android to HarmonyOS"
                },
                "summary": "Seamless user interface (i.e., UI) translation has emerged as a pivotal\ntechnique for modern mobile developers, addressing the challenge of developing\nseparate UI applications for Android and HarmonyOS platforms due to fundamental\ndifferences in layout structures and development paradigms. In this paper, we\npresent UITrans, the first automated UI translation tool designed for Android\nto HarmonyOS. UITrans leverages an LLM-driven multi-agent reflective\ncollaboration framework to convert Android XML layouts into HarmonyOS ArkUI\nlayouts. It not only maps component-level and page-level elements to ArkUI\nequivalents but also handles project-level challenges, including complex\nlayouts and interaction logic. Our evaluation of six Android applications\ndemonstrates that our UITrans achieves translation success rates of over 90.1%,\n89.3%, and 89.2% at the component, page, and project levels, respectively.\nUITrans is available at https://github.com/OpenSELab/UITrans and the demo video\ncan be viewed at https://www.youtube.com/watch?v=iqKOSmCnJG0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seamless user interface (i.e., UI) translation has emerged as a pivotal\ntechnique for modern mobile developers, addressing the challenge of developing\nseparate UI applications for Android and HarmonyOS platforms due to fundamental\ndifferences in layout structures and development paradigms. In this paper, we\npresent UITrans, the first automated UI translation tool designed for Android\nto HarmonyOS. UITrans leverages an LLM-driven multi-agent reflective\ncollaboration framework to convert Android XML layouts into HarmonyOS ArkUI\nlayouts. It not only maps component-level and page-level elements to ArkUI\nequivalents but also handles project-level challenges, including complex\nlayouts and interaction logic. Our evaluation of six Android applications\ndemonstrates that our UITrans achieves translation success rates of over 90.1%,\n89.3%, and 89.2% at the component, page, and project levels, respectively.\nUITrans is available at https://github.com/OpenSELab/UITrans and the demo video\ncan be viewed at https://www.youtube.com/watch?v=iqKOSmCnJG0."
                },
                "authors": [
                    {
                        "name": "Lina Gong"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Yujun Huang"
                    },
                    {
                        "name": "Di Cui"
                    },
                    {
                        "name": "Mingqiang Wei"
                    }
                ],
                "author_detail": {
                    "name": "Mingqiang Wei"
                },
                "author": "Mingqiang Wei",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03009v1",
                "updated": "2025-02-05T09:11:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    11,
                    13,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T09:11:13Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    11,
                    13,
                    2,
                    36,
                    0
                ],
                "title": "Scaling Laws for Upcycling Mixture-of-Experts Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Upcycling Mixture-of-Experts Language Models"
                },
                "summary": "Pretraining large language models (LLMs) is resource-intensive, often\nrequiring months of training time even with high-end GPU clusters. There are\ntwo approaches of mitigating such computational demands: reusing smaller models\nto train larger ones (upcycling), and training computationally efficient models\nlike mixture-of-experts (MoE). In this paper, we study the upcycling of LLMs to\nMoE models, of which the scaling behavior remains underexplored. Through\nextensive experiments, we identify empirical scaling laws that describe how\nperformance depends on dataset size and model configuration. Particularly, we\nshow that, while scaling these factors improves performance, there is a novel\ninteraction term between the dense and upcycled training dataset that limits\nthe efficiency of upcycling at large computational budgets. Based on these\nfindings, we provide guidance to scale upcycling, and establish conditions\nunder which upcycling outperforms from-scratch trainings within budget\nconstraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretraining large language models (LLMs) is resource-intensive, often\nrequiring months of training time even with high-end GPU clusters. There are\ntwo approaches of mitigating such computational demands: reusing smaller models\nto train larger ones (upcycling), and training computationally efficient models\nlike mixture-of-experts (MoE). In this paper, we study the upcycling of LLMs to\nMoE models, of which the scaling behavior remains underexplored. Through\nextensive experiments, we identify empirical scaling laws that describe how\nperformance depends on dataset size and model configuration. Particularly, we\nshow that, while scaling these factors improves performance, there is a novel\ninteraction term between the dense and upcycled training dataset that limits\nthe efficiency of upcycling at large computational budgets. Based on these\nfindings, we provide guidance to scale upcycling, and establish conditions\nunder which upcycling outperforms from-scratch trainings within budget\nconstraints."
                },
                "authors": [
                    {
                        "name": "Seng Pei Liew"
                    },
                    {
                        "name": "Takuya Kato"
                    },
                    {
                        "name": "Sho Takase"
                    }
                ],
                "author_detail": {
                    "name": "Sho Takase"
                },
                "author": "Sho Takase",
                "arxiv_comment": "15 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03004v1",
                "updated": "2025-02-05T08:58:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    58,
                    35,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T08:58:35Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    58,
                    35,
                    2,
                    36,
                    0
                ],
                "title": "MedBioLM: Optimizing Medical and Biological QA with Fine-Tuned Large\n  Language Models and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedBioLM: Optimizing Medical and Biological QA with Fine-Tuned Large\n  Language Models and Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nnatural language processing tasks. However, their application to specialized\ndomains such as medicine and biology requires further optimization to ensure\nfactual accuracy, reliability, and contextual depth. We introduce MedBioLM, a\ndomain-adapted biomedical question-answering model designed to enhance both\nshort-form and long-form queries. By integrating fine-tuning and\nretrieval-augmented generation (RAG), MedBioLM dynamically incorporates\ndomain-specific knowledge, improving reasoning abilities and factual accuracy.\nTo evaluate its effectiveness, we fine-tuned the model on diverse biomedical QA\ndatasets, covering structured multiple-choice assessments and complex clinical\nreasoning tasks. Fine-tuning significantly improves accuracy on benchmark\ndatasets, while RAG enhances factual consistency. These results highlight the\npotential of domain-optimized LLMs in advancing biomedical research, medical\neducation, and clinical decision support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nnatural language processing tasks. However, their application to specialized\ndomains such as medicine and biology requires further optimization to ensure\nfactual accuracy, reliability, and contextual depth. We introduce MedBioLM, a\ndomain-adapted biomedical question-answering model designed to enhance both\nshort-form and long-form queries. By integrating fine-tuning and\nretrieval-augmented generation (RAG), MedBioLM dynamically incorporates\ndomain-specific knowledge, improving reasoning abilities and factual accuracy.\nTo evaluate its effectiveness, we fine-tuned the model on diverse biomedical QA\ndatasets, covering structured multiple-choice assessments and complex clinical\nreasoning tasks. Fine-tuning significantly improves accuracy on benchmark\ndatasets, while RAG enhances factual consistency. These results highlight the\npotential of domain-optimized LLMs in advancing biomedical research, medical\neducation, and clinical decision support."
                },
                "authors": [
                    {
                        "name": "Seonok Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seonok Kim"
                },
                "author": "Seonok Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03000v1",
                "updated": "2025-02-05T08:52:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    52,
                    37,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T08:52:37Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    52,
                    37,
                    2,
                    36,
                    0
                ],
                "title": "Armadillo: An Efficient Framework for Numerical Linear Algebra",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Armadillo: An Efficient Framework for Numerical Linear Algebra"
                },
                "summary": "A major challenge in the deployment of scientific software solutions is the\nadaptation of research prototypes to production-grade code. While high-level\nlanguages like MATLAB are useful for rapid prototyping, they lack the resource\nefficiency required for scalable production applications, necessitating\ntranslation into lower level languages like C++. Further, for machine learning\nand signal processing applications, the underlying linear algebra primitives,\ngenerally provided by the standard BLAS and LAPACK libraries, are unwieldy and\ndifficult to use, requiring manual memory management and other tedium. To\naddress this challenge, the Armadillo C++ linear algebra library provides an\nintuitive interface for writing linear algebra expressions that are easily\ncompiled into efficient production-grade implementations. We describe the\nexpression optimisations we have implemented in Armadillo, exploiting template\nmetaprogramming. We demonstrate that these optimisations result in considerable\nefficiency gains on a variety of benchmark linear algebra expressions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major challenge in the deployment of scientific software solutions is the\nadaptation of research prototypes to production-grade code. While high-level\nlanguages like MATLAB are useful for rapid prototyping, they lack the resource\nefficiency required for scalable production applications, necessitating\ntranslation into lower level languages like C++. Further, for machine learning\nand signal processing applications, the underlying linear algebra primitives,\ngenerally provided by the standard BLAS and LAPACK libraries, are unwieldy and\ndifficult to use, requiring manual memory management and other tedium. To\naddress this challenge, the Armadillo C++ linear algebra library provides an\nintuitive interface for writing linear algebra expressions that are easily\ncompiled into efficient production-grade implementations. We describe the\nexpression optimisations we have implemented in Armadillo, exploiting template\nmetaprogramming. We demonstrate that these optimisations result in considerable\nefficiency gains on a variety of benchmark linear algebra expressions."
                },
                "authors": [
                    {
                        "name": "Conrad Sanderson"
                    },
                    {
                        "name": "Ryan Curtin"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Curtin"
                },
                "author": "Ryan Curtin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N99, 65Y04, 65Y15, 65F45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; G.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13257v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13257v3",
                "updated": "2025-02-05T08:44:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    44,
                    2,
                    2,
                    36,
                    0
                ],
                "published": "2024-08-23T17:59:51Z",
                "published_parsed": [
                    2024,
                    8,
                    23,
                    17,
                    59,
                    51,
                    4,
                    236,
                    0
                ],
                "title": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\n  Real-World Scenarios that are Difficult for Humans?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution\n  Real-World Scenarios that are Difficult for Humans?"
                },
                "summary": "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than $300$K images from public datasets and the Internet,\nfiltering $13,366$ high-quality images for annotation. This involves the\nefforts of professional $25$ annotators and $7$ experts in MLLMs, contributing\nto $29,429$ question-answer pairs that cover $43$ subtasks across $5$\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving $28$ prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n$60\\%$ accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than $300$K images from public datasets and the Internet,\nfiltering $13,366$ high-quality images for annotation. This involves the\nefforts of professional $25$ annotators and $7$ experts in MLLMs, contributing\nto $29,429$ question-answer pairs that cover $43$ subtasks across $5$\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving $28$ prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n$60\\%$ accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ ."
                },
                "authors": [
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Huanyu Zhang"
                    },
                    {
                        "name": "Haochen Tian"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Shuangqing Zhang"
                    },
                    {
                        "name": "Junfei Wu"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Rong Jin"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "Project Page: https://mme-realworld.github.io/; accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13257v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13257v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02988v1",
                "updated": "2025-02-05T08:35:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    35,
                    55,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T08:35:55Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    35,
                    55,
                    2,
                    36,
                    0
                ],
                "title": "Training an LLM-as-a-Judge Model: Pipeline, Insights, and Practical\n  Lessons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training an LLM-as-a-Judge Model: Pipeline, Insights, and Practical\n  Lessons"
                },
                "summary": "The rapid advancement of large language models (LLMs) has opened new\npossibilities for their adoption as evaluative judges. This paper introduces\nThemis, a fine-tuned LLM judge that delivers sophisticated context-aware\nevaluations. We provide a comprehensive overview of the development pipeline\nfor Themis, highlighting its scenario-dependent evaluation prompts and two\nnovel methods for controlled instruction generation. These designs enable\nThemis to effectively distill evaluative skills from teacher models, while\nretaining flexibility for continuous development. We introduce two\nhuman-labeled benchmarks for meta-evaluation, demonstrating that Themis can\nachieve high alignment with human preferences in an economical manner.\nAdditionally, we explore insights into the LLM-as-a-judge paradigm, revealing\nnuances in performance and the varied effects of reference answers. Notably, we\nobserve that pure knowledge distillation from strong LLMs, though common, does\nnot guarantee performance improvement through scaling. We propose a mitigation\nstrategy based on instruction-following difficulty. Furthermore, we provide\npractical guidelines covering data balancing, prompt customization,\nmulti-objective training, and metric aggregation. We aim for our method and\nfindings, along with the fine-tuning data, benchmarks, and model checkpoints,\nto support future research and development in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) has opened new\npossibilities for their adoption as evaluative judges. This paper introduces\nThemis, a fine-tuned LLM judge that delivers sophisticated context-aware\nevaluations. We provide a comprehensive overview of the development pipeline\nfor Themis, highlighting its scenario-dependent evaluation prompts and two\nnovel methods for controlled instruction generation. These designs enable\nThemis to effectively distill evaluative skills from teacher models, while\nretaining flexibility for continuous development. We introduce two\nhuman-labeled benchmarks for meta-evaluation, demonstrating that Themis can\nachieve high alignment with human preferences in an economical manner.\nAdditionally, we explore insights into the LLM-as-a-judge paradigm, revealing\nnuances in performance and the varied effects of reference answers. Notably, we\nobserve that pure knowledge distillation from strong LLMs, though common, does\nnot guarantee performance improvement through scaling. We propose a mitigation\nstrategy based on instruction-following difficulty. Furthermore, we provide\npractical guidelines covering data balancing, prompt customization,\nmulti-objective training, and metric aggregation. We aim for our method and\nfindings, along with the fine-tuning data, benchmarks, and model checkpoints,\nto support future research and development in this area."
                },
                "authors": [
                    {
                        "name": "Renjun Hu"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Libin Meng"
                    },
                    {
                        "name": "Jiaxin Xia"
                    },
                    {
                        "name": "Yi Zong"
                    },
                    {
                        "name": "Xing Shi"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_doi": "10.1145/3701716.3715265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.02988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "accepted at WWW'25 (Industrial Track), extended version",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v2",
                "updated": "2025-02-05T08:22:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    22,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "Accepted to NAACL2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10020v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10020v3",
                "updated": "2025-02-05T08:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    11,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2024-03-15T05:06:21Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    5,
                    6,
                    21,
                    4,
                    75,
                    0
                ],
                "title": "Lost in Overlap: Exploring Logit-based Watermark Collision in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Overlap: Exploring Logit-based Watermark Collision in LLMs"
                },
                "summary": "The proliferation of large language models (LLMs) in generating content\nraises concerns about text copyright. Watermarking methods, particularly\nlogit-based approaches, embed imperceptible identifiers into text to address\nthese challenges. However, the widespread usage of watermarking across diverse\nLLMs has led to an inevitable issue known as watermark collision during common\ntasks, such as paraphrasing or translation. In this paper, we introduce\nwatermark collision as a novel and general philosophy for watermark attacks,\naimed at enhancing attack performance on top of any other attacking methods. We\nalso provide a comprehensive demonstration that watermark collision poses a\nthreat to all logit-based watermark algorithms, impacting not only specific\nattack scenarios but also downstream applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) in generating content\nraises concerns about text copyright. Watermarking methods, particularly\nlogit-based approaches, embed imperceptible identifiers into text to address\nthese challenges. However, the widespread usage of watermarking across diverse\nLLMs has led to an inevitable issue known as watermark collision during common\ntasks, such as paraphrasing or translation. In this paper, we introduce\nwatermark collision as a novel and general philosophy for watermark attacks,\naimed at enhancing attack performance on top of any other attacking methods. We\nalso provide a comprehensive demonstration that watermark collision poses a\nthreat to all logit-based watermark algorithms, impacting not only specific\nattack scenarios but also downstream applications."
                },
                "authors": [
                    {
                        "name": "Yiyang Luo"
                    },
                    {
                        "name": "Ke Lin"
                    },
                    {
                        "name": "Chao Gu"
                    },
                    {
                        "name": "Jiahui Hou"
                    },
                    {
                        "name": "Lijie Wen"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "Long Paper, 9 pages, accepted at NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10020v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10020v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v2",
                "updated": "2025-02-05T08:10:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    10,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring"
                },
                "summary": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02967v1",
                "updated": "2025-02-05T08:07:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    7,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T08:07:38Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    7,
                    38,
                    2,
                    36,
                    0
                ],
                "title": "Demonstrating a Control Framework for Physical Human-Robot Interaction\n  Toward Industrial Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demonstrating a Control Framework for Physical Human-Robot Interaction\n  Toward Industrial Applications"
                },
                "summary": "Human-Robot Interaction (pHRI) is critical for implementing Industry 5.0\nwhich focuses on human-centric approaches. However, few studies explore the\npractical alignment of pHRI to industrial grade performance. This paper\nintroduces a versatile control framework designed to bridge this gap by\nincorporating the torque-based control modes: compliance control, null-space\ncompliance, dual compliance, all in static and dynamic scenarios. Thanks to our\nsecond-order Quadratic Programming (QP) formulation, strict kinematic and\ncollision constraints are integrated into the system as safety features, and a\nweighted hierarchy guarantees singularity-robust task tracking performance. The\nframework is implemented on a Kinova Gen3 collaborative robot (cobot) equipped\nwith a Bota force/torque sensor. A DualShock 4 game controller is attached at\nthe robot's end-effector to demonstrate the framework's capabilities. This\nsetup enables seamless dynamic switching between the modes, and real-time\nadjustment of parameters, such as transitioning between position and torque\ncontrol or selecting a more robust custom-developed low-level torque controller\nover the default one.Built on the open-source robotic control software mc_rtc,\nto ensure reproducibility for both research and industrial deployment, this\nframework demonstrates industrial-grade performance and repeatability,\nshowcasing its potential as a robust pHRI control system for industrial\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Robot Interaction (pHRI) is critical for implementing Industry 5.0\nwhich focuses on human-centric approaches. However, few studies explore the\npractical alignment of pHRI to industrial grade performance. This paper\nintroduces a versatile control framework designed to bridge this gap by\nincorporating the torque-based control modes: compliance control, null-space\ncompliance, dual compliance, all in static and dynamic scenarios. Thanks to our\nsecond-order Quadratic Programming (QP) formulation, strict kinematic and\ncollision constraints are integrated into the system as safety features, and a\nweighted hierarchy guarantees singularity-robust task tracking performance. The\nframework is implemented on a Kinova Gen3 collaborative robot (cobot) equipped\nwith a Bota force/torque sensor. A DualShock 4 game controller is attached at\nthe robot's end-effector to demonstrate the framework's capabilities. This\nsetup enables seamless dynamic switching between the modes, and real-time\nadjustment of parameters, such as transitioning between position and torque\ncontrol or selecting a more robust custom-developed low-level torque controller\nover the default one.Built on the open-source robotic control software mc_rtc,\nto ensure reproducibility for both research and industrial deployment, this\nframework demonstrates industrial-grade performance and repeatability,\nshowcasing its potential as a robust pHRI control system for industrial\nenvironments."
                },
                "authors": [
                    {
                        "name": "Bastien Muraccioli"
                    },
                    {
                        "name": "Celerier Mathieu"
                    },
                    {
                        "name": "Benallegue Mehdi"
                    },
                    {
                        "name": "Venture Gentiane"
                    }
                ],
                "author_detail": {
                    "name": "Venture Gentiane"
                },
                "arxiv_affiliation": "CNRS-AIST JRL, UTokyo",
                "author": "Venture Gentiane",
                "arxiv_comment": "Demo Paper submitted to Robotics: Science and Systems (RSS2025),\n  pending review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02966v1",
                "updated": "2025-02-05T08:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    7,
                    4,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T08:07:04Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    7,
                    4,
                    2,
                    36,
                    0
                ],
                "title": "FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for\n  Enabling Fair LLM-Based Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for\n  Enabling Fair LLM-Based Recommender Systems"
                },
                "summary": "We propose FACTER, a fairness-aware framework for LLM-based recommendation\nsystems that integrates conformal prediction with dynamic prompt engineering.\nBy introducing an adaptive semantic variance threshold and a\nviolation-triggered mechanism, FACTER automatically tightens fairness\nconstraints whenever biased patterns emerge. We further develop an adversarial\nprompt generator that leverages historical violations to reduce repeated\ndemographic biases without retraining the LLM. Empirical results on MovieLens\nand Amazon show that FACTER substantially reduces fairness violations (up to\n95.5%) while maintaining strong recommendation accuracy, revealing semantic\nvariance as a potent proxy of bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose FACTER, a fairness-aware framework for LLM-based recommendation\nsystems that integrates conformal prediction with dynamic prompt engineering.\nBy introducing an adaptive semantic variance threshold and a\nviolation-triggered mechanism, FACTER automatically tightens fairness\nconstraints whenever biased patterns emerge. We further develop an adversarial\nprompt generator that leverages historical violations to reduce repeated\ndemographic biases without retraining the LLM. Empirical results on MovieLens\nand Amazon show that FACTER substantially reduces fairness violations (up to\n95.5%) while maintaining strong recommendation accuracy, revealing semantic\nvariance as a potent proxy of bias."
                },
                "authors": [
                    {
                        "name": "Arya Fayyazi"
                    },
                    {
                        "name": "Mehdi Kamal"
                    },
                    {
                        "name": "Massoud Pedram"
                    }
                ],
                "author_detail": {
                    "name": "Massoud Pedram"
                },
                "author": "Massoud Pedram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07471v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07471v3",
                "updated": "2025-02-05T08:05:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    5,
                    42,
                    2,
                    36,
                    0
                ],
                "published": "2024-08-14T11:29:47Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    11,
                    29,
                    47,
                    2,
                    227,
                    0
                ],
                "title": "Bridging and Modeling Correlations in Pairwise Data for Direct\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging and Modeling Correlations in Pairwise Data for Direct\n  Preference Optimization"
                },
                "summary": "Direct preference optimization (DPO), a widely adopted offline preference\noptimization algorithm, aims to align large language models (LLMs) with\nhuman-desired behaviors using pairwise preference data. However, the generation\nof the winning response and the losing response within pairwise data are\ntypically isolated, leading to weak correlations between them as well as\nsuboptimal alignment performance. To address this issue, we propose an\neffective framework for Bridging and Modeling Correlations in pairwise data,\nnamed BMC. Firstly, we increase the consistency and informativeness of the\npairwise preference signals through targeted modifications, synthesizing a\npseudo-winning response by improving the losing response with the winning\nresponse as a reference. Secondly, we identify that DPO alone is insufficient\nto model these correlations and capture nuanced variations. Therefore, we\npropose learning token-level correlations by dynamically leveraging the policy\nmodel's confidence during training. Comprehensive experiments on QA, math, and\ninstruction-following tasks demonstrate the effectiveness of our approach,\nsignificantly surpassing competitive baselines, including DPO. Additionally,\nour in-depth quantitative analysis reveals the reasons behind our method's\nsuperior performance over DPO and showcases its versatility to other DPO\nvariants. We release our repository at https://github.com/YJiangcm/BMC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct preference optimization (DPO), a widely adopted offline preference\noptimization algorithm, aims to align large language models (LLMs) with\nhuman-desired behaviors using pairwise preference data. However, the generation\nof the winning response and the losing response within pairwise data are\ntypically isolated, leading to weak correlations between them as well as\nsuboptimal alignment performance. To address this issue, we propose an\neffective framework for Bridging and Modeling Correlations in pairwise data,\nnamed BMC. Firstly, we increase the consistency and informativeness of the\npairwise preference signals through targeted modifications, synthesizing a\npseudo-winning response by improving the losing response with the winning\nresponse as a reference. Secondly, we identify that DPO alone is insufficient\nto model these correlations and capture nuanced variations. Therefore, we\npropose learning token-level correlations by dynamically leveraging the policy\nmodel's confidence during training. Comprehensive experiments on QA, math, and\ninstruction-following tasks demonstrate the effectiveness of our approach,\nsignificantly surpassing competitive baselines, including DPO. Additionally,\nour in-depth quantitative analysis reveals the reasons behind our method's\nsuperior performance over DPO and showcases its versatility to other DPO\nvariants. We release our repository at https://github.com/YJiangcm/BMC."
                },
                "authors": [
                    {
                        "name": "Yuxin Jiang"
                    },
                    {
                        "name": "Bo Huang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Xingshan Zeng"
                    },
                    {
                        "name": "Liangyou Li"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "arxiv_comment": "20 pages, 9 figures, 12 tables. Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07471v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07471v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02960v1",
                "updated": "2025-02-05T07:54:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    7,
                    54,
                    7,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T07:54:07Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    7,
                    54,
                    7,
                    2,
                    36,
                    0
                ],
                "title": "Large Language Model Adversarial Landscape Through the Lens of Attack\n  Objectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Adversarial Landscape Through the Lens of Attack\n  Objectives"
                },
                "summary": "Large Language Models (LLMs) represent a transformative leap in artificial\nintelligence, enabling the comprehension, generation, and nuanced interaction\nwith human language on an unparalleled scale. However, LLMs are increasingly\nvulnerable to a range of adversarial attacks that threaten their privacy,\nreliability, security, and trustworthiness. These attacks can distort outputs,\ninject biases, leak sensitive information, or disrupt the normal functioning of\nLLMs, posing significant challenges across various applications.\n  In this paper, we provide a novel comprehensive analysis of the adversarial\nlandscape of LLMs, framed through the lens of attack objectives. By\nconcentrating on the core goals of adversarial actors, we offer a fresh\nperspective that examines threats from the angles of privacy, integrity,\navailability, and misuse, moving beyond conventional taxonomies that focus\nsolely on attack techniques. This objective-driven adversarial landscape not\nonly highlights the strategic intent behind different adversarial approaches\nbut also sheds light on the evolving nature of these threats and the\neffectiveness of current defenses. Our analysis aims to guide researchers and\npractitioners in better understanding, anticipating, and mitigating these\nattacks, ultimately contributing to the development of more resilient and\nrobust LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a transformative leap in artificial\nintelligence, enabling the comprehension, generation, and nuanced interaction\nwith human language on an unparalleled scale. However, LLMs are increasingly\nvulnerable to a range of adversarial attacks that threaten their privacy,\nreliability, security, and trustworthiness. These attacks can distort outputs,\ninject biases, leak sensitive information, or disrupt the normal functioning of\nLLMs, posing significant challenges across various applications.\n  In this paper, we provide a novel comprehensive analysis of the adversarial\nlandscape of LLMs, framed through the lens of attack objectives. By\nconcentrating on the core goals of adversarial actors, we offer a fresh\nperspective that examines threats from the angles of privacy, integrity,\navailability, and misuse, moving beyond conventional taxonomies that focus\nsolely on attack techniques. This objective-driven adversarial landscape not\nonly highlights the strategic intent behind different adversarial approaches\nbut also sheds light on the evolving nature of these threats and the\neffectiveness of current defenses. Our analysis aims to guide researchers and\npractitioners in better understanding, anticipating, and mitigating these\nattacks, ultimately contributing to the development of more resilient and\nrobust LLM systems."
                },
                "authors": [
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Kane Walter"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    }
                ],
                "author_detail": {
                    "name": "Alsharif Abuadbba"
                },
                "author": "Alsharif Abuadbba",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14202v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14202v3",
                "updated": "2025-02-05T07:52:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    7,
                    52,
                    14,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-18T06:35:17Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    35,
                    17,
                    4,
                    292,
                    0
                ],
                "title": "Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay\n  Scoring with Rationale Generated by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay\n  Scoring with Rationale Generated by LLMs"
                },
                "summary": "Existing automated essay scoring (AES) has solely relied on essay text\nwithout using explanatory rationales for the scores, thereby forgoing an\nopportunity to capture the specific aspects evaluated by rubric indicators in a\nfine-grained manner. This paper introduces Rationale-based Multiple Trait\nScoring (RMTS), a novel approach for multi-trait essay scoring that integrates\nprompt-engineering-based large language models (LLMs) with a fine-tuning-based\nessay scoring model using a smaller large language model (S-LLM). RMTS uses an\nLLM-based trait-wise rationale generation system where a separate LLM agent\ngenerates trait-specific rationales based on rubric guidelines, which the\nscoring model uses to accurately predict multi-trait scores. Extensive\nexperiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,\nshow that RMTS significantly outperforms state-of-the-art models and vanilla\nS-LLMs in trait-specific scoring. By assisting quantitative assessment with\nfine-grained qualitative rationales, RMTS enhances the trait-wise reliability,\nproviding partial explanations about essays. The code is available at\nhttps://github.com/BBeeChu/RMTS.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing automated essay scoring (AES) has solely relied on essay text\nwithout using explanatory rationales for the scores, thereby forgoing an\nopportunity to capture the specific aspects evaluated by rubric indicators in a\nfine-grained manner. This paper introduces Rationale-based Multiple Trait\nScoring (RMTS), a novel approach for multi-trait essay scoring that integrates\nprompt-engineering-based large language models (LLMs) with a fine-tuning-based\nessay scoring model using a smaller large language model (S-LLM). RMTS uses an\nLLM-based trait-wise rationale generation system where a separate LLM agent\ngenerates trait-specific rationales based on rubric guidelines, which the\nscoring model uses to accurately predict multi-trait scores. Extensive\nexperiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,\nshow that RMTS significantly outperforms state-of-the-art models and vanilla\nS-LLMs in trait-specific scoring. By assisting quantitative assessment with\nfine-grained qualitative rationales, RMTS enhances the trait-wise reliability,\nproviding partial explanations about essays. The code is available at\nhttps://github.com/BBeeChu/RMTS.git."
                },
                "authors": [
                    {
                        "name": "SeongYeub Chu"
                    },
                    {
                        "name": "JongWoo Kim"
                    },
                    {
                        "name": "Bryan Wong"
                    },
                    {
                        "name": "MunYong Yi"
                    }
                ],
                "author_detail": {
                    "name": "MunYong Yi"
                },
                "author": "MunYong Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14202v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14202v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02958v1",
                "updated": "2025-02-05T07:51:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    7,
                    51,
                    32,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T07:51:32Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    7,
                    51,
                    32,
                    2,
                    36,
                    0
                ],
                "title": "Position: Editing Large Language Models Poses Serious Safety Risks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Editing Large Language Models Poses Serious Safety Risks"
                },
                "summary": "Large Language Models (LLMs) contain large amounts of facts about the world.\nThese facts can become outdated over time, which has led to the development of\nknowledge editing methods (KEs) that can change specific facts in LLMs with\nlimited side effects. This position paper argues that editing LLMs poses\nserious safety risks that have been largely overlooked. First, we note the fact\nthat KEs are widely available, computationally inexpensive, highly performant,\nand stealthy makes them an attractive tool for malicious actors. Second, we\ndiscuss malicious use cases of KEs, showing how KEs can be easily adapted for a\nvariety of malicious purposes. Third, we highlight vulnerabilities in the AI\necosystem that allow unrestricted uploading and downloading of updated models\nwithout verification. Fourth, we argue that a lack of social and institutional\nawareness exacerbates this risk, and discuss the implications for different\nstakeholders. We call on the community to (i) research tamper-resistant models\nand countermeasures against malicious model editing, and (ii) actively engage\nin securing the AI ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) contain large amounts of facts about the world.\nThese facts can become outdated over time, which has led to the development of\nknowledge editing methods (KEs) that can change specific facts in LLMs with\nlimited side effects. This position paper argues that editing LLMs poses\nserious safety risks that have been largely overlooked. First, we note the fact\nthat KEs are widely available, computationally inexpensive, highly performant,\nand stealthy makes them an attractive tool for malicious actors. Second, we\ndiscuss malicious use cases of KEs, showing how KEs can be easily adapted for a\nvariety of malicious purposes. Third, we highlight vulnerabilities in the AI\necosystem that allow unrestricted uploading and downloading of updated models\nwithout verification. Fourth, we argue that a lack of social and institutional\nawareness exacerbates this risk, and discuss the implications for different\nstakeholders. We call on the community to (i) research tamper-resistant models\nand countermeasures against malicious model editing, and (ii) actively engage\nin securing the AI ecosystem."
                },
                "authors": [
                    {
                        "name": "Paul Youssef"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Daniel Braun"
                    },
                    {
                        "name": "Jrg Schltterer"
                    },
                    {
                        "name": "Christin Seifert"
                    }
                ],
                "author_detail": {
                    "name": "Christin Seifert"
                },
                "author": "Christin Seifert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02481v2",
                "updated": "2025-02-05T07:48:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    7,
                    48,
                    33,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-04T16:57:03Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    57,
                    3,
                    1,
                    35,
                    0
                ],
                "title": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study"
                },
                "summary": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo."
                },
                "authors": [
                    {
                        "name": "Menglong Cui"
                    },
                    {
                        "name": "Pengzhi Gao"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_comment": "Accept to NAACL2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02298v3",
                "updated": "2025-02-05T07:23:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    7,
                    23,
                    59,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-03T08:34:17Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    34,
                    17,
                    3,
                    277,
                    0
                ],
                "title": "Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse\n  Representation Adjustment in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse\n  Representation Adjustment in Large Language Models"
                },
                "summary": "As large language models (LLMs) become integral to various applications,\nensuring both their safety and utility is paramount. Jailbreak attacks, which\nmanipulate LLMs into generating harmful content, pose significant challenges to\nthis balance. Existing defenses, such as prompt engineering and safety\nfine-tuning, often introduce computational overhead, increase inference\nlatency, and lack runtime flexibility. Moreover, overly restrictive safety\nmeasures can degrade model utility by causing refusals of benign queries. In\nthis paper, we introduce Jailbreak Antidote, a method that enables real-time\nadjustment of LLM safety preferences by manipulating a sparse subset of the\nmodel's internal states during inference. By shifting the model's hidden\nrepresentations along a safety direction with varying strengths, we achieve\nflexible control over the safety-utility balance without additional token\noverhead or inference delays. Our analysis reveals that safety-related\ninformation in LLMs is sparsely distributed; adjusting approximately 5% of the\ninternal state is as effective as modifying the entire state. Extensive\nexperiments on nine LLMs (ranging from 2 billion to 72 billion parameters),\nevaluated against ten jailbreak attack methods and compared with six defense\nstrategies, validate the effectiveness and efficiency of our approach. By\ndirectly manipulating internal states during reasoning, Jailbreak Antidote\noffers a lightweight, scalable solution that enhances LLM safety while\npreserving utility, opening new possibilities for real-time safety mechanisms\nin widely-deployed AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become integral to various applications,\nensuring both their safety and utility is paramount. Jailbreak attacks, which\nmanipulate LLMs into generating harmful content, pose significant challenges to\nthis balance. Existing defenses, such as prompt engineering and safety\nfine-tuning, often introduce computational overhead, increase inference\nlatency, and lack runtime flexibility. Moreover, overly restrictive safety\nmeasures can degrade model utility by causing refusals of benign queries. In\nthis paper, we introduce Jailbreak Antidote, a method that enables real-time\nadjustment of LLM safety preferences by manipulating a sparse subset of the\nmodel's internal states during inference. By shifting the model's hidden\nrepresentations along a safety direction with varying strengths, we achieve\nflexible control over the safety-utility balance without additional token\noverhead or inference delays. Our analysis reveals that safety-related\ninformation in LLMs is sparsely distributed; adjusting approximately 5% of the\ninternal state is as effective as modifying the entire state. Extensive\nexperiments on nine LLMs (ranging from 2 billion to 72 billion parameters),\nevaluated against ten jailbreak attack methods and compared with six defense\nstrategies, validate the effectiveness and efficiency of our approach. By\ndirectly manipulating internal states during reasoning, Jailbreak Antidote\noffers a lightweight, scalable solution that enhances LLM safety while\npreserving utility, opening new possibilities for real-time safety mechanisms\nin widely-deployed AI systems."
                },
                "authors": [
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Yiting Dong"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "Accepted by ICLR2025. url: https://openreview.net/forum?id=s20W12XTF8",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02945v1",
                "updated": "2025-02-05T07:21:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    7,
                    21,
                    49,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T07:21:49Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    7,
                    21,
                    49,
                    2,
                    36,
                    0
                ],
                "title": "LLM-KT: Aligning Large Language Models with Knowledge Tracing using a\n  Plug-and-Play Instruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-KT: Aligning Large Language Models with Knowledge Tracing using a\n  Plug-and-Play Instruction"
                },
                "summary": "The knowledge tracing (KT) problem is an extremely important topic in\npersonalized education, which aims to predict whether students can correctly\nanswer the next question based on their past question-answer records. Prior\nwork on this task mainly focused on learning the sequence of behaviors based on\nthe IDs or textual information. However, these studies usually fail to capture\nstudents' sufficient behavioral patterns without reasoning with rich world\nknowledge about questions. In this paper, we propose a large language models\n(LLMs)-based framework for KT, named \\texttt{\\textbf{LLM-KT}}, to integrate the\nstrengths of LLMs and traditional sequence interaction models. For task-level\nalignment, we design Plug-and-Play instruction to align LLMs with KT,\nleveraging LLMs' rich knowledge and powerful reasoning capacity. For\nmodality-level alignment, we design the plug-in context and sequence to\nintegrate multiple modalities learned by traditional methods. To capture the\nlong context of history records, we present a plug-in context to flexibly\ninsert the compressed context embedding into LLMs using question-specific and\nconcept-specific tokens. Furthermore, we introduce a plug-in sequence to\nenhance LLMs with sequence interaction behavior representation learned by\ntraditional sequence models using a sequence adapter. Extensive experiments\nshow that \\texttt{\\textbf{LLM-KT}} obtains state-of-the-art performance on four\ntypical datasets by comparing it with approximately 20 strong baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The knowledge tracing (KT) problem is an extremely important topic in\npersonalized education, which aims to predict whether students can correctly\nanswer the next question based on their past question-answer records. Prior\nwork on this task mainly focused on learning the sequence of behaviors based on\nthe IDs or textual information. However, these studies usually fail to capture\nstudents' sufficient behavioral patterns without reasoning with rich world\nknowledge about questions. In this paper, we propose a large language models\n(LLMs)-based framework for KT, named \\texttt{\\textbf{LLM-KT}}, to integrate the\nstrengths of LLMs and traditional sequence interaction models. For task-level\nalignment, we design Plug-and-Play instruction to align LLMs with KT,\nleveraging LLMs' rich knowledge and powerful reasoning capacity. For\nmodality-level alignment, we design the plug-in context and sequence to\nintegrate multiple modalities learned by traditional methods. To capture the\nlong context of history records, we present a plug-in context to flexibly\ninsert the compressed context embedding into LLMs using question-specific and\nconcept-specific tokens. Furthermore, we introduce a plug-in sequence to\nenhance LLMs with sequence interaction behavior representation learned by\ntraditional sequence models using a sequence adapter. Extensive experiments\nshow that \\texttt{\\textbf{LLM-KT}} obtains state-of-the-art performance on four\ntypical datasets by comparing it with approximately 20 strong baselines."
                },
                "authors": [
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Aimin Zhou"
                    },
                    {
                        "name": "Qinchun Bai"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01658v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01658v3",
                "updated": "2025-02-05T07:09:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    7,
                    9,
                    32,
                    2,
                    36,
                    0
                ],
                "published": "2024-09-03T07:01:37Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    7,
                    1,
                    37,
                    1,
                    247,
                    0
                ],
                "title": "From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language\n  Models with Pinpoint Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language\n  Models with Pinpoint Tuning"
                },
                "summary": "Large Language Models (LLMs) tend to prioritize adherence to user prompts\nover providing veracious responses, leading to the sycophancy issue. When\nchallenged by users, LLMs tend to admit mistakes and provide inaccurate\nresponses even if they initially provided the correct answer. Recent works\npropose to employ supervised fine-tuning (SFT) to mitigate the sycophancy\nissue, while it typically leads to the degeneration of LLMs' general\ncapability. To address the challenge, we propose a novel supervised pinpoint\ntuning (SPT), where the region-of-interest modules are tuned for a given\nobjective. Specifically, SPT first reveals and verifies a small percentage\n(<5%) of the basic modules, which significantly affect a particular behavior of\nLLMs. i.e., sycophancy. Subsequently, SPT merely fine-tunes these identified\nmodules while freezing the rest. To verify the effectiveness of the proposed\nSPT, we conduct comprehensive experiments, demonstrating that SPT significantly\nmitigates the sycophancy issue of LLMs (even better than SFT). Moreover, SPT\nintroduces limited or even no side effects on the general capability of LLMs.\nOur results shed light on how to precisely, effectively, and efficiently\nexplain and improve the targeted ability of LLMs. Code and data are available\nat https://github.com/yellowtownhz/sycophancy-interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) tend to prioritize adherence to user prompts\nover providing veracious responses, leading to the sycophancy issue. When\nchallenged by users, LLMs tend to admit mistakes and provide inaccurate\nresponses even if they initially provided the correct answer. Recent works\npropose to employ supervised fine-tuning (SFT) to mitigate the sycophancy\nissue, while it typically leads to the degeneration of LLMs' general\ncapability. To address the challenge, we propose a novel supervised pinpoint\ntuning (SPT), where the region-of-interest modules are tuned for a given\nobjective. Specifically, SPT first reveals and verifies a small percentage\n(<5%) of the basic modules, which significantly affect a particular behavior of\nLLMs. i.e., sycophancy. Subsequently, SPT merely fine-tunes these identified\nmodules while freezing the rest. To verify the effectiveness of the proposed\nSPT, we conduct comprehensive experiments, demonstrating that SPT significantly\nmitigates the sycophancy issue of LLMs (even better than SFT). Moreover, SPT\nintroduces limited or even no side effects on the general capability of LLMs.\nOur results shed light on how to precisely, effectively, and efficiently\nexplain and improve the targeted ability of LLMs. Code and data are available\nat https://github.com/yellowtownhz/sycophancy-interpretability."
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Binbin Lin"
                    },
                    {
                        "name": "Houqiang Li"
                    },
                    {
                        "name": "Le Lu"
                    },
                    {
                        "name": "Xinmei Tian"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Yonggang Zhang"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "accepted by ICML 2024, code and data are available at\n  https://github.com/yellowtownhz/sycophancy-interpretability",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01658v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01658v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00473v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00473v2",
                "updated": "2025-02-05T07:01:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    7,
                    1,
                    26,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-01T16:00:08Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    16,
                    0,
                    8,
                    5,
                    32,
                    0
                ],
                "title": "Weak-to-Strong Diffusion with Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak-to-Strong Diffusion with Reflection"
                },
                "summary": "The goal of diffusion generative models is to align the learned distribution\nwith the real data distribution through gradient score matching. However,\ninherent limitations in training data quality, modeling strategies, and\narchitectural design lead to inevitable gap between generated outputs and real\ndata. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel\nframework that utilizes the estimated difference between existing weak and\nstrong models (i.e., weak-to-strong difference) to approximate the gap between\nan ideal model and a strong model. By employing a reflective operation that\nalternates between denoising and inversion with weak-to-strong difference, we\ntheoretically understand that W2SD steers latent variables along sampling\ntrajectories toward regions of the real data distribution. W2SD is highly\nflexible and broadly applicable, enabling diverse improvements through the\nstrategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5,\ngood experts vs. bad experts in MoE). Extensive experiments demonstrate that\nW2SD significantly improves human preference, aesthetic quality, and prompt\nadherence, achieving SOTA performance across various modalities (e.g., image,\nvideo), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For\nexample, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to\n90% over the original results. Moreover, the performance gains achieved by W2SD\nmarkedly outweigh its additional computational overhead, while the cumulative\nimprovements from different weak-to-strong difference further solidify its\npractical utility and deployability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The goal of diffusion generative models is to align the learned distribution\nwith the real data distribution through gradient score matching. However,\ninherent limitations in training data quality, modeling strategies, and\narchitectural design lead to inevitable gap between generated outputs and real\ndata. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel\nframework that utilizes the estimated difference between existing weak and\nstrong models (i.e., weak-to-strong difference) to approximate the gap between\nan ideal model and a strong model. By employing a reflective operation that\nalternates between denoising and inversion with weak-to-strong difference, we\ntheoretically understand that W2SD steers latent variables along sampling\ntrajectories toward regions of the real data distribution. W2SD is highly\nflexible and broadly applicable, enabling diverse improvements through the\nstrategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5,\ngood experts vs. bad experts in MoE). Extensive experiments demonstrate that\nW2SD significantly improves human preference, aesthetic quality, and prompt\nadherence, achieving SOTA performance across various modalities (e.g., image,\nvideo), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For\nexample, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to\n90% over the original results. Moreover, the performance gains achieved by W2SD\nmarkedly outweigh its additional computational overhead, while the cumulative\nimprovements from different weak-to-strong difference further solidify its\npractical utility and deployability."
                },
                "authors": [
                    {
                        "name": "Lichen Bai"
                    },
                    {
                        "name": "Masashi Sugiyama"
                    },
                    {
                        "name": "Zeke Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Xie"
                },
                "author": "Zeke Xie",
                "arxiv_comment": "20 pages, 19 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00473v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00473v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18410v2",
                "updated": "2025-02-05T06:56:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    6,
                    56,
                    47,
                    2,
                    36,
                    0
                ],
                "published": "2024-04-29T03:58:12Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    3,
                    58,
                    12,
                    0,
                    120,
                    0
                ],
                "title": "Mixture-of-Instructions: Aligning Large Language Models via Mixture\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Instructions: Aligning Large Language Models via Mixture\n  Prompting"
                },
                "summary": "With the proliferation of large language models (LLMs), the comprehensive\nalignment of such models across multiple tasks has emerged as a critical area\nof research. Existing alignment methodologies primarily address single task,\nsuch as multi-turn dialogue, coding, mathematical problem-solving, and tool\nusage. Although there is a large amount of high-quality data available for\nthose tasks, most of them provide only questions and answers without including\nthe system prompt. Though a detailed analysis of the Qwen language model, we\nfound that the system prompt has a significant impact on both training and\ninference processes of LLM. We attributes this phenomenon to overfitting to the\nsystem prompt. In address this issue, we introduce a novel technique termed\nMixture-of-Instructions (MoI), which employs a strategy of instruction packing\ncombined with diverse system prompts to boost the alignment efficiency of\nlanguage models. We have also compiled a diverse set of seven benchmark\ndatasets to rigorously evaluate the alignment efficacy of the MoI-enhanced\nlanguage model. Our methodology was applied to the open-source Qwen-7B-chat\nmodel, culminating in the development of Qwen-SFT-MoI. This enhanced model\ndemonstrates significant advancements in generative capabilities across coding,\nmathematics, and tool use tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the proliferation of large language models (LLMs), the comprehensive\nalignment of such models across multiple tasks has emerged as a critical area\nof research. Existing alignment methodologies primarily address single task,\nsuch as multi-turn dialogue, coding, mathematical problem-solving, and tool\nusage. Although there is a large amount of high-quality data available for\nthose tasks, most of them provide only questions and answers without including\nthe system prompt. Though a detailed analysis of the Qwen language model, we\nfound that the system prompt has a significant impact on both training and\ninference processes of LLM. We attributes this phenomenon to overfitting to the\nsystem prompt. In address this issue, we introduce a novel technique termed\nMixture-of-Instructions (MoI), which employs a strategy of instruction packing\ncombined with diverse system prompts to boost the alignment efficiency of\nlanguage models. We have also compiled a diverse set of seven benchmark\ndatasets to rigorously evaluate the alignment efficacy of the MoI-enhanced\nlanguage model. Our methodology was applied to the open-source Qwen-7B-chat\nmodel, culminating in the development of Qwen-SFT-MoI. This enhanced model\ndemonstrates significant advancements in generative capabilities across coding,\nmathematics, and tool use tasks."
                },
                "authors": [
                    {
                        "name": "Bowen Xu"
                    },
                    {
                        "name": "Shaoyu Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Lulu Hu"
                    }
                ],
                "author_detail": {
                    "name": "Lulu Hu"
                },
                "author": "Lulu Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00165v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00165v3",
                "updated": "2025-02-05T06:45:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    6,
                    45,
                    46,
                    2,
                    36,
                    0
                ],
                "published": "2024-02-29T22:26:07Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    22,
                    26,
                    7,
                    3,
                    60,
                    0
                ],
                "title": "TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text\n  Classification with Minimal Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text\n  Classification with Minimal Supervision"
                },
                "summary": "Hierarchical text classification aims to categorize each document into a set\nof classes in a label taxonomy, which is a fundamental web text mining task\nwith broad applications such as web content analysis and semantic indexing.\nMost earlier works focus on fully or semi-supervised methods that require a\nlarge amount of human annotated data which is costly and time-consuming to\nacquire. To alleviate human efforts, in this paper, we work on hierarchical\ntext classification with a minimal amount of supervision: using the sole class\nname of each node as the only supervision. Recently, large language models\n(LLM) have shown competitive performance on various tasks through zero-shot\nprompting, but this method performs poorly in the hierarchical setting because\nit is ineffective to include the large and structured label space in a prompt.\nOn the other hand, previous weakly-supervised hierarchical text classification\nmethods only utilize the raw taxonomy skeleton and ignore the rich information\nhidden in the text corpus that can serve as additional class-indicative\nfeatures. To tackle the above challenges, we propose TELEClass, Taxonomy\nEnrichment and LLM-Enhanced weakly-supervised hierarchical text Classification,\nwhich combines the general knowledge of LLMs and task-specific features mined\nfrom an unlabeled corpus. TELEClass automatically enriches the raw taxonomy\nwith class-indicative features for better label space understanding and\nutilizes novel LLM-based data annotation and generation methods specifically\ntailored for the hierarchical setting. Experiments show that TELEClass can\nsignificantly outperform previous baselines while achieving comparable\nperformance to zero-shot prompting of LLMs with drastically less inference\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical text classification aims to categorize each document into a set\nof classes in a label taxonomy, which is a fundamental web text mining task\nwith broad applications such as web content analysis and semantic indexing.\nMost earlier works focus on fully or semi-supervised methods that require a\nlarge amount of human annotated data which is costly and time-consuming to\nacquire. To alleviate human efforts, in this paper, we work on hierarchical\ntext classification with a minimal amount of supervision: using the sole class\nname of each node as the only supervision. Recently, large language models\n(LLM) have shown competitive performance on various tasks through zero-shot\nprompting, but this method performs poorly in the hierarchical setting because\nit is ineffective to include the large and structured label space in a prompt.\nOn the other hand, previous weakly-supervised hierarchical text classification\nmethods only utilize the raw taxonomy skeleton and ignore the rich information\nhidden in the text corpus that can serve as additional class-indicative\nfeatures. To tackle the above challenges, we propose TELEClass, Taxonomy\nEnrichment and LLM-Enhanced weakly-supervised hierarchical text Classification,\nwhich combines the general knowledge of LLMs and task-specific features mined\nfrom an unlabeled corpus. TELEClass automatically enriches the raw taxonomy\nwith class-indicative features for better label space understanding and\nutilizes novel LLM-based data annotation and generation methods specifically\ntailored for the hierarchical setting. Experiments show that TELEClass can\nsignificantly outperform previous baselines while achieving comparable\nperformance to zero-shot prompting of LLMs with drastically less inference\ncost."
                },
                "authors": [
                    {
                        "name": "Yunyi Zhang"
                    },
                    {
                        "name": "Ruozhen Yang"
                    },
                    {
                        "name": "Xueqiang Xu"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jinfeng Xiao"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "Accepted to WWW 2025 Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00165v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00165v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02928v1",
                "updated": "2025-02-05T06:43:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    6,
                    43,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T06:43:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    6,
                    43,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "Large Language Model Guided Self-Debugging Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Guided Self-Debugging Code Generation"
                },
                "summary": "Automated code generation is gaining significant importance in intelligent\ncomputer programming and system deployment. However, current approaches often\nface challenges in computational efficiency and lack robust mechanisms for code\nparsing and error correction. In this work, we propose a novel framework,\nPyCapsule, with a simple yet effective two-agent pipeline and efficient\nself-debugging modules for Python code generation. PyCapsule features\nsophisticated prompt inference, iterative error handling, and case testing,\nensuring high generation stability, safety, and correctness. Empirically,\nPyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3%\non HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art\nmethods. We also observe a decrease in normalized success rate given more\nself-debugging attempts, potentially affected by limited and noisy error\nfeedback in retention. PyCapsule demonstrates broader impacts on advancing\nlightweight and efficient code generation for artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated code generation is gaining significant importance in intelligent\ncomputer programming and system deployment. However, current approaches often\nface challenges in computational efficiency and lack robust mechanisms for code\nparsing and error correction. In this work, we propose a novel framework,\nPyCapsule, with a simple yet effective two-agent pipeline and efficient\nself-debugging modules for Python code generation. PyCapsule features\nsophisticated prompt inference, iterative error handling, and case testing,\nensuring high generation stability, safety, and correctness. Empirically,\nPyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3%\non HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art\nmethods. We also observe a decrease in normalized success rate given more\nself-debugging attempts, potentially affected by limited and noisy error\nfeedback in retention. PyCapsule demonstrates broader impacts on advancing\nlightweight and efficient code generation for artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Muntasir Adnan"
                    },
                    {
                        "name": "Zhiwei Xu"
                    },
                    {
                        "name": "Carlos C. N. Kuhn"
                    }
                ],
                "author_detail": {
                    "name": "Carlos C. N. Kuhn"
                },
                "author": "Carlos C. N. Kuhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11629v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11629v6",
                "updated": "2025-02-05T06:42:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    6,
                    42,
                    43,
                    2,
                    36,
                    0
                ],
                "published": "2024-06-17T15:11:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    11,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary\n  Empirical Study"
                },
                "summary": "Utilizing Large Language Models (LLMs) as evaluators to assess the\nperformance of LLMs has garnered attention. However, this kind of evaluation\napproach is affected by potential biases within LLMs, raising concerns about\nthe accuracy and reliability of the evaluation results of LLMs. To address this\nproblem, we propose and study two many-shot In-Context Learning (ICL) prompt\ntemplates to help LLM evaluators mitigate potential biases: Many-Shot with\nReference (MSwR) and Many-Shot without Reference (MSoR). Specifically, the\nformer utilizes in-context examples with model-generated evaluation rationales\nas references, while the latter does not include these references. Using these\nprompt designs, we investigate the impact of increasing the number of\nin-context examples on the consistency and quality of the evaluation results.\nExperimental results show that advanced LLMs, such as GPT-4o, perform better in\nthe many-shot regime than in the zero-shot and few-shot regimes. Furthermore,\nwhen using GPT-4o as an evaluator in the many-shot regime, adopting MSwR as the\nprompt template performs better than MSoR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models (LLMs) as evaluators to assess the\nperformance of LLMs has garnered attention. However, this kind of evaluation\napproach is affected by potential biases within LLMs, raising concerns about\nthe accuracy and reliability of the evaluation results of LLMs. To address this\nproblem, we propose and study two many-shot In-Context Learning (ICL) prompt\ntemplates to help LLM evaluators mitigate potential biases: Many-Shot with\nReference (MSwR) and Many-Shot without Reference (MSoR). Specifically, the\nformer utilizes in-context examples with model-generated evaluation rationales\nas references, while the latter does not include these references. Using these\nprompt designs, we investigate the impact of increasing the number of\nin-context examples on the consistency and quality of the evaluation results.\nExperimental results show that advanced LLMs, such as GPT-4o, perform better in\nthe many-shot regime than in the zero-shot and few-shot regimes. Furthermore,\nwhen using GPT-4o as an evaluator in the many-shot regime, adopting MSwR as the\nprompt template performs better than MSoR."
                },
                "authors": [
                    {
                        "name": "Mingyang Song"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Xuan Luo"
                    },
                    {
                        "name": "Yue Pan"
                    }
                ],
                "author_detail": {
                    "name": "Yue Pan"
                },
                "author": "Yue Pan",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11629v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11629v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02913v1",
                "updated": "2025-02-05T06:20:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    6,
                    20,
                    20,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T06:20:20Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    6,
                    20,
                    20,
                    2,
                    36,
                    0
                ],
                "title": "Privacy Token: Surprised to Find Out What You Accidentally Revealed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Token: Surprised to Find Out What You Accidentally Revealed"
                },
                "summary": "The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of deep learning models in privacy-sensitive\ndomains has amplified concerns regarding privacy risks, particularly those\nstemming from gradient leakage during training. Current privacy assessments\nprimarily rely on post-training attack simulations. However, these methods are\ninherently reactive, unable to encompass all potential attack scenarios, and\noften based on idealized adversarial assumptions. These limitations underscore\nthe need for proactive approaches to privacy risk assessment during the\ntraining process. To address this gap, we propose the concept of privacy\ntokens, which are derived directly from private gradients during training.\nPrivacy tokens encapsulate gradient features and, when combined with data\nfeatures, offer valuable insights into the extent of private information\nleakage from training data, enabling real-time measurement of privacy risks\nwithout relying on adversarial attack simulations. Additionally, we employ\nMutual Information (MI) as a robust metric to quantify the relationship between\ntraining data and gradients, providing precise and continuous assessments of\nprivacy leakage throughout the training process. Extensive experiments validate\nour framework, demonstrating the effectiveness of privacy tokens and MI in\nidentifying and quantifying privacy risks. This proactive approach marks a\nsignificant advancement in privacy monitoring, promoting the safer deployment\nof deep learning models in sensitive applications."
                },
                "authors": [
                    {
                        "name": "Jiayang Meng"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Xin Shi"
                    },
                    {
                        "name": "Qingyu Huang"
                    },
                    {
                        "name": "Chen Hou"
                    },
                    {
                        "name": "Hong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hong Chen"
                },
                "author": "Hong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16920v2",
                "updated": "2025-02-05T06:12:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    6,
                    12,
                    13,
                    2,
                    36,
                    0
                ],
                "published": "2024-07-24T01:04:34Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    1,
                    4,
                    34,
                    2,
                    206,
                    0
                ],
                "title": "Train-Attention: Meta-Learning Where to Focus in Continual Knowledge\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train-Attention: Meta-Learning Where to Focus in Continual Knowledge\n  Learning"
                },
                "summary": "Previous studies on continual knowledge learning (CKL) in large language\nmodels (LLMs) have predominantly focused on approaches such as regularization,\narchitectural modifications, and rehearsal techniques to mitigate catastrophic\nforgetting. However, these methods naively inherit the inefficiencies of\nstandard training procedures, indiscriminately applying uniform weight across\nall tokens, which can lead to unnecessary parameter updates and increased\nforgetting. To address these shortcomings, we propose a novel CKL approach\ntermed Train-Attention-Augmented Language Model (TAALM), which enhances\nlearning efficiency by dynamically predicting and applying weights to tokens\nbased on their usefulness. This method employs a meta-learning framework that\noptimizes token importance predictions, facilitating targeted knowledge updates\nand minimizing forgetting. Also, we observe that existing benchmarks do not\nclearly exhibit the trade-off between learning and retaining, therefore we\npropose a new benchmark, \\textsc{LAMA-ckl}, to address this issue. Through\nexperiments conducted on both newly introduced and established CKL benchmarks,\nTAALM proves the state-of-the-art performance upon the baselines, and also\nshows synergistic compatibility when integrated with previous CKL approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous studies on continual knowledge learning (CKL) in large language\nmodels (LLMs) have predominantly focused on approaches such as regularization,\narchitectural modifications, and rehearsal techniques to mitigate catastrophic\nforgetting. However, these methods naively inherit the inefficiencies of\nstandard training procedures, indiscriminately applying uniform weight across\nall tokens, which can lead to unnecessary parameter updates and increased\nforgetting. To address these shortcomings, we propose a novel CKL approach\ntermed Train-Attention-Augmented Language Model (TAALM), which enhances\nlearning efficiency by dynamically predicting and applying weights to tokens\nbased on their usefulness. This method employs a meta-learning framework that\noptimizes token importance predictions, facilitating targeted knowledge updates\nand minimizing forgetting. Also, we observe that existing benchmarks do not\nclearly exhibit the trade-off between learning and retaining, therefore we\npropose a new benchmark, \\textsc{LAMA-ckl}, to address this issue. Through\nexperiments conducted on both newly introduced and established CKL benchmarks,\nTAALM proves the state-of-the-art performance upon the baselines, and also\nshows synergistic compatibility when integrated with previous CKL approaches."
                },
                "authors": [
                    {
                        "name": "Yeongbin Seo"
                    },
                    {
                        "name": "Dongha Lee"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Jinyoung Yeo"
                },
                "author": "Jinyoung Yeo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02909v1",
                "updated": "2025-02-05T06:11:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    6,
                    11,
                    55,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T06:11:55Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    6,
                    11,
                    55,
                    2,
                    36,
                    0
                ],
                "title": "SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in\n  LLMs"
                },
                "summary": "We propose SPARC, a lightweight continual learning framework for large\nlanguage models (LLMs) that enables efficient task adaptation through prompt\ntuning in a lower-dimensional space. By leveraging principal component analysis\n(PCA), we identify a compact subspace of the training data. Optimizing prompts\nin this lower-dimensional space enhances training efficiency, as it focuses\nupdates on the most relevant features while reducing computational overhead.\nFurthermore, since the model's internal structure remains unaltered, the\nextensive knowledge gained from pretraining is fully preserved, ensuring that\npreviously learned information is not compromised during adaptation. Our method\nachieves high knowledge retention in both task-incremental and\ndomain-incremental continual learning setups while fine-tuning only 0.04% of\nthe model's parameters. Additionally, by integrating LoRA, we enhance\nadaptability to computational constraints, allowing for a tradeoff between\naccuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate\nthat our PCA-based prompt tuning combined with LoRA maintains full knowledge\nretention while improving accuracy, utilizing only 1% of the model's\nparameters. These results establish our approach as a scalable and\nresource-efficient solution for continual learning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SPARC, a lightweight continual learning framework for large\nlanguage models (LLMs) that enables efficient task adaptation through prompt\ntuning in a lower-dimensional space. By leveraging principal component analysis\n(PCA), we identify a compact subspace of the training data. Optimizing prompts\nin this lower-dimensional space enhances training efficiency, as it focuses\nupdates on the most relevant features while reducing computational overhead.\nFurthermore, since the model's internal structure remains unaltered, the\nextensive knowledge gained from pretraining is fully preserved, ensuring that\npreviously learned information is not compromised during adaptation. Our method\nachieves high knowledge retention in both task-incremental and\ndomain-incremental continual learning setups while fine-tuning only 0.04% of\nthe model's parameters. Additionally, by integrating LoRA, we enhance\nadaptability to computational constraints, allowing for a tradeoff between\naccuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate\nthat our PCA-based prompt tuning combined with LoRA maintains full knowledge\nretention while improving accuracy, utilizing only 1% of the model's\nparameters. These results establish our approach as a scalable and\nresource-efficient solution for continual learning in LLMs."
                },
                "authors": [
                    {
                        "name": "Dinithi Jayasuriya"
                    },
                    {
                        "name": "Sina Tayebati"
                    },
                    {
                        "name": "Davide Ettori"
                    },
                    {
                        "name": "Ranganath Krishnan"
                    },
                    {
                        "name": "Amit Ranjan Trivedi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Ranjan Trivedi"
                },
                "arxiv_affiliation": "Intel Labs, Oregon",
                "author": "Amit Ranjan Trivedi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02908v1",
                "updated": "2025-02-05T06:09:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    6,
                    9,
                    26,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T06:09:26Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    6,
                    9,
                    26,
                    2,
                    36,
                    0
                ],
                "title": "COSMosFL: Ensemble of Small Language Models for Fault Localisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSMosFL: Ensemble of Small Language Models for Fault Localisation"
                },
                "summary": "LLMs are rapidly being adopted to build powerful tools and agents for\nsoftware engineering, but most of them rely heavily on extremely large\nclosed-source models. This, in turn, can hinder wider adoption due to security\nissues as well as financial cost and environmental impact. Recently, a number\nof open source Small Language Models (SLMs) are being released and gaining\ntraction. While SLMs are smaller, more energy-efficient, and therefore easier\nto locally deploy, they tend to show worse performance when compared to larger\nclosed LLMs. We present COSMos, a task-level LLM ensemble technique that uses\nvoting mechanism, to provide a broader range of choice between SLMs and LLMs.\nWe instantiate COSMos with an LLM-based Fault Localisation technique, AutoFL,\nand report the cost-benefit trade-off between LLM accuracy and various costs\nsuch as energy consumption, inference time, and the number of tokens used. An\nempirical evaluation using Defects4J shows that COSMos can build effective\nensembles that can achieve Pareto-optimality in terms of FL accuracy and\ninference cost, when compared to individual models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are rapidly being adopted to build powerful tools and agents for\nsoftware engineering, but most of them rely heavily on extremely large\nclosed-source models. This, in turn, can hinder wider adoption due to security\nissues as well as financial cost and environmental impact. Recently, a number\nof open source Small Language Models (SLMs) are being released and gaining\ntraction. While SLMs are smaller, more energy-efficient, and therefore easier\nto locally deploy, they tend to show worse performance when compared to larger\nclosed LLMs. We present COSMos, a task-level LLM ensemble technique that uses\nvoting mechanism, to provide a broader range of choice between SLMs and LLMs.\nWe instantiate COSMos with an LLM-based Fault Localisation technique, AutoFL,\nand report the cost-benefit trade-off between LLM accuracy and various costs\nsuch as energy consumption, inference time, and the number of tokens used. An\nempirical evaluation using Defects4J shows that COSMos can build effective\nensembles that can achieve Pareto-optimality in terms of FL accuracy and\ninference cost, when compared to individual models."
                },
                "authors": [
                    {
                        "name": "Hyunjoon Cho"
                    },
                    {
                        "name": "Sungmin Kang"
                    },
                    {
                        "name": "Gabin An"
                    },
                    {
                        "name": "Shin Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Shin Yoo"
                },
                "author": "Shin Yoo",
                "arxiv_comment": "LLM4Code 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01768v2",
                "updated": "2025-02-05T05:57:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    5,
                    57,
                    44,
                    2,
                    36,
                    0
                ],
                "published": "2024-05-02T22:37:38Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    22,
                    37,
                    38,
                    3,
                    123,
                    0
                ],
                "title": "CoS: Enhancing Personalization and Mitigating Bias with Context Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoS: Enhancing Personalization and Mitigating Bias with Context Steering"
                },
                "summary": "When querying a large language model (LLM), the context, i.e. personal,\ndemographic, and cultural information specific to an end-user, can\nsignificantly shape the response of the LLM. For example, asking the model to\nexplain Newton's second law with the context \"I am a toddler\" yields a\ndifferent answer compared to the context \"I am a physics professor.\" Proper\nusage of the context enables the LLM to generate personalized responses,\nwhereas inappropriate contextual influence can lead to stereotypical and\npotentially harmful generations (e.g. associating \"female\" with \"housekeeper\").\nIn practice, striking the right balance when leveraging context is a nuanced\nand challenging problem that is often situation-dependent. One common approach\nto address this challenge is to fine-tune LLMs on contextually appropriate\nresponses. However, this approach is expensive, time-consuming, and not\ncontrollable for end-users in different situations. In this work, we propose\nContext Steering (CoS) - a simple training-free method that can be easily\napplied to autoregressive LLMs at inference time. By measuring the contextual\ninfluence in terms of token prediction likelihood and modulating it, our method\nenables practitioners to determine the appropriate level of contextual\ninfluence based on their specific use case and end-user base. We showcase a\nvariety of applications of CoS including amplifying the contextual influence to\nachieve better personalization and mitigating unwanted influence for reducing\nmodel bias. In addition, we show that we can combine CoS with Bayesian\nInference to quantify the extent of hate speech on the internet. We demonstrate\nthe effectiveness of CoS on state-of-the-art LLMs and benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When querying a large language model (LLM), the context, i.e. personal,\ndemographic, and cultural information specific to an end-user, can\nsignificantly shape the response of the LLM. For example, asking the model to\nexplain Newton's second law with the context \"I am a toddler\" yields a\ndifferent answer compared to the context \"I am a physics professor.\" Proper\nusage of the context enables the LLM to generate personalized responses,\nwhereas inappropriate contextual influence can lead to stereotypical and\npotentially harmful generations (e.g. associating \"female\" with \"housekeeper\").\nIn practice, striking the right balance when leveraging context is a nuanced\nand challenging problem that is often situation-dependent. One common approach\nto address this challenge is to fine-tune LLMs on contextually appropriate\nresponses. However, this approach is expensive, time-consuming, and not\ncontrollable for end-users in different situations. In this work, we propose\nContext Steering (CoS) - a simple training-free method that can be easily\napplied to autoregressive LLMs at inference time. By measuring the contextual\ninfluence in terms of token prediction likelihood and modulating it, our method\nenables practitioners to determine the appropriate level of contextual\ninfluence based on their specific use case and end-user base. We showcase a\nvariety of applications of CoS including amplifying the contextual influence to\nachieve better personalization and mitigating unwanted influence for reducing\nmodel bias. In addition, we show that we can combine CoS with Bayesian\nInference to quantify the extent of hate speech on the internet. We demonstrate\nthe effectiveness of CoS on state-of-the-art LLMs and benchmarks."
                },
                "authors": [
                    {
                        "name": "Jerry Zhi-Yang He"
                    },
                    {
                        "name": "Sashrika Pandey"
                    },
                    {
                        "name": "Mariah L. Schrum"
                    },
                    {
                        "name": "Anca Dragan"
                    }
                ],
                "author_detail": {
                    "name": "Anca Dragan"
                },
                "author": "Anca Dragan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]