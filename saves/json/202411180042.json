[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.09688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v1",
                "updated": "2024-11-14T18:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v2",
                "updated": "2024-11-14T17:46:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    46,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v2",
                "updated": "2024-11-14T15:40:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    40,
                    59,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v1",
                "updated": "2024-11-14T08:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Pkd-tree: Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pkd-tree: Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v4",
                "updated": "2024-11-13T16:33:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    16,
                    33,
                    33,
                    2,
                    318,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The personalized recommendation system's continuous size growth poses new\nchallenges for model inference. Although weight-sharing algorithms have been\nproposed to reduce embedding table capacity, they increase memory access.\nRecent advancements in processing-in-memory (PIM) successfully enhance the\nrecommendation system's throughput by exploiting memory parallelism, but our\nanalysis shows that those algorithms introduce CPU-PIM communication overhead\ninto prior PIM systems, compromising the PIM throughput. We propose\nProactivePIM, a specialized memory architecture integrated with PIM technology\ntailored to accelerate the weight-sharing algorithms. ProacitvePIM integrates\nan SRAM cache within the PIM with an efficient prefetching scheme to leverage a\nunique locality of the algorithm and eliminate CPU-PIM communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The personalized recommendation system's continuous size growth poses new\nchallenges for model inference. Although weight-sharing algorithms have been\nproposed to reduce embedding table capacity, they increase memory access.\nRecent advancements in processing-in-memory (PIM) successfully enhance the\nrecommendation system's throughput by exploiting memory parallelism, but our\nanalysis shows that those algorithms introduce CPU-PIM communication overhead\ninto prior PIM systems, compromising the PIM throughput. We propose\nProactivePIM, a specialized memory architecture integrated with PIM technology\ntailored to accelerate the weight-sharing algorithms. ProacitvePIM integrates\nan SRAM cache within the PIM with an efficient prefetching scheme to leverage a\nunique locality of the algorithm and eliminate CPU-PIM communication."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08672v1",
                "updated": "2024-11-13T15:07:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T15:07:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks"
                },
                "summary": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Zhibin Gao"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "conference paper with 6 pages and 5 figures. arXiv admin note: text\n  overlap with arXiv:2411.01458",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08312v1",
                "updated": "2024-11-13T03:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T03:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "title": "A Novel Extensible Simulation Framework for CXL-Enabled Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Extensible Simulation Framework for CXL-Enabled Systems"
                },
                "summary": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3."
                },
                "authors": [
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Mingzhe Zhang"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "arxiv_affiliation": "Peking University",
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08203v1",
                "updated": "2024-11-12T21:50:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T21:50:03Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "title": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing"
                },
                "summary": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads."
                },
                "authors": [
                    {
                        "name": "Jacopo Tagliabue"
                    },
                    {
                        "name": "Ryan Curtin"
                    },
                    {
                        "name": "Ciro Greco"
                    }
                ],
                "author_detail": {
                    "name": "Ciro Greco"
                },
                "author": "Ciro Greco",
                "arxiv_comment": "Pre-print of the paper accepted at DEMAI@IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06219v3",
                "updated": "2024-11-12T08:18:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    18,
                    45,
                    1,
                    317,
                    0
                ],
                "published": "2024-05-10T03:06:24Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    3,
                    6,
                    24,
                    4,
                    131,
                    0
                ],
                "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding."
                },
                "authors": [
                    {
                        "name": "Haojie Duanmu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Jiangfei Duan"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v1",
                "updated": "2024-11-12T08:17:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06659v1",
                "updated": "2024-11-11T01:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:53:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning"
                },
                "summary": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git ."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "arxiv_comment": "16 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v2",
                "updated": "2024-11-10T23:04:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    23,
                    4,
                    12,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v4",
                "updated": "2024-11-10T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    58,
                    7,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04873v2",
                "updated": "2024-11-10T10:08:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    10,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "published": "2024-06-07T12:12:25Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    12,
                    12,
                    25,
                    4,
                    159,
                    0
                ],
                "title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior"
                },
                "summary": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main"
                },
                "authors": [
                    {
                        "name": "Tanvir Mahmud"
                    },
                    {
                        "name": "Mustafa Munir"
                    },
                    {
                        "name": "Radu Marculescu"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "arxiv_comment": "Accepted in WACV 2025. Project page:\n  https://tanvir-utexas.github.io/AdaVE_Demo/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v1",
                "updated": "2024-11-10T08:31:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v1",
                "updated": "2024-11-10T05:12:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02542v2",
                "updated": "2024-11-07T18:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    58,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    58,
                    3,
                    1,
                    156,
                    0
                ],
                "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loki: Low-rank Keys for Efficient Sparse Attention"
                },
                "summary": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods."
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "Proceedings of the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (Main Conference Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v1",
                "updated": "2024-11-07T14:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v1",
                "updated": "2024-11-05T15:22:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v1",
                "updated": "2024-10-30T02:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "The code is coming soon! For sure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08043v1",
                "updated": "2024-10-30T02:18:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:18:59Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "title": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications"
                },
                "summary": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks."
                },
                "authors": [
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Ding Yuan"
                    },
                    {
                        "name": "Xueshang Feng"
                    },
                    {
                        "name": "Stefaan Poedts"
                    },
                    {
                        "name": "Zhengyang Zou"
                    },
                    {
                        "name": "Song Feng"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Tong Yin"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yin"
                },
                "author": "Tong Yin",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.space-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20337v1",
                "updated": "2024-10-27T04:31:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "published": "2024-10-27T04:31:35Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "title": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms"
                },
                "summary": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided."
                },
                "authors": [
                    {
                        "name": "Lorenzo De Stefani"
                    },
                    {
                        "name": "Vedant Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vedant Gupta"
                },
                "author": "Vedant Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04216v3",
                "updated": "2024-10-26T22:19:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    19,
                    4,
                    5,
                    300,
                    0
                ],
                "published": "2024-02-06T18:17:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    17,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks"
                },
                "summary": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Md Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20149v1",
                "updated": "2024-10-26T11:20:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "published": "2024-10-26T11:20:02Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models"
                },
                "summary": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}."
                },
                "authors": [
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20004v1",
                "updated": "2024-10-25T23:17:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T23:17:56Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "title": "Lightweight, Secure and Stateful Serverless Computing with PSL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight, Secure and Stateful Serverless Computing with PSL"
                },
                "summary": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks."
                },
                "authors": [
                    {
                        "name": "Alexander Thomas"
                    },
                    {
                        "name": "Shubham Mishra"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "John Kubiatowicz"
                    }
                ],
                "author_detail": {
                    "name": "John Kubiatowicz"
                },
                "author": "John Kubiatowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05317v2",
                "updated": "2024-10-25T21:09:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    21,
                    9,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-08T01:35:11Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    35,
                    11,
                    5,
                    160,
                    0
                ],
                "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoCo: Dropping In Convolutions for Long Context Compression"
                },
                "summary": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v2",
                "updated": "2024-10-25T19:45:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    45,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19937v1",
                "updated": "2024-10-25T19:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T19:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction"
                },
                "summary": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Changjiang Li"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v2",
                "updated": "2024-10-25T19:18:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Fast Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v5",
                "updated": "2024-10-25T18:29:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    29,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19123v1",
                "updated": "2024-10-24T19:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T19:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design"
                },
                "summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.09689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09689v1",
                "updated": "2024-11-14T18:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    55,
                    26,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    55,
                    26,
                    3,
                    319,
                    0
                ],
                "title": "LLM Hallucination Reasoning with Zero-shot Knowledge Test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Hallucination Reasoning with Zero-shot Knowledge Test"
                },
                "summary": "LLM hallucination, where LLMs occasionally generate unfaithful text, poses\nsignificant challenges for their practical applications. Most existing\ndetection methods rely on external knowledge, LLM fine-tuning, or\nhallucination-labeled datasets, and they do not distinguish between different\ntypes of hallucinations, which are crucial for improving detection performance.\nWe introduce a new task, Hallucination Reasoning, which classifies\nLLM-generated text into one of three categories: aligned, misaligned, and\nfabricated. Our novel zero-shot method assesses whether LLM has enough\nknowledge about a given prompt and text. Our experiments conducted on new\ndatasets demonstrate the effectiveness of our method in hallucination reasoning\nand underscore its importance for enhancing detection performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM hallucination, where LLMs occasionally generate unfaithful text, poses\nsignificant challenges for their practical applications. Most existing\ndetection methods rely on external knowledge, LLM fine-tuning, or\nhallucination-labeled datasets, and they do not distinguish between different\ntypes of hallucinations, which are crucial for improving detection performance.\nWe introduce a new task, Hallucination Reasoning, which classifies\nLLM-generated text into one of three categories: aligned, misaligned, and\nfabricated. Our novel zero-shot method assesses whether LLM has enough\nknowledge about a given prompt and text. Our experiments conducted on new\ndatasets demonstrate the effectiveness of our method in hallucination reasoning\nand underscore its importance for enhancing detection performance."
                },
                "authors": [
                    {
                        "name": "Seongmin Lee"
                    },
                    {
                        "name": "Hsiang Hsu"
                    },
                    {
                        "name": "Chun-Fu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Fu Chen"
                },
                "author": "Chun-Fu Chen",
                "arxiv_comment": "12 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v1",
                "updated": "2024-11-14T18:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05777v2",
                "updated": "2024-11-14T18:35:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    35,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-08T18:43:15Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    43,
                    15,
                    4,
                    313,
                    0
                ],
                "title": "Quantitative Assessment of Intersectional Empathetic Bias and\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative Assessment of Intersectional Empathetic Bias and\n  Understanding"
                },
                "summary": "A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results."
                },
                "authors": [
                    {
                        "name": "Vojtech Formanek"
                    },
                    {
                        "name": "Ondrej Sotolar"
                    }
                ],
                "author_detail": {
                    "name": "Ondrej Sotolar"
                },
                "author": "Ondrej Sotolar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09661v1",
                "updated": "2024-11-14T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    31,
                    39,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    31,
                    39,
                    3,
                    319,
                    0
                ],
                "title": "Adaptive Decoding via Latent Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Decoding via Latent Preference Optimization"
                },
                "summary": "During language model decoding, it is known that using higher temperature\nsampling gives more creative responses, while lower temperatures are more\nfactually accurate. However, such models are commonly applied to general\ninstruction following, which involves both creative and fact seeking tasks,\nusing a single fixed temperature across all examples and tokens. In this work,\nwe introduce Adaptive Decoding, a layer added to the model to select the\nsampling temperature dynamically at inference time, at either the token or\nexample level, in order to optimize performance. To learn its parameters we\nintroduce Latent Preference Optimization (LPO) a general approach to train\ndiscrete latent variables such as choices of temperature. Our method\noutperforms all fixed decoding temperatures across a range of tasks that\nrequire different temperatures, including UltraFeedback, Creative Story\nWriting, and GSM8K.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "During language model decoding, it is known that using higher temperature\nsampling gives more creative responses, while lower temperatures are more\nfactually accurate. However, such models are commonly applied to general\ninstruction following, which involves both creative and fact seeking tasks,\nusing a single fixed temperature across all examples and tokens. In this work,\nwe introduce Adaptive Decoding, a layer added to the model to select the\nsampling temperature dynamically at inference time, at either the token or\nexample level, in order to optimize performance. To learn its parameters we\nintroduce Latent Preference Optimization (LPO) a general approach to train\ndiscrete latent variables such as choices of temperature. Our method\noutperforms all fixed decoding temperatures across a range of tasks that\nrequire different temperatures, including UltraFeedback, Creative Story\nWriting, and GSM8K."
                },
                "authors": [
                    {
                        "name": "Shehzaad Dhuliawala"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Ping Yu"
                    },
                    {
                        "name": "Asli Celikyilmaz"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "Jack Lanchantin"
                    }
                ],
                "author_detail": {
                    "name": "Jack Lanchantin"
                },
                "author": "Jack Lanchantin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09658v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09658v1",
                "updated": "2024-11-14T18:29:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    29,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:29:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    29,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Motion Before Action: Diffusing Object Motion as Manipulation Condition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion Before Action: Diffusing Object Motion as Manipulation Condition"
                },
                "summary": "Inferring object motion representations from observations enhances the\nperformance of robotic manipulation tasks. This paper introduces a new paradigm\nfor robot imitation learning that generates action sequences by reasoning about\nobject motion from visual observations. We propose MBA (Motion Before Action),\na novel module that employs two cascaded diffusion processes for object motion\ngeneration and robot action generation under object motion guidance. MBA first\npredicts the future pose sequence of the object based on observations, then\nuses this sequence as a condition to guide robot action generation. Designed as\na plug-and-play component, MBA can be flexibly integrated into existing robotic\nmanipulation policies with diffusion action heads. Extensive experiments in\nboth simulated and real-world environments demonstrate that our approach\nsubstantially improves the performance of existing policies across a wide range\nof manipulation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring object motion representations from observations enhances the\nperformance of robotic manipulation tasks. This paper introduces a new paradigm\nfor robot imitation learning that generates action sequences by reasoning about\nobject motion from visual observations. We propose MBA (Motion Before Action),\na novel module that employs two cascaded diffusion processes for object motion\ngeneration and robot action generation under object motion guidance. MBA first\npredicts the future pose sequence of the object based on observations, then\nuses this sequence as a condition to guide robot action generation. Designed as\na plug-and-play component, MBA can be flexibly integrated into existing robotic\nmanipulation policies with diffusion action heads. Extensive experiments in\nboth simulated and real-world environments demonstrate that our approach\nsubstantially improves the performance of existing policies across a wide range\nof manipulation tasks."
                },
                "authors": [
                    {
                        "name": "Yup Su"
                    },
                    {
                        "name": "Xinyu Zhan"
                    },
                    {
                        "name": "Hongjie Fang"
                    },
                    {
                        "name": "Yong-Lu Li"
                    },
                    {
                        "name": "Cewu Lu"
                    },
                    {
                        "name": "Lixin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lixin Yang"
                },
                "author": "Lixin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09658v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09658v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03862v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03862v3",
                "updated": "2024-11-14T18:27:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    27,
                    39,
                    3,
                    319,
                    0
                ],
                "published": "2024-04-05T02:27:09Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    2,
                    27,
                    9,
                    4,
                    96,
                    0
                ],
                "title": "Verifiable by Design: Aligning Language Models to Quote from\n  Pre-Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiable by Design: Aligning Language Models to Quote from\n  Pre-Training Data"
                },
                "summary": "To trust the fluent generations of large language models (LLMs), humans must\nbe able to verify their correctness against trusted, external sources. Recent\nefforts, such as providing citations via retrieved documents or post-hoc\nprovenance, enhance verifiability but provide no guarantees on their\ncorrectness. To address these limitations, we tackle the verifiability goal\nwith a different philosophy: trivializing the verification process by\ndeveloping models that quote verbatim statements from trusted sources in their\npre-training data. We propose Quote-Tuning, which demonstrates the feasibility\nof aligning models to quote. The core of Quote-Tuning is a fast membership\ninference function that efficiently verifies text against trusted corpora. We\nleverage this tool to design a reward function to quantify quotes in model\nresponses, and curate datasets for preference learning. Experiments show that\nQuote-Tuning significantly increases verbatim quotes from high-quality\ndocuments by up to 130% relative to base models while maintaining response\nquality. Quote-Tuning is applicable in different tasks, generalizes to\nout-of-domain data and diverse model families, and provides additional benefits\nto truthfulness. Our method not only serves as a hassle-free method to increase\nquoting but also opens up avenues for improving LLM trustworthiness through\nbetter verifiability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To trust the fluent generations of large language models (LLMs), humans must\nbe able to verify their correctness against trusted, external sources. Recent\nefforts, such as providing citations via retrieved documents or post-hoc\nprovenance, enhance verifiability but provide no guarantees on their\ncorrectness. To address these limitations, we tackle the verifiability goal\nwith a different philosophy: trivializing the verification process by\ndeveloping models that quote verbatim statements from trusted sources in their\npre-training data. We propose Quote-Tuning, which demonstrates the feasibility\nof aligning models to quote. The core of Quote-Tuning is a fast membership\ninference function that efficiently verifies text against trusted corpora. We\nleverage this tool to design a reward function to quantify quotes in model\nresponses, and curate datasets for preference learning. Experiments show that\nQuote-Tuning significantly increases verbatim quotes from high-quality\ndocuments by up to 130% relative to base models while maintaining response\nquality. Quote-Tuning is applicable in different tasks, generalizes to\nout-of-domain data and diverse model families, and provides additional benefits\nto truthfulness. Our method not only serves as a hassle-free method to increase\nquoting but also opens up avenues for improving LLM trustworthiness through\nbetter verifiability."
                },
                "authors": [
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Marc Marone"
                    },
                    {
                        "name": "Tianjian Li"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03862v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03862v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09654v1",
                "updated": "2024-11-14T18:21:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    21,
                    10,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:21:10Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    21,
                    10,
                    3,
                    319,
                    0
                ],
                "title": "Differentiable Land Model Reveals Global Environmental Controls on\n  Ecological Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Land Model Reveals Global Environmental Controls on\n  Ecological Parameters"
                },
                "summary": "Accurate modeling of terrestrial carbon and water exchange requires robust\necological parameters that capture vegetation responses and adaptations to the\nlocal environment. The current generation of land models use Plant Functional\nTypes (PFTs) to discretize vegetation functional diversity, but these coarse\ncategorizations often overlook fine-scale variations shaped by local climate,\nsoil, and forest age factors. The lack of governing equations for plant\nadaptation demands a paradigm shift in how we integrate diverse Earth\nobservations to uncover ecological functional dependence on changing\nenvironments. To address this challenge, we developed DifferLand, a\ndifferentiable, hybrid physics and machine learning model that infers the\nspatial distributions of ecological parameters and their relationships with\nenvironmental factors constrained by satellite and in-situ observations. Our\nmodel unifies top-down and bottom-up observational constraints with\nprocess-based knowledge to generate a global analysis of ecological functions\nand their adaptation to environmental gradients. We found PFTs account for less\nthan half of the explainable spatial parameter variations controlling carbon\nfluxes and vegetation states. The remaining parameter variability is largely\ndriven by local climate and forest demography factors, and the learned\nenvironment-parameter relationships lead to enhanced spatial generalization at\nunseen locations. DifferLand identified growing season length, leaf economics,\nand agricultural intensity as the three orthogonal spatial gradients underlying\nparameter variations. Our novel framework can lead to new insights on global\ncarbon cycling by learning directly from data and expanding our understanding\nof local responses of ecosystems to environmental drivers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate modeling of terrestrial carbon and water exchange requires robust\necological parameters that capture vegetation responses and adaptations to the\nlocal environment. The current generation of land models use Plant Functional\nTypes (PFTs) to discretize vegetation functional diversity, but these coarse\ncategorizations often overlook fine-scale variations shaped by local climate,\nsoil, and forest age factors. The lack of governing equations for plant\nadaptation demands a paradigm shift in how we integrate diverse Earth\nobservations to uncover ecological functional dependence on changing\nenvironments. To address this challenge, we developed DifferLand, a\ndifferentiable, hybrid physics and machine learning model that infers the\nspatial distributions of ecological parameters and their relationships with\nenvironmental factors constrained by satellite and in-situ observations. Our\nmodel unifies top-down and bottom-up observational constraints with\nprocess-based knowledge to generate a global analysis of ecological functions\nand their adaptation to environmental gradients. We found PFTs account for less\nthan half of the explainable spatial parameter variations controlling carbon\nfluxes and vegetation states. The remaining parameter variability is largely\ndriven by local climate and forest demography factors, and the learned\nenvironment-parameter relationships lead to enhanced spatial generalization at\nunseen locations. DifferLand identified growing season length, leaf economics,\nand agricultural intensity as the three orthogonal spatial gradients underlying\nparameter variations. Our novel framework can lead to new insights on global\ncarbon cycling by learning directly from data and expanding our understanding\nof local responses of ecosystems to environmental drivers."
                },
                "authors": [
                    {
                        "name": "Jianing Fang"
                    },
                    {
                        "name": "Kevin Bowman"
                    },
                    {
                        "name": "Wenli Zhao"
                    },
                    {
                        "name": "Xu Lian"
                    },
                    {
                        "name": "Pierre Gentine"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Gentine"
                },
                "author": "Pierre Gentine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09648v1",
                "updated": "2024-11-14T18:17:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    17,
                    30,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:17:30Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    17,
                    30,
                    3,
                    319,
                    0
                ],
                "title": "Med-Bot: An AI-Powered Assistant to Provide Accurate and Reliable\n  Medical Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-Bot: An AI-Powered Assistant to Provide Accurate and Reliable\n  Medical Information"
                },
                "summary": "This paper introduces Med-Bot, an AI-powered chatbot designed to provide\nusers with accurate and reliable medical information. Utilizing advanced\nlibraries and frameworks such as PyTorch, Chromadb, Langchain and Autogptq,\nMed-Bot is built to handle the complexities of natural language understanding\nin a healthcare context. The integration of llamaassisted data processing and\nAutoGPT-Q provides enhanced performance in processing and responding to queries\nbased on PDFs of medical literature, ensuring that users receive precise and\ntrustworthy information. This research details the methodologies employed in\ndeveloping Med-Bot and evaluates its effectiveness in disseminating healthcare\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Med-Bot, an AI-powered chatbot designed to provide\nusers with accurate and reliable medical information. Utilizing advanced\nlibraries and frameworks such as PyTorch, Chromadb, Langchain and Autogptq,\nMed-Bot is built to handle the complexities of natural language understanding\nin a healthcare context. The integration of llamaassisted data processing and\nAutoGPT-Q provides enhanced performance in processing and responding to queries\nbased on PDFs of medical literature, ensuring that users receive precise and\ntrustworthy information. This research details the methodologies employed in\ndeveloping Med-Bot and evaluates its effectiveness in disseminating healthcare\ninformation."
                },
                "authors": [
                    {
                        "name": "Ahan Bhatt"
                    },
                    {
                        "name": "Nandan Vaghela"
                    }
                ],
                "author_detail": {
                    "name": "Nandan Vaghela"
                },
                "author": "Nandan Vaghela",
                "arxiv_comment": "3 figures, 5 pages Keywords-LLM, AI-powered healthcare, Medical\n  chatbot, Context-based interaction, Llama-assisted data processing,\n  AutoGPT-Q, PyTorch, TensorFlow, Reliable medical information, Machine\n  learning in healthcare, Conversational AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04783v2",
                "updated": "2024-11-14T18:14:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    14,
                    0,
                    3,
                    319,
                    0
                ],
                "published": "2024-03-02T16:52:22Z",
                "published_parsed": [
                    2024,
                    3,
                    2,
                    16,
                    52,
                    22,
                    5,
                    62,
                    0
                ],
                "title": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks"
                },
                "summary": "Despite extensive pre-training in moral alignment to prevent generating\nharmful information, large language models (LLMs) remain vulnerable to\njailbreak attacks. In this paper, we propose AutoDefense, a multi-agent defense\nframework that filters harmful responses from LLMs. With the response-filtering\nmechanism, our framework is robust against different jailbreak attack prompts,\nand can be used to defend different victim models. AutoDefense assigns\ndifferent roles to LLM agents and employs them to complete the defense task\ncollaboratively. The division in tasks enhances the overall\ninstruction-following of LLMs and enables the integration of other defense\ncomponents as tools. With AutoDefense, small open-source LMs can serve as\nagents and defend larger models against jailbreak attacks. Our experiments show\nthat AutoDefense can effectively defense against different jailbreak attacks,\nwhile maintaining the performance at normal user request. For example, we\nreduce the attack success rate on GPT-3.5 from 55.74% to 7.95% using\nLLaMA-2-13b with a 3-agent system. Our code and data are publicly available at\nhttps://github.com/XHMY/AutoDefense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite extensive pre-training in moral alignment to prevent generating\nharmful information, large language models (LLMs) remain vulnerable to\njailbreak attacks. In this paper, we propose AutoDefense, a multi-agent defense\nframework that filters harmful responses from LLMs. With the response-filtering\nmechanism, our framework is robust against different jailbreak attack prompts,\nand can be used to defend different victim models. AutoDefense assigns\ndifferent roles to LLM agents and employs them to complete the defense task\ncollaboratively. The division in tasks enhances the overall\ninstruction-following of LLMs and enables the integration of other defense\ncomponents as tools. With AutoDefense, small open-source LMs can serve as\nagents and defend larger models against jailbreak attacks. Our experiments show\nthat AutoDefense can effectively defense against different jailbreak attacks,\nwhile maintaining the performance at normal user request. For example, we\nreduce the attack success rate on GPT-3.5 from 55.74% to 7.95% using\nLLaMA-2-13b with a 3-agent system. Our code and data are publicly available at\nhttps://github.com/XHMY/AutoDefense."
                },
                "authors": [
                    {
                        "name": "Yifan Zeng"
                    },
                    {
                        "name": "Yiran Wu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Huazheng Wang"
                    },
                    {
                        "name": "Qingyun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyun Wu"
                },
                "author": "Qingyun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04573v2",
                "updated": "2024-11-14T18:01:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    1,
                    10,
                    3,
                    319,
                    0
                ],
                "published": "2024-07-05T15:08:44Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    15,
                    8,
                    44,
                    4,
                    187,
                    0
                ],
                "title": "VRSD: Rethinking Similarity and Diversity for Retrieval in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VRSD: Rethinking Similarity and Diversity for Retrieval in Large\n  Language Models"
                },
                "summary": "Vector retrieval algorithms are essential for semantic queries within the\nrapidly evolving landscape of Large Language Models (LLMs). The ability to\nretrieve vectors that satisfy both similarity and diversity criteria\nsubstantially enhances the performance of LLMs. Although Maximal Marginal\nRelevance (MMR) is widely employed in retrieval scenarios requiring relevance\nand diversity, variations in the parameter $\\lambda$ lead to fluctuations that\ncomplicate the optimization trajectory in vector spaces. This obscures the\ndirection of improvement and highlights the lack of a robust theoretical\nanalysis regarding similarity and diversity constraints in retrieval processes.\nTo address these challenges, this paper introduces a novel approach that\ncharacterizes both constraints through the relationship between the sum vector\nand the query vector. The proximity of these vectors ensures the similarity\nconstraint, while requiring individual vectors within the sum vector to diverge\nin their alignment with the query vector satisfies the diversity constraint. We\nfirst formulate a new combinatorial optimization problem, selecting k vectors\nfrom a candidate set such that their sum vector maximally aligns with the query\nvector, and demonstrate that this problem is NP-complete. This result\nunderscores the inherent difficulty of simultaneously achieving similarity and\ndiversity in vector retrieval, thereby providing a theoretical foundation for\nfuture research. Subsequently, we present the heuristic algorithm Vectors\nRetrieval with Similarity and Diversity, VRSD, which features a clear\noptimization objective and eliminates the need for preset parameters. VRSD also\nachieves a modest reduction in time complexity compared to MMR. Empirical\nvalidation confirms that VRSD significantly outperforms MMR across various\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector retrieval algorithms are essential for semantic queries within the\nrapidly evolving landscape of Large Language Models (LLMs). The ability to\nretrieve vectors that satisfy both similarity and diversity criteria\nsubstantially enhances the performance of LLMs. Although Maximal Marginal\nRelevance (MMR) is widely employed in retrieval scenarios requiring relevance\nand diversity, variations in the parameter $\\lambda$ lead to fluctuations that\ncomplicate the optimization trajectory in vector spaces. This obscures the\ndirection of improvement and highlights the lack of a robust theoretical\nanalysis regarding similarity and diversity constraints in retrieval processes.\nTo address these challenges, this paper introduces a novel approach that\ncharacterizes both constraints through the relationship between the sum vector\nand the query vector. The proximity of these vectors ensures the similarity\nconstraint, while requiring individual vectors within the sum vector to diverge\nin their alignment with the query vector satisfies the diversity constraint. We\nfirst formulate a new combinatorial optimization problem, selecting k vectors\nfrom a candidate set such that their sum vector maximally aligns with the query\nvector, and demonstrate that this problem is NP-complete. This result\nunderscores the inherent difficulty of simultaneously achieving similarity and\ndiversity in vector retrieval, thereby providing a theoretical foundation for\nfuture research. Subsequently, we present the heuristic algorithm Vectors\nRetrieval with Similarity and Diversity, VRSD, which features a clear\noptimization objective and eliminates the need for preset parameters. VRSD also\nachieves a modest reduction in time complexity compared to MMR. Empirical\nvalidation confirms that VRSD significantly outperforms MMR across various\ndatasets."
                },
                "authors": [
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.11738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.11738v3",
                "updated": "2024-11-14T17:50:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    50,
                    53,
                    3,
                    319,
                    0
                ],
                "published": "2023-08-22T18:58:21Z",
                "published_parsed": [
                    2023,
                    8,
                    22,
                    18,
                    58,
                    21,
                    1,
                    234,
                    0
                ],
                "title": "Lifted Inference beyond First-Order Logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifted Inference beyond First-Order Logic"
                },
                "summary": "Weighted First Order Model Counting (WFOMC) is fundamental to probabilistic\ninference in statistical relational learning models. As WFOMC is known to be\nintractable in general ($\\#$P-complete), logical fragments that admit\npolynomial time WFOMC are of significant interest. Such fragments are called\ndomain liftable. Recent works have shown that the two-variable fragment of\nfirst order logic extended with counting quantifiers ($\\mathrm{C^2}$) is\ndomain-liftable. However, many properties of real-world data, like acyclicity\nin citation networks and connectivity in social networks, cannot be modeled in\n$\\mathrm{C^2}$, or first order logic in general. In this work, we expand the\ndomain liftability of $\\mathrm{C^2}$ with multiple such properties. We show\nthat any $\\mathrm{C^2}$ sentence remains domain liftable when one of its\nrelations is restricted to represent a directed acyclic graph, a connected\ngraph, a tree (resp. a directed tree) or a forest (resp. a directed forest).\nAll our results rely on a novel and general methodology of \"counting by\nsplitting\". Besides their application to probabilistic inference, our results\nprovide a general framework for counting combinatorial structures. We expand a\nvast array of previous results in discrete mathematics literature on directed\nacyclic graphs, phylogenetic networks, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weighted First Order Model Counting (WFOMC) is fundamental to probabilistic\ninference in statistical relational learning models. As WFOMC is known to be\nintractable in general ($\\#$P-complete), logical fragments that admit\npolynomial time WFOMC are of significant interest. Such fragments are called\ndomain liftable. Recent works have shown that the two-variable fragment of\nfirst order logic extended with counting quantifiers ($\\mathrm{C^2}$) is\ndomain-liftable. However, many properties of real-world data, like acyclicity\nin citation networks and connectivity in social networks, cannot be modeled in\n$\\mathrm{C^2}$, or first order logic in general. In this work, we expand the\ndomain liftability of $\\mathrm{C^2}$ with multiple such properties. We show\nthat any $\\mathrm{C^2}$ sentence remains domain liftable when one of its\nrelations is restricted to represent a directed acyclic graph, a connected\ngraph, a tree (resp. a directed tree) or a forest (resp. a directed forest).\nAll our results rely on a novel and general methodology of \"counting by\nsplitting\". Besides their application to probabilistic inference, our results\nprovide a general framework for counting combinatorial structures. We expand a\nvast array of previous results in discrete mathematics literature on directed\nacyclic graphs, phylogenetic networks, etc."
                },
                "authors": [
                    {
                        "name": "Sagar Malhotra"
                    },
                    {
                        "name": "Davide Bizzaro"
                    },
                    {
                        "name": "Luciano Serafini"
                    }
                ],
                "author_detail": {
                    "name": "Luciano Serafini"
                },
                "author": "Luciano Serafini",
                "arxiv_comment": "Under Review at the Artificial Intelligence Journal. Added two new\n  lemmas for counting by splitting in the Main approach section. Added\n  experiments with Markov Logic.arXiv admin note: text overlap with\n  arXiv:2302.09830",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.11738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.11738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09625v1",
                "updated": "2024-11-14T17:49:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    49,
                    27,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:49:27Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    49,
                    27,
                    3,
                    319,
                    0
                ],
                "title": "Local deployment of large-scale music AI models on commodity hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local deployment of large-scale music AI models on commodity hardware"
                },
                "summary": "We present the MIDInfinite, a web application capable of generating symbolic\nmusic using a large-scale generative AI model locally on commodity hardware.\nCreating this demo involved porting the Anticipatory Music Transformer, a large\nlanguage model (LLM) pre-trained on the Lakh MIDI dataset, to the Machine\nLearning Compilation (MLC) framework. Once the model is ported, MLC facilitates\ninference on a variety of runtimes including C++, mobile, and the browser. We\nenvision that MLC has the potential to bridge the gap between the landscape of\nincreasingly capable music AI models and technology more familiar to music\nsoftware developers. As a proof of concept, we build a web application that\nallows users to generate endless streams of multi-instrumental MIDI in the\nbrowser, either from scratch or conditioned on a prompt. On commodity hardware\n(an M3 Macbook Pro), our demo can generate 51 notes per second, which is faster\nthan real-time playback for 72.9% of generations, and increases to 86.3% with 2\nseconds of upfront buffering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the MIDInfinite, a web application capable of generating symbolic\nmusic using a large-scale generative AI model locally on commodity hardware.\nCreating this demo involved porting the Anticipatory Music Transformer, a large\nlanguage model (LLM) pre-trained on the Lakh MIDI dataset, to the Machine\nLearning Compilation (MLC) framework. Once the model is ported, MLC facilitates\ninference on a variety of runtimes including C++, mobile, and the browser. We\nenvision that MLC has the potential to bridge the gap between the landscape of\nincreasingly capable music AI models and technology more familiar to music\nsoftware developers. As a proof of concept, we build a web application that\nallows users to generate endless streams of multi-instrumental MIDI in the\nbrowser, either from scratch or conditioned on a prompt. On commodity hardware\n(an M3 Macbook Pro), our demo can generate 51 notes per second, which is faster\nthan real-time playback for 72.9% of generations, and increases to 86.3% with 2\nseconds of upfront buffering."
                },
                "authors": [
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Charlie Ruan"
                    },
                    {
                        "name": "Zihe Zhao"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Chris Donahue"
                    }
                ],
                "author_detail": {
                    "name": "Chris Donahue"
                },
                "author": "Chris Donahue",
                "arxiv_comment": "2 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09624v1",
                "updated": "2024-11-14T17:49:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    49,
                    16,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:49:16Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    49,
                    16,
                    3,
                    319,
                    0
                ],
                "title": "Photonic frequency multiplexed next-generation reservoir computer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photonic frequency multiplexed next-generation reservoir computer"
                },
                "summary": "In this work, we introduce and experimentally demonstrate a photonic\nfrequency-multiplexed next generation reservoir computer (FM-NGRC) capable of\nperforming real-time inference at GHz speed. NGRCs apply a feed-forward\narchitecture to produce a feature vector directly from the input data over a\nfixed number of time steps. This feature vector, analogous to the reservoir\nstate in a conventional RC, is used to perform inference by applying a decision\nlayer trained by linear regression. Photonic NGRC provides a flexible platform\nfor real-time inference by forgoing the need for explicit feedback loops\ninherent to a physical reservoir. The FM-NGRC introduced here defines the\nmemory structure using an optical frequency comb and dispersive fiber while the\nsinusoidal response of electro-optic Mach-Zehnder interferometers controls the\nnonlinear transform applied to elements of the feature vector. A programmable\nwaveshaper modulates each comb tooth independently to apply the trained\ndecision layer weights in the analog domain. We apply the FM-NGRC to solve the\nbenchmark nonlinear channel equalization task; after theoretically determining\nfeature vectors that enable high-accuracy distortion compensation, we construct\nan FM-NGRC that generates these vectors to experimentally demonstrate real-time\nchannel equalization at 5 GS/s with a symbol error rate of $\\sim 2\\times\n10^{-3}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce and experimentally demonstrate a photonic\nfrequency-multiplexed next generation reservoir computer (FM-NGRC) capable of\nperforming real-time inference at GHz speed. NGRCs apply a feed-forward\narchitecture to produce a feature vector directly from the input data over a\nfixed number of time steps. This feature vector, analogous to the reservoir\nstate in a conventional RC, is used to perform inference by applying a decision\nlayer trained by linear regression. Photonic NGRC provides a flexible platform\nfor real-time inference by forgoing the need for explicit feedback loops\ninherent to a physical reservoir. The FM-NGRC introduced here defines the\nmemory structure using an optical frequency comb and dispersive fiber while the\nsinusoidal response of electro-optic Mach-Zehnder interferometers controls the\nnonlinear transform applied to elements of the feature vector. A programmable\nwaveshaper modulates each comb tooth independently to apply the trained\ndecision layer weights in the analog domain. We apply the FM-NGRC to solve the\nbenchmark nonlinear channel equalization task; after theoretically determining\nfeature vectors that enable high-accuracy distortion compensation, we construct\nan FM-NGRC that generates these vectors to experimentally demonstrate real-time\nchannel equalization at 5 GS/s with a symbol error rate of $\\sim 2\\times\n10^{-3}$."
                },
                "authors": [
                    {
                        "name": "Nicholas Cox"
                    },
                    {
                        "name": "Joseph Murray"
                    },
                    {
                        "name": "Joseph Hart"
                    },
                    {
                        "name": "Brandon Redding"
                    }
                ],
                "author_detail": {
                    "name": "Brandon Redding"
                },
                "author": "Brandon Redding",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09613v1",
                "updated": "2024-11-14T17:33:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    33,
                    36,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:33:36Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    33,
                    36,
                    3,
                    319,
                    0
                ],
                "title": "PTR: Precision-Driven Tool Recommendation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PTR: Precision-Driven Tool Recommendation for Large Language Models"
                },
                "summary": "By augmenting Large Language Models (LLMs) with external tools, their\ncapacity to solve complex problems has been significantly enhanced. However,\ndespite ongoing advancements in the parsing capabilities of LLMs, incorporating\nall available tools simultaneously in the prompt remains impractical due to the\nvast number of external tools. Consequently, it is essential to provide LLMs\nwith a precise set of tools tailored to the specific task, considering both\nquantity and quality. Current tool retrieval methods primarily focus on\nrefining the ranking list of tools and directly packaging a fixed number of\ntop-ranked tools as the tool set. However, these approaches often fail to equip\nLLMs with the optimal set of tools prior to execution, since the optimal number\nof tools for different tasks could be different, resulting in inefficiencies\nsuch as redundant or unsuitable tools, which impede immediate access to the\nmost relevant tools. This paper addresses the challenge of recommending precise\ntoolsets for LLMs. We introduce the problem of tool recommendation, define its\nscope, and propose a novel Precision-driven Tool Recommendation (PTR) approach.\nPTR captures an initial, concise set of tools by leveraging historical tool\nbundle usage and dynamically adjusts the tool set by performing tool matching,\nculminating in a multi-view-based tool addition. Additionally, we present a new\ndataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness\nof tool recommendation for LLMs. We further validate our design choices through\ncomprehensive experiments, demonstrating promising accuracy across two open\nbenchmarks and our RecTools dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By augmenting Large Language Models (LLMs) with external tools, their\ncapacity to solve complex problems has been significantly enhanced. However,\ndespite ongoing advancements in the parsing capabilities of LLMs, incorporating\nall available tools simultaneously in the prompt remains impractical due to the\nvast number of external tools. Consequently, it is essential to provide LLMs\nwith a precise set of tools tailored to the specific task, considering both\nquantity and quality. Current tool retrieval methods primarily focus on\nrefining the ranking list of tools and directly packaging a fixed number of\ntop-ranked tools as the tool set. However, these approaches often fail to equip\nLLMs with the optimal set of tools prior to execution, since the optimal number\nof tools for different tasks could be different, resulting in inefficiencies\nsuch as redundant or unsuitable tools, which impede immediate access to the\nmost relevant tools. This paper addresses the challenge of recommending precise\ntoolsets for LLMs. We introduce the problem of tool recommendation, define its\nscope, and propose a novel Precision-driven Tool Recommendation (PTR) approach.\nPTR captures an initial, concise set of tools by leveraging historical tool\nbundle usage and dynamically adjusts the tool set by performing tool matching,\nculminating in a multi-view-based tool addition. Additionally, we present a new\ndataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness\nof tool recommendation for LLMs. We further validate our design choices through\ncomprehensive experiments, demonstrating promising accuracy across two open\nbenchmarks and our RecTools dataset."
                },
                "authors": [
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09601v1",
                "updated": "2024-11-14T17:21:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    21,
                    2,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:21:02Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    21,
                    2,
                    3,
                    319,
                    0
                ],
                "title": "Accelerating Knowledge Graph and Ontology Engineering with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Knowledge Graph and Ontology Engineering with Large\n  Language Models"
                },
                "summary": "Large Language Models bear the promise of significant acceleration of key\nKnowledge Graph and Ontology Engineering tasks, including ontology modeling,\nextension, modification, population, alignment, as well as entity\ndisambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering\nas a new and coming area of research, and argue that modular approaches to\nontologies will be of central importance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models bear the promise of significant acceleration of key\nKnowledge Graph and Ontology Engineering tasks, including ontology modeling,\nextension, modification, population, alignment, as well as entity\ndisambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering\nas a new and coming area of research, and argue that modular approaches to\nontologies will be of central importance."
                },
                "authors": [
                    {
                        "name": "Cogan Shimizu"
                    },
                    {
                        "name": "Pascal Hitzler"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Hitzler"
                },
                "author": "Pascal Hitzler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09595v1",
                "updated": "2024-11-14T17:08:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    8,
                    23,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:08:23Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    8,
                    23,
                    3,
                    319,
                    0
                ],
                "title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models"
                },
                "summary": "This work explores expanding the capabilities of large language models (LLMs)\npretrained on text to generate 3D meshes within a unified model. This offers\nkey advantages of (1) leveraging spatial knowledge already embedded in LLMs,\nderived from textual sources like 3D tutorials, and (2) enabling conversational\n3D generation and mesh understanding. A primary challenge is effectively\ntokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly.\nTo address this, we introduce LLaMA-Mesh, a novel approach that represents the\nvertex coordinates and face definitions of 3D meshes as plain text, allowing\ndirect integration with LLMs without expanding the vocabulary. We construct a\nsupervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate\n3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs\nas required, and (3) understand and interpret 3D meshes. Our work is the first\nto demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge\nfor 3D mesh generation in a text-based format, effectively unifying the 3D and\ntext modalities. LLaMA-Mesh achieves mesh generation quality on par with models\ntrained from scratch while maintaining strong text generation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores expanding the capabilities of large language models (LLMs)\npretrained on text to generate 3D meshes within a unified model. This offers\nkey advantages of (1) leveraging spatial knowledge already embedded in LLMs,\nderived from textual sources like 3D tutorials, and (2) enabling conversational\n3D generation and mesh understanding. A primary challenge is effectively\ntokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly.\nTo address this, we introduce LLaMA-Mesh, a novel approach that represents the\nvertex coordinates and face definitions of 3D meshes as plain text, allowing\ndirect integration with LLMs without expanding the vocabulary. We construct a\nsupervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate\n3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs\nas required, and (3) understand and interpret 3D meshes. Our work is the first\nto demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge\nfor 3D mesh generation in a text-based format, effectively unifying the 3D and\ntext modalities. LLaMA-Mesh achieves mesh generation quality on par with models\ntrained from scratch while maintaining strong text generation performance."
                },
                "authors": [
                    {
                        "name": "Zhengyi Wang"
                    },
                    {
                        "name": "Jonathan Lorraine"
                    },
                    {
                        "name": "Yikai Wang"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Xiaohui Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Zeng"
                },
                "author": "Xiaohui Zeng",
                "arxiv_comment": "See the project website at\n  https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5; I.2.10; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09590v1",
                "updated": "2024-11-14T17:01:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    1,
                    24,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:01:24Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    1,
                    24,
                    3,
                    319,
                    0
                ],
                "title": "Adopting RAG for LLM-Aided Future Vehicle Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adopting RAG for LLM-Aided Future Vehicle Design"
                },
                "summary": "In this paper, we explore the integration of Large Language Models (LLMs)\nwith Retrieval-Augmented Generation (RAG) to enhance automated design and\nsoftware development in the automotive industry. We present two case studies: a\nstandardization compliance chatbot and a design copilot, both utilizing RAG to\nprovide accurate, context-aware responses. We evaluate four LLMs-GPT-4o,\nLLAMA3, Mistral, and Mixtral -- comparing their answering accuracy and\nexecution time. Our results demonstrate that while GPT-4 offers superior\nperformance, LLAMA3 and Mistral also show promising capabilities for local\ndeployment, addressing data privacy concerns in automotive applications. This\nstudy highlights the potential of RAG-augmented LLMs in improving design\nworkflows and compliance in automotive engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the integration of Large Language Models (LLMs)\nwith Retrieval-Augmented Generation (RAG) to enhance automated design and\nsoftware development in the automotive industry. We present two case studies: a\nstandardization compliance chatbot and a design copilot, both utilizing RAG to\nprovide accurate, context-aware responses. We evaluate four LLMs-GPT-4o,\nLLAMA3, Mistral, and Mixtral -- comparing their answering accuracy and\nexecution time. Our results demonstrate that while GPT-4 offers superior\nperformance, LLAMA3 and Mistral also show promising capabilities for local\ndeployment, addressing data privacy concerns in automotive applications. This\nstudy highlights the potential of RAG-augmented LLMs in improving design\nworkflows and compliance in automotive engineering."
                },
                "authors": [
                    {
                        "name": "Vahid Zolfaghari"
                    },
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Krzysztof Lebioda"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "arxiv_comment": "Conference paper accepted in IEEE FLLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09439v1",
                "updated": "2024-11-14T16:58:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    58,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:58:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    58,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Spider: Any-to-Many Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spider: Any-to-Many Multimodal LLM"
                },
                "summary": "Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models\n(LLMs), enabling the integration of various modalities. However, Any-to-Any\nMLLMs are limited to generating pairwise modalities 'Text + X' within a single\nresponse, such as Text + {Image or Audio or Video}. To address this limitation,\nwe introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG)\nframework, which can generate an arbitrary combination of modalities 'Text +\nXs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our\nSpider integrates three core components: a Base Model for basic X-to-X (i.e.,\nAny-to-Any) modality processing, a novel Efficient Decoders-Controller for\ncontrolling multimodal Decoders to generate Xs (many-modal) contents, and an\nAny-to-Many Instruction Template designed for producing Xs signal prompts. To\ntrain Spider, we constructed a novel Text-formatted Many-Modal (TMM) dataset,\nwhich facilitates the learning of the X-to-Xs (i.e., Any-to-Many) capability\nnecessary for AMMG. Ultimately, the well-trained Spider generates a pseudo\nX-to-Xs dataset, the first-ever X-to-Xs many-modal dataset, enhancing the\npotential for AMMG task in future research. Overall, this work not only pushes\nthe boundary of multimodal interaction but also provides rich data support for\nadvancing the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models\n(LLMs), enabling the integration of various modalities. However, Any-to-Any\nMLLMs are limited to generating pairwise modalities 'Text + X' within a single\nresponse, such as Text + {Image or Audio or Video}. To address this limitation,\nwe introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG)\nframework, which can generate an arbitrary combination of modalities 'Text +\nXs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our\nSpider integrates three core components: a Base Model for basic X-to-X (i.e.,\nAny-to-Any) modality processing, a novel Efficient Decoders-Controller for\ncontrolling multimodal Decoders to generate Xs (many-modal) contents, and an\nAny-to-Many Instruction Template designed for producing Xs signal prompts. To\ntrain Spider, we constructed a novel Text-formatted Many-Modal (TMM) dataset,\nwhich facilitates the learning of the X-to-Xs (i.e., Any-to-Many) capability\nnecessary for AMMG. Ultimately, the well-trained Spider generates a pseudo\nX-to-Xs dataset, the first-ever X-to-Xs many-modal dataset, enhancing the\npotential for AMMG task in future research. Overall, this work not only pushes\nthe boundary of multimodal interaction but also provides rich data support for\nadvancing the field."
                },
                "authors": [
                    {
                        "name": "Jinxiang Lai"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Xiaocheng Lu"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01767v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01767v3",
                "updated": "2024-11-14T16:46:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    46,
                    17,
                    3,
                    319,
                    0
                ],
                "published": "2024-06-03T20:16:35Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    20,
                    16,
                    35,
                    0,
                    155,
                    0
                ],
                "title": "Region-aware Grasp Framework with Normalized Grasp Space for Efficient\n  6-DoF Grasping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Region-aware Grasp Framework with Normalized Grasp Space for Efficient\n  6-DoF Grasping"
                },
                "summary": "A series of region-based methods succeed in extracting regional features and\nenhancing grasp detection quality. However, faced with a cluttered scene with\npotential collision, the definition of the grasp-relevant region stays\ninconsistent, and the relationship between grasps and regional spaces remains\nincompletely investigated. In this paper, we propose Normalized Grasp Space\n(NGS) from a novel region-aware viewpoint, unifying the grasp representation\nwithin a normalized regional space and benefiting the generalizability of\nmethods. Leveraging the NGS, we find that CNNs are underestimated for 3D\nfeature extraction and 6-DoF grasp detection in clutter scenes and build a\nhighly efficient Region-aware Normalized Grasp Network (RNGNet). Experiments on\nthe public benchmark show that our method achieves significant >20% performance\ngains while attaining a real-time inference speed of approximately 50 FPS.\nReal-world cluttered scene clearance experiments underscore the effectiveness\nof our method. Further, human-to-robot handover and dynamic object grasping\nexperiments demonstrate the potential of our proposed method for closed-loop\ngrasping in dynamic scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A series of region-based methods succeed in extracting regional features and\nenhancing grasp detection quality. However, faced with a cluttered scene with\npotential collision, the definition of the grasp-relevant region stays\ninconsistent, and the relationship between grasps and regional spaces remains\nincompletely investigated. In this paper, we propose Normalized Grasp Space\n(NGS) from a novel region-aware viewpoint, unifying the grasp representation\nwithin a normalized regional space and benefiting the generalizability of\nmethods. Leveraging the NGS, we find that CNNs are underestimated for 3D\nfeature extraction and 6-DoF grasp detection in clutter scenes and build a\nhighly efficient Region-aware Normalized Grasp Network (RNGNet). Experiments on\nthe public benchmark show that our method achieves significant >20% performance\ngains while attaining a real-time inference speed of approximately 50 FPS.\nReal-world cluttered scene clearance experiments underscore the effectiveness\nof our method. Further, human-to-robot handover and dynamic object grasping\nexperiments demonstrate the potential of our proposed method for closed-loop\ngrasping in dynamic scenarios."
                },
                "authors": [
                    {
                        "name": "Siang Chen"
                    },
                    {
                        "name": "Pengwei Xie"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Dingchang Hu"
                    },
                    {
                        "name": "Yixiang Dai"
                    },
                    {
                        "name": "Guijin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guijin Wang"
                },
                "author": "Guijin Wang",
                "arxiv_comment": "Accepted by CoRL2024, final camera-ready version will be updated soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01767v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01767v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09580v1",
                "updated": "2024-11-14T16:42:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    42,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:42:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    42,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Software Performance Engineering for Foundation Model-Powered Software\n  (FMware)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Performance Engineering for Foundation Model-Powered Software\n  (FMware)"
                },
                "summary": "The rise of Foundation Models (FMs) like Large Language Models (LLMs) is\nrevolutionizing software development. Despite the impressive prototypes,\ntransforming FMware into production-ready products demands complex engineering\nacross various domains. A critical but overlooked aspect is performance\nengineering, which aims at ensuring FMware meets performance goals such as\nthroughput and latency to avoid user dissatisfaction and financial loss. Often,\nperformance considerations are an afterthought, leading to costly optimization\nefforts post-deployment. FMware's high computational resource demands highlight\nthe need for efficient hardware use. Continuous performance engineering is\nessential to prevent degradation. This paper highlights the significance of\nSoftware Performance Engineering (SPE) in FMware, identifying four key\nchallenges: cognitive architecture design, communication protocols, tuning and\noptimization, and deployment. These challenges are based on literature surveys\nand experiences from developing an in-house FMware system. We discuss problems,\ncurrent practices, and innovative paths for the software engineering community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Foundation Models (FMs) like Large Language Models (LLMs) is\nrevolutionizing software development. Despite the impressive prototypes,\ntransforming FMware into production-ready products demands complex engineering\nacross various domains. A critical but overlooked aspect is performance\nengineering, which aims at ensuring FMware meets performance goals such as\nthroughput and latency to avoid user dissatisfaction and financial loss. Often,\nperformance considerations are an afterthought, leading to costly optimization\nefforts post-deployment. FMware's high computational resource demands highlight\nthe need for efficient hardware use. Continuous performance engineering is\nessential to prevent degradation. This paper highlights the significance of\nSoftware Performance Engineering (SPE) in FMware, identifying four key\nchallenges: cognitive architecture design, communication protocols, tuning and\noptimization, and deployment. These challenges are based on literature surveys\nand experiences from developing an in-house FMware system. We discuss problems,\ncurrent practices, and innovative paths for the software engineering community."
                },
                "authors": [
                    {
                        "name": "Haoxiang Zhang"
                    },
                    {
                        "name": "Shi Chang"
                    },
                    {
                        "name": "Arthur Leung"
                    },
                    {
                        "name": "Kishanthan Thangarajah"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Hanan Lutfiyya"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22980v2",
                "updated": "2024-11-14T16:40:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    40,
                    0,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-30T12:45:12Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    45,
                    12,
                    2,
                    304,
                    0
                ],
                "title": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation"
                },
                "summary": "6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments. Benefiting from our end-to-end\nmethodology and efficient network design, our approach surpasses previous\nmethods in model inference efficiency and achieves real-time 6-Dof grasp\ndetection on edge devices. Furthermore, real-world experiments validate the\neffectiveness of our method, achieving a satisfactory 94% object grasping\nsuccess rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments. Benefiting from our end-to-end\nmethodology and efficient network design, our approach surpasses previous\nmethods in model inference efficiency and achieves real-time 6-Dof grasp\ndetection on edge devices. Furthermore, real-world experiments validate the\neffectiveness of our method, achieving a satisfactory 94% object grasping\nsuccess rate."
                },
                "authors": [
                    {
                        "name": "Kaiqin Yang"
                    },
                    {
                        "name": "Yixiang Dai"
                    },
                    {
                        "name": "Guijin Wang"
                    },
                    {
                        "name": "Siang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siang Chen"
                },
                "author": "Siang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01881v2",
                "updated": "2024-11-14T16:17:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    17,
                    40,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-04T08:24:56Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    8,
                    24,
                    56,
                    0,
                    309,
                    0
                ],
                "title": "Causal Discovery and Classification Using Lempel-Ziv Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Discovery and Classification Using Lempel-Ziv Complexity"
                },
                "summary": "Inferring causal relationships in the decision-making processes of machine\nlearning algorithms is a crucial step toward achieving explainable Artificial\nIntelligence (AI). In this research, we introduce a novel causality measure and\na distance metric derived from Lempel-Ziv (LZ) complexity. We explore how the\nproposed causality measure can be used in decision trees by enabling splits\nbased on features that most strongly \\textit{cause} the outcome. We further\nevaluate the effectiveness of the causality-based decision tree and the\ndistance-based decision tree in comparison to a traditional decision tree using\nGini impurity. While the proposed methods demonstrate comparable classification\nperformance overall, the causality-based decision tree significantly\noutperforms both the distance-based decision tree and the Gini-based decision\ntree on datasets generated from causal models. This result indicates that the\nproposed approach can capture insights beyond those of classical decision\ntrees, especially in causally structured data. Based on the features used in\nthe LZ causal measure based decision tree, we introduce a causal strength for\neach features in the dataset so as to infer the predominant causal variables\nfor the occurrence of the outcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring causal relationships in the decision-making processes of machine\nlearning algorithms is a crucial step toward achieving explainable Artificial\nIntelligence (AI). In this research, we introduce a novel causality measure and\na distance metric derived from Lempel-Ziv (LZ) complexity. We explore how the\nproposed causality measure can be used in decision trees by enabling splits\nbased on features that most strongly \\textit{cause} the outcome. We further\nevaluate the effectiveness of the causality-based decision tree and the\ndistance-based decision tree in comparison to a traditional decision tree using\nGini impurity. While the proposed methods demonstrate comparable classification\nperformance overall, the causality-based decision tree significantly\noutperforms both the distance-based decision tree and the Gini-based decision\ntree on datasets generated from causal models. This result indicates that the\nproposed approach can capture insights beyond those of classical decision\ntrees, especially in causally structured data. Based on the features used in\nthe LZ causal measure based decision tree, we introduce a causal strength for\neach features in the dataset so as to infer the predominant causal variables\nfor the occurrence of the outcome."
                },
                "authors": [
                    {
                        "name": "Dhruthi"
                    },
                    {
                        "name": "Nithin Nagaraj"
                    },
                    {
                        "name": "Harikrishnan N B"
                    }
                ],
                "author_detail": {
                    "name": "Harikrishnan N B"
                },
                "author": "Harikrishnan N B",
                "arxiv_comment": "17 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09539v1",
                "updated": "2024-11-14T15:55:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    55,
                    37,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T15:55:37Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    55,
                    37,
                    3,
                    319,
                    0
                ],
                "title": "A Practical Guide to Fine-tuning Language Models with Limited Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Guide to Fine-tuning Language Models with Limited Data"
                },
                "summary": "Employing pre-trained Large Language Models (LLMs) has become the de facto\nstandard in Natural Language Processing (NLP) despite their extensive data\nrequirements. Motivated by the recent surge in research focused on training\nLLMs with limited data, particularly in low-resource domains and languages,\nthis paper surveys recent transfer learning approaches to optimize model\nperformance in downstream tasks where data is scarce. We first address initial\nand continued pre-training strategies to better leverage prior knowledge in\nunseen domains and languages. We then examine how to maximize the utility of\nlimited data during fine-tuning and few-shot learning. The final section takes\na task-specific perspective, reviewing models and methods suited for different\nlevels of data scarcity. Our goal is to provide practitioners with practical\nguidelines for overcoming the challenges posed by constrained data while also\nhighlighting promising directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing pre-trained Large Language Models (LLMs) has become the de facto\nstandard in Natural Language Processing (NLP) despite their extensive data\nrequirements. Motivated by the recent surge in research focused on training\nLLMs with limited data, particularly in low-resource domains and languages,\nthis paper surveys recent transfer learning approaches to optimize model\nperformance in downstream tasks where data is scarce. We first address initial\nand continued pre-training strategies to better leverage prior knowledge in\nunseen domains and languages. We then examine how to maximize the utility of\nlimited data during fine-tuning and few-shot learning. The final section takes\na task-specific perspective, reviewing models and methods suited for different\nlevels of data scarcity. Our goal is to provide practitioners with practical\nguidelines for overcoming the challenges posed by constrained data while also\nhighlighting promising directions for future research."
                },
                "authors": [
                    {
                        "name": "Márton Szép"
                    },
                    {
                        "name": "Daniel Rueckert"
                    },
                    {
                        "name": "Rüdiger von Eisenhart-Rothe"
                    },
                    {
                        "name": "Florian Hinterwimmer"
                    }
                ],
                "author_detail": {
                    "name": "Florian Hinterwimmer"
                },
                "author": "Florian Hinterwimmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08278v2",
                "updated": "2024-11-14T15:49:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    49,
                    46,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-13T01:33:05Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    1,
                    33,
                    5,
                    2,
                    318,
                    0
                ],
                "title": "Knowledge Bases in Support of Large Language Models for Processing Web\n  News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Bases in Support of Large Language Models for Processing Web\n  News"
                },
                "summary": "Large Language Models (LLMs) have received considerable interest in wide\napplications lately. During pre-training via massive datasets, such a model\nimplicitly memorizes the factual knowledge of trained datasets in its hidden\nparameters. However, knowledge held implicitly in parameters often makes its\nuse by downstream applications ineffective due to the lack of common-sense\nreasoning. In this article, we introduce a general framework that permits to\nbuild knowledge bases with an aid of LLMs, tailored for processing Web news.\nThe framework applies a rule-based News Information Extractor (NewsIE) to news\nitems for extracting their relational tuples, referred to as knowledge bases,\nwhich are then graph-convoluted with the implicit knowledge facts of news items\nobtained by LLMs, for their classification. It involves two lightweight\ncomponents: 1) NewsIE: for extracting the structural information of every news\nitem, in the form of relational tuples; 2) BERTGraph: for graph convoluting the\nimplicit knowledge facts with relational tuples extracted by NewsIE. We have\nevaluated our framework under different news-related datasets for news category\nclassification, with promising experimental results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have received considerable interest in wide\napplications lately. During pre-training via massive datasets, such a model\nimplicitly memorizes the factual knowledge of trained datasets in its hidden\nparameters. However, knowledge held implicitly in parameters often makes its\nuse by downstream applications ineffective due to the lack of common-sense\nreasoning. In this article, we introduce a general framework that permits to\nbuild knowledge bases with an aid of LLMs, tailored for processing Web news.\nThe framework applies a rule-based News Information Extractor (NewsIE) to news\nitems for extracting their relational tuples, referred to as knowledge bases,\nwhich are then graph-convoluted with the implicit knowledge facts of news items\nobtained by LLMs, for their classification. It involves two lightweight\ncomponents: 1) NewsIE: for extracting the structural information of every news\nitem, in the form of relational tuples; 2) BERTGraph: for graph convoluting the\nimplicit knowledge facts with relational tuples extracted by NewsIE. We have\nevaluated our framework under different news-related datasets for news category\nclassification, with promising experimental results."
                },
                "authors": [
                    {
                        "name": "Yihe Zhang"
                    },
                    {
                        "name": "Nabin Pakka"
                    },
                    {
                        "name": "Nian-Feng Tzeng"
                    }
                ],
                "author_detail": {
                    "name": "Nian-Feng Tzeng"
                },
                "author": "Nian-Feng Tzeng",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09523v1",
                "updated": "2024-11-14T15:40:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    40,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T15:40:04Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    40,
                    4,
                    3,
                    319,
                    0
                ],
                "title": "Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats\n  in LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats\n  in LLM-Based Agents"
                },
                "summary": "With the continuous development of large language models (LLMs),\ntransformer-based models have made groundbreaking advances in numerous natural\nlanguage processing (NLP) tasks, leading to the emergence of a series of agents\nthat use LLMs as their control hub. While LLMs have achieved success in various\ntasks, they face numerous security and privacy threats, which become even more\nsevere in the agent scenarios. To enhance the reliability of LLM-based\napplications, a range of research has emerged to assess and mitigate these\nrisks from different perspectives.\n  To help researchers gain a comprehensive understanding of various risks, this\nsurvey collects and analyzes the different threats faced by these agents. To\naddress the challenges posed by previous taxonomies in handling cross-module\nand cross-stage threats, we propose a novel taxonomy framework based on the\nsources and impacts. Additionally, we identify six key features of LLM-based\nagents, based on which we summarize the current research progress and analyze\ntheir limitations. Subsequently, we select four representative agents as case\nstudies to analyze the risks they may face in practical use. Finally, based on\nthe aforementioned analyses, we propose future research directions from the\nperspectives of data, methodology, and policy, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous development of large language models (LLMs),\ntransformer-based models have made groundbreaking advances in numerous natural\nlanguage processing (NLP) tasks, leading to the emergence of a series of agents\nthat use LLMs as their control hub. While LLMs have achieved success in various\ntasks, they face numerous security and privacy threats, which become even more\nsevere in the agent scenarios. To enhance the reliability of LLM-based\napplications, a range of research has emerged to assess and mitigate these\nrisks from different perspectives.\n  To help researchers gain a comprehensive understanding of various risks, this\nsurvey collects and analyzes the different threats faced by these agents. To\naddress the challenges posed by previous taxonomies in handling cross-module\nand cross-stage threats, we propose a novel taxonomy framework based on the\nsources and impacts. Additionally, we identify six key features of LLM-based\nagents, based on which we summarize the current research progress and analyze\ntheir limitations. Subsequently, we select four representative agents as case\nstudies to analyze the risks they may face in practical use. Finally, based on\nthe aforementioned analyses, we propose future research directions from the\nperspectives of data, methodology, and policy, respectively."
                },
                "authors": [
                    {
                        "name": "Yuyou Gan"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Zhe Ma"
                    },
                    {
                        "name": "Ping He"
                    },
                    {
                        "name": "Rui Zeng"
                    },
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Qingming Li"
                    },
                    {
                        "name": "Chunyi Zhou"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Ting Wang"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Yingcai Wu"
                    },
                    {
                        "name": "Shouling Ji"
                    }
                ],
                "author_detail": {
                    "name": "Shouling Ji"
                },
                "author": "Shouling Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.05612v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.05612v2",
                "updated": "2024-11-14T15:22:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    22,
                    27,
                    3,
                    319,
                    0
                ],
                "published": "2023-06-09T01:11:50Z",
                "published_parsed": [
                    2023,
                    6,
                    9,
                    1,
                    11,
                    50,
                    4,
                    160,
                    0
                ],
                "title": "Spatial Re-parameterization for N:M Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Re-parameterization for N:M Sparsity"
                },
                "summary": "This paper presents a Spatial Re-parameterization (SpRe) method for the N:M\nsparsity in CNNs. SpRe is stemmed from an observation regarding the restricted\nvariety in spatial sparsity present in N:M sparsity compared with unstructured\nsparsity. Particularly, N:M sparsity exhibits a fixed sparsity rate within the\nspatial domains due to its distinctive pattern that mandates N non-zero\ncomponents among M successive weights in the input channel dimension of\nconvolution filters. On the contrary, we observe that unstructured sparsity\ndisplays a substantial divergence in sparsity across the spatial domains, which\nwe experimentally verified to be very crucial for its robust performance\nretention compared with N:M sparsity. Therefore, SpRe employs the\nspatial-sparsity distribution of unstructured sparsity to assign an extra\nbranch in conjunction with the original N:M branch at training time, which\nallows the N:M sparse network to sustain a similar distribution of spatial\nsparsity with unstructured sparsity. During inference, the extra branch can be\nfurther re-parameterized into the main N:M branch, without exerting any\ndistortion on the sparse pattern or additional computation costs. SpRe has\nachieved a commendable feat by matching the performance of N:M sparsity methods\nwith state-of-the-art unstructured sparsity methods across various benchmarks.\nCode and models are anonymously available at\n\\url{https://github.com/zyxxmu/SpRe}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Spatial Re-parameterization (SpRe) method for the N:M\nsparsity in CNNs. SpRe is stemmed from an observation regarding the restricted\nvariety in spatial sparsity present in N:M sparsity compared with unstructured\nsparsity. Particularly, N:M sparsity exhibits a fixed sparsity rate within the\nspatial domains due to its distinctive pattern that mandates N non-zero\ncomponents among M successive weights in the input channel dimension of\nconvolution filters. On the contrary, we observe that unstructured sparsity\ndisplays a substantial divergence in sparsity across the spatial domains, which\nwe experimentally verified to be very crucial for its robust performance\nretention compared with N:M sparsity. Therefore, SpRe employs the\nspatial-sparsity distribution of unstructured sparsity to assign an extra\nbranch in conjunction with the original N:M branch at training time, which\nallows the N:M sparse network to sustain a similar distribution of spatial\nsparsity with unstructured sparsity. During inference, the extra branch can be\nfurther re-parameterized into the main N:M branch, without exerting any\ndistortion on the sparse pattern or additional computation costs. SpRe has\nachieved a commendable feat by matching the performance of N:M sparsity methods\nwith state-of-the-art unstructured sparsity methods across various benchmarks.\nCode and models are anonymously available at\n\\url{https://github.com/zyxxmu/SpRe}."
                },
                "authors": [
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Mingliang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.05612v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.05612v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09510v1",
                "updated": "2024-11-14T15:19:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    19,
                    1,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T15:19:01Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    19,
                    1,
                    3,
                    319,
                    0
                ],
                "title": "Communication Compression for Tensor Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication Compression for Tensor Parallel LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have pushed the frontier of artificial\nintelligence but are comprised of hundreds of billions of parameters and\noperations. For faster inference latency, LLMs are deployed on multiple\nhardware accelerators through various Model Parallelism strategies. Our paper\nlooks into the details on one such strategy - Tensor Parallel - and proposes to\nreduce latency by compressing inter-accelerator communication. We leverage fine\ngrained quantization techniques to compress selected activations by 3.5 - 4.5x.\nOur proposed method leads up to 2x reduction of time-to-first-token (TTFT) with\nnegligible model performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have pushed the frontier of artificial\nintelligence but are comprised of hundreds of billions of parameters and\noperations. For faster inference latency, LLMs are deployed on multiple\nhardware accelerators through various Model Parallelism strategies. Our paper\nlooks into the details on one such strategy - Tensor Parallel - and proposes to\nreduce latency by compressing inter-accelerator communication. We leverage fine\ngrained quantization techniques to compress selected activations by 3.5 - 4.5x.\nOur proposed method leads up to 2x reduction of time-to-first-token (TTFT) with\nnegligible model performance degradation."
                },
                "authors": [
                    {
                        "name": "Jan Hansen-Palmus"
                    },
                    {
                        "name": "Michael Truong-Le"
                    },
                    {
                        "name": "Oliver Hausdörfer"
                    },
                    {
                        "name": "Alok Verma"
                    }
                ],
                "author_detail": {
                    "name": "Alok Verma"
                },
                "author": "Alok Verma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09502v1",
                "updated": "2024-11-14T15:13:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    13,
                    13,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T15:13:13Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    13,
                    13,
                    3,
                    319,
                    0
                ],
                "title": "Golden Noise for Diffusion Models: A Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Golden Noise for Diffusion Models: A Learning Framework"
                },
                "summary": "Text-to-image diffusion model is a popular paradigm that synthesizes\npersonalized images by providing a text prompt and a random Gaussian noise.\nWhile people observe that some noises are ``golden noises'' that can achieve\nbetter text-image alignment and higher human preference than others, we still\nlack a machine learning framework to obtain those golden noises. To learn\ngolden noises for diffusion sampling, we mainly make three contributions in\nthis paper. First, we identify a new concept termed the \\textit{noise prompt},\nwhich aims at turning a random Gaussian noise into a golden noise by adding a\nsmall desirable perturbation derived from the text prompt. Following the\nconcept, we first formulate the \\textit{noise prompt learning} framework that\nsystematically learns ``prompted'' golden noise associated with a text prompt\nfor diffusion models. Second, we design a noise prompt data collection pipeline\nand collect a large-scale \\textit{noise prompt dataset}~(NPD) that contains\n100k pairs of random noises and golden noises with the associated text prompts.\nWith the prepared NPD as the training dataset, we trained a small \\textit{noise\nprompt network}~(NPNet) that can directly learn to transform a random noise\ninto a golden noise. The learned golden noise perturbation can be considered as\na kind of prompt for noise, as it is rich in semantic information and tailored\nto the given text prompt. Third, our extensive experiments demonstrate the\nimpressive effectiveness and generalization of NPNet on improving the quality\nof synthesized images across various diffusion models, including SDXL,\nDreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small and\nefficient controller that acts as a plug-and-play module with very limited\nadditional inference and computational costs, as it just provides a golden\nnoise instead of a random noise without accessing the original pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image diffusion model is a popular paradigm that synthesizes\npersonalized images by providing a text prompt and a random Gaussian noise.\nWhile people observe that some noises are ``golden noises'' that can achieve\nbetter text-image alignment and higher human preference than others, we still\nlack a machine learning framework to obtain those golden noises. To learn\ngolden noises for diffusion sampling, we mainly make three contributions in\nthis paper. First, we identify a new concept termed the \\textit{noise prompt},\nwhich aims at turning a random Gaussian noise into a golden noise by adding a\nsmall desirable perturbation derived from the text prompt. Following the\nconcept, we first formulate the \\textit{noise prompt learning} framework that\nsystematically learns ``prompted'' golden noise associated with a text prompt\nfor diffusion models. Second, we design a noise prompt data collection pipeline\nand collect a large-scale \\textit{noise prompt dataset}~(NPD) that contains\n100k pairs of random noises and golden noises with the associated text prompts.\nWith the prepared NPD as the training dataset, we trained a small \\textit{noise\nprompt network}~(NPNet) that can directly learn to transform a random noise\ninto a golden noise. The learned golden noise perturbation can be considered as\na kind of prompt for noise, as it is rich in semantic information and tailored\nto the given text prompt. Third, our extensive experiments demonstrate the\nimpressive effectiveness and generalization of NPNet on improving the quality\nof synthesized images across various diffusion models, including SDXL,\nDreamShaper-xl-v2-turbo, and Hunyuan-DiT. Moreover, NPNet is a small and\nefficient controller that acts as a plug-and-play module with very limited\nadditional inference and computational costs, as it just provides a golden\nnoise instead of a random noise without accessing the original pipeline."
                },
                "authors": [
                    {
                        "name": "Zikai Zhou"
                    },
                    {
                        "name": "Shitong Shao"
                    },
                    {
                        "name": "Lichen Bai"
                    },
                    {
                        "name": "Zhiqiang Xu"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Zeke Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Xie"
                },
                "author": "Zeke Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01440v3",
                "updated": "2024-11-14T15:04:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    4,
                    33,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-02T11:42:49Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    11,
                    42,
                    49,
                    2,
                    276,
                    0
                ],
                "title": "Closed-Loop Long-Horizon Robotic Planning via Equilibrium Sequence\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closed-Loop Long-Horizon Robotic Planning via Equilibrium Sequence\n  Modeling"
                },
                "summary": "In the endeavor to make autonomous robots take actions, task planning is a\nmajor challenge that requires translating high-level task descriptions into\nlong-horizon action sequences. Despite recent advances in language model\nagents, they remain prone to planning errors and limited in their ability to\nplan ahead. To address these limitations in robotic planning, we advocate a\nself-refining scheme that iteratively refines a draft plan until an equilibrium\nis reached. Remarkably, this process can be optimized end-to-end from an\nanalytical perspective without the need to curate additional verifiers or\nreward models, allowing us to train self-refining planners in a simple\nsupervised learning fashion. Meanwhile, a nested equilibrium sequence modeling\nprocedure is devised for efficient closed-loop planning that incorporates\nuseful feedback from the environment (or an internal world model). Our method\nis evaluated on the VirtualHome-Env benchmark, showing advanced performance\nwith better scaling for inference computation. Code is available at\nhttps://github.com/Singularity0104/equilibrium-planner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the endeavor to make autonomous robots take actions, task planning is a\nmajor challenge that requires translating high-level task descriptions into\nlong-horizon action sequences. Despite recent advances in language model\nagents, they remain prone to planning errors and limited in their ability to\nplan ahead. To address these limitations in robotic planning, we advocate a\nself-refining scheme that iteratively refines a draft plan until an equilibrium\nis reached. Remarkably, this process can be optimized end-to-end from an\nanalytical perspective without the need to curate additional verifiers or\nreward models, allowing us to train self-refining planners in a simple\nsupervised learning fashion. Meanwhile, a nested equilibrium sequence modeling\nprocedure is devised for efficient closed-loop planning that incorporates\nuseful feedback from the environment (or an internal world model). Our method\nis evaluated on the VirtualHome-Env benchmark, showing advanced performance\nwith better scaling for inference computation. Code is available at\nhttps://github.com/Singularity0104/equilibrium-planner."
                },
                "authors": [
                    {
                        "name": "Jinghan Li"
                    },
                    {
                        "name": "Zhicheng Sun"
                    },
                    {
                        "name": "Fei Li"
                    },
                    {
                        "name": "Cao Sheng"
                    },
                    {
                        "name": "Jiazhong Yu"
                    },
                    {
                        "name": "Yadong Mu"
                    }
                ],
                "author_detail": {
                    "name": "Yadong Mu"
                },
                "author": "Yadong Mu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09492v1",
                "updated": "2024-11-14T14:58:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    58,
                    38,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:58:38Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    58,
                    38,
                    3,
                    319,
                    0
                ],
                "title": "MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in\n  LLMs"
                },
                "summary": "Large language models (LLMs) excel in high-resource languages but face\nnotable challenges in low-resource languages like Mongolian. This paper\naddresses these challenges by categorizing capabilities into language abilities\n(syntax and semantics) and cognitive abilities (knowledge and reasoning). To\nsystematically evaluate these areas, we developed MM-Eval, a specialized\ndataset based on Modern Mongolian Language Textbook I and enriched with WebQSP\nand MGSM datasets.\n  Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat,\nLlama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all models\nperformed better on syntactic tasks than semantic tasks, highlighting a gap in\ndeeper language understanding; and 2) knowledge tasks showed a moderate\ndecline, suggesting that models can transfer general knowledge from\nhigh-resource to low-resource contexts.\n  The release of MM-Eval, comprising 569 syntax, 677 semantics, 344 knowledge,\nand 250 reasoning tasks, offers valuable insights for advancing NLP and LLMs in\nlow-resource languages like Mongolian. The dataset is available at\nhttps://github.com/joenahm/MM-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in high-resource languages but face\nnotable challenges in low-resource languages like Mongolian. This paper\naddresses these challenges by categorizing capabilities into language abilities\n(syntax and semantics) and cognitive abilities (knowledge and reasoning). To\nsystematically evaluate these areas, we developed MM-Eval, a specialized\ndataset based on Modern Mongolian Language Textbook I and enriched with WebQSP\nand MGSM datasets.\n  Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat,\nLlama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all models\nperformed better on syntactic tasks than semantic tasks, highlighting a gap in\ndeeper language understanding; and 2) knowledge tasks showed a moderate\ndecline, suggesting that models can transfer general knowledge from\nhigh-resource to low-resource contexts.\n  The release of MM-Eval, comprising 569 syntax, 677 semantics, 344 knowledge,\nand 250 reasoning tasks, offers valuable insights for advancing NLP and LLMs in\nlow-resource languages like Mongolian. The dataset is available at\nhttps://github.com/joenahm/MM-Eval."
                },
                "authors": [
                    {
                        "name": "Mengyuan Zhang"
                    },
                    {
                        "name": "Ruihui Wang"
                    },
                    {
                        "name": "Bo Xia"
                    },
                    {
                        "name": "Yuan Sun"
                    },
                    {
                        "name": "Xiaobing Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobing Zhao"
                },
                "author": "Xiaobing Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01083v2",
                "updated": "2024-11-14T14:52:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    52,
                    54,
                    3,
                    319,
                    0
                ],
                "published": "2024-09-02T09:11:28Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    9,
                    11,
                    28,
                    0,
                    246,
                    0
                ],
                "title": "Affordance-based Robot Manipulation with Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordance-based Robot Manipulation with Flow Matching"
                },
                "summary": "We present a framework for assistive robot manipulation, which focuses on two\nfundamental challenges: first, efficiently adapting large-scale models to\ndownstream scene affordance understanding tasks, especially in daily living\nscenarios where gathering multi-task data involving humans requires strenuous\neffort; second, effectively learning robot trajectories by grounding the visual\naffordance model. We tackle the first challenge by employing a\nparameter-efficient prompt tuning method that prepends learnable text prompts\nto the frozen vision model to predict manipulation affordances in multi-task\nscenarios. Then we propose to learn robot trajectories guided by affordances in\na supervised Flow Matching method. Flow matching represents a robot visuomotor\npolicy as a conditional process of flowing random waypoints to desired robot\ntrajectories. Finally, we introduce a real-world dataset with 10 tasks across\nActivities of Daily Living to test our framework. Our extensive evaluation\nhighlights that the proposed prompt tuning method for learning manipulation\naffordance with language prompter achieves competitive performance and even\noutperforms other finetuning protocols across data scales, while satisfying\nparameter efficiency. Learning multi-task robot trajectories with flow matching\npolicy also leads to consistently better generalization performance and faster\ninference than alternative behavior cloning methods, especially given\nmultimodal robot action distributions. Our framework seamlessly unifies\naffordance model learning and trajectory generation with flow matching for\nrobot manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a framework for assistive robot manipulation, which focuses on two\nfundamental challenges: first, efficiently adapting large-scale models to\ndownstream scene affordance understanding tasks, especially in daily living\nscenarios where gathering multi-task data involving humans requires strenuous\neffort; second, effectively learning robot trajectories by grounding the visual\naffordance model. We tackle the first challenge by employing a\nparameter-efficient prompt tuning method that prepends learnable text prompts\nto the frozen vision model to predict manipulation affordances in multi-task\nscenarios. Then we propose to learn robot trajectories guided by affordances in\na supervised Flow Matching method. Flow matching represents a robot visuomotor\npolicy as a conditional process of flowing random waypoints to desired robot\ntrajectories. Finally, we introduce a real-world dataset with 10 tasks across\nActivities of Daily Living to test our framework. Our extensive evaluation\nhighlights that the proposed prompt tuning method for learning manipulation\naffordance with language prompter achieves competitive performance and even\noutperforms other finetuning protocols across data scales, while satisfying\nparameter efficiency. Learning multi-task robot trajectories with flow matching\npolicy also leads to consistently better generalization performance and faster\ninference than alternative behavior cloning methods, especially given\nmultimodal robot action distributions. Our framework seamlessly unifies\naffordance model learning and trajectory generation with flow matching for\nrobot manipulation."
                },
                "authors": [
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Michael Gienger"
                    }
                ],
                "author_detail": {
                    "name": "Michael Gienger"
                },
                "author": "Michael Gienger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09483v1",
                "updated": "2024-11-14T14:37:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    37,
                    47,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:37:47Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    37,
                    47,
                    3,
                    319,
                    0
                ],
                "title": "Sparse Bayesian Generative Modeling for Compressive Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Bayesian Generative Modeling for Compressive Sensing"
                },
                "summary": "This work addresses the fundamental linear inverse problem in compressive\nsensing (CS) by introducing a new type of regularizing generative prior. Our\nproposed method utilizes ideas from classical dictionary-based CS and, in\nparticular, sparse Bayesian learning (SBL), to integrate a strong\nregularization towards sparse solutions. At the same time, by leveraging the\nnotion of conditional Gaussianity, it also incorporates the adaptability from\ngenerative models to training data. However, unlike most state-of-the-art\ngenerative models, it is able to learn from a few compressed and noisy data\nsamples and requires no optimization algorithm for solving the inverse problem.\nAdditionally, similar to Dirichlet prior networks, our model parameterizes a\nconjugate prior enabling its application for uncertainty quantification. We\nsupport our approach theoretically through the concept of variational inference\nand validate it empirically using different types of compressible signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work addresses the fundamental linear inverse problem in compressive\nsensing (CS) by introducing a new type of regularizing generative prior. Our\nproposed method utilizes ideas from classical dictionary-based CS and, in\nparticular, sparse Bayesian learning (SBL), to integrate a strong\nregularization towards sparse solutions. At the same time, by leveraging the\nnotion of conditional Gaussianity, it also incorporates the adaptability from\ngenerative models to training data. However, unlike most state-of-the-art\ngenerative models, it is able to learn from a few compressed and noisy data\nsamples and requires no optimization algorithm for solving the inverse problem.\nAdditionally, similar to Dirichlet prior networks, our model parameterizes a\nconjugate prior enabling its application for uncertainty quantification. We\nsupport our approach theoretically through the concept of variational inference\nand validate it empirically using different types of compressible signals."
                },
                "authors": [
                    {
                        "name": "Benedikt Böck"
                    },
                    {
                        "name": "Sadaf Syed"
                    },
                    {
                        "name": "Wolfgang Utschick"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Utschick"
                },
                "author": "Wolfgang Utschick",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06900v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06900v5",
                "updated": "2024-11-14T14:28:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    58,
                    3,
                    319,
                    0
                ],
                "published": "2024-02-10T07:55:27Z",
                "published_parsed": [
                    2024,
                    2,
                    10,
                    7,
                    55,
                    27,
                    5,
                    41,
                    0
                ],
                "title": "Can LLMs Recognize Toxicity? A Structured Investigation Framework and\n  Toxicity Metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Recognize Toxicity? A Structured Investigation Framework and\n  Toxicity Metric"
                },
                "summary": "In the pursuit of developing Large Language Models (LLMs) that adhere to\nsocietal standards, it is imperative to detect the toxicity in the generated\ntext. The majority of existing toxicity metrics rely on encoder models trained\non specific toxicity datasets, which are susceptible to out-of-distribution\n(OOD) problems and depend on the dataset's definition of toxicity. In this\npaper, we introduce a robust metric grounded on LLMs to flexibly measure\ntoxicity according to the given definition. We first analyze the toxicity\nfactors, followed by an examination of the intrinsic toxic attributes of LLMs\nto ascertain their suitability as evaluators. Finally, we evaluate the\nperformance of our metric with detailed analysis. Our empirical results\ndemonstrate outstanding performance in measuring toxicity within verified\nfactors, improving on conventional metrics by 12 points in the F1 score. Our\nfindings also indicate that upstream toxicity significantly influences\ndownstream metrics, suggesting that LLMs are unsuitable for toxicity\nevaluations within unverified factors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the pursuit of developing Large Language Models (LLMs) that adhere to\nsocietal standards, it is imperative to detect the toxicity in the generated\ntext. The majority of existing toxicity metrics rely on encoder models trained\non specific toxicity datasets, which are susceptible to out-of-distribution\n(OOD) problems and depend on the dataset's definition of toxicity. In this\npaper, we introduce a robust metric grounded on LLMs to flexibly measure\ntoxicity according to the given definition. We first analyze the toxicity\nfactors, followed by an examination of the intrinsic toxic attributes of LLMs\nto ascertain their suitability as evaluators. Finally, we evaluate the\nperformance of our metric with detailed analysis. Our empirical results\ndemonstrate outstanding performance in measuring toxicity within verified\nfactors, improving on conventional metrics by 12 points in the F1 score. Our\nfindings also indicate that upstream toxicity significantly influences\ndownstream metrics, suggesting that LLMs are unsuitable for toxicity\nevaluations within unverified factors."
                },
                "authors": [
                    {
                        "name": "Hyukhun Koh"
                    },
                    {
                        "name": "Dohyung Kim"
                    },
                    {
                        "name": "Minwoo Lee"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "arxiv_comment": "8 page long",
                "arxiv_journal_ref": "EMNLP2024 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06900v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06900v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07668v2",
                "updated": "2024-11-14T14:28:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    24,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-12T09:35:23Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    9,
                    35,
                    23,
                    1,
                    317,
                    0
                ],
                "title": "Towards Evaluation Guidelines for Empirical Studies involving LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluation Guidelines for Empirical Studies involving LLMs"
                },
                "summary": "In the short period since the release of ChatGPT in November 2022, large\nlanguage models (LLMs) have changed the software engineering research\nlandscape. While there are numerous opportunities to use LLMs for supporting\nresearch or software engineering tasks, solid science needs rigorous empirical\nevaluations. However, so far, there are no specific guidelines for conducting\nand assessing studies involving LLMs in software engineering research. Our\nfocus is on empirical studies that either use LLMs as part of the research\nprocess (e.g., for data annotation) or studies that evaluate existing or new\ntools that are based on LLMs. This paper contributes the first set of\nguidelines for such studies. Our goal is to start a discussion in the software\nengineering research community to reach a common understanding of what our\ncommunity standards are for high-quality empirical studies involving LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the short period since the release of ChatGPT in November 2022, large\nlanguage models (LLMs) have changed the software engineering research\nlandscape. While there are numerous opportunities to use LLMs for supporting\nresearch or software engineering tasks, solid science needs rigorous empirical\nevaluations. However, so far, there are no specific guidelines for conducting\nand assessing studies involving LLMs in software engineering research. Our\nfocus is on empirical studies that either use LLMs as part of the research\nprocess (e.g., for data annotation) or studies that evaluate existing or new\ntools that are based on LLMs. This paper contributes the first set of\nguidelines for such studies. Our goal is to start a discussion in the software\nengineering research community to reach a common understanding of what our\ncommunity standards are for high-quality empirical studies involving LLMs."
                },
                "authors": [
                    {
                        "name": "Stefan Wagner"
                    },
                    {
                        "name": "Marvin Muñoz Barón"
                    },
                    {
                        "name": "Davide Falessi"
                    },
                    {
                        "name": "Sebastian Baltes"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Baltes"
                },
                "author": "Sebastian Baltes",
                "arxiv_comment": "4 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03227v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03227v4",
                "updated": "2024-11-14T14:11:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    11,
                    57,
                    3,
                    319,
                    0
                ],
                "published": "2024-02-05T17:38:49Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    17,
                    38,
                    49,
                    0,
                    36,
                    0
                ],
                "title": "IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of\n  brain MR images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of\n  brain MR images"
                },
                "summary": "In MRI studies, the aggregation of imaging data from multiple acquisition\nsites enhances sample size but may introduce site-related variabilities that\nhinder consistency in subsequent analyses. Deep learning methods for image\ntranslation have emerged as a solution for harmonizing MR images across sites.\nIn this study, we introduce IGUANe (Image Generation with Unified Adversarial\nNetworks), an original 3D model that leverages the strengths of domain\ntranslation and straightforward application of style transfer methods for\nmulticenter brain MR image harmonization. IGUANe extends CycleGAN by\nintegrating an arbitrary number of domains for training through a many-to-one\narchitecture. The framework based on domain pairs enables the implementation of\nsampling strategies that prevent confusion between site-related and biological\nvariabilities. During inference, the model can be applied to any image, even\nfrom an unknown acquisition site, making it a universal generator for\nharmonization. Trained on a dataset comprising T1-weighted images from 11\ndifferent scanners, IGUANe was evaluated on data from unseen sites. The\nassessments included the transformation of MR images with traveling subjects,\nthe preservation of pairwise distances between MR images within domains, the\nevolution of volumetric patterns related to age and Alzheimer$'$s disease (AD),\nand the performance in age regression and patient classification tasks.\nComparisons with other harmonization and normalization methods suggest that\nIGUANe better preserves individual information in MR images and is more\nsuitable for maintaining and reinforcing variabilities related to age and AD.\nFuture studies may further assess IGUANe in other multicenter contexts, either\nusing the same model or retraining it for applications to different image\nmodalities. IGUANe is available at\nhttps://github.com/RocaVincent/iguane_harmonization.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In MRI studies, the aggregation of imaging data from multiple acquisition\nsites enhances sample size but may introduce site-related variabilities that\nhinder consistency in subsequent analyses. Deep learning methods for image\ntranslation have emerged as a solution for harmonizing MR images across sites.\nIn this study, we introduce IGUANe (Image Generation with Unified Adversarial\nNetworks), an original 3D model that leverages the strengths of domain\ntranslation and straightforward application of style transfer methods for\nmulticenter brain MR image harmonization. IGUANe extends CycleGAN by\nintegrating an arbitrary number of domains for training through a many-to-one\narchitecture. The framework based on domain pairs enables the implementation of\nsampling strategies that prevent confusion between site-related and biological\nvariabilities. During inference, the model can be applied to any image, even\nfrom an unknown acquisition site, making it a universal generator for\nharmonization. Trained on a dataset comprising T1-weighted images from 11\ndifferent scanners, IGUANe was evaluated on data from unseen sites. The\nassessments included the transformation of MR images with traveling subjects,\nthe preservation of pairwise distances between MR images within domains, the\nevolution of volumetric patterns related to age and Alzheimer$'$s disease (AD),\nand the performance in age regression and patient classification tasks.\nComparisons with other harmonization and normalization methods suggest that\nIGUANe better preserves individual information in MR images and is more\nsuitable for maintaining and reinforcing variabilities related to age and AD.\nFuture studies may further assess IGUANe in other multicenter contexts, either\nusing the same model or retraining it for applications to different image\nmodalities. IGUANe is available at\nhttps://github.com/RocaVincent/iguane_harmonization.git."
                },
                "authors": [
                    {
                        "name": "Vincent Roca"
                    },
                    {
                        "name": "Grégory Kuchcinski"
                    },
                    {
                        "name": "Jean-Pierre Pruvo"
                    },
                    {
                        "name": "Dorian Manouvriez"
                    },
                    {
                        "name": "Renaud Lopes"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Lopes"
                },
                "author": "Renaud Lopes",
                "arxiv_doi": "10.1016/j.media.2024.103388",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.media.2024.103388",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.03227v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03227v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "29 pages, 14 figures",
                "arxiv_journal_ref": "Medical Image Analysis 99 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08586v2",
                "updated": "2024-11-14T14:07:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    7,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-13T13:09:14Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    13,
                    9,
                    14,
                    2,
                    318,
                    0
                ],
                "title": "Optimizing Automatic Summarization of Long Clinical Records Using\n  Dynamic Context Extension:Testing and Evaluation of the NBCE Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Automatic Summarization of Long Clinical Records Using\n  Dynamic Context Extension:Testing and Evaluation of the NBCE Method"
                },
                "summary": "Summarizing patient clinical notes is vital for reducing documentation\nburdens. Current manual summarization makes medical staff struggle. We propose\nan automatic method using LLMs, but long inputs cause LLMs to lose context,\nreducing output quality especially in small size model. We used a 7B model,\nopen-calm-7b, enhanced with Native Bayes Context Extend and a redesigned\ndecoding mechanism to reference one sentence at a time, keeping inputs within\ncontext windows, 2048 tokens. Our improved model achieved near parity with\nGoogle's over 175B Gemini on ROUGE-L metrics with 200 samples, indicating\nstrong performance using less resources, enhancing automated EMR summarization\nfeasibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summarizing patient clinical notes is vital for reducing documentation\nburdens. Current manual summarization makes medical staff struggle. We propose\nan automatic method using LLMs, but long inputs cause LLMs to lose context,\nreducing output quality especially in small size model. We used a 7B model,\nopen-calm-7b, enhanced with Native Bayes Context Extend and a redesigned\ndecoding mechanism to reference one sentence at a time, keeping inputs within\ncontext windows, 2048 tokens. Our improved model achieved near parity with\nGoogle's over 175B Gemini on ROUGE-L metrics with 200 samples, indicating\nstrong performance using less resources, enhancing automated EMR summarization\nfeasibility."
                },
                "authors": [
                    {
                        "name": "Guoqing Zhang"
                    },
                    {
                        "name": "Keita Fukuyama"
                    },
                    {
                        "name": "Kazumasa Kishimoto"
                    },
                    {
                        "name": "Tomohiro Kuroda"
                    }
                ],
                "author_detail": {
                    "name": "Tomohiro Kuroda"
                },
                "author": "Tomohiro Kuroda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15933v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15933v2",
                "updated": "2024-11-14T13:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    59,
                    15,
                    3,
                    319,
                    0
                ],
                "published": "2024-09-24T09:57:25Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    57,
                    25,
                    1,
                    268,
                    0
                ],
                "title": "SLIMER-IT: Zero-Shot NER on Italian Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLIMER-IT: Zero-Shot NER on Italian Language"
                },
                "summary": "Traditional approaches to Named Entity Recognition (NER) frame the task into\na BIO sequence labeling problem. Although these systems often excel in the\ndownstream task at hand, they require extensive annotated data and struggle to\ngeneralize to out-of-distribution input domains and unseen entity types. On the\ncontrary, Large Language Models (LLMs) have demonstrated strong zero-shot\ncapabilities. While several works address Zero-Shot NER in English, little has\nbeen done in other languages. In this paper, we define an evaluation framework\nfor Zero-Shot NER, applying it to the Italian language. Furthermore, we\nintroduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning\napproach for zero-shot NER leveraging prompts enriched with definition and\nguidelines. Comparisons with other state-of-the-art models, demonstrate the\nsuperiority of SLIMER-IT on never-seen-before entity tags.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional approaches to Named Entity Recognition (NER) frame the task into\na BIO sequence labeling problem. Although these systems often excel in the\ndownstream task at hand, they require extensive annotated data and struggle to\ngeneralize to out-of-distribution input domains and unseen entity types. On the\ncontrary, Large Language Models (LLMs) have demonstrated strong zero-shot\ncapabilities. While several works address Zero-Shot NER in English, little has\nbeen done in other languages. In this paper, we define an evaluation framework\nfor Zero-Shot NER, applying it to the Italian language. Furthermore, we\nintroduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning\napproach for zero-shot NER leveraging prompts enriched with definition and\nguidelines. Comparisons with other state-of-the-art models, demonstrate the\nsuperiority of SLIMER-IT on never-seen-before entity tags."
                },
                "authors": [
                    {
                        "name": "Andrew Zamai"
                    },
                    {
                        "name": "Leonardo Rigutini"
                    },
                    {
                        "name": "Marco Maggini"
                    },
                    {
                        "name": "Andrea Zugarini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zugarini"
                },
                "author": "Andrea Zugarini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15933v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15933v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15696v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15696v2",
                "updated": "2024-11-14T13:57:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    57,
                    37,
                    3,
                    319,
                    0
                ],
                "published": "2024-08-28T10:51:18Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    10,
                    51,
                    18,
                    2,
                    241,
                    0
                ],
                "title": "Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: a case study on Baidu, Ernie and Qwen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: a case study on Baidu, Ernie and Qwen"
                },
                "summary": "Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective."
                },
                "authors": [
                    {
                        "name": "Geng Liu"
                    },
                    {
                        "name": "Carlo Alberto Bono"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15696v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15696v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05556v2",
                "updated": "2024-11-14T13:47:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    47,
                    10,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-08T13:24:42Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    42,
                    4,
                    313,
                    0
                ],
                "title": "Gaussian process modelling of infectious diseases using the Greta\n  software package and GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian process modelling of infectious diseases using the Greta\n  software package and GPUs"
                },
                "summary": "Gaussian process are a widely-used statistical tool for conducting\nnon-parametric inference in applied sciences, with many computational packages\navailable to fit to data and predict future observations. We study the use of\nthe Greta software for Bayesian inference to apply Gaussian process regression\nto spatio-temporal data of infectious disease outbreaks and predict future\ndisease spread. Greta builds on Tensorflow, making it comparatively easy to\ntake advantage of the significant gain in speed offered by GPUs. In these\ncomplex spatio-temporal models, we show a reduction of up to 70\\% in\ncomputational time relative to fitting the same models on CPUs. We show how the\nchoice of covariance kernel impacts the ability to infer spread and extrapolate\nto unobserved spatial and temporal units. The inference pipeline is applied to\nweekly incidence data on tuberculosis in the East and West Midlands regions of\nEngland over a period of two years.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian process are a widely-used statistical tool for conducting\nnon-parametric inference in applied sciences, with many computational packages\navailable to fit to data and predict future observations. We study the use of\nthe Greta software for Bayesian inference to apply Gaussian process regression\nto spatio-temporal data of infectious disease outbreaks and predict future\ndisease spread. Greta builds on Tensorflow, making it comparatively easy to\ntake advantage of the significant gain in speed offered by GPUs. In these\ncomplex spatio-temporal models, we show a reduction of up to 70\\% in\ncomputational time relative to fitting the same models on CPUs. We show how the\nchoice of covariance kernel impacts the ability to infer spread and extrapolate\nto unobserved spatial and temporal units. The inference pipeline is applied to\nweekly incidence data on tuberculosis in the East and West Midlands regions of\nEngland over a period of two years."
                },
                "authors": [
                    {
                        "name": "Eva Gunn"
                    },
                    {
                        "name": "Nikhil Sengupta"
                    },
                    {
                        "name": "Ben Swallow"
                    }
                ],
                "author_detail": {
                    "name": "Ben Swallow"
                },
                "author": "Ben Swallow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06651v2",
                "updated": "2024-11-14T13:26:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    26,
                    35,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-11T01:36:48Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    36,
                    48,
                    0,
                    316,
                    0
                ],
                "title": "Machine learning-enabled velocity model building with uncertainty\n  quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning-enabled velocity model building with uncertainty\n  quantification"
                },
                "summary": "Accurately characterizing migration velocity models is crucial for a wide\nrange of geophysical applications, from hydrocarbon exploration to monitoring\nof CO2 sequestration projects. Traditional velocity model building methods such\nas Full-Waveform Inversion (FWI) are powerful but often struggle with the\ninherent complexities of the inverse problem, including noise, limited\nbandwidth, receiver aperture and computational constraints. To address these\nchallenges, we propose a scalable methodology that integrates generative\nmodeling, in the form of Diffusion networks, with physics-informed summary\nstatistics, making it suitable for complicated imaging problems including field\ndatasets. By defining these summary statistics in terms of subsurface-offset\nimage volumes for poor initial velocity models, our approach allows for\ncomputationally efficient generation of Bayesian posterior samples for\nmigration velocity models that offer a useful assessment of uncertainty. To\nvalidate our approach, we introduce a battery of tests that measure the quality\nof the inferred velocity models, as well as the quality of the inferred\nuncertainties. With modern synthetic datasets, we reconfirm gains from using\nsubsurface-image gathers as the conditioning observable. For complex velocity\nmodel building involving salt, we propose a new iterative workflow that refines\namortized posterior approximations with salt flooding and demonstrate how the\nuncertainty in the velocity model can be propagated to the final product\nreverse time migrated images. Finally, we present a proof of concept on field\ndatasets to show that our method can scale to industry-sized problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately characterizing migration velocity models is crucial for a wide\nrange of geophysical applications, from hydrocarbon exploration to monitoring\nof CO2 sequestration projects. Traditional velocity model building methods such\nas Full-Waveform Inversion (FWI) are powerful but often struggle with the\ninherent complexities of the inverse problem, including noise, limited\nbandwidth, receiver aperture and computational constraints. To address these\nchallenges, we propose a scalable methodology that integrates generative\nmodeling, in the form of Diffusion networks, with physics-informed summary\nstatistics, making it suitable for complicated imaging problems including field\ndatasets. By defining these summary statistics in terms of subsurface-offset\nimage volumes for poor initial velocity models, our approach allows for\ncomputationally efficient generation of Bayesian posterior samples for\nmigration velocity models that offer a useful assessment of uncertainty. To\nvalidate our approach, we introduce a battery of tests that measure the quality\nof the inferred velocity models, as well as the quality of the inferred\nuncertainties. With modern synthetic datasets, we reconfirm gains from using\nsubsurface-image gathers as the conditioning observable. For complex velocity\nmodel building involving salt, we propose a new iterative workflow that refines\namortized posterior approximations with salt flooding and demonstrate how the\nuncertainty in the velocity model can be propagated to the final product\nreverse time migrated images. Finally, we present a proof of concept on field\ndatasets to show that our method can scale to industry-sized problems."
                },
                "authors": [
                    {
                        "name": "Rafael Orozco"
                    },
                    {
                        "name": "Huseyin Tuna Erdinc"
                    },
                    {
                        "name": "Yunlin Zeng"
                    },
                    {
                        "name": "Mathias Louboutin"
                    },
                    {
                        "name": "Felix J. Herrmann"
                    }
                ],
                "author_detail": {
                    "name": "Felix J. Herrmann"
                },
                "author": "Felix J. Herrmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09413v1",
                "updated": "2024-11-14T13:07:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    7,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:07:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    7,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Script-centric behavior understanding for assisted autism spectrum\n  disorder diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Script-centric behavior understanding for assisted autism spectrum\n  disorder diagnosis"
                },
                "summary": "Observing and analyzing children's social behaviors is crucial for the early\ndiagnosis of Autism Spectrum Disorders (ASD). This work focuses on\nautomatically detecting ASD using computer vision techniques and large language\nmodels (LLMs). Existing methods typically rely on supervised learning. However,\nthe scarcity of ASD diagnostic datasets and the lack of interpretability in\ndiagnostic results significantly limits its clinical application. To address\nthese challenges, we introduce a novel unsupervised approach based on\nscript-centric behavior understanding. Our pipeline converts video content into\nscripts that describe the behavior of characters, leveraging the\ngeneralizability of large language models to detect ASD in a zero-shot or\nfew-shot manner. Specifically, we propose a scripts transcription module for\nmultimodal behavior data textualization and a domain prompts module to bridge\nLLMs. Our method achieves an accuracy of 92.00\\% in diagnosing ASD in children\nwith an average age of 24 months, surpassing the performance of supervised\nlearning methods by 3.58\\% absolutely. Extensive experiments confirm the\neffectiveness of our approach and suggest its potential for advancing ASD\nresearch through LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observing and analyzing children's social behaviors is crucial for the early\ndiagnosis of Autism Spectrum Disorders (ASD). This work focuses on\nautomatically detecting ASD using computer vision techniques and large language\nmodels (LLMs). Existing methods typically rely on supervised learning. However,\nthe scarcity of ASD diagnostic datasets and the lack of interpretability in\ndiagnostic results significantly limits its clinical application. To address\nthese challenges, we introduce a novel unsupervised approach based on\nscript-centric behavior understanding. Our pipeline converts video content into\nscripts that describe the behavior of characters, leveraging the\ngeneralizability of large language models to detect ASD in a zero-shot or\nfew-shot manner. Specifically, we propose a scripts transcription module for\nmultimodal behavior data textualization and a domain prompts module to bridge\nLLMs. Our method achieves an accuracy of 92.00\\% in diagnosing ASD in children\nwith an average age of 24 months, surpassing the performance of supervised\nlearning methods by 3.58\\% absolutely. Extensive experiments confirm the\neffectiveness of our approach and suggest its potential for advancing ASD\nresearch through LLMs."
                },
                "authors": [
                    {
                        "name": "Wenxing Liu"
                    },
                    {
                        "name": "Yueran Pan"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "5 pages, 4 figures, submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09410v1",
                "updated": "2024-11-14T13:00:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    0,
                    23,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:00:23Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    0,
                    23,
                    3,
                    319,
                    0
                ],
                "title": "LLM-assisted Explicit and Implicit Multi-interest Learning Framework for\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-assisted Explicit and Implicit Multi-interest Learning Framework for\n  Sequential Recommendation"
                },
                "summary": "Multi-interest modeling in current recommender systems (RS) is mainly based\non user behavioral data, capturing user interest preferences from multiple\ndimensions. However, since behavioral data is implicit and often highly sparse,\nit is challenging to understand users' complex and diverse interests. Recent\nstudies have shown that the rich semantic information in the text can\neffectively supplement the deficiencies of behavioral data. Despite this, it is\nstill difficult for small models to directly extract semantic features\nassociated with users' deep interests. That is, how to effectively align\nsemantics with behavioral information to form a more comprehensive and accurate\nunderstanding of user interests has become a critical research problem.To\naddress this, we propose an LLM-assisted explicit and implicit multi-interest\nlearning framework (named EIMF) to model user interests on two levels: behavior\nand semantics. The framework consists of two parts: Implicit Behavioral\nInterest Module (IBIM) and Explicit Semantic Interest Module (ESIM). The\ntraditional multi-interest RS model in IBIM can learn users' implicit\nbehavioral interests from interactions with items. In ESIM, we first adopt a\nclustering algorithm to select typical samples and design a prompting strategy\non LLM to obtain explicit semantic interests. Furthermore, in the training\nphase, the semantic interests of typical samples can enhance the representation\nlearning of behavioral interests based on the multi-task learning on semantic\nprediction and modality alignment. Therefore, in the inference stage, accurate\nrecommendations can be achieved with only the user's behavioral data. Extensive\nexperiments on real-world datasets demonstrate the effectiveness of the\nproposed EIMF framework, which effectively and efficiently combines small\nmodels with LLM to improve the accuracy of multi-interest modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-interest modeling in current recommender systems (RS) is mainly based\non user behavioral data, capturing user interest preferences from multiple\ndimensions. However, since behavioral data is implicit and often highly sparse,\nit is challenging to understand users' complex and diverse interests. Recent\nstudies have shown that the rich semantic information in the text can\neffectively supplement the deficiencies of behavioral data. Despite this, it is\nstill difficult for small models to directly extract semantic features\nassociated with users' deep interests. That is, how to effectively align\nsemantics with behavioral information to form a more comprehensive and accurate\nunderstanding of user interests has become a critical research problem.To\naddress this, we propose an LLM-assisted explicit and implicit multi-interest\nlearning framework (named EIMF) to model user interests on two levels: behavior\nand semantics. The framework consists of two parts: Implicit Behavioral\nInterest Module (IBIM) and Explicit Semantic Interest Module (ESIM). The\ntraditional multi-interest RS model in IBIM can learn users' implicit\nbehavioral interests from interactions with items. In ESIM, we first adopt a\nclustering algorithm to select typical samples and design a prompting strategy\non LLM to obtain explicit semantic interests. Furthermore, in the training\nphase, the semantic interests of typical samples can enhance the representation\nlearning of behavioral interests based on the multi-task learning on semantic\nprediction and modality alignment. Therefore, in the inference stage, accurate\nrecommendations can be achieved with only the user's behavioral data. Extensive\nexperiments on real-world datasets demonstrate the effectiveness of the\nproposed EIMF framework, which effectively and efficiently combines small\nmodels with LLM to improve the accuracy of multi-interest modeling."
                },
                "authors": [
                    {
                        "name": "Shutong Qiao"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21991v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21991v5",
                "updated": "2024-11-14T12:19:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    12,
                    19,
                    26,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-29T12:22:07Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    22,
                    7,
                    1,
                    303,
                    0
                ],
                "title": "From Explicit Rules to Implicit Reasoning in an Interpretable Violence\n  Monitoring System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Explicit Rules to Implicit Reasoning in an Interpretable Violence\n  Monitoring System"
                },
                "summary": "Recently, research based on pre-trained models has demonstrated outstanding\nperformance in violence surveillance tasks. However, most of them were\nblack-box systems which faced challenges regarding explainability during\ntraining and inference processes. An important question is how to incorporate\nexplicit knowledge into these implicit models, thereby designing expertdriven\nand interpretable violence surveillance systems. This paper proposes a new\nparadigm for weakly supervised violence monitoring (WSVM) called Rule base\nViolence Monitoring (RuleVM). The proposed RuleVM uses a dual-branch structure\nwith different designs for images and text. One of the branches is called the\nimplicit branch, which uses only visual features for coarse-grained binary\nclassification. In this branch, image feature extraction is divided into two\nchannels: one responsible for extracting scene frames and the other focusing on\nextracting actions. The other branch is called the explicit branch, which\nutilizes language-image alignment to perform fine-grained classification. For\nthe language channel design in the explicit branch, the proposed RuleVM uses\nthe state-of-the-art YOLOWorld model to detect objects in video frames, and\nassociation rules are identified through data mining methods as descriptions of\nthe video. Leveraging the dual-branch architecture, RuleVM achieves\ninterpretable coarse-grained and fine-grained violence surveillance. Extensive\nexperiments were conducted on two commonly used benchmarks, and the results\nshow that RuleVM achieved the best performance in both coarse-grained and\nfinegrained monitoring, significantly outperforming existing state-ofthe-art\nmethods. Moreover, interpretability experiments uncovered some interesting\nrules, such as the observation that as the number of people increases, the risk\nlevel of violent behavior also rises.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, research based on pre-trained models has demonstrated outstanding\nperformance in violence surveillance tasks. However, most of them were\nblack-box systems which faced challenges regarding explainability during\ntraining and inference processes. An important question is how to incorporate\nexplicit knowledge into these implicit models, thereby designing expertdriven\nand interpretable violence surveillance systems. This paper proposes a new\nparadigm for weakly supervised violence monitoring (WSVM) called Rule base\nViolence Monitoring (RuleVM). The proposed RuleVM uses a dual-branch structure\nwith different designs for images and text. One of the branches is called the\nimplicit branch, which uses only visual features for coarse-grained binary\nclassification. In this branch, image feature extraction is divided into two\nchannels: one responsible for extracting scene frames and the other focusing on\nextracting actions. The other branch is called the explicit branch, which\nutilizes language-image alignment to perform fine-grained classification. For\nthe language channel design in the explicit branch, the proposed RuleVM uses\nthe state-of-the-art YOLOWorld model to detect objects in video frames, and\nassociation rules are identified through data mining methods as descriptions of\nthe video. Leveraging the dual-branch architecture, RuleVM achieves\ninterpretable coarse-grained and fine-grained violence surveillance. Extensive\nexperiments were conducted on two commonly used benchmarks, and the results\nshow that RuleVM achieved the best performance in both coarse-grained and\nfinegrained monitoring, significantly outperforming existing state-ofthe-art\nmethods. Moreover, interpretability experiments uncovered some interesting\nrules, such as the observation that as the number of people increases, the risk\nlevel of violent behavior also rises."
                },
                "authors": [
                    {
                        "name": "Wen-Dong Jiang"
                    },
                    {
                        "name": "Chih-Yung Chang"
                    },
                    {
                        "name": "Ssu-Chi Kuai"
                    },
                    {
                        "name": "Diptendu Sinha Roy"
                    }
                ],
                "author_detail": {
                    "name": "Diptendu Sinha Roy"
                },
                "author": "Diptendu Sinha Roy",
                "arxiv_comment": "12 pages,7 figures IEEE TSMCA (Under review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21991v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21991v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12514v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12514v4",
                "updated": "2024-11-14T12:03:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    12,
                    3,
                    37,
                    3,
                    319,
                    0
                ],
                "published": "2024-09-19T07:10:18Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    7,
                    10,
                    18,
                    3,
                    263,
                    0
                ],
                "title": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Jinming Li"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Zhiyuan Xu"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Ran Cheng"
                    },
                    {
                        "name": "Chaomin Shen"
                    },
                    {
                        "name": "Yaxin Peng"
                    },
                    {
                        "name": "Feifei Feng"
                    },
                    {
                        "name": "Jian Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Tang"
                },
                "author": "Jian Tang",
                "arxiv_comment": "add more citations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12514v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12514v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07940v3",
                "updated": "2024-11-14T11:51:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    11,
                    51,
                    0,
                    3,
                    319,
                    0
                ],
                "published": "2024-03-11T02:06:30Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    2,
                    6,
                    30,
                    0,
                    71,
                    0
                ],
                "title": "InfiBench: Evaluating the Question-Answering Capabilities of Code Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiBench: Evaluating the Question-Answering Capabilities of Code Large\n  Language Models"
                },
                "summary": "Large Language Models for code (code LLMs) have witnessed tremendous progress\nin recent years. With the rapid development of code LLMs, many popular\nevaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to\nmeasure the performance of code LLMs with a particular focus on code generation\ntasks. However, they are insufficient to cover the full range of expected\ncapabilities of code LLMs, which span beyond code generation to answering\ndiverse coding-related questions. To fill this gap, we propose InfiBench, the\nfirst large-scale freeform question-answering (QA) benchmark for code to our\nknowledge, comprising 234 carefully selected high-quality Stack Overflow\nquestions that span across 15 programming languages. InfiBench uses four types\nof model-free automatic metrics to evaluate response correctness where domain\nexperts carefully concretize the criterion for each question. We conduct a\nsystematic evaluation for over 100 latest code LLMs on InfiBench, leading to a\nseries of novel and insightful findings. Our detailed analyses showcase\npotential directions for further advancement of code LLMs. InfiBench is fully\nopen source at https://infi-coder.github.io/infibench and continuously\nexpanding to foster more scientific and systematic practices for code LLM\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for code (code LLMs) have witnessed tremendous progress\nin recent years. With the rapid development of code LLMs, many popular\nevaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to\nmeasure the performance of code LLMs with a particular focus on code generation\ntasks. However, they are insufficient to cover the full range of expected\ncapabilities of code LLMs, which span beyond code generation to answering\ndiverse coding-related questions. To fill this gap, we propose InfiBench, the\nfirst large-scale freeform question-answering (QA) benchmark for code to our\nknowledge, comprising 234 carefully selected high-quality Stack Overflow\nquestions that span across 15 programming languages. InfiBench uses four types\nof model-free automatic metrics to evaluate response correctness where domain\nexperts carefully concretize the criterion for each question. We conduct a\nsystematic evaluation for over 100 latest code LLMs on InfiBench, leading to a\nseries of novel and insightful findings. Our detailed analyses showcase\npotential directions for further advancement of code LLMs. InfiBench is fully\nopen source at https://infi-coder.github.io/infibench and continuously\nexpanding to foster more scientific and systematic practices for code LLM\nevaluation."
                },
                "authors": [
                    {
                        "name": "Linyi Li"
                    },
                    {
                        "name": "Shijie Geng"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Yibo He"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Ziyue Hua"
                    },
                    {
                        "name": "Guanghan Ning"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "arxiv_comment": "31 pages. Appear at NeurIPS 2024 Datasets and Benchmarks track.\n  Project website: https://infi-coder.github.io/infibench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00341v2",
                "updated": "2024-11-14T11:30:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    11,
                    30,
                    49,
                    3,
                    319,
                    0
                ],
                "published": "2024-08-01T07:25:15Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    25,
                    15,
                    3,
                    214,
                    0
                ],
                "title": "Enhancing Attack Resilience in Real-Time Systems through Variable\n  Control Task Sampling Rates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Attack Resilience in Real-Time Systems through Variable\n  Control Task Sampling Rates"
                },
                "summary": "Cyber-physical systems (CPSs) in modern real-time applications integrate\nnumerous control units linked through communication networks, each responsible\nfor executing a mix of real-time safety-critical and non-critical tasks. To\nensure predictable timing behaviour, most safety-critical tasks are scheduled\nwith fixed sampling periods, which supports rigorous safety and performance\nanalyses. However, this deterministic execution can be exploited by attackers\nto launch inference-based attacks on safety-critical tasks. This paper\naddresses the challenge of preventing such timing inference or schedule-based\nattacks by dynamically adjusting the execution rates of safety-critical tasks\nwhile maintaining their performance. We propose a novel schedule vulnerability\nanalysis methodology, enabling runtime switching between valid schedules for\nvarious control task sampling rates. Leveraging this approach, we present the\nMulti-Rate Attack-Aware Randomized Scheduling (MAARS) framework for preemptive\nfixed-priority schedulers, designed to reduce the success rate of timing\ninference attacks on real-time systems. To our knowledge, this is the first\nmethod that combines attack-aware schedule randomization with preserved control\nand scheduling integrity. The framework's efficacy in attack prevention is\nevaluated on automotive benchmarks using a Hardware-in-the-Loop (HiL) setup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber-physical systems (CPSs) in modern real-time applications integrate\nnumerous control units linked through communication networks, each responsible\nfor executing a mix of real-time safety-critical and non-critical tasks. To\nensure predictable timing behaviour, most safety-critical tasks are scheduled\nwith fixed sampling periods, which supports rigorous safety and performance\nanalyses. However, this deterministic execution can be exploited by attackers\nto launch inference-based attacks on safety-critical tasks. This paper\naddresses the challenge of preventing such timing inference or schedule-based\nattacks by dynamically adjusting the execution rates of safety-critical tasks\nwhile maintaining their performance. We propose a novel schedule vulnerability\nanalysis methodology, enabling runtime switching between valid schedules for\nvarious control task sampling rates. Leveraging this approach, we present the\nMulti-Rate Attack-Aware Randomized Scheduling (MAARS) framework for preemptive\nfixed-priority schedulers, designed to reduce the success rate of timing\ninference attacks on real-time systems. To our knowledge, this is the first\nmethod that combines attack-aware schedule randomization with preserved control\nand scheduling integrity. The framework's efficacy in attack prevention is\nevaluated on automotive benchmarks using a Hardware-in-the-Loop (HiL) setup."
                },
                "authors": [
                    {
                        "name": "Arkaprava Sain"
                    },
                    {
                        "name": "Sunandan Adhikary"
                    },
                    {
                        "name": "Ipsita Koley"
                    },
                    {
                        "name": "Soumyajit Dey"
                    }
                ],
                "author_detail": {
                    "name": "Soumyajit Dey"
                },
                "author": "Soumyajit Dey",
                "arxiv_comment": "12 pages including references, Total 10 figures (with 3 having\n  subfigures)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10959v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10959v4",
                "updated": "2024-11-14T11:23:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    11,
                    23,
                    50,
                    3,
                    319,
                    0
                ],
                "published": "2024-07-15T17:55:36Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    55,
                    36,
                    0,
                    197,
                    0
                ],
                "title": "A Unified Probabilistic Approach to Traffic Conflict Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Probabilistic Approach to Traffic Conflict Detection"
                },
                "summary": "Traffic conflict detection is essential for proactive road safety by\nidentifying potential collisions before they occur. Existing methods rely on\nsurrogate safety measures tailored to specific interactions (e.g.,\ncar-following, side-swiping, or path-crossing) and require varying thresholds\nin different traffic conditions. This variation leads to inconsistencies and\nlimited adaptability of conflict detection in evolving traffic environments.\nConsequently, a need persists for consistent detection of traffic conflicts\nacross interaction contexts. To address this need, this study proposes a\nunified probabilistic approach. The proposed approach establishes a unified\nframework of traffic conflict detection, where traffic conflicts are formulated\nas context-dependent extreme events of road user interactions. The detection of\nconflicts is then decomposed into a series of statistical learning tasks:\nrepresenting interaction contexts, inferring proximity distributions, and\nassessing extreme collision risk. The unified formulation accommodates diverse\nhypotheses of traffic conflicts and the learning tasks enable data-driven\nanalysis of factors such as motion states of road users, environment\nconditions, and participant characteristics. Jointly, this approach supports\nconsistent and comprehensive evaluation of the collision risk emerging in road\nuser interactions. Our experiments using real-world trajectory data show that\nthe approach provides effective collision warnings, generalises across distinct\ndatasets and traffic environments, covers a broad range of conflict types, and\ncaptures a long-tailed distribution of conflict intensity. The findings\nhighlight its potential to enhance the safety assessment of traffic\ninfrastructures and policies, improve collision warning systems for autonomous\ndriving, and deepen the understanding of road user behaviour in safety-critical\ninteractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic conflict detection is essential for proactive road safety by\nidentifying potential collisions before they occur. Existing methods rely on\nsurrogate safety measures tailored to specific interactions (e.g.,\ncar-following, side-swiping, or path-crossing) and require varying thresholds\nin different traffic conditions. This variation leads to inconsistencies and\nlimited adaptability of conflict detection in evolving traffic environments.\nConsequently, a need persists for consistent detection of traffic conflicts\nacross interaction contexts. To address this need, this study proposes a\nunified probabilistic approach. The proposed approach establishes a unified\nframework of traffic conflict detection, where traffic conflicts are formulated\nas context-dependent extreme events of road user interactions. The detection of\nconflicts is then decomposed into a series of statistical learning tasks:\nrepresenting interaction contexts, inferring proximity distributions, and\nassessing extreme collision risk. The unified formulation accommodates diverse\nhypotheses of traffic conflicts and the learning tasks enable data-driven\nanalysis of factors such as motion states of road users, environment\nconditions, and participant characteristics. Jointly, this approach supports\nconsistent and comprehensive evaluation of the collision risk emerging in road\nuser interactions. Our experiments using real-world trajectory data show that\nthe approach provides effective collision warnings, generalises across distinct\ndatasets and traffic environments, covers a broad range of conflict types, and\ncaptures a long-tailed distribution of conflict intensity. The findings\nhighlight its potential to enhance the safety assessment of traffic\ninfrastructures and policies, improve collision warning systems for autonomous\ndriving, and deepen the understanding of road user behaviour in safety-critical\ninteractions."
                },
                "authors": [
                    {
                        "name": "Yiru Jiao"
                    },
                    {
                        "name": "Simeon C. Calvert"
                    },
                    {
                        "name": "Sander van Cranenburgh"
                    },
                    {
                        "name": "Hans van Lint"
                    }
                ],
                "author_detail": {
                    "name": "Hans van Lint"
                },
                "author": "Hans van Lint",
                "arxiv_comment": "21 pages, 10 figures, under revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10959v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10959v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09369v1",
                "updated": "2024-11-14T11:23:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    11,
                    23,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T11:23:04Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    11,
                    23,
                    4,
                    3,
                    319,
                    0
                ],
                "title": "Low-Energy Cosmic Rays and Associated MeV Gamma-Ray Emissions in the\n  Protoplanetary System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Energy Cosmic Rays and Associated MeV Gamma-Ray Emissions in the\n  Protoplanetary System"
                },
                "summary": "Low-energy cosmic rays (LECRs) play a crucial role in the formation of\nplanetary systems, and detecting and reconstructing the properties of early\nLECRs is essential for understanding the mechanisms of planetary system\nformation. Given that LECRs interact with the surrounding medium to produce\nnuclear de-excitation line emissions, which are gamma-ray emissions with energy\nmainly within 0.1--10 MeV and are unaffected by stellar wind modulation, these\nemissions can accurately reflect the properties of LECRs. This study introduces\nan innovative method for using gamma-ray emissions to infer LECR properties. We\nemployed the Parker transport equation to simulate the propagation and spectral\nevolution of LECRs in a protoplanetary disk and calculated the characteristic\ngamma-ray emissions resulting from interactions between LECRs and disk\nmaterial. These gamma-ray emissions encapsulate the spectral information of\nLECRs, providing a powerful tool to reconstruct the cosmic ray environment at\nthat time. This method, supported by further theoretical developments and\nobservations, will fundamentally enhance our understanding of the impact of CRs\non the origin and evolution of planetary systems and address significant\nscientific questions regarding the cosmic ray environment at the origin of\nlife.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-energy cosmic rays (LECRs) play a crucial role in the formation of\nplanetary systems, and detecting and reconstructing the properties of early\nLECRs is essential for understanding the mechanisms of planetary system\nformation. Given that LECRs interact with the surrounding medium to produce\nnuclear de-excitation line emissions, which are gamma-ray emissions with energy\nmainly within 0.1--10 MeV and are unaffected by stellar wind modulation, these\nemissions can accurately reflect the properties of LECRs. This study introduces\nan innovative method for using gamma-ray emissions to infer LECR properties. We\nemployed the Parker transport equation to simulate the propagation and spectral\nevolution of LECRs in a protoplanetary disk and calculated the characteristic\ngamma-ray emissions resulting from interactions between LECRs and disk\nmaterial. These gamma-ray emissions encapsulate the spectral information of\nLECRs, providing a powerful tool to reconstruct the cosmic ray environment at\nthat time. This method, supported by further theoretical developments and\nobservations, will fundamentally enhance our understanding of the impact of CRs\non the origin and evolution of planetary systems and address significant\nscientific questions regarding the cosmic ray environment at the origin of\nlife."
                },
                "authors": [
                    {
                        "name": "Xulei Sun"
                    },
                    {
                        "name": "Shuying Zheng"
                    },
                    {
                        "name": "Zhaodong Shi"
                    },
                    {
                        "name": "Bing Liu"
                    },
                    {
                        "name": "Ruizhi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ruizhi Yang"
                },
                "author": "Ruizhi Yang",
                "arxiv_doi": "10.3390/universe10080310",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/universe10080310",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.09369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published by Universe. 13 pages, 3 figures, 1 table",
                "arxiv_journal_ref": "Universe 2024, 10(8), 310",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18406v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18406v2",
                "updated": "2024-11-14T10:55:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    55,
                    14,
                    3,
                    319,
                    0
                ],
                "published": "2024-06-26T14:57:38Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    14,
                    57,
                    38,
                    2,
                    178,
                    0
                ],
                "title": "IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying\n  and Reweighting Context-Aware Neurons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying\n  and Reweighting Context-Aware Neurons"
                },
                "summary": "It is widely acknowledged that large language models (LLMs) encode a vast\nreservoir of knowledge after being trained on mass data. Recent studies\ndisclose knowledge conflicts in LLM generation, wherein outdated or incorrect\nparametric knowledge (i.e., encoded knowledge) contradicts new knowledge\nprovided in the context. To mitigate such knowledge conflicts, we propose a\nnovel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to\ncapitalize on neurons that are crucial in processing contextual cues.\nSpecifically, IRCAN first identifies neurons that significantly contribute to\ncontext processing, utilizing a context-aware attribution score derived from\nintegrated gradients. Subsequently, the identified context-aware neurons are\nstrengthened via reweighting. In doing so, we steer LLMs to generate\ncontext-sensitive outputs with respect to the new knowledge provided in the\ncontext. Extensive experiments conducted across a variety of models and tasks\ndemonstrate that IRCAN not only achieves remarkable improvements in handling\nknowledge conflicts but also offers a scalable, plug-and-play solution that can\nbe integrated seamlessly with existing models. Our codes are released at\nhttps://github.com/danshi777/IRCAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is widely acknowledged that large language models (LLMs) encode a vast\nreservoir of knowledge after being trained on mass data. Recent studies\ndisclose knowledge conflicts in LLM generation, wherein outdated or incorrect\nparametric knowledge (i.e., encoded knowledge) contradicts new knowledge\nprovided in the context. To mitigate such knowledge conflicts, we propose a\nnovel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to\ncapitalize on neurons that are crucial in processing contextual cues.\nSpecifically, IRCAN first identifies neurons that significantly contribute to\ncontext processing, utilizing a context-aware attribution score derived from\nintegrated gradients. Subsequently, the identified context-aware neurons are\nstrengthened via reweighting. In doing so, we steer LLMs to generate\ncontext-sensitive outputs with respect to the new knowledge provided in the\ncontext. Extensive experiments conducted across a variety of models and tasks\ndemonstrate that IRCAN not only achieves remarkable improvements in handling\nknowledge conflicts but also offers a scalable, plug-and-play solution that can\nbe integrated seamlessly with existing models. Our codes are released at\nhttps://github.com/danshi777/IRCAN."
                },
                "authors": [
                    {
                        "name": "Dan Shi"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Tianhao Shen"
                    },
                    {
                        "name": "Weilong Dong"
                    },
                    {
                        "name": "Xinwei Wu"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18406v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18406v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09341v1",
                "updated": "2024-11-14T10:37:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    37,
                    34,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T10:37:34Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    37,
                    34,
                    3,
                    319,
                    0
                ],
                "title": "Approximated Variational Bayesian Inverse Reinforcement Learning for\n  Large Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximated Variational Bayesian Inverse Reinforcement Learning for\n  Large Language Model Alignment"
                },
                "summary": "The alignment of large language models (LLMs) is crucial for generating\nhelpful and harmless content. Existing approaches leverage preference-based\nhuman feedback data to learn the reward function and align the LLM with the\nfeedback data. However, these approaches focus on modeling the reward\ndifference between the chosen and rejected demonstrations, rather than directly\nmodeling the true reward from each demonstration. Moreover, these approaches\nassume that the reward is only obtained at the end of the sentence, which\noverlooks the modeling of intermediate rewards. These issues lead to\ninsufficient use of training signals in the feedback data, limiting the\nrepresentation and generalization ability of the reward and potentially\nresulting in reward hacking. In this paper, we formulate LLM alignment as a\nBayesian Inverse Reinforcement Learning (BIRL) problem and propose a novel\ntraining objective, Approximated Variational Alignment (AVA), to perform LLM\nalignment through Approximated Variational Reward Imitation Learning (AVRIL).\nThe BIRL formulation facilitates intermediate reward modeling and direct reward\nmodeling on each single demonstration, which enhances the utilization of\ntraining signals in the feedback data. Experiments show that AVA outperforms\nexisting LLM alignment approaches in reward modeling, RL fine-tuning, and\ndirect optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of large language models (LLMs) is crucial for generating\nhelpful and harmless content. Existing approaches leverage preference-based\nhuman feedback data to learn the reward function and align the LLM with the\nfeedback data. However, these approaches focus on modeling the reward\ndifference between the chosen and rejected demonstrations, rather than directly\nmodeling the true reward from each demonstration. Moreover, these approaches\nassume that the reward is only obtained at the end of the sentence, which\noverlooks the modeling of intermediate rewards. These issues lead to\ninsufficient use of training signals in the feedback data, limiting the\nrepresentation and generalization ability of the reward and potentially\nresulting in reward hacking. In this paper, we formulate LLM alignment as a\nBayesian Inverse Reinforcement Learning (BIRL) problem and propose a novel\ntraining objective, Approximated Variational Alignment (AVA), to perform LLM\nalignment through Approximated Variational Reward Imitation Learning (AVRIL).\nThe BIRL formulation facilitates intermediate reward modeling and direct reward\nmodeling on each single demonstration, which enhances the utilization of\ntraining signals in the feedback data. Experiments show that AVA outperforms\nexisting LLM alignment approaches in reward modeling, RL fine-tuning, and\ndirect optimization."
                },
                "authors": [
                    {
                        "name": "Yuang Cai"
                    },
                    {
                        "name": "Yuyu Yuan"
                    },
                    {
                        "name": "Jinsheng Shi"
                    },
                    {
                        "name": "Qinhong Lin"
                    }
                ],
                "author_detail": {
                    "name": "Qinhong Lin"
                },
                "author": "Qinhong Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09339v1",
                "updated": "2024-11-14T10:36:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    36,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T10:36:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    36,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Re-Parameterization of Lightweight Transformer for On-Device Speech\n  Emotion Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-Parameterization of Lightweight Transformer for On-Device Speech\n  Emotion Recognition"
                },
                "summary": "With the increasing implementation of machine learning models on edge or\nInternet-of-Things (IoT) devices, deploying advanced models on\nresource-constrained IoT devices remains challenging. Transformer models, a\ncurrently dominant neural architecture, have achieved great success in broad\ndomains but their complexity hinders its deployment on IoT devices with limited\ncomputation capability and storage size. Although many model compression\napproaches have been explored, they often suffer from notorious performance\ndegradation. To address this issue, we introduce a new method, namely\nTransformer Re-parameterization, to boost the performance of lightweight\nTransformer models. It consists of two processes: the High-Rank Factorization\n(HRF) process in the training stage and the deHigh-Rank Factorization (deHRF)\nprocess in the inference stage. In the former process, we insert an additional\nlinear layer before the Feed-Forward Network (FFN) of the lightweight\nTransformer. It is supposed that the inserted HRF layers can enhance the model\nlearning capability. In the later process, the auxiliary HRF layer will be\nmerged together with the following FFN layer into one linear layer and thus\nrecover the original structure of the lightweight model. To examine the\neffectiveness of the proposed method, we evaluate it on three widely used\nTransformer variants, i.e., ConvTransformer, Conformer, and SpeechFormer\nnetworks, in the application of speech emotion recognition on the IEMOCAP, M3ED\nand DAIC-WOZ datasets. Experimental results show that our proposed method\nconsistently improves the performance of lightweight Transformers, even making\nthem comparable to large models. The proposed re-parameterization approach\nenables advanced Transformer models to be deployed on resource-constrained IoT\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing implementation of machine learning models on edge or\nInternet-of-Things (IoT) devices, deploying advanced models on\nresource-constrained IoT devices remains challenging. Transformer models, a\ncurrently dominant neural architecture, have achieved great success in broad\ndomains but their complexity hinders its deployment on IoT devices with limited\ncomputation capability and storage size. Although many model compression\napproaches have been explored, they often suffer from notorious performance\ndegradation. To address this issue, we introduce a new method, namely\nTransformer Re-parameterization, to boost the performance of lightweight\nTransformer models. It consists of two processes: the High-Rank Factorization\n(HRF) process in the training stage and the deHigh-Rank Factorization (deHRF)\nprocess in the inference stage. In the former process, we insert an additional\nlinear layer before the Feed-Forward Network (FFN) of the lightweight\nTransformer. It is supposed that the inserted HRF layers can enhance the model\nlearning capability. In the later process, the auxiliary HRF layer will be\nmerged together with the following FFN layer into one linear layer and thus\nrecover the original structure of the lightweight model. To examine the\neffectiveness of the proposed method, we evaluate it on three widely used\nTransformer variants, i.e., ConvTransformer, Conformer, and SpeechFormer\nnetworks, in the application of speech emotion recognition on the IEMOCAP, M3ED\nand DAIC-WOZ datasets. Experimental results show that our proposed method\nconsistently improves the performance of lightweight Transformers, even making\nthem comparable to large models. The proposed re-parameterization approach\nenables advanced Transformer models to be deployed on resource-constrained IoT\ndevices."
                },
                "authors": [
                    {
                        "name": "Zixing Zhang"
                    },
                    {
                        "name": "Zhongren Dong"
                    },
                    {
                        "name": "Weixiang Xu"
                    },
                    {
                        "name": "Jing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jing Han"
                },
                "author": "Jing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09328v1",
                "updated": "2024-11-14T10:19:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    19,
                    55,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T10:19:55Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    19,
                    55,
                    3,
                    319,
                    0
                ],
                "title": "A Flexible Framework for Grant-Free Random Access in Cell-Free Massive\n  MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Flexible Framework for Grant-Free Random Access in Cell-Free Massive\n  MIMO Systems"
                },
                "summary": "We propose a novel generalized framework for grant-free random-access (GFRA)\nin cell-free massive multiple input multiple-output systems where multiple\ngeographically separated access points (APs) or base stations (BSs) aim to\ndetect sporadically active user-equipment (UEs). Unlike a conventional\narchitecture in which all the active UEs transmit their signature or pilot\nsequences of equal length, we admit a flexible pilot length for each UE, which\nalso enables a seamless integration into conventional grant-based wireless\nsystems. We formulate the joint UE activity detection and the distributed\nchannel estimation as a sparse support and signal recovery problem, and\ndescribe a Bayesian learning procedure to solve it. We develop a scheme to fuse\nthe posterior statistics of the latent variables inferred by each AP to jointly\ndetect the UEs' activities, and utilize them to further refine the channel\nestimates. In addition, we allude to an interesting point which enables this\nflexible GFRA framework to encode the information bits from the active UEs. We\nnumerically evaluate the normalized mean square error and the probability of\nmiss-detection performances obtained by the Bayesian algorithm and show that\nthe latent-variable fusion enhances the detection and the channel estimation\nperformances by a large margin. We also benchmark against a genie-aided\nalgorithm which has a prior knowledge of the UEs' activities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel generalized framework for grant-free random-access (GFRA)\nin cell-free massive multiple input multiple-output systems where multiple\ngeographically separated access points (APs) or base stations (BSs) aim to\ndetect sporadically active user-equipment (UEs). Unlike a conventional\narchitecture in which all the active UEs transmit their signature or pilot\nsequences of equal length, we admit a flexible pilot length for each UE, which\nalso enables a seamless integration into conventional grant-based wireless\nsystems. We formulate the joint UE activity detection and the distributed\nchannel estimation as a sparse support and signal recovery problem, and\ndescribe a Bayesian learning procedure to solve it. We develop a scheme to fuse\nthe posterior statistics of the latent variables inferred by each AP to jointly\ndetect the UEs' activities, and utilize them to further refine the channel\nestimates. In addition, we allude to an interesting point which enables this\nflexible GFRA framework to encode the information bits from the active UEs. We\nnumerically evaluate the normalized mean square error and the probability of\nmiss-detection performances obtained by the Bayesian algorithm and show that\nthe latent-variable fusion enhances the detection and the channel estimation\nperformances by a large margin. We also benchmark against a genie-aided\nalgorithm which has a prior knowledge of the UEs' activities."
                },
                "authors": [
                    {
                        "name": "Sai Subramanyam Thoota"
                    },
                    {
                        "name": "Erik G. Larsson"
                    }
                ],
                "author_detail": {
                    "name": "Erik G. Larsson"
                },
                "author": "Erik G. Larsson",
                "arxiv_comment": "Published in the Proceedings of SPAWC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09318v1",
                "updated": "2024-11-14T10:00:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    0,
                    33,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T10:00:33Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    0,
                    33,
                    3,
                    319,
                    0
                ],
                "title": "DriveThru: a Document Extraction Platform and Benchmark Datasets for\n  Indonesian Local Language Archives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriveThru: a Document Extraction Platform and Benchmark Datasets for\n  Indonesian Local Language Archives"
                },
                "summary": "Indonesia is one of the most diverse countries linguistically. However,\ndespite this linguistic diversity, Indonesian languages remain underrepresented\nin Natural Language Processing (NLP) research and technologies. In the past two\nyears, several efforts have been conducted to construct NLP resources for\nIndonesian languages. However, most of these efforts have been focused on\ncreating manual resources thus difficult to scale to more languages. Although\nmany Indonesian languages do not have a web presence, locally there are\nresources that document these languages well in printed forms such as books,\nmagazines, and newspapers. Digitizing these existing resources will enable\nscaling of Indonesian language resource construction to many more languages. In\nthis paper, we propose an alternative method of creating datasets by digitizing\ndocuments, which have not previously been used to build digital language\nresources in Indonesia. DriveThru is a platform for extracting document content\nutilizing Optical Character Recognition (OCR) techniques in its system to\nprovide language resource building with less manual effort and cost. This paper\nalso studies the utility of current state-of-the-art LLM for post-OCR\ncorrection to show the capability of increasing the character accuracy rate\n(CAR) and word accuracy rate (WAR) compared to off-the-shelf OCR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indonesia is one of the most diverse countries linguistically. However,\ndespite this linguistic diversity, Indonesian languages remain underrepresented\nin Natural Language Processing (NLP) research and technologies. In the past two\nyears, several efforts have been conducted to construct NLP resources for\nIndonesian languages. However, most of these efforts have been focused on\ncreating manual resources thus difficult to scale to more languages. Although\nmany Indonesian languages do not have a web presence, locally there are\nresources that document these languages well in printed forms such as books,\nmagazines, and newspapers. Digitizing these existing resources will enable\nscaling of Indonesian language resource construction to many more languages. In\nthis paper, we propose an alternative method of creating datasets by digitizing\ndocuments, which have not previously been used to build digital language\nresources in Indonesia. DriveThru is a platform for extracting document content\nutilizing Optical Character Recognition (OCR) techniques in its system to\nprovide language resource building with less manual effort and cost. This paper\nalso studies the utility of current state-of-the-art LLM for post-OCR\ncorrection to show the capability of increasing the character accuracy rate\n(CAR) and word accuracy rate (WAR) compared to off-the-shelf OCR."
                },
                "authors": [
                    {
                        "name": "MohammadRifqi Farhansyah"
                    },
                    {
                        "name": "Muhammad Zuhdi Fikri Johari"
                    },
                    {
                        "name": "Afinzaki Amiral"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Kumara Ari Yuana"
                    },
                    {
                        "name": "Derry Tanti Wijaya"
                    }
                ],
                "author_detail": {
                    "name": "Derry Tanti Wijaya"
                },
                "author": "Derry Tanti Wijaya",
                "arxiv_comment": "12 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09312v1",
                "updated": "2024-11-14T09:38:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    38,
                    58,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:38:58Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    38,
                    58,
                    3,
                    319,
                    0
                ],
                "title": "Approximate Probabilistic Inference forTime-Series Data A Robust Latent\n  Gaussian Model With Temporal Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Probabilistic Inference forTime-Series Data A Robust Latent\n  Gaussian Model With Temporal Awareness"
                },
                "summary": "The development of robust generative models for highly varied non-stationary\ntime series data is a complex yet important problem. Traditional models for\ntime series data prediction, such as Long Short-Term Memory (LSTM), are\ninefficient and generalize poorly as they cannot capture complex temporal\nrelationships. In this paper, we present a probabilistic generative model that\ncan be trained to capture temporal information, and that is robust to data\nerrors. We call it Time Deep Latent Gaussian Model (tDLGM). Its novel\narchitecture is inspired by Deep Latent Gaussian Model (DLGM). Our model is\ntrained to minimize a loss function based on the negative log loss. One\ncontributing factor to Time Deep Latent Gaussian Model (tDLGM) robustness is\nour regularizer, which accounts for data trends. Experiments conducted show\nthat tDLGM is able to reconstruct and generate complex time series data, and\nthat it is robust against to noise and faulty data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of robust generative models for highly varied non-stationary\ntime series data is a complex yet important problem. Traditional models for\ntime series data prediction, such as Long Short-Term Memory (LSTM), are\ninefficient and generalize poorly as they cannot capture complex temporal\nrelationships. In this paper, we present a probabilistic generative model that\ncan be trained to capture temporal information, and that is robust to data\nerrors. We call it Time Deep Latent Gaussian Model (tDLGM). Its novel\narchitecture is inspired by Deep Latent Gaussian Model (DLGM). Our model is\ntrained to minimize a loss function based on the negative log loss. One\ncontributing factor to Time Deep Latent Gaussian Model (tDLGM) robustness is\nour regularizer, which accounts for data trends. Experiments conducted show\nthat tDLGM is able to reconstruct and generate complex time series data, and\nthat it is robust against to noise and faulty data."
                },
                "authors": [
                    {
                        "name": "Anton Johansson"
                    },
                    {
                        "name": "Arunselvan Ramaswamy"
                    }
                ],
                "author_detail": {
                    "name": "Arunselvan Ramaswamy"
                },
                "author": "Arunselvan Ramaswamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05521v2",
                "updated": "2024-11-14T09:28:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    28,
                    49,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-08T12:27:13Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    27,
                    13,
                    4,
                    313,
                    0
                ],
                "title": "SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark"
                },
                "summary": "Electronic health records (EHRs) are stored in various database systems with\ndifferent database models on heterogeneous storage architectures, such as\nrelational databases, document stores, or graph databases. These different\ndatabase models have a big impact on query complexity and performance. While\nthis has been a known fact in database research, its implications for the\ngrowing number of Text-to-Query systems have surprisingly not been investigated\nso far. In this paper, we present SM3-Text-to-Query, the first multi-model\nmedical Text-to-Query benchmark based on synthetic patient data from Synthea,\nfollowing the SNOMED-CT taxonomy -- a widely used knowledge graph ontology\ncovering medical terminology. SM3-Text-to-Query provides data representations\nfor relational databases (PostgreSQL), document stores (MongoDB), and graph\ndatabases (Neo4j and GraphDB (RDF)), allowing the evaluation across four\npopular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically\nand manually develop 408 template questions, which we augment to construct a\nbenchmark of 10K diverse natural language question/query pairs for these four\nquery languages (40K pairs overall). On our dataset, we evaluate several common\nin-context-learning (ICL) approaches for a set of representative closed and\nopen-source LLMs. Our evaluation sheds light on the trade-offs between database\nmodels and query languages for different ICL strategies and LLMs. Last,\nSM3-Text-to-Query is easily extendable to additional query languages or real,\nstandard-based patient databases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic health records (EHRs) are stored in various database systems with\ndifferent database models on heterogeneous storage architectures, such as\nrelational databases, document stores, or graph databases. These different\ndatabase models have a big impact on query complexity and performance. While\nthis has been a known fact in database research, its implications for the\ngrowing number of Text-to-Query systems have surprisingly not been investigated\nso far. In this paper, we present SM3-Text-to-Query, the first multi-model\nmedical Text-to-Query benchmark based on synthetic patient data from Synthea,\nfollowing the SNOMED-CT taxonomy -- a widely used knowledge graph ontology\ncovering medical terminology. SM3-Text-to-Query provides data representations\nfor relational databases (PostgreSQL), document stores (MongoDB), and graph\ndatabases (Neo4j and GraphDB (RDF)), allowing the evaluation across four\npopular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically\nand manually develop 408 template questions, which we augment to construct a\nbenchmark of 10K diverse natural language question/query pairs for these four\nquery languages (40K pairs overall). On our dataset, we evaluate several common\nin-context-learning (ICL) approaches for a set of representative closed and\nopen-source LLMs. Our evaluation sheds light on the trade-offs between database\nmodels and query languages for different ICL strategies and LLMs. Last,\nSM3-Text-to-Query is easily extendable to additional query languages or real,\nstandard-based patient databases."
                },
                "authors": [
                    {
                        "name": "Sithursan Sivasubramaniam"
                    },
                    {
                        "name": "Cedric Osei-Akoto"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Kurt Stockinger"
                    },
                    {
                        "name": "Jonathan Fuerst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Fuerst"
                },
                "author": "Jonathan Fuerst",
                "arxiv_comment": "NeurIPS 2024 Track Datasets and Benchmarks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14979v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14979v4",
                "updated": "2024-11-14T09:17:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    17,
                    48,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-19T05:01:56Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    5,
                    1,
                    56,
                    5,
                    293,
                    0
                ],
                "title": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration From Cognitive Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration From Cognitive Psychology"
                },
                "summary": "The cognitive mechanism by which Large Language Models (LLMs) solve\nmathematical problems remains a widely debated and unresolved issue. Currently,\nthere is little interpretable experimental evidence that connects LLMs'\nproblem-solving with human cognitive psychology.To determine if LLMs possess\nhuman-like mathematical reasoning, we modified the problems used in the human\nCognitive Reflection Test (CRT). Our results show that, even with the use of\nChains of Thought (CoT) prompts, mainstream LLMs, including the latest o1 model\n(noted for its reasoning capabilities), have a high error rate when solving\nthese modified CRT problems. Specifically, the average accuracy rate dropped by\nup to 50% compared to the original questions.Further analysis of LLMs'\nincorrect answers suggests that they primarily rely on pattern matching from\ntheir training data, which aligns more with human intuition (System 1 thinking)\nrather than with human-like reasoning (System 2 thinking). This finding\nchallenges the belief that LLMs have genuine mathematical reasoning abilities\ncomparable to humans. As a result, this work may adjust overly optimistic views\non LLMs' progress towards artificial general intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cognitive mechanism by which Large Language Models (LLMs) solve\nmathematical problems remains a widely debated and unresolved issue. Currently,\nthere is little interpretable experimental evidence that connects LLMs'\nproblem-solving with human cognitive psychology.To determine if LLMs possess\nhuman-like mathematical reasoning, we modified the problems used in the human\nCognitive Reflection Test (CRT). Our results show that, even with the use of\nChains of Thought (CoT) prompts, mainstream LLMs, including the latest o1 model\n(noted for its reasoning capabilities), have a high error rate when solving\nthese modified CRT problems. Specifically, the average accuracy rate dropped by\nup to 50% compared to the original questions.Further analysis of LLMs'\nincorrect answers suggests that they primarily rely on pattern matching from\ntheir training data, which aligns more with human intuition (System 1 thinking)\nrather than with human-like reasoning (System 2 thinking). This finding\nchallenges the belief that LLMs have genuine mathematical reasoning abilities\ncomparable to humans. As a result, this work may adjust overly optimistic views\non LLMs' progress towards artificial general intelligence."
                },
                "authors": [
                    {
                        "name": "Wei Xie"
                    },
                    {
                        "name": "Shuoyoucheng Ma"
                    },
                    {
                        "name": "Zhenhua Wang"
                    },
                    {
                        "name": "Enze Wang"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Xiaobing Sun"
                    },
                    {
                        "name": "Baosheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Baosheng Wang"
                },
                "author": "Baosheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14979v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14979v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21862v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21862v2",
                "updated": "2024-11-14T09:17:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    17,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-29T08:56:29Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    8,
                    56,
                    29,
                    1,
                    303,
                    0
                ],
                "title": "Hierarchical mixtures of Unigram models for short text clustering: the\n  role of Beta-Liouville priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical mixtures of Unigram models for short text clustering: the\n  role of Beta-Liouville priors"
                },
                "summary": "This paper presents a variant of the Multinomial mixture model tailored for\nthe unsupervised classification of short text data. Traditionally, the\nMultinomial probability vector in this hierarchical model is assigned a\nDirichlet prior distribution. Here, however, we explore an alternative\nprior--the Beta-Liouville distribution--which offers a more flexible\ncorrelation structure than the Dirichlet. We examine the theoretical properties\nof the Beta-Liouville distribution, focusing on its conjugacy with the\nMultinomial likelihood. This property enables the derivation of update\nequations for a CAVI (Coordinate Ascent Variational Inference) variational\nalgorithm, facilitating the approximate posterior estimation of model\nparameters. Additionally, we propose a stochastic variant of the CAVI algorithm\nthat enhances scalability. The paper concludes with data examples that\ndemonstrate effective strategies for setting the Beta-Liouville\nhyperparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a variant of the Multinomial mixture model tailored for\nthe unsupervised classification of short text data. Traditionally, the\nMultinomial probability vector in this hierarchical model is assigned a\nDirichlet prior distribution. Here, however, we explore an alternative\nprior--the Beta-Liouville distribution--which offers a more flexible\ncorrelation structure than the Dirichlet. We examine the theoretical properties\nof the Beta-Liouville distribution, focusing on its conjugacy with the\nMultinomial likelihood. This property enables the derivation of update\nequations for a CAVI (Coordinate Ascent Variational Inference) variational\nalgorithm, facilitating the approximate posterior estimation of model\nparameters. Additionally, we propose a stochastic variant of the CAVI algorithm\nthat enhances scalability. The paper concludes with data examples that\ndemonstrate effective strategies for setting the Beta-Liouville\nhyperparameters."
                },
                "authors": [
                    {
                        "name": "Massimo Bilancia"
                    },
                    {
                        "name": "Samuele Magro"
                    }
                ],
                "author_detail": {
                    "name": "Samuele Magro"
                },
                "author": "Samuele Magro",
                "arxiv_comment": "32 pages, 4 figures. Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21862v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21862v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09297v1",
                "updated": "2024-11-14T09:16:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    16,
                    48,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:16:48Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    16,
                    48,
                    3,
                    319,
                    0
                ],
                "title": "DTELS: Towards Dynamic Granularity of Timeline Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DTELS: Towards Dynamic Granularity of Timeline Summarization"
                },
                "summary": "The rapid proliferation of online news has posed significant challenges in\ntracking the continuous development of news topics. Traditional timeline\nsummarization constructs a chronological summary of the events but often lacks\nthe flexibility to meet the diverse granularity needs. To overcome this\nlimitation, we introduce a new paradigm, Dynamic-granularity TimELine\nSummarization, (DTELS), which aims to construct adaptive timelines based on\nuser instructions or requirements. This paper establishes a comprehensive\nbenchmark for DTLES that includes: (1) an evaluation framework grounded in\njournalistic standards to assess the timeline quality across four dimensions:\nInformativeness, Granular Consistency, Factuality, and Coherence; (2) a\nlarge-scale, multi-source dataset with multiple granularity timeline\nannotations based on a consensus process to facilitate authority; (3) extensive\nexperiments and analysis with two proposed solutions based on Large Language\nModels (LLMs) and existing state-of-the-art TLS methods. The experimental\nresults demonstrate the effectiveness of LLM-based solutions. However, even the\nmost advanced LLMs struggle to consistently generate timelines that are both\ninformative and granularly consistent, highlighting the challenges of the DTELS\ntask.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of online news has posed significant challenges in\ntracking the continuous development of news topics. Traditional timeline\nsummarization constructs a chronological summary of the events but often lacks\nthe flexibility to meet the diverse granularity needs. To overcome this\nlimitation, we introduce a new paradigm, Dynamic-granularity TimELine\nSummarization, (DTELS), which aims to construct adaptive timelines based on\nuser instructions or requirements. This paper establishes a comprehensive\nbenchmark for DTLES that includes: (1) an evaluation framework grounded in\njournalistic standards to assess the timeline quality across four dimensions:\nInformativeness, Granular Consistency, Factuality, and Coherence; (2) a\nlarge-scale, multi-source dataset with multiple granularity timeline\nannotations based on a consensus process to facilitate authority; (3) extensive\nexperiments and analysis with two proposed solutions based on Large Language\nModels (LLMs) and existing state-of-the-art TLS methods. The experimental\nresults demonstrate the effectiveness of LLM-based solutions. However, even the\nmost advanced LLMs struggle to consistently generate timelines that are both\ninformative and granularly consistent, highlighting the challenges of the DTELS\ntask."
                },
                "authors": [
                    {
                        "name": "Chenlong Zhang"
                    },
                    {
                        "name": "Tong Zhou"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Zhuoran Jin"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.01607v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.01607v3",
                "updated": "2024-11-14T09:04:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    4,
                    1,
                    3,
                    319,
                    0
                ],
                "published": "2023-02-03T09:10:31Z",
                "published_parsed": [
                    2023,
                    2,
                    3,
                    9,
                    10,
                    31,
                    4,
                    34,
                    0
                ],
                "title": "dynamite: An R Package for Dynamic Multivariate Panel Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dynamite: An R Package for Dynamic Multivariate Panel Models"
                },
                "summary": "dynamite is an R package for Bayesian inference of intensive panel (time\nseries) data comprising multiple measurements per multiple individuals measured\nin time. The package supports joint modeling of multiple response variables,\ntime-varying and time-invariant effects, a wide range of discrete and\ncontinuous distributions, group-specific random effects, latent factors, and\ncustomization of prior distributions of the model parameters. Models in the\npackage are defined via a user-friendly formula interface, and estimation of\nthe posterior distribution of the model parameters takes advantage of\nstate-of-the-art Markov chain Monte Carlo methods. The package enables\nefficient computation of both individual-level and aggregated predictions and\noffers a comprehensive suite of tools for visualization and model diagnostics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dynamite is an R package for Bayesian inference of intensive panel (time\nseries) data comprising multiple measurements per multiple individuals measured\nin time. The package supports joint modeling of multiple response variables,\ntime-varying and time-invariant effects, a wide range of discrete and\ncontinuous distributions, group-specific random effects, latent factors, and\ncustomization of prior distributions of the model parameters. Models in the\npackage are defined via a user-friendly formula interface, and estimation of\nthe posterior distribution of the model parameters takes advantage of\nstate-of-the-art Markov chain Monte Carlo methods. The package enables\nefficient computation of both individual-level and aggregated predictions and\noffers a comprehensive suite of tools for visualization and model diagnostics."
                },
                "authors": [
                    {
                        "name": "Santtu Tikka"
                    },
                    {
                        "name": "Jouni Helske"
                    }
                ],
                "author_detail": {
                    "name": "Jouni Helske"
                },
                "author": "Jouni Helske",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.01607v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.01607v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09289v1",
                "updated": "2024-11-14T09:03:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    3,
                    54,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:03:54Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    3,
                    54,
                    3,
                    319,
                    0
                ],
                "title": "StreamAdapter: Efficient Test Time Adaptation from Contextual Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAdapter: Efficient Test Time Adaptation from Contextual Streams"
                },
                "summary": "In-context learning (ICL) allows large language models (LLMs) to adapt to new\ntasks directly from the given demonstrations without requiring gradient\nupdates. While recent advances have expanded context windows to accommodate\nmore demonstrations, this approach increases inference costs without\nnecessarily improving performance. To mitigate these issues, We propose\nStreamAdapter, a novel approach that directly updates model parameters from\ncontext at test time, eliminating the need for explicit in-context\ndemonstrations. StreamAdapter employs context mapping and weight absorption\nmechanisms to dynamically transform ICL demonstrations into parameter updates\nwith minimal additional parameters. By reducing reliance on numerous in-context\nexamples, StreamAdapter significantly reduce inference costs and allows for\nefficient inference with constant time complexity, regardless of demonstration\ncount. Extensive experiments across diverse tasks and model architectures\ndemonstrate that StreamAdapter achieves comparable or superior adaptation\ncapability to ICL while requiring significantly fewer demonstrations. The\nsuperior task adaptation and context encoding capabilities of StreamAdapter on\nboth language understanding and generation tasks provides a new perspective for\nadapting LLMs at test time using context, allowing for more efficient\nadaptation across scenarios and more cost-effective inference",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) allows large language models (LLMs) to adapt to new\ntasks directly from the given demonstrations without requiring gradient\nupdates. While recent advances have expanded context windows to accommodate\nmore demonstrations, this approach increases inference costs without\nnecessarily improving performance. To mitigate these issues, We propose\nStreamAdapter, a novel approach that directly updates model parameters from\ncontext at test time, eliminating the need for explicit in-context\ndemonstrations. StreamAdapter employs context mapping and weight absorption\nmechanisms to dynamically transform ICL demonstrations into parameter updates\nwith minimal additional parameters. By reducing reliance on numerous in-context\nexamples, StreamAdapter significantly reduce inference costs and allows for\nefficient inference with constant time complexity, regardless of demonstration\ncount. Extensive experiments across diverse tasks and model architectures\ndemonstrate that StreamAdapter achieves comparable or superior adaptation\ncapability to ICL while requiring significantly fewer demonstrations. The\nsuperior task adaptation and context encoding capabilities of StreamAdapter on\nboth language understanding and generation tasks provides a new perspective for\nadapting LLMs at test time using context, allowing for more efficient\nadaptation across scenarios and more cost-effective inference"
                },
                "authors": [
                    {
                        "name": "Dilxat Muhtar"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Yaming Yang"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Yadong Lu"
                    },
                    {
                        "name": "Jianfeng Liu"
                    },
                    {
                        "name": "Yuefeng Zhan"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Weiwei Deng"
                    },
                    {
                        "name": "Feng Sun"
                    },
                    {
                        "name": "Xueliang Zhang"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "22 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09287v1",
                "updated": "2024-11-14T08:55:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    55,
                    14,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:55:14Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    55,
                    14,
                    3,
                    319,
                    0
                ],
                "title": "The Communication-Friendly Privacy-Preserving Machine Learning against\n  Malicious Adversaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Communication-Friendly Privacy-Preserving Machine Learning against\n  Malicious Adversaries"
                },
                "summary": "With the increasing emphasis on privacy regulations, such as GDPR, protecting\nindividual privacy and ensuring compliance have become critical concerns for\nboth individuals and organizations. Privacy-preserving machine learning (PPML)\nis an innovative approach that allows for secure data analysis while\nsafeguarding sensitive information. It enables organizations to extract\nvaluable insights from data without compromising privacy. Secure multi-party\ncomputation (MPC) is a key tool in PPML, as it allows multiple parties to\njointly compute functions without revealing their private inputs, making it\nessential in multi-server environments. We address the performance overhead of\nexisting maliciously secure protocols, particularly in finite rings like\n$\\mathbb{Z}_{2^\\ell}$, by introducing an efficient protocol for secure linear\nfunction evaluation. We implement our maliciously secure MPC protocol on GPUs,\nsignificantly improving its efficiency and scalability. We extend the protocol\nto handle linear and non-linear layers, ensuring compatibility with a wide\nrange of machine-learning models. Finally, we comprehensively evaluate machine\nlearning models by integrating our protocol into the workflow, enabling secure\nand efficient inference across simple and complex models, such as convolutional\nneural networks (CNNs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing emphasis on privacy regulations, such as GDPR, protecting\nindividual privacy and ensuring compliance have become critical concerns for\nboth individuals and organizations. Privacy-preserving machine learning (PPML)\nis an innovative approach that allows for secure data analysis while\nsafeguarding sensitive information. It enables organizations to extract\nvaluable insights from data without compromising privacy. Secure multi-party\ncomputation (MPC) is a key tool in PPML, as it allows multiple parties to\njointly compute functions without revealing their private inputs, making it\nessential in multi-server environments. We address the performance overhead of\nexisting maliciously secure protocols, particularly in finite rings like\n$\\mathbb{Z}_{2^\\ell}$, by introducing an efficient protocol for secure linear\nfunction evaluation. We implement our maliciously secure MPC protocol on GPUs,\nsignificantly improving its efficiency and scalability. We extend the protocol\nto handle linear and non-linear layers, ensuring compatibility with a wide\nrange of machine-learning models. Finally, we comprehensively evaluate machine\nlearning models by integrating our protocol into the workflow, enabling secure\nand efficient inference across simple and complex models, such as convolutional\nneural networks (CNNs)."
                },
                "authors": [
                    {
                        "name": "Tianpei Lu"
                    },
                    {
                        "name": "Bingsheng Zhang"
                    },
                    {
                        "name": "Lichun Li"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02367v2",
                "updated": "2024-11-14T08:39:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    39,
                    54,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-03T10:25:23Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    25,
                    23,
                    3,
                    277,
                    0
                ],
                "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference\n  Acceleration"
                },
                "summary": "The transformer architecture predominates across various models. As the heart\nof the transformer, attention has a computational complexity of O(N^2),\ncompared to O(N) for linear transformations. When handling large sequence\nlengths, attention becomes the primary time-consuming component. Although\nquantization has proven to be an effective method for accelerating model\ninference, existing quantization methods primarily focus on optimizing the\nlinear layer. In response, we first analyze the feasibility of quantization in\nattention detailedly. Following that, we propose SageAttention, a highly\nefficient and accurate quantization method for attention. The OPS (operations\nper second) of our approach outperforms FlashAttention2 and xformers by about\n2.1 times and 2.7 times, respectively. SageAttention also achieves superior\naccuracy performance over FlashAttention3. Comprehensive experiments confirm\nthat our approach incurs almost no end-to-end metrics loss across diverse\nmodels, including those for large language processing, image generation, and\nvideo generation. The codes are available at\nhttps://github.com/thu-ml/SageAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer architecture predominates across various models. As the heart\nof the transformer, attention has a computational complexity of O(N^2),\ncompared to O(N) for linear transformations. When handling large sequence\nlengths, attention becomes the primary time-consuming component. Although\nquantization has proven to be an effective method for accelerating model\ninference, existing quantization methods primarily focus on optimizing the\nlinear layer. In response, we first analyze the feasibility of quantization in\nattention detailedly. Following that, we propose SageAttention, a highly\nefficient and accurate quantization method for attention. The OPS (operations\nper second) of our approach outperforms FlashAttention2 and xformers by about\n2.1 times and 2.7 times, respectively. SageAttention also achieves superior\naccuracy performance over FlashAttention3. Comprehensive experiments confirm\nthat our approach incurs almost no end-to-end metrics loss across diverse\nmodels, including those for large language processing, image generation, and\nvideo generation. The codes are available at\nhttps://github.com/thu-ml/SageAttention."
                },
                "authors": [
                    {
                        "name": "Jintao Zhang"
                    },
                    {
                        "name": "Jia wei"
                    },
                    {
                        "name": "Haofeng Huang"
                    },
                    {
                        "name": "Pengle Zhang"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Jianfei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Chen"
                },
                "author": "Jianfei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09269v1",
                "updated": "2024-11-14T08:12:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    12,
                    36,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:12:36Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    12,
                    36,
                    3,
                    319,
                    0
                ],
                "title": "Harnessing multiple LLMs for Information Retrieval: A case study on Deep\n  Learning methodologies in Biodiversity publications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing multiple LLMs for Information Retrieval: A case study on Deep\n  Learning methodologies in Biodiversity publications"
                },
                "summary": "Deep Learning (DL) techniques are increasingly applied in scientific studies\nacross various domains to address complex research questions. However, the\nmethodological details of these DL models are often hidden in the unstructured\ntext. As a result, critical information about how these models are designed,\ntrained, and evaluated is challenging to access and comprehend. To address this\nissue, in this work, we use five different open-source Large Language Models\n(LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B,\nand Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG)\napproach to extract and process DL methodological details from scientific\npublications automatically. We built a voting classifier from the outputs of\nfive LLMs to accurately report DL methodological information. We tested our\napproach using biodiversity publications, building upon our previous research.\nTo validate our pipeline, we employed two datasets of DL-related biodiversity\npublications: a curated set of 100 publications from our prior work and a set\nof 364 publications from the Ecological Informatics journal. Our results\ndemonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of\nDL methodological information, achieving an accuracy of 69.5% (417 out of 600\ncomparisons) based solely on textual content from publications. This\nperformance was assessed against human annotators who had access to code,\nfigures, tables, and other supplementary information. Although demonstrated in\nbiodiversity, our methodology is not limited to this field; it can be applied\nacross other scientific domains where detailed methodological reporting is\nessential for advancing knowledge and ensuring reproducibility. This study\npresents a scalable and reliable approach for automating information\nextraction, facilitating better reproducibility and knowledge transfer across\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning (DL) techniques are increasingly applied in scientific studies\nacross various domains to address complex research questions. However, the\nmethodological details of these DL models are often hidden in the unstructured\ntext. As a result, critical information about how these models are designed,\ntrained, and evaluated is challenging to access and comprehend. To address this\nissue, in this work, we use five different open-source Large Language Models\n(LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B,\nand Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG)\napproach to extract and process DL methodological details from scientific\npublications automatically. We built a voting classifier from the outputs of\nfive LLMs to accurately report DL methodological information. We tested our\napproach using biodiversity publications, building upon our previous research.\nTo validate our pipeline, we employed two datasets of DL-related biodiversity\npublications: a curated set of 100 publications from our prior work and a set\nof 364 publications from the Ecological Informatics journal. Our results\ndemonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of\nDL methodological information, achieving an accuracy of 69.5% (417 out of 600\ncomparisons) based solely on textual content from publications. This\nperformance was assessed against human annotators who had access to code,\nfigures, tables, and other supplementary information. Although demonstrated in\nbiodiversity, our methodology is not limited to this field; it can be applied\nacross other scientific domains where detailed methodological reporting is\nessential for advancing knowledge and ensuring reproducibility. This study\npresents a scalable and reliable approach for automating information\nextraction, facilitating better reproducibility and knowledge transfer across\nstudies."
                },
                "authors": [
                    {
                        "name": "Vamsi Krishna Kommineni"
                    },
                    {
                        "name": "Birgitta König-Ries"
                    },
                    {
                        "name": "Sheeba Samuel"
                    }
                ],
                "author_detail": {
                    "name": "Sheeba Samuel"
                },
                "author": "Sheeba Samuel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09266v1",
                "updated": "2024-11-14T08:07:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    7,
                    2,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:07:02Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    7,
                    2,
                    3,
                    319,
                    0
                ],
                "title": "How Good is ChatGPT at Audiovisual Deepfake Detection: A Comparative\n  Study of ChatGPT, AI Models and Human Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Good is ChatGPT at Audiovisual Deepfake Detection: A Comparative\n  Study of ChatGPT, AI Models and Human Perception"
                },
                "summary": "Multimodal deepfakes involving audiovisual manipulations are a growing threat\nbecause they are difficult to detect with the naked eye or using unimodal deep\nlearningbased forgery detection methods. Audiovisual forensic models, while\nmore capable than unimodal models, require large training datasets and are\ncomputationally expensive for training and inference. Furthermore, these models\nlack interpretability and often do not generalize well to unseen manipulations.\nIn this study, we examine the detection capabilities of a large language model\n(LLM) (i.e., ChatGPT) to identify and account for any possible visual and\nauditory artifacts and manipulations in audiovisual deepfake content. Extensive\nexperiments are conducted on videos from a benchmark multimodal deepfake\ndataset to evaluate the detection performance of ChatGPT and compare it with\nthe detection capabilities of state-of-the-art multimodal forensic models and\nhumans. Experimental results demonstrate the importance of domain knowledge and\nprompt engineering for video forgery detection tasks using LLMs. Unlike\napproaches based on end-to-end learning, ChatGPT can account for spatial and\nspatiotemporal artifacts and inconsistencies that may exist within or across\nmodalities. Additionally, we discuss the limitations of ChatGPT for multimedia\nforensic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal deepfakes involving audiovisual manipulations are a growing threat\nbecause they are difficult to detect with the naked eye or using unimodal deep\nlearningbased forgery detection methods. Audiovisual forensic models, while\nmore capable than unimodal models, require large training datasets and are\ncomputationally expensive for training and inference. Furthermore, these models\nlack interpretability and often do not generalize well to unseen manipulations.\nIn this study, we examine the detection capabilities of a large language model\n(LLM) (i.e., ChatGPT) to identify and account for any possible visual and\nauditory artifacts and manipulations in audiovisual deepfake content. Extensive\nexperiments are conducted on videos from a benchmark multimodal deepfake\ndataset to evaluate the detection performance of ChatGPT and compare it with\nthe detection capabilities of state-of-the-art multimodal forensic models and\nhumans. Experimental results demonstrate the importance of domain knowledge and\nprompt engineering for video forgery detection tasks using LLMs. Unlike\napproaches based on end-to-end learning, ChatGPT can account for spatial and\nspatiotemporal artifacts and inconsistencies that may exist within or across\nmodalities. Additionally, we discuss the limitations of ChatGPT for multimedia\nforensic tasks."
                },
                "authors": [
                    {
                        "name": "Sahibzada Adil Shahzad"
                    },
                    {
                        "name": "Ammarah Hashmi"
                    },
                    {
                        "name": "Yan-Tsung Peng"
                    },
                    {
                        "name": "Yu Tsao"
                    },
                    {
                        "name": "Hsin-Min Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hsin-Min Wang"
                },
                "author": "Hsin-Min Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09261v1",
                "updated": "2024-11-14T07:58:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    58,
                    44,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T07:58:44Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    58,
                    44,
                    3,
                    319,
                    0
                ],
                "title": "Automating Autograding: Large Language Models as Test Suite Generators\n  for Introductory Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Autograding: Large Language Models as Test Suite Generators\n  for Introductory Programming"
                },
                "summary": "Automatically graded programming assignments provide instant feedback to\nstudents and significantly reduce manual grading time for instructors. However,\ncreating comprehensive suites of test cases for programming problems within\nautomatic graders can be time-consuming and complex. The effort needed to\ndefine test suites may deter some instructors from creating additional problems\nor lead to inadequate test coverage, potentially resulting in misleading\nfeedback on student solutions. Such limitations may reduce student access to\nthe well-documented benefits of timely feedback when learning programming.\n  In this work, we evaluate the effectiveness of using Large Language Models\n(LLMs), as part of a larger workflow, to automatically generate test suites for\nCS1-level programming problems. Each problem's statement and reference solution\nare provided to GPT-4 to produce a test suite that can be used by an\nautograder. We evaluate our proposed approach using a sample of 26 problems,\nand more than 25,000 attempted solutions to those problems, submitted by\nstudents in an introductory programming course. We compare the performance of\nthe LLM-generated test suites against the instructor-created test suites for\neach problem. Our findings reveal that LLM-generated test suites can correctly\nidentify most valid solutions, and for most problems are at least as\ncomprehensive as the instructor test suites. Additionally, the LLM-generated\ntest suites exposed ambiguities in some problem statements, underscoring their\npotential to improve both autograding and instructional design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically graded programming assignments provide instant feedback to\nstudents and significantly reduce manual grading time for instructors. However,\ncreating comprehensive suites of test cases for programming problems within\nautomatic graders can be time-consuming and complex. The effort needed to\ndefine test suites may deter some instructors from creating additional problems\nor lead to inadequate test coverage, potentially resulting in misleading\nfeedback on student solutions. Such limitations may reduce student access to\nthe well-documented benefits of timely feedback when learning programming.\n  In this work, we evaluate the effectiveness of using Large Language Models\n(LLMs), as part of a larger workflow, to automatically generate test suites for\nCS1-level programming problems. Each problem's statement and reference solution\nare provided to GPT-4 to produce a test suite that can be used by an\nautograder. We evaluate our proposed approach using a sample of 26 problems,\nand more than 25,000 attempted solutions to those problems, submitted by\nstudents in an introductory programming course. We compare the performance of\nthe LLM-generated test suites against the instructor-created test suites for\neach problem. Our findings reveal that LLM-generated test suites can correctly\nidentify most valid solutions, and for most problems are at least as\ncomprehensive as the instructor test suites. Additionally, the LLM-generated\ntest suites exposed ambiguities in some problem statements, underscoring their\npotential to improve both autograding and instructional design."
                },
                "authors": [
                    {
                        "name": "Umar Alkafaween"
                    },
                    {
                        "name": "Ibrahim Albluwi"
                    },
                    {
                        "name": "Paul Denny"
                    }
                ],
                "author_detail": {
                    "name": "Paul Denny"
                },
                "author": "Paul Denny",
                "arxiv_comment": "Submitted to Journal of Computer Assisted Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10126v2",
                "updated": "2024-11-14T07:43:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    43,
                    14,
                    3,
                    319,
                    0
                ],
                "published": "2023-11-16T13:07:47Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    13,
                    7,
                    47,
                    3,
                    320,
                    0
                ],
                "title": "I&S-ViT: An Inclusive & Stable Method for Pushing the Limit of\n  Post-Training ViTs Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I&S-ViT: An Inclusive & Stable Method for Pushing the Limit of\n  Post-Training ViTs Quantization"
                },
                "summary": "Albeit the scalable performance of vision transformers (ViTs), the dense\ncomputational costs (training & inference) undermine their position in\nindustrial applications. Post-training quantization (PTQ), tuning ViTs with a\ntiny dataset and running in a low-bit format, well addresses the cost issue but\nunluckily bears more performance drops in lower-bit cases. In this paper, we\nintroduce I&S-ViT, a novel method that regulates the PTQ of ViTs in an\ninclusive and stable fashion. I&S-ViT first identifies two issues in the PTQ of\nViTs: (1) Quantization inefficiency in the prevalent log2 quantizer for\npost-Softmax activations; (2) Rugged and magnified loss landscape in\ncoarse-grained quantization granularity for post-LayerNorm activations. Then,\nI&S-ViT addresses these issues by introducing: (1) A novel shift-uniform-log2\nquantizer (SULQ) that incorporates a shift mechanism followed by uniform\nquantization to achieve both an inclusive domain representation and accurate\ndistribution approximation; (2) A three-stage smooth optimization strategy\n(SOS) that amalgamates the strengths of channel-wise and layer-wise\nquantization to enable stable learning. Comprehensive evaluations across\ndiverse vision tasks validate I&S-ViT' superiority over existing PTQ of ViTs\nmethods, particularly in low-bit scenarios. For instance, I&S-ViT elevates the\nperformance of 3-bit ViT-B by an impressive 50.68%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Albeit the scalable performance of vision transformers (ViTs), the dense\ncomputational costs (training & inference) undermine their position in\nindustrial applications. Post-training quantization (PTQ), tuning ViTs with a\ntiny dataset and running in a low-bit format, well addresses the cost issue but\nunluckily bears more performance drops in lower-bit cases. In this paper, we\nintroduce I&S-ViT, a novel method that regulates the PTQ of ViTs in an\ninclusive and stable fashion. I&S-ViT first identifies two issues in the PTQ of\nViTs: (1) Quantization inefficiency in the prevalent log2 quantizer for\npost-Softmax activations; (2) Rugged and magnified loss landscape in\ncoarse-grained quantization granularity for post-LayerNorm activations. Then,\nI&S-ViT addresses these issues by introducing: (1) A novel shift-uniform-log2\nquantizer (SULQ) that incorporates a shift mechanism followed by uniform\nquantization to achieve both an inclusive domain representation and accurate\ndistribution approximation; (2) A three-stage smooth optimization strategy\n(SOS) that amalgamates the strengths of channel-wise and layer-wise\nquantization to enable stable learning. Comprehensive evaluations across\ndiverse vision tasks validate I&S-ViT' superiority over existing PTQ of ViTs\nmethods, particularly in low-bit scenarios. For instance, I&S-ViT elevates the\nperformance of 3-bit ViT-B by an impressive 50.68%."
                },
                "authors": [
                    {
                        "name": "Yunshan Zhong"
                    },
                    {
                        "name": "Jiawei Hu"
                    },
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.10126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09255v1",
                "updated": "2024-11-14T07:41:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    41,
                    34,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T07:41:34Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    41,
                    34,
                    3,
                    319,
                    0
                ],
                "title": "DAHL: Domain-specific Automated Hallucination Evaluation of Long-Form\n  Text through a Benchmark Dataset in Biomedicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAHL: Domain-specific Automated Hallucination Evaluation of Long-Form\n  Text through a Benchmark Dataset in Biomedicine"
                },
                "summary": "We introduce DAHL, a benchmark dataset and automated evaluation system\ndesigned to assess hallucination in long-form text generation, specifically\nwithin the biomedical domain. Our benchmark dataset, meticulously curated from\nbiomedical research papers, consists of 8,573 questions across 29 categories.\nDAHL evaluates fact-conflicting hallucinations in Large Language Models (LLMs)\nby deconstructing responses into atomic units, each representing a single piece\nof information. The accuracy of these responses is averaged to produce the DAHL\nScore, offering a more in-depth evaluation of hallucinations compared to\nprevious methods that rely on multiple-choice tasks. We conduct experiments\nwith 8 different models, finding that larger models tend to hallucinate less;\nhowever, beyond a model size of 7 to 8 billion parameters, further scaling does\nnot significantly improve factual accuracy. The DAHL Score holds potential as\nan efficient alternative to human-annotated preference labels, being able to be\nexpanded to other specialized domains. We release the dataset and code in\npublic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DAHL, a benchmark dataset and automated evaluation system\ndesigned to assess hallucination in long-form text generation, specifically\nwithin the biomedical domain. Our benchmark dataset, meticulously curated from\nbiomedical research papers, consists of 8,573 questions across 29 categories.\nDAHL evaluates fact-conflicting hallucinations in Large Language Models (LLMs)\nby deconstructing responses into atomic units, each representing a single piece\nof information. The accuracy of these responses is averaged to produce the DAHL\nScore, offering a more in-depth evaluation of hallucinations compared to\nprevious methods that rely on multiple-choice tasks. We conduct experiments\nwith 8 different models, finding that larger models tend to hallucinate less;\nhowever, beyond a model size of 7 to 8 billion parameters, further scaling does\nnot significantly improve factual accuracy. The DAHL Score holds potential as\nan efficient alternative to human-annotated preference labels, being able to be\nexpanded to other specialized domains. We release the dataset and code in\npublic."
                },
                "authors": [
                    {
                        "name": "Jean Seo"
                    },
                    {
                        "name": "Jongwon Lim"
                    },
                    {
                        "name": "Dongjun Jang"
                    },
                    {
                        "name": "Hyopil Shin"
                    }
                ],
                "author_detail": {
                    "name": "Hyopil Shin"
                },
                "author": "Hyopil Shin",
                "arxiv_comment": "EMNLP2024/FEVER",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09249v1",
                "updated": "2024-11-14T07:28:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    28,
                    9,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T07:28:09Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    28,
                    9,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Financial Domain Adaptation of Language Models via Model\n  Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Financial Domain Adaptation of Language Models via Model\n  Augmentation"
                },
                "summary": "The domain adaptation of language models, including large language models\n(LLMs), has become increasingly important as the use of such models continues\nto expand. This study demonstrates the effectiveness of Composition to Augment\nLanguage Models (CALM) in adapting to the financial domain. CALM is a model to\nextend the capabilities of existing models by introducing cross-attention\nbetween two LLMs with different functions. In our experiments, we developed a\nCALM to enhance the financial performance of an LLM with strong response\ncapabilities by leveraging a financial-specialized LLM. Notably, the CALM was\ntrained using a financial dataset different from the one used to train the\nfinancial-specialized LLM, confirming CALM's ability to adapt to various\ndatasets. The models were evaluated through quantitative Japanese financial\nbenchmarks and qualitative response comparisons, demonstrating that CALM\nenables superior responses with higher scores than the original models and\nbaselines. Additionally, comparative experiments on connection points revealed\nthat connecting the middle layers of the models is most effective in\nfacilitating adaptation to the financial domain. These findings confirm that\nCALM is a practical approach for adapting LLMs to the financial domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The domain adaptation of language models, including large language models\n(LLMs), has become increasingly important as the use of such models continues\nto expand. This study demonstrates the effectiveness of Composition to Augment\nLanguage Models (CALM) in adapting to the financial domain. CALM is a model to\nextend the capabilities of existing models by introducing cross-attention\nbetween two LLMs with different functions. In our experiments, we developed a\nCALM to enhance the financial performance of an LLM with strong response\ncapabilities by leveraging a financial-specialized LLM. Notably, the CALM was\ntrained using a financial dataset different from the one used to train the\nfinancial-specialized LLM, confirming CALM's ability to adapt to various\ndatasets. The models were evaluated through quantitative Japanese financial\nbenchmarks and qualitative response comparisons, demonstrating that CALM\nenables superior responses with higher scores than the original models and\nbaselines. Additionally, comparative experiments on connection points revealed\nthat connecting the middle layers of the models is most effective in\nfacilitating adaptation to the financial domain. These findings confirm that\nCALM is a practical approach for adapting LLMs to the financial domain."
                },
                "authors": [
                    {
                        "name": "Kota Tanabe"
                    },
                    {
                        "name": "Masanori Hirano"
                    },
                    {
                        "name": "Kazuki Matoya"
                    },
                    {
                        "name": "Kentaro Imajo"
                    },
                    {
                        "name": "Hiroki Sakaji"
                    },
                    {
                        "name": "Itsuki Noda"
                    }
                ],
                "author_detail": {
                    "name": "Itsuki Noda"
                },
                "author": "Itsuki Noda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09248v1",
                "updated": "2024-11-14T07:28:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    28,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T07:28:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    28,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Constraint on Lorentz Invariance Violation for spectral lag transition\n  in GRB 160625B using profile likelihood",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint on Lorentz Invariance Violation for spectral lag transition\n  in GRB 160625B using profile likelihood"
                },
                "summary": "We reanalyze the spectral lag data for of GRB 160625B using frequentist\ninference to constrain the energy scale ($E_{QG}$) of Lorentz Invariance\nViolation (LIV). For this purpose, we use profile likelihood to deal with the\nastrophysical nuisance parameters. This is in contrast to Bayesian inference\nimplemented in previous works, where marginalization was carried out over the\nnuisance parameters. We show that with profile likelihood, we do not find a\nglobal minimum for $\\chi^2$ as a function of $E_{QG}$ below the Planck scale\nfor both the linear and quadratic models of LIV, whereas bounded credible\nintervals were obtained using Bayesian inference. Therefore, we can set lower\nlimits in a straightforward manner. We find that $E_{QG} \\geq 3.7 \\times\n10^{16}$ GeV and $E_{QG} \\geq 2.6 \\times 10^7$ GeV at 68\\% c.l., for linear and\nquadratic LIV, respectively. Therefore, this is the first proof of principles\napplication of profile likelihood method to the analysis of GRB spectral lag\ndata to constrain LIV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We reanalyze the spectral lag data for of GRB 160625B using frequentist\ninference to constrain the energy scale ($E_{QG}$) of Lorentz Invariance\nViolation (LIV). For this purpose, we use profile likelihood to deal with the\nastrophysical nuisance parameters. This is in contrast to Bayesian inference\nimplemented in previous works, where marginalization was carried out over the\nnuisance parameters. We show that with profile likelihood, we do not find a\nglobal minimum for $\\chi^2$ as a function of $E_{QG}$ below the Planck scale\nfor both the linear and quadratic models of LIV, whereas bounded credible\nintervals were obtained using Bayesian inference. Therefore, we can set lower\nlimits in a straightforward manner. We find that $E_{QG} \\geq 3.7 \\times\n10^{16}$ GeV and $E_{QG} \\geq 2.6 \\times 10^7$ GeV at 68\\% c.l., for linear and\nquadratic LIV, respectively. Therefore, this is the first proof of principles\napplication of profile likelihood method to the analysis of GRB spectral lag\ndata to constrain LIV."
                },
                "authors": [
                    {
                        "name": "Shantanu Desai"
                    },
                    {
                        "name": "Shalini Ganguly"
                    }
                ],
                "author_detail": {
                    "name": "Shalini Ganguly"
                },
                "author": "Shalini Ganguly",
                "arxiv_comment": "4 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09242v1",
                "updated": "2024-11-14T07:16:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    16,
                    23,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T07:16:23Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    16,
                    23,
                    3,
                    319,
                    0
                ],
                "title": "FluidML: Fast and Memory Efficient Inference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FluidML: Fast and Memory Efficient Inference Optimization"
                },
                "summary": "Machine learning models deployed on edge devices have enabled numerous\nexciting new applications, such as humanoid robots, AR glasses, and autonomous\nvehicles. However, the computing resources available on these edge devices are\nnot catching up with the ever-growing number of parameters in these models. As\nthe models become bigger and more complicated, the novel yet sophisticated\nstructure challenges the inference runtime optimization. We present FluidML, a\ngeneric runtime memory management and optimization framework that can flexibly\ntransform the model execution blueprint to achieve faster and more\nmemory-efficient inference. Evaluations across different platforms show that\nFluidML can consistently reduce the end-to-end inference latency by up to\n25.38% for popular language models and reduce peak memory usage by up to\n41.47%, compared to state-of-the-art approaches. FluidML is of ~30K line of\ncodes, built for general-purpose usage, and will be released as an open-source\ninference runtime optimization framework to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models deployed on edge devices have enabled numerous\nexciting new applications, such as humanoid robots, AR glasses, and autonomous\nvehicles. However, the computing resources available on these edge devices are\nnot catching up with the ever-growing number of parameters in these models. As\nthe models become bigger and more complicated, the novel yet sophisticated\nstructure challenges the inference runtime optimization. We present FluidML, a\ngeneric runtime memory management and optimization framework that can flexibly\ntransform the model execution blueprint to achieve faster and more\nmemory-efficient inference. Evaluations across different platforms show that\nFluidML can consistently reduce the end-to-end inference latency by up to\n25.38% for popular language models and reduce peak memory usage by up to\n41.47%, compared to state-of-the-art approaches. FluidML is of ~30K line of\ncodes, built for general-purpose usage, and will be released as an open-source\ninference runtime optimization framework to the community."
                },
                "authors": [
                    {
                        "name": "Jinjie Liu"
                    },
                    {
                        "name": "Hang Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Qiu"
                },
                "author": "Hang Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10880v2",
                "updated": "2024-11-14T07:01:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    1,
                    7,
                    3,
                    319,
                    0
                ],
                "published": "2024-06-16T10:04:19Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    10,
                    4,
                    19,
                    6,
                    168,
                    0
                ],
                "title": "Exploring the Potential of Multimodal LLM with Knowledge-Intensive\n  Multimodal ASR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Multimodal LLM with Knowledge-Intensive\n  Multimodal ASR"
                },
                "summary": "Recent advancements in multimodal large language models (MLLMs) have made\nsignificant progress in integrating information across various modalities, yet\nreal-world applications in educational and scientific domains remain\nchallenging. This paper introduces the Multimodal Scientific ASR (MS-ASR) task,\nwhich focuses on transcribing scientific conference videos by leveraging visual\ninformation from slides to enhance the accuracy of technical terminologies.\nRealized that traditional metrics like WER fall short in assessing performance\naccurately, prompting the proposal of severity-aware WER (SWER) that considers\nthe content type and severity of ASR errors. We propose the Scientific Vision\nAugmented ASR (SciVASR) framework as a baseline method, enabling MLLMs to\nimprove transcript quality through post-editing. Evaluations of\nstate-of-the-art MLLMs, including GPT-4o, show a 45% improvement over\nspeech-only baselines, highlighting the importance of multimodal information\nintegration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multimodal large language models (MLLMs) have made\nsignificant progress in integrating information across various modalities, yet\nreal-world applications in educational and scientific domains remain\nchallenging. This paper introduces the Multimodal Scientific ASR (MS-ASR) task,\nwhich focuses on transcribing scientific conference videos by leveraging visual\ninformation from slides to enhance the accuracy of technical terminologies.\nRealized that traditional metrics like WER fall short in assessing performance\naccurately, prompting the proposal of severity-aware WER (SWER) that considers\nthe content type and severity of ASR errors. We propose the Scientific Vision\nAugmented ASR (SciVASR) framework as a baseline method, enabling MLLMs to\nimprove transcript quality through post-editing. Evaluations of\nstate-of-the-art MLLMs, including GPT-4o, show a 45% improvement over\nspeech-only baselines, highlighting the importance of multimodal information\nintegration."
                },
                "authors": [
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "arxiv_comment": "Accepted to EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00996v2",
                "updated": "2024-11-14T06:55:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    55,
                    27,
                    3,
                    319,
                    0
                ],
                "published": "2024-07-01T06:22:38Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    6,
                    22,
                    38,
                    0,
                    183,
                    0
                ],
                "title": "Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?"
                },
                "summary": "Small Language Models (SLMs) are generally considered more compact versions\nof large language models (LLMs). This study investigates the ability of SLMs\nwith parameters between 1 and 3 billion to learn, retain, and subsequently\neliminate different types of noise present in the data. Four pre-trained SLMs\nwere utilized for this: Olmo 1B, Qwen1.5 1.8B, Gemma 2B, and Phi2 2.7B. The\nmodels were instruction-tuned on noise-free data and tested using in-context\nexamples to determine if they could learn noise through examples. Subsequently,\nnoise patterns were introduced in instruction tuning to evaluate the noise\nlearning, unlearning, and retention capabilities of the models. Olmo, the\nsmallest model, was highly sensitive to noise, quickly adapting to noisy\npatterns. Phi2 resisted learning character-level and transliteration noise,\nlikely due to its carefully curated, structured, and high-quality pretraining\ndata. Gemma excelled with transliteration noise, likely benefiting from its\nmultilingual pretraining. The findings can be used to develop robust training\nstrategies for SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models (SLMs) are generally considered more compact versions\nof large language models (LLMs). This study investigates the ability of SLMs\nwith parameters between 1 and 3 billion to learn, retain, and subsequently\neliminate different types of noise present in the data. Four pre-trained SLMs\nwere utilized for this: Olmo 1B, Qwen1.5 1.8B, Gemma 2B, and Phi2 2.7B. The\nmodels were instruction-tuned on noise-free data and tested using in-context\nexamples to determine if they could learn noise through examples. Subsequently,\nnoise patterns were introduced in instruction tuning to evaluate the noise\nlearning, unlearning, and retention capabilities of the models. Olmo, the\nsmallest model, was highly sensitive to noise, quickly adapting to noisy\npatterns. Phi2 resisted learning character-level and transliteration noise,\nlikely due to its carefully curated, structured, and high-quality pretraining\ndata. Gemma excelled with transliteration noise, likely benefiting from its\nmultilingual pretraining. The findings can be used to develop robust training\nstrategies for SLMs."
                },
                "authors": [
                    {
                        "name": "Nicy Scaria"
                    },
                    {
                        "name": "Silvester John Joseph Kennedy"
                    },
                    {
                        "name": "Deepak Subramani"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Subramani"
                },
                "author": "Deepak Subramani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06403v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06403v2",
                "updated": "2024-11-14T06:54:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    54,
                    46,
                    3,
                    319,
                    0
                ],
                "published": "2024-01-12T06:54:22Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    6,
                    54,
                    22,
                    4,
                    12,
                    0
                ],
                "title": "Fourier analysis of spatial point processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fourier analysis of spatial point processes"
                },
                "summary": "In this article, we develop comprehensive frequency domain methods for\nestimating and inferring the second-order structure of spatial point processes.\nThe main element here is on utilizing the discrete Fourier transform (DFT) of\nthe point pattern and its tapered counterpart. Under second-order stationarity,\nwe show that both the DFTs and the tapered DFTs are asymptotically jointly\nindependent Gaussian even when the DFTs share the same limiting frequencies.\nBased on these results, we establish an $\\alpha$-mixing central limit theorem\nfor a statistic formulated as a quadratic form of the tapered DFT. As\napplications, we derive the asymptotic distribution of the kernel spectral\ndensity estimator and establish a frequency domain inferential method for\nparametric stationary point processes. For the latter, the resulting model\nparameter estimator is computationally tractable and yields meaningful\ninterpretations even in the case of model misspecification. We investigate the\nfinite sample performance of our estimator through simulations, considering\nscenarios of both correctly specified and misspecified models. Furthermore, we\nextend our proposed DFT-based frequency domain methods to a class of\nnon-stationary spatial point processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we develop comprehensive frequency domain methods for\nestimating and inferring the second-order structure of spatial point processes.\nThe main element here is on utilizing the discrete Fourier transform (DFT) of\nthe point pattern and its tapered counterpart. Under second-order stationarity,\nwe show that both the DFTs and the tapered DFTs are asymptotically jointly\nindependent Gaussian even when the DFTs share the same limiting frequencies.\nBased on these results, we establish an $\\alpha$-mixing central limit theorem\nfor a statistic formulated as a quadratic form of the tapered DFT. As\napplications, we derive the asymptotic distribution of the kernel spectral\ndensity estimator and establish a frequency domain inferential method for\nparametric stationary point processes. For the latter, the resulting model\nparameter estimator is computationally tractable and yields meaningful\ninterpretations even in the case of model misspecification. We investigate the\nfinite sample performance of our estimator through simulations, considering\nscenarios of both correctly specified and misspecified models. Furthermore, we\nextend our proposed DFT-based frequency domain methods to a class of\nnon-stationary spatial point processes."
                },
                "authors": [
                    {
                        "name": "Junho Yang"
                    },
                    {
                        "name": "Yongtao Guan"
                    }
                ],
                "author_detail": {
                    "name": "Yongtao Guan"
                },
                "author": "Yongtao Guan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06403v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06403v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09228v1",
                "updated": "2024-11-14T06:53:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    53,
                    0,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T06:53:00Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    53,
                    0,
                    3,
                    319,
                    0
                ],
                "title": "Injection Attacks Against End-to-End Encrypted Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Injection Attacks Against End-to-End Encrypted Applications"
                },
                "summary": "We explore an emerging threat model for end-to-end (E2E) encrypted\napplications: an adversary sends chosen messages to a target client, thereby\n\"injecting\" adversarial content into the application state. Such state is\nsubsequently encrypted and synchronized to an adversarially-visible storage. By\nobserving the lengths of the resulting cloud-stored ciphertexts, the attacker\nbacks out confidential information. We investigate this injection threat model\nin the context of state-of-the-art encrypted messaging applications that\nsupport E2E encrypted backups. We show proof-of-concept attacks that can\nrecover information about E2E encrypted messages or attachments sent via\nWhatsApp, assuming the ability to compromise the target user's Google or Apple\naccount (which gives access to encrypted backups). We also show weaknesses in\nSignal's encrypted backup design that would allow injection attacks to infer\nmetadata including a target user's number of contacts and conversations, should\nthe adversary somehow obtain access to the user's encrypted Signal backup.\nWhile we do not believe our results should be of immediate concern for users of\nthese messaging applications, our results do suggest that more work is needed\nto build tools that enjoy strong E2E security guarantees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore an emerging threat model for end-to-end (E2E) encrypted\napplications: an adversary sends chosen messages to a target client, thereby\n\"injecting\" adversarial content into the application state. Such state is\nsubsequently encrypted and synchronized to an adversarially-visible storage. By\nobserving the lengths of the resulting cloud-stored ciphertexts, the attacker\nbacks out confidential information. We investigate this injection threat model\nin the context of state-of-the-art encrypted messaging applications that\nsupport E2E encrypted backups. We show proof-of-concept attacks that can\nrecover information about E2E encrypted messages or attachments sent via\nWhatsApp, assuming the ability to compromise the target user's Google or Apple\naccount (which gives access to encrypted backups). We also show weaknesses in\nSignal's encrypted backup design that would allow injection attacks to infer\nmetadata including a target user's number of contacts and conversations, should\nthe adversary somehow obtain access to the user's encrypted Signal backup.\nWhile we do not believe our results should be of immediate concern for users of\nthese messaging applications, our results do suggest that more work is needed\nto build tools that enjoy strong E2E security guarantees."
                },
                "authors": [
                    {
                        "name": "Andrés Fábrega"
                    },
                    {
                        "name": "Carolina Ortega Pérez"
                    },
                    {
                        "name": "Armin Namavari"
                    },
                    {
                        "name": "Ben Nassi"
                    },
                    {
                        "name": "Rachit Agarwal"
                    },
                    {
                        "name": "Thomas Ristenpart"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Ristenpart"
                },
                "author": "Thomas Ristenpart",
                "arxiv_comment": "Published in IEEE Security and Privacy 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03735v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03735v4",
                "updated": "2024-11-14T06:42:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    42,
                    51,
                    3,
                    319,
                    0
                ],
                "published": "2024-01-08T08:54:22Z",
                "published_parsed": [
                    2024,
                    1,
                    8,
                    8,
                    54,
                    22,
                    0,
                    8,
                    0
                ],
                "title": "Language Models Encode the Value of Numbers Linearly",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Encode the Value of Numbers Linearly"
                },
                "summary": "Large language models (LLMs) have exhibited impressive competence in various\ntasks, but their internal mechanisms on mathematical problems are still\nunder-explored. In this paper, we study a fundamental question: how language\nmodels encode the value of numbers, a basic element in math. To study the\nquestion, we construct a synthetic dataset comprising addition problems and\nutilize linear probes to read out input numbers from the hidden states.\nExperimental results support the existence of encoded number values in LLMs on\ndifferent layers, and these values can be extracted via linear probes. Further\nexperiments show that LLMs store their calculation results in a similar manner,\nand we can intervene the output via simple vector additions, proving the causal\nconnection between encoded numbers and language model outputs. Our research\nprovides evidence that LLMs encode the value of numbers linearly, offering\ninsights for better exploring, designing, and utilizing numeric information in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive competence in various\ntasks, but their internal mechanisms on mathematical problems are still\nunder-explored. In this paper, we study a fundamental question: how language\nmodels encode the value of numbers, a basic element in math. To study the\nquestion, we construct a synthetic dataset comprising addition problems and\nutilize linear probes to read out input numbers from the hidden states.\nExperimental results support the existence of encoded number values in LLMs on\ndifferent layers, and these values can be extracted via linear probes. Further\nexperiments show that LLMs store their calculation results in a similar manner,\nand we can intervene the output via simple vector additions, proving the causal\nconnection between encoded numbers and language model outputs. Our research\nprovides evidence that LLMs encode the value of numbers linearly, offering\ninsights for better exploring, designing, and utilizing numeric information in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Fangwei Zhu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "The code and data are available at\n  https://github.com/solitaryzero/NumProbe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03735v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03735v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09224v1",
                "updated": "2024-11-14T06:40:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    40,
                    55,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T06:40:55Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    40,
                    55,
                    3,
                    319,
                    0
                ],
                "title": "Programming with AI: Evaluating ChatGPT, Gemini, AlphaCode, and GitHub\n  Copilot for Programmers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming with AI: Evaluating ChatGPT, Gemini, AlphaCode, and GitHub\n  Copilot for Programmers"
                },
                "summary": "Our everyday lives now heavily rely on artificial intelligence (AI) powered\nlarge language models (LLMs). Like regular users, programmers are also\nbenefiting from the newest large language models. In response to the critical\nrole that AI models play in modern software development, this study presents a\nthorough evaluation of leading programming assistants, including ChatGPT,\nGemini(Bard AI), AlphaCode, and GitHub Copilot. The evaluation is based on\ntasks like natural language processing and code generation accuracy in\ndifferent programming languages like Java, Python and C++. Based on the\nresults, it has emphasized their strengths and weaknesses and the importance of\nfurther modifications to increase the reliability and accuracy of the latest\npopular models. Although these AI assistants illustrate a high level of\nprogress in language understanding and code generation, along with ethical\nconsiderations and responsible usage, they provoke a necessity for discussion.\nWith time, developing more refined AI technology is essential for achieving\nadvanced solutions in various fields, especially with the knowledge of the\nfeature intricacies of these models and their implications. This study offers a\ncomparison of different LLMs and provides essential feedback on the rapidly\nchanging area of AI models. It also emphasizes the need for ethical\ndevelopmental practices to actualize AI models' full potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our everyday lives now heavily rely on artificial intelligence (AI) powered\nlarge language models (LLMs). Like regular users, programmers are also\nbenefiting from the newest large language models. In response to the critical\nrole that AI models play in modern software development, this study presents a\nthorough evaluation of leading programming assistants, including ChatGPT,\nGemini(Bard AI), AlphaCode, and GitHub Copilot. The evaluation is based on\ntasks like natural language processing and code generation accuracy in\ndifferent programming languages like Java, Python and C++. Based on the\nresults, it has emphasized their strengths and weaknesses and the importance of\nfurther modifications to increase the reliability and accuracy of the latest\npopular models. Although these AI assistants illustrate a high level of\nprogress in language understanding and code generation, along with ethical\nconsiderations and responsible usage, they provoke a necessity for discussion.\nWith time, developing more refined AI technology is essential for achieving\nadvanced solutions in various fields, especially with the knowledge of the\nfeature intricacies of these models and their implications. This study offers a\ncomparison of different LLMs and provides essential feedback on the rapidly\nchanging area of AI models. It also emphasizes the need for ethical\ndevelopmental practices to actualize AI models' full potential."
                },
                "authors": [
                    {
                        "name": "Md Kamrul Siam"
                    },
                    {
                        "name": "Huanying Gu"
                    },
                    {
                        "name": "Jerry Q. Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jerry Q. Cheng"
                },
                "author": "Jerry Q. Cheng",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09221v1",
                "updated": "2024-11-14T06:37:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    37,
                    43,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T06:37:43Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    37,
                    43,
                    3,
                    319,
                    0
                ],
                "title": "Difference-in-Differences with Sample Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Difference-in-Differences with Sample Selection"
                },
                "summary": "Endogenous treatment and sample selection are two concomitant sources of\nendogeneity that challenge the validity of causal inference. In this paper, we\nfocus on the partial identification of treatment effects within a standard\ntwo-period difference-in-differences framework when the outcome is observed for\nan endogenously selected subpopulation. The identification strategy embeds\nLee's (2009) bounding approach based on principal stratification, which divides\nthe population into latent subgroups based on selection behaviour in\ncounterfactual treatment states in both periods. We establish identification\nresults for four latent types and illustrate the proposed approach by applying\nit to estimate 1) the effect of a job training program on earnings and 2) the\neffect of a working-from-home policy on employee performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endogenous treatment and sample selection are two concomitant sources of\nendogeneity that challenge the validity of causal inference. In this paper, we\nfocus on the partial identification of treatment effects within a standard\ntwo-period difference-in-differences framework when the outcome is observed for\nan endogenously selected subpopulation. The identification strategy embeds\nLee's (2009) bounding approach based on principal stratification, which divides\nthe population into latent subgroups based on selection behaviour in\ncounterfactual treatment states in both periods. We establish identification\nresults for four latent types and illustrate the proposed approach by applying\nit to estimate 1) the effect of a job training program on earnings and 2) the\neffect of a working-from-home policy on employee performance."
                },
                "authors": [
                    {
                        "name": "Gayani Rathnayake"
                    },
                    {
                        "name": "Akanksha Negi"
                    },
                    {
                        "name": "Otavio Bartalotti"
                    },
                    {
                        "name": "Xueyan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xueyan Zhao"
                },
                "author": "Xueyan Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07032v2",
                "updated": "2024-11-14T06:36:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    36,
                    57,
                    3,
                    319,
                    0
                ],
                "published": "2024-03-11T04:56:10Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    4,
                    56,
                    10,
                    0,
                    71,
                    0
                ],
                "title": "STARFlow: Spatial Temporal Feature Re-embedding with Attentive Learning\n  for Real-world Scene Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STARFlow: Spatial Temporal Feature Re-embedding with Attentive Learning\n  for Real-world Scene Flow"
                },
                "summary": "Scene flow prediction is a crucial underlying task in understanding dynamic\nscenes as it offers fundamental motion information. However, contemporary scene\nflow methods encounter three major challenges. Firstly, flow estimation solely\nbased on local receptive fields lacks long-dependency matching of point pairs.\nTo address this issue, we propose global attentive flow embedding to match\nall-to-all point pairs in both feature space and Euclidean space, providing\nglobal initialization before local refinement. Secondly, there are deformations\nexisting in non-rigid objects after warping, which leads to variations in the\nspatiotemporal relation between the consecutive frames. For a more precise\nestimation of residual flow, a spatial temporal feature re-embedding module is\ndevised to acquire the sequence features after deformation. Furthermore,\nprevious methods perform poor generalization due to the significant domain gap\nbetween the synthesized and LiDAR-scanned datasets. We leverage novel domain\nadaptive losses to effectively bridge the gap of motion inference from\nsynthetic to real-world. Experiments demonstrate that our approach achieves\nstate-of-the-art performance across various datasets, with particularly\noutstanding results on real-world LiDAR-scanned datasets. Our code is available\nat https://github.com/O-VIGIA/StarFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene flow prediction is a crucial underlying task in understanding dynamic\nscenes as it offers fundamental motion information. However, contemporary scene\nflow methods encounter three major challenges. Firstly, flow estimation solely\nbased on local receptive fields lacks long-dependency matching of point pairs.\nTo address this issue, we propose global attentive flow embedding to match\nall-to-all point pairs in both feature space and Euclidean space, providing\nglobal initialization before local refinement. Secondly, there are deformations\nexisting in non-rigid objects after warping, which leads to variations in the\nspatiotemporal relation between the consecutive frames. For a more precise\nestimation of residual flow, a spatial temporal feature re-embedding module is\ndevised to acquire the sequence features after deformation. Furthermore,\nprevious methods perform poor generalization due to the significant domain gap\nbetween the synthesized and LiDAR-scanned datasets. We leverage novel domain\nadaptive losses to effectively bridge the gap of motion inference from\nsynthetic to real-world. Experiments demonstrate that our approach achieves\nstate-of-the-art performance across various datasets, with particularly\noutstanding results on real-world LiDAR-scanned datasets. Our code is available\nat https://github.com/O-VIGIA/StarFlow."
                },
                "authors": [
                    {
                        "name": "Zhiyang Lu"
                    },
                    {
                        "name": "Qinghan Chen"
                    },
                    {
                        "name": "Ming Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ming Cheng"
                },
                "author": "Ming Cheng",
                "arxiv_comment": "This paper was renamed to:\"SSRFlow: Semantic-aware Fusion with\n  Spatial Temporal Re-embedding for Real-world Scene Flow\" [arXiv:2408.07825]\n  and was accepted in 3DV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09217v1",
                "updated": "2024-11-14T06:28:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    28,
                    57,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T06:28:57Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    28,
                    57,
                    3,
                    319,
                    0
                ],
                "title": "SmartInv: Multimodal Learning for Smart Contract Invariant Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartInv: Multimodal Learning for Smart Contract Invariant Inference"
                },
                "summary": "Smart contracts are software programs that enable diverse business activities\non the blockchain. Recent research has identified new classes of \"machine\nun-auditable\" bugs that arise from both transactional contexts and source code.\nExisting detection methods require human understanding of underlying\ntransaction logic and manual reasoning across different sources of context\n(i.e. modalities), such as code, dynamic transaction executions, and natural\nlanguage specifying the expected transaction behavior.\n  To automate the detection of ``machine un-auditable'' bugs, we present\nSmartInv, an accurate and fast smart contract invariant inference framework.\nOur key insight is that the expected behavior of smart contracts, as specified\nby invariants, relies on understanding and reasoning across multimodal\ninformation, such as source code and natural language. We propose a new\nprompting strategy to foundation models, Tier of Thought (ToT), to reason\nacross multiple modalities of smart contracts and ultimately to generate\ninvariants. By checking the violation of these generated invariants, SmartInv\ncan identify potential vulnerabilities.\n  We evaluate SmartInv on real-world contracts and re-discover bugs that\nresulted in multi-million dollar losses over the past 2.5 years (from January\n1, 2021 to May 31, 2023). Our extensive evaluation shows that SmartInv\ngenerates (3.5X) more bug-critical invariants and detects (4$\\times$) more\ncritical bugs compared to the state-of-the-art tools in significantly (150X)\nless time. \\sys uncovers 119 zero-day vulnerabilities from the 89,621\nreal-world contracts. Among them, five are critical zero-day bugs confirmed by\ndevelopers as ``high severity.''",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts are software programs that enable diverse business activities\non the blockchain. Recent research has identified new classes of \"machine\nun-auditable\" bugs that arise from both transactional contexts and source code.\nExisting detection methods require human understanding of underlying\ntransaction logic and manual reasoning across different sources of context\n(i.e. modalities), such as code, dynamic transaction executions, and natural\nlanguage specifying the expected transaction behavior.\n  To automate the detection of ``machine un-auditable'' bugs, we present\nSmartInv, an accurate and fast smart contract invariant inference framework.\nOur key insight is that the expected behavior of smart contracts, as specified\nby invariants, relies on understanding and reasoning across multimodal\ninformation, such as source code and natural language. We propose a new\nprompting strategy to foundation models, Tier of Thought (ToT), to reason\nacross multiple modalities of smart contracts and ultimately to generate\ninvariants. By checking the violation of these generated invariants, SmartInv\ncan identify potential vulnerabilities.\n  We evaluate SmartInv on real-world contracts and re-discover bugs that\nresulted in multi-million dollar losses over the past 2.5 years (from January\n1, 2021 to May 31, 2023). Our extensive evaluation shows that SmartInv\ngenerates (3.5X) more bug-critical invariants and detects (4$\\times$) more\ncritical bugs compared to the state-of-the-art tools in significantly (150X)\nless time. \\sys uncovers 119 zero-day vulnerabilities from the 89,621\nreal-world contracts. Among them, five are critical zero-day bugs confirmed by\ndevelopers as ``high severity.''"
                },
                "authors": [
                    {
                        "name": "Sally Junsong Wang"
                    },
                    {
                        "name": "Kexin Pei"
                    },
                    {
                        "name": "Junfeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Yang"
                },
                "author": "Junfeng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09213v1",
                "updated": "2024-11-14T06:19:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    19,
                    18,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T06:19:18Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    19,
                    18,
                    3,
                    319,
                    0
                ],
                "title": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation\n  Systems for Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation\n  Systems for Medical Question Answering"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a promising approach to\nenhance the performance of large language models (LLMs) in knowledge-intensive\ntasks such as those from medical domain. However, the sensitive nature of the\nmedical domain necessitates a completely accurate and trustworthy system. While\nexisting RAG benchmarks primarily focus on the standard retrieve-answer\nsetting, they overlook many practical scenarios that measure crucial aspects of\na reliable medical system. This paper addresses this gap by providing a\ncomprehensive evaluation framework for medical question-answering (QA) systems\nin a RAG setting for these situations, including sufficiency, integration, and\nrobustness. We introduce Medical Retrieval-Augmented Generation Benchmark\n(MedRGB) that provides various supplementary elements to four medical QA\ndatasets for testing LLMs' ability to handle these specific scenarios.\nUtilizing MedRGB, we conduct extensive evaluations of both state-of-the-art\ncommercial LLMs and open-source models across multiple retrieval conditions.\nOur experimental results reveals current models' limited ability to handle\nnoise and misinformation in the retrieved documents. We further analyze the\nLLMs' reasoning processes to provides valuable insights and future directions\nfor developing RAG systems in this critical medical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a promising approach to\nenhance the performance of large language models (LLMs) in knowledge-intensive\ntasks such as those from medical domain. However, the sensitive nature of the\nmedical domain necessitates a completely accurate and trustworthy system. While\nexisting RAG benchmarks primarily focus on the standard retrieve-answer\nsetting, they overlook many practical scenarios that measure crucial aspects of\na reliable medical system. This paper addresses this gap by providing a\ncomprehensive evaluation framework for medical question-answering (QA) systems\nin a RAG setting for these situations, including sufficiency, integration, and\nrobustness. We introduce Medical Retrieval-Augmented Generation Benchmark\n(MedRGB) that provides various supplementary elements to four medical QA\ndatasets for testing LLMs' ability to handle these specific scenarios.\nUtilizing MedRGB, we conduct extensive evaluations of both state-of-the-art\ncommercial LLMs and open-source models across multiple retrieval conditions.\nOur experimental results reveals current models' limited ability to handle\nnoise and misinformation in the retrieved documents. We further analyze the\nLLMs' reasoning processes to provides valuable insights and future directions\nfor developing RAG systems in this critical medical domain."
                },
                "authors": [
                    {
                        "name": "Nghia Trung Ngo"
                    },
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09209v1",
                "updated": "2024-11-14T06:13:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    13,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T06:13:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    13,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "JoyVASA: Portrait and Animal Image Animation with Diffusion-Based\n  Audio-Driven Facial Dynamics and Head Motion Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JoyVASA: Portrait and Animal Image Animation with Diffusion-Based\n  Audio-Driven Facial Dynamics and Head Motion Generation"
                },
                "summary": "Audio-driven portrait animation has made significant advances with\ndiffusion-based models, improving video quality and lipsync accuracy. However,\nthe increasing complexity of these models has led to inefficiencies in training\nand inference, as well as constraints on video length and inter-frame\ncontinuity. In this paper, we propose JoyVASA, a diffusion-based method for\ngenerating facial dynamics and head motion in audio-driven facial animation.\nSpecifically, in the first stage, we introduce a decoupled facial\nrepresentation framework that separates dynamic facial expressions from static\n3D facial representations. This decoupling allows the system to generate longer\nvideos by combining any static 3D facial representation with dynamic motion\nsequences. Then, in the second stage, a diffusion transformer is trained to\ngenerate motion sequences directly from audio cues, independent of character\nidentity. Finally, a generator trained in the first stage uses the 3D facial\nrepresentation and the generated motion sequences as inputs to render\nhigh-quality animations. With the decoupled facial representation and the\nidentity-independent motion generation process, JoyVASA extends beyond human\nportraits to animate animal faces seamlessly. The model is trained on a hybrid\ndataset of private Chinese and public English data, enabling multilingual\nsupport. Experimental results validate the effectiveness of our approach.\nFuture work will focus on improving real-time performance and refining\nexpression control, further expanding the applications in portrait animation.\nThe code will be available at: https://jdhalgo.github.io/JoyVASA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-driven portrait animation has made significant advances with\ndiffusion-based models, improving video quality and lipsync accuracy. However,\nthe increasing complexity of these models has led to inefficiencies in training\nand inference, as well as constraints on video length and inter-frame\ncontinuity. In this paper, we propose JoyVASA, a diffusion-based method for\ngenerating facial dynamics and head motion in audio-driven facial animation.\nSpecifically, in the first stage, we introduce a decoupled facial\nrepresentation framework that separates dynamic facial expressions from static\n3D facial representations. This decoupling allows the system to generate longer\nvideos by combining any static 3D facial representation with dynamic motion\nsequences. Then, in the second stage, a diffusion transformer is trained to\ngenerate motion sequences directly from audio cues, independent of character\nidentity. Finally, a generator trained in the first stage uses the 3D facial\nrepresentation and the generated motion sequences as inputs to render\nhigh-quality animations. With the decoupled facial representation and the\nidentity-independent motion generation process, JoyVASA extends beyond human\nportraits to animate animal faces seamlessly. The model is trained on a hybrid\ndataset of private Chinese and public English data, enabling multilingual\nsupport. Experimental results validate the effectiveness of our approach.\nFuture work will focus on improving real-time performance and refining\nexpression control, further expanding the applications in portrait animation.\nThe code will be available at: https://jdhalgo.github.io/JoyVASA."
                },
                "authors": [
                    {
                        "name": "Xuyang Cao"
                    },
                    {
                        "name": "Sheng Shi"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Yang Yao"
                    },
                    {
                        "name": "Jintao Fei"
                    },
                    {
                        "name": "Minyu Gao"
                    },
                    {
                        "name": "Guoxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoxin Wang"
                },
                "author": "Guoxin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10570v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10570v6",
                "updated": "2024-11-14T06:09:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    9,
                    47,
                    3,
                    319,
                    0
                ],
                "published": "2023-10-16T16:45:12Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    16,
                    45,
                    12,
                    0,
                    289,
                    0
                ],
                "title": "On Context Utilization in Summarization with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Context Utilization in Summarization with Large Language Models"
                },
                "summary": "Large language models (LLMs) excel in abstractive summarization tasks,\ndelivering fluent and pertinent summaries. Recent advancements have extended\ntheir capabilities to handle long-input contexts, exceeding 100k tokens.\nHowever, in question answering, language models exhibit uneven utilization of\ntheir input context. They tend to favor the initial and final segments,\nresulting in a U-shaped performance pattern concerning where the answer is\nlocated within the input. This bias raises concerns, particularly in\nsummarization where crucial content may be dispersed throughout the source\ndocument(s). Besides, in summarization, mapping facts from the source to the\nsummary is not trivial as salient content is usually re-phrased. In this paper,\nwe conduct the first comprehensive study on context utilization and position\nbias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5\nevaluation metrics. We introduce a new evaluation benchmark called MiddleSum on\nthe which we benchmark two alternative inference methods to alleviate position\nbias: hierarchical summarization and incremental summarization. Our code and\ndata can be found here: https://github.com/ntunlp/MiddleSum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in abstractive summarization tasks,\ndelivering fluent and pertinent summaries. Recent advancements have extended\ntheir capabilities to handle long-input contexts, exceeding 100k tokens.\nHowever, in question answering, language models exhibit uneven utilization of\ntheir input context. They tend to favor the initial and final segments,\nresulting in a U-shaped performance pattern concerning where the answer is\nlocated within the input. This bias raises concerns, particularly in\nsummarization where crucial content may be dispersed throughout the source\ndocument(s). Besides, in summarization, mapping facts from the source to the\nsummary is not trivial as salient content is usually re-phrased. In this paper,\nwe conduct the first comprehensive study on context utilization and position\nbias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5\nevaluation metrics. We introduce a new evaluation benchmark called MiddleSum on\nthe which we benchmark two alternative inference methods to alleviate position\nbias: hierarchical summarization and incremental summarization. Our code and\ndata can be found here: https://github.com/ntunlp/MiddleSum."
                },
                "authors": [
                    {
                        "name": "Mathieu Ravaut"
                    },
                    {
                        "name": "Aixin Sun"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "ACL 2024. 9 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10570v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10570v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00476v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00476v3",
                "updated": "2024-11-14T06:06:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    6,
                    9,
                    3,
                    319,
                    0
                ],
                "published": "2024-06-29T15:47:28Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    15,
                    47,
                    28,
                    5,
                    181,
                    0
                ],
                "title": "Large Language Models for Power Scheduling: A User-Centric Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Power Scheduling: A User-Centric Approach"
                },
                "summary": "While traditional optimization and scheduling schemes are designed to meet\nfixed, predefined system requirements, future systems are moving toward\nuser-driven approaches and personalized services, aiming to achieve high\nquality-of-experience (QoE) and flexibility. This challenge is particularly\npronounced in wireless and digitalized energy networks, where users'\nrequirements have largely not been taken into consideration due to the lack of\na common language between users and machines. The emergence of powerful large\nlanguage models (LLMs) marks a radical departure from traditional\nsystem-centric methods into more advanced user-centric approaches by providing\na natural communication interface between users and devices. In this paper, for\nthe first time, we introduce a novel architecture for resource scheduling\nproblems by constructing three LLM agents to convert an arbitrary user's voice\nrequest (VRQ) into a resource allocation vector. Specifically, we design an LLM\nintent recognition agent to translate the request into an optimization problem\n(OP), an LLM OP parameter identification agent, and an LLM OP solving agent. To\nevaluate system performance, we construct a database of typical VRQs in the\ncontext of electric vehicle (EV) charging. As a proof of concept, we primarily\nuse Llama 3 8B. Through testing with different prompt engineering scenarios,\nthe obtained results demonstrate the efficiency of the proposed architecture.\nThe conducted performance analysis allows key insights to be extracted. For\ninstance, having a larger set of candidate OPs to model the real-world problem\nmight degrade the final performance because of a higher recognition/OP\nclassification noise level. All results and codes are open source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While traditional optimization and scheduling schemes are designed to meet\nfixed, predefined system requirements, future systems are moving toward\nuser-driven approaches and personalized services, aiming to achieve high\nquality-of-experience (QoE) and flexibility. This challenge is particularly\npronounced in wireless and digitalized energy networks, where users'\nrequirements have largely not been taken into consideration due to the lack of\na common language between users and machines. The emergence of powerful large\nlanguage models (LLMs) marks a radical departure from traditional\nsystem-centric methods into more advanced user-centric approaches by providing\na natural communication interface between users and devices. In this paper, for\nthe first time, we introduce a novel architecture for resource scheduling\nproblems by constructing three LLM agents to convert an arbitrary user's voice\nrequest (VRQ) into a resource allocation vector. Specifically, we design an LLM\nintent recognition agent to translate the request into an optimization problem\n(OP), an LLM OP parameter identification agent, and an LLM OP solving agent. To\nevaluate system performance, we construct a database of typical VRQs in the\ncontext of electric vehicle (EV) charging. As a proof of concept, we primarily\nuse Llama 3 8B. Through testing with different prompt engineering scenarios,\nthe obtained results demonstrate the efficiency of the proposed architecture.\nThe conducted performance analysis allows key insights to be extracted. For\ninstance, having a larger set of candidate OPs to model the real-world problem\nmight degrade the final performance because of a higher recognition/OP\nclassification noise level. All results and codes are open source."
                },
                "authors": [
                    {
                        "name": "Thomas Mongaillard"
                    },
                    {
                        "name": "Samson Lasaulce"
                    },
                    {
                        "name": "Othman Hicheur"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Lina Bariah"
                    },
                    {
                        "name": "Vineeth S. Varma"
                    },
                    {
                        "name": "Hang Zou"
                    },
                    {
                        "name": "Qiyang Zhao"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00476v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00476v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08504v2",
                "updated": "2024-11-14T05:51:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    5,
                    51,
                    26,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-13T10:42:11Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    10,
                    42,
                    11,
                    2,
                    318,
                    0
                ],
                "title": "Towards Objective and Unbiased Decision Assessments with LLM-Enhanced\n  Hierarchical Attention Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Objective and Unbiased Decision Assessments with LLM-Enhanced\n  Hierarchical Attention Networks"
                },
                "summary": "How objective and unbiased are we while making decisions? This work\ninvestigates cognitive bias identification in high-stake decision making\nprocess by human experts, questioning its effectiveness in real-world settings,\nsuch as candidates assessments for university admission. We begin with a\nstatistical analysis assessing correlations among different decision points\namong in the current process, which discovers discrepancies that imply\ncognitive bias and inconsistency in decisions. This motivates our exploration\nof bias-aware AI-augmented workflow that surpass human judgment. We propose\nBGM-HAN, an enhanced Hierarchical Attention Network with Byte-Pair Encoding,\nGated Residual Connections and Multi-Head Attention. Using it as a backbone\nmodel, we further propose a Shortlist-Analyse-Recommend (SAR) agentic workflow,\nwhich simulate real-world decision-making. In our experiments, both the\nproposed model and the agentic workflow significantly improves on both human\njudgment and alternative models, validated with real-world data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How objective and unbiased are we while making decisions? This work\ninvestigates cognitive bias identification in high-stake decision making\nprocess by human experts, questioning its effectiveness in real-world settings,\nsuch as candidates assessments for university admission. We begin with a\nstatistical analysis assessing correlations among different decision points\namong in the current process, which discovers discrepancies that imply\ncognitive bias and inconsistency in decisions. This motivates our exploration\nof bias-aware AI-augmented workflow that surpass human judgment. We propose\nBGM-HAN, an enhanced Hierarchical Attention Network with Byte-Pair Encoding,\nGated Residual Connections and Multi-Head Attention. Using it as a backbone\nmodel, we further propose a Shortlist-Analyse-Recommend (SAR) agentic workflow,\nwhich simulate real-world decision-making. In our experiments, both the\nproposed model and the agentic workflow significantly improves on both human\njudgment and alternative models, validated with real-world data."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "arxiv_comment": "Source code is available at: https://github.com/junhua/bgm-han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09195v1",
                "updated": "2024-11-14T05:34:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    5,
                    34,
                    30,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T05:34:30Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    5,
                    34,
                    30,
                    3,
                    319,
                    0
                ],
                "title": "The origin channels of hierarchical binary black hole mergers in the\n  LIGO-Virgo-KAGRA O1, O2, and O3 runs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The origin channels of hierarchical binary black hole mergers in the\n  LIGO-Virgo-KAGRA O1, O2, and O3 runs"
                },
                "summary": "We infer the origin channels of hierarchical mergers observed in the\nLIGO-Virgo-KAGRA (LVK) O1, O2, and O3 runs using a hierarchical Bayesian\nanalysis under a parametric population model. By assuming the active galactic\nnucleus (ANG) disk and nuclear star cluster (NSC) channels, we find that NSCs\nlikely dominate the hierarchical merger rate in the universe, corresponding to\na fraction of $f_{\\rm NSC}=0.87_{-0.29}^{+0.10}$ at 90\\% credible intervals in\nour fiducial model; AGN disks may contribute up to nearly half of hierarchical\nmergers detectable with LVK, specifically $f_{\\rm\ndet,AGN}=0.34_{-0.26}^{+0.38}$. We investigate the impact of the escape speed,\nalong with other population parameters on the branching fraction, suggesting\nthat the mass, mass ratio, and spin of the sources play significant roles in\npopulation analysis. We show that hierarchical mergers constitute at least\n$\\sim$$10\\%$ of the gravitational wave events detected by LVK during the O1-O3\nruns. Furthermore, we demonstrate that it is challenging to effectively infer\ndetailed information about the host environment based solely on the\ndistribution of black hole merger parameters if multiple formation channels are\nconsidered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We infer the origin channels of hierarchical mergers observed in the\nLIGO-Virgo-KAGRA (LVK) O1, O2, and O3 runs using a hierarchical Bayesian\nanalysis under a parametric population model. By assuming the active galactic\nnucleus (ANG) disk and nuclear star cluster (NSC) channels, we find that NSCs\nlikely dominate the hierarchical merger rate in the universe, corresponding to\na fraction of $f_{\\rm NSC}=0.87_{-0.29}^{+0.10}$ at 90\\% credible intervals in\nour fiducial model; AGN disks may contribute up to nearly half of hierarchical\nmergers detectable with LVK, specifically $f_{\\rm\ndet,AGN}=0.34_{-0.26}^{+0.38}$. We investigate the impact of the escape speed,\nalong with other population parameters on the branching fraction, suggesting\nthat the mass, mass ratio, and spin of the sources play significant roles in\npopulation analysis. We show that hierarchical mergers constitute at least\n$\\sim$$10\\%$ of the gravitational wave events detected by LVK during the O1-O3\nruns. Furthermore, we demonstrate that it is challenging to effectively infer\ndetailed information about the host environment based solely on the\ndistribution of black hole merger parameters if multiple formation channels are\nconsidered."
                },
                "authors": [
                    {
                        "name": "Guo-Peng Li"
                    },
                    {
                        "name": "Xi-Long Fan"
                    }
                ],
                "author_detail": {
                    "name": "Xi-Long Fan"
                },
                "author": "Xi-Long Fan",
                "arxiv_comment": "11 pages, 2 figures, 2 tables, and comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06493v2",
                "updated": "2024-11-14T05:34:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    5,
                    34,
                    13,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-10T15:21:30Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    21,
                    30,
                    6,
                    315,
                    0
                ],
                "title": "LProtector: An LLM-driven Vulnerability Detection System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LProtector: An LLM-driven Vulnerability Detection System"
                },
                "summary": "This paper presents LProtector, an automated vulnerability detection system\nfor C/C++ codebases driven by the large language model (LLM) GPT-4o and\nRetrieval-Augmented Generation (RAG). As software complexity grows, traditional\nmethods face challenges in detecting vulnerabilities effectively. LProtector\nleverages GPT-4o's powerful code comprehension and generation capabilities to\nperform binary classification and identify vulnerabilities within target\ncodebases. We conducted experiments on the Big-Vul dataset, showing that\nLProtector outperforms two state-of-the-art baselines in terms of F1 score,\ndemonstrating the potential of integrating LLMs with vulnerability detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents LProtector, an automated vulnerability detection system\nfor C/C++ codebases driven by the large language model (LLM) GPT-4o and\nRetrieval-Augmented Generation (RAG). As software complexity grows, traditional\nmethods face challenges in detecting vulnerabilities effectively. LProtector\nleverages GPT-4o's powerful code comprehension and generation capabilities to\nperform binary classification and identify vulnerabilities within target\ncodebases. We conducted experiments on the Big-Vul dataset, showing that\nLProtector outperforms two state-of-the-art baselines in terms of F1 score,\ndemonstrating the potential of integrating LLMs with vulnerability detection."
                },
                "authors": [
                    {
                        "name": "Ze Sheng"
                    },
                    {
                        "name": "Fenghua Wu"
                    },
                    {
                        "name": "Xiangwu Zuo"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Yuxin Qiao"
                    },
                    {
                        "name": "Lei Hang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Hang"
                },
                "author": "Lei Hang",
                "arxiv_comment": "5 pages, 4 figures. This is a preprint version of the article. The\n  final version will be published in the proceedings of the IEEE conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11713v2",
                "updated": "2024-11-14T04:59:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    4,
                    59,
                    49,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-15T15:47:10Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    47,
                    10,
                    1,
                    289,
                    0
                ],
                "title": "Enhancing Statistical Validity and Power in Hybrid Controlled Trials: A\n  Randomization Inference Approach with Conformal Selective Borrowing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Statistical Validity and Power in Hybrid Controlled Trials: A\n  Randomization Inference Approach with Conformal Selective Borrowing"
                },
                "summary": "Randomized controlled trials (RCTs) are the gold standard for causal\ninference but may lack power because of small populations in rare diseases and\nlimited participation in common diseases due to equipoise concerns. Hybrid\ncontrolled trials, which integrate external controls (ECs) from historical\nstudies or large observational data, improve statistical efficiency and are\nappealing for drug evaluations. However, non-randomized ECs can introduce\nbiases and inflate the type I error rate, especially when the RCT sample size\nis small. To address this, we propose a Fisher randomization test (FRT) that\nemploys a semiparametric efficient test statistic combining RCT and EC data,\nwith assignments resampled using the actual randomization procedure. The\nproposed FRT controls the type I error rate even with unmeasured confounding\namong ECs. However, borrowing biased ECs can reduce FRT power, so we introduce\nconformal selective borrowing (CSB) to individually borrow comparable ECs. We\npropose an adaptive procedure to determine the selection threshold, minimizing\nthe mean squared error of a class of CSB estimators and enhancing FRT power.\nThe advantages of our method are demonstrated through simulations and an\napplication to a lung cancer RCT with ECs from the National Cancer Database.\nOur method is available in the R package intFRT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized controlled trials (RCTs) are the gold standard for causal\ninference but may lack power because of small populations in rare diseases and\nlimited participation in common diseases due to equipoise concerns. Hybrid\ncontrolled trials, which integrate external controls (ECs) from historical\nstudies or large observational data, improve statistical efficiency and are\nappealing for drug evaluations. However, non-randomized ECs can introduce\nbiases and inflate the type I error rate, especially when the RCT sample size\nis small. To address this, we propose a Fisher randomization test (FRT) that\nemploys a semiparametric efficient test statistic combining RCT and EC data,\nwith assignments resampled using the actual randomization procedure. The\nproposed FRT controls the type I error rate even with unmeasured confounding\namong ECs. However, borrowing biased ECs can reduce FRT power, so we introduce\nconformal selective borrowing (CSB) to individually borrow comparable ECs. We\npropose an adaptive procedure to determine the selection threshold, minimizing\nthe mean squared error of a class of CSB estimators and enhancing FRT power.\nThe advantages of our method are demonstrated through simulations and an\napplication to a lung cancer RCT with ECs from the National Cancer Database.\nOur method is available in the R package intFRT."
                },
                "authors": [
                    {
                        "name": "Ke Zhu"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Xiaofei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofei Wang"
                },
                "author": "Xiaofei Wang",
                "arxiv_comment": "Update the MSE estimation in the adaptive selection threshold\n  procedure, along with the associated non-asymptotic theory and numerical\n  results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11214v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11214v3",
                "updated": "2024-11-14T03:53:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    3,
                    53,
                    56,
                    3,
                    319,
                    0
                ],
                "published": "2024-06-17T05:13:25Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    5,
                    13,
                    25,
                    0,
                    169,
                    0
                ],
                "title": "Problematic Tokens: Tokenizer Bias in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problematic Tokens: Tokenizer Bias in Large Language Models"
                },
                "summary": "Recent advancements in large language models(LLMs), such as GPT-4 and GPT-4o,\nhave shown exceptional performance, especially in languages with abundant\nresources like English, thanks to extensive datasets that ensure robust\ntraining. Conversely, these models exhibit limitations when processing\nunder-resourced languages such as Chinese and Korean, where issues including\nhallucinatory responses remain prevalent. This paper traces the roots of these\ndisparities to the tokenization process inherent to these models. Specifically,\nit explores how the tokenizers vocabulary, often used to speed up the\ntokenization process and reduce tokens but constructed independently of the\nactual model training data, inadequately represents non-English languages. This\nmisrepresentation results in the propagation of under-trained or untrained\ntokens, which perpetuate biases and pose serious concerns related to data\nsecurity and ethical standards. We aim to dissect the tokenization mechanics of\nGPT-4o, illustrating how its simplified token-handling methods amplify these\nrisks and offer strategic solutions to mitigate associated security and ethical\nissues. Through this study, we emphasize the critical need to rethink\ntokenization frameworks to foster more equitable and secure AI technologies.\nThe code and data are available at https://github.com/yeyimilk/LLMGPT4o",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models(LLMs), such as GPT-4 and GPT-4o,\nhave shown exceptional performance, especially in languages with abundant\nresources like English, thanks to extensive datasets that ensure robust\ntraining. Conversely, these models exhibit limitations when processing\nunder-resourced languages such as Chinese and Korean, where issues including\nhallucinatory responses remain prevalent. This paper traces the roots of these\ndisparities to the tokenization process inherent to these models. Specifically,\nit explores how the tokenizers vocabulary, often used to speed up the\ntokenization process and reduce tokens but constructed independently of the\nactual model training data, inadequately represents non-English languages. This\nmisrepresentation results in the propagation of under-trained or untrained\ntokens, which perpetuate biases and pose serious concerns related to data\nsecurity and ethical standards. We aim to dissect the tokenization mechanics of\nGPT-4o, illustrating how its simplified token-handling methods amplify these\nrisks and offer strategic solutions to mitigate associated security and ethical\nissues. Through this study, we emphasize the critical need to rethink\ntokenization frameworks to foster more equitable and secure AI technologies.\nThe code and data are available at https://github.com/yeyimilk/LLMGPT4o"
                },
                "authors": [
                    {
                        "name": "Jin Yang"
                    },
                    {
                        "name": "Zhiqiang Wang"
                    },
                    {
                        "name": "Yanbin Lin"
                    },
                    {
                        "name": "Zunduo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zunduo Zhao"
                },
                "author": "Zunduo Zhao",
                "arxiv_comment": "11th IEEE Special session on Privacy and Security of Big Data (PSBD\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11214v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11214v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09159v1",
                "updated": "2024-11-14T03:27:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    3,
                    27,
                    43,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T03:27:43Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    3,
                    27,
                    43,
                    3,
                    319,
                    0
                ],
                "title": "PIMCOMP: An End-to-End DNN Compiler for Processing-In-Memory\n  Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIMCOMP: An End-to-End DNN Compiler for Processing-In-Memory\n  Accelerators"
                },
                "summary": "Various processing-in-memory (PIM) accelerators based on various devices,\nmicro-architectures, and interfaces have been proposed to accelerate deep\nneural networks (DNNs). How to deploy DNNs onto PIM-based accelerators is the\nkey to explore PIM's high performance and energy efficiency. The scale of DNN\nmodels, the diversity of PIM accelerators, and the complexity of deployment are\nfar beyond the human deployment capability. Hence, an automatic deployment\nmethodology is indispensable. In this work, we propose PIMCOMP, an end-to-end\nDNN compiler tailored for PIM accelerators, achieving efficient deployment of\nDNN models on PIM hardware. PIMCOMP can adapt to various PIM architectures by\nusing an abstract configurable PIM accelerator template with a set of\npseudo-instructions, which is a high-level abstraction of the hardware's\nfundamental functionalities. Through a generic multi-level optimization\nframework, PIMCOMP realizes an end-to-end conversion from a high-level DNN\ndescription to pseudo-instructions, which can be further converted to specific\nhardware intrinsics/primitives. The compilation addresses two critical issues\nin PIM-accelerated inference from a system perspective: resource utilization\nand dataflow scheduling. PIMCOMP adopts a flexible unfolding format to reshape\nand partition convolutional layers, adopts a weight-layout guided\ncomputation-storage-mapping approach to enhance resource utilization, and\nbalances the system's computation, memory access, and communication\ncharacteristics. For dataflow scheduling, we design two scheduling algorithms\nwith different inter-layer pipeline granularities to support varying\napplication scenarios while ensuring high computational parallelism.\nExperiments demonstrate that PIMCOMP improves throughput, latency, and energy\nefficiency across various architectures. PIMCOMP is open-sourced at\n\\url{https://github.com/sunxt99/PIMCOMP-NN}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various processing-in-memory (PIM) accelerators based on various devices,\nmicro-architectures, and interfaces have been proposed to accelerate deep\nneural networks (DNNs). How to deploy DNNs onto PIM-based accelerators is the\nkey to explore PIM's high performance and energy efficiency. The scale of DNN\nmodels, the diversity of PIM accelerators, and the complexity of deployment are\nfar beyond the human deployment capability. Hence, an automatic deployment\nmethodology is indispensable. In this work, we propose PIMCOMP, an end-to-end\nDNN compiler tailored for PIM accelerators, achieving efficient deployment of\nDNN models on PIM hardware. PIMCOMP can adapt to various PIM architectures by\nusing an abstract configurable PIM accelerator template with a set of\npseudo-instructions, which is a high-level abstraction of the hardware's\nfundamental functionalities. Through a generic multi-level optimization\nframework, PIMCOMP realizes an end-to-end conversion from a high-level DNN\ndescription to pseudo-instructions, which can be further converted to specific\nhardware intrinsics/primitives. The compilation addresses two critical issues\nin PIM-accelerated inference from a system perspective: resource utilization\nand dataflow scheduling. PIMCOMP adopts a flexible unfolding format to reshape\nand partition convolutional layers, adopts a weight-layout guided\ncomputation-storage-mapping approach to enhance resource utilization, and\nbalances the system's computation, memory access, and communication\ncharacteristics. For dataflow scheduling, we design two scheduling algorithms\nwith different inter-layer pipeline granularities to support varying\napplication scenarios while ensuring high computational parallelism.\nExperiments demonstrate that PIMCOMP improves throughput, latency, and energy\nefficiency across various architectures. PIMCOMP is open-sourced at\n\\url{https://github.com/sunxt99/PIMCOMP-NN}."
                },
                "authors": [
                    {
                        "name": "Xiaotian Sun"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Wanqian Li"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Xiaoming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Chen"
                },
                "author": "Xiaoming Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12382v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12382v3",
                "updated": "2024-11-14T03:10:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    3,
                    10,
                    45,
                    3,
                    319,
                    0
                ],
                "published": "2024-06-18T08:14:28Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    8,
                    14,
                    28,
                    1,
                    170,
                    0
                ],
                "title": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions"
                },
                "summary": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Yuanzhe Zhang"
                    },
                    {
                        "name": "Yanchao Hao"
                    },
                    {
                        "name": "Shengping Liu"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12382v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12382v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.16552v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.16552v4",
                "updated": "2024-11-14T03:04:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    3,
                    4,
                    44,
                    3,
                    319,
                    0
                ],
                "published": "2022-11-29T19:23:19Z",
                "published_parsed": [
                    2022,
                    11,
                    29,
                    19,
                    23,
                    19,
                    1,
                    333,
                    0
                ],
                "title": "Bayesian inference for aggregated Hawkes processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference for aggregated Hawkes processes"
                },
                "summary": "The Hawkes process, a self-exciting point process, has a wide range of\napplications in modeling earthquakes, social networks and stock markets. The\nestablished estimation process requires that researchers have access to the\nexact time stamps and spatial information. However, available data are often\nrounded or aggregated. We develop a Bayesian estimation procedure for the\nparameters of a Hawkes process based on aggregated data. Our approach is\ndeveloped for temporal, spatio-temporal, and mutually exciting Hawkes processes\nwhere data are available over discrete time periods and regions. We show\ntheoretically that the parameters of the Hawkes process are identifiable from\naggregated data under general specifications. We demonstrate the method on\nsimulated data under various model specifications in the presence of one or\nmore interacting processes, and under varying coarseness of data aggregation.\nFinally, we examine the internal and cross-excitation effects of airstrikes and\ninsurgent violence events from February 2007 to June 2008, with some data\naggregated by day.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hawkes process, a self-exciting point process, has a wide range of\napplications in modeling earthquakes, social networks and stock markets. The\nestablished estimation process requires that researchers have access to the\nexact time stamps and spatial information. However, available data are often\nrounded or aggregated. We develop a Bayesian estimation procedure for the\nparameters of a Hawkes process based on aggregated data. Our approach is\ndeveloped for temporal, spatio-temporal, and mutually exciting Hawkes processes\nwhere data are available over discrete time periods and regions. We show\ntheoretically that the parameters of the Hawkes process are identifiable from\naggregated data under general specifications. We demonstrate the method on\nsimulated data under various model specifications in the presence of one or\nmore interacting processes, and under varying coarseness of data aggregation.\nFinally, we examine the internal and cross-excitation effects of airstrikes and\ninsurgent violence events from February 2007 to June 2008, with some data\naggregated by day."
                },
                "authors": [
                    {
                        "name": "Lingxiao Zhou"
                    },
                    {
                        "name": "Georgia Papadogeorgou"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Papadogeorgou"
                },
                "author": "Georgia Papadogeorgou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.16552v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.16552v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09148v1",
                "updated": "2024-11-14T02:58:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    2,
                    58,
                    54,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T02:58:54Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    2,
                    58,
                    54,
                    3,
                    319,
                    0
                ],
                "title": "Toward Democratized Generative AI in Next-Generation Mobile Edge\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Democratized Generative AI in Next-Generation Mobile Edge\n  Networks"
                },
                "summary": "The rapid development of generative AI technologies, including large language\nmodels (LLMs), has brought transformative changes to various fields. However,\ndeploying such advanced models on mobile and edge devices remains challenging\ndue to their high computational, memory, communication, and energy\nrequirements. To address these challenges, we propose a model-centric framework\nfor democratizing generative AI deployment on mobile and edge networks. First,\nwe comprehensively review key compact model strategies, such as quantization,\nmodel pruning, and knowledge distillation, and present key performance metrics\nto optimize generative AI for mobile deployment. Next, we provide a focused\nreview of mobile and edge networks, emphasizing the specific challenges and\nrequirements of these environments. We further conduct a case study\ndemonstrating the effectiveness of these strategies by deploying LLMs on real\nmobile edge devices. Experimental results highlight the practicality of\ndemocratized LLMs, with significant improvements in generalization accuracy,\nhallucination rate, accessibility, and resource consumption. Finally, we\ndiscuss potential research directions to further advance the deployment of\ngenerative AI in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of generative AI technologies, including large language\nmodels (LLMs), has brought transformative changes to various fields. However,\ndeploying such advanced models on mobile and edge devices remains challenging\ndue to their high computational, memory, communication, and energy\nrequirements. To address these challenges, we propose a model-centric framework\nfor democratizing generative AI deployment on mobile and edge networks. First,\nwe comprehensively review key compact model strategies, such as quantization,\nmodel pruning, and knowledge distillation, and present key performance metrics\nto optimize generative AI for mobile deployment. Next, we provide a focused\nreview of mobile and edge networks, emphasizing the specific challenges and\nrequirements of these environments. We further conduct a case study\ndemonstrating the effectiveness of these strategies by deploying LLMs on real\nmobile edge devices. Experimental results highlight the practicality of\ndemocratized LLMs, with significant improvements in generalization accuracy,\nhallucination rate, accessibility, and resource consumption. Finally, we\ndiscuss potential research directions to further advance the deployment of\ngenerative AI in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Jiayi He"
                    },
                    {
                        "name": "Xiaofeng Luo"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Yonghui Li"
                    },
                    {
                        "name": "Biplab Sikdar"
                    }
                ],
                "author_detail": {
                    "name": "Biplab Sikdar"
                },
                "author": "Biplab Sikdar",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08733v2",
                "updated": "2024-11-14T02:36:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    2,
                    36,
                    58,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-13T16:15:38Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    16,
                    15,
                    38,
                    2,
                    318,
                    0
                ],
                "title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free\n  Self-Alignment of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free\n  Self-Alignment of Language Models"
                },
                "summary": "Aligning Large Language Models (LLMs) traditionally relies on costly training\nand human preference annotations. Self-alignment seeks to reduce these expenses\nby enabling models to align themselves. To further lower costs and achieve\nalignment without any expensive tuning or annotations, we introduce a new\ntuning-free approach for self-alignment, Dynamic Rewarding with Prompt\nOptimization (DRPO). Our approach leverages a search-based optimization\nframework that allows LLMs to iteratively self-improve and craft the optimal\nalignment instructions, all without additional training or human intervention.\nThe core of DRPO is a dynamic rewarding mechanism, which identifies and\nrectifies model-specific alignment weaknesses, allowing LLMs to adapt\nefficiently to diverse alignment challenges. Empirical evaluations on eight\nrecent LLMs, both open- and closed-sourced, demonstrate that DRPO significantly\nenhances alignment performance, with base models outperforming their\nSFT/RLHF-tuned counterparts. Moreover, the prompts automatically optimized by\nDRPO surpass those curated by human experts, further validating the\neffectiveness of our approach. Our findings highlight the great potential of\ncurrent LLMs to achieve adaptive self-alignment through inference-time\noptimization, complementing tuning-based alignment methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models (LLMs) traditionally relies on costly training\nand human preference annotations. Self-alignment seeks to reduce these expenses\nby enabling models to align themselves. To further lower costs and achieve\nalignment without any expensive tuning or annotations, we introduce a new\ntuning-free approach for self-alignment, Dynamic Rewarding with Prompt\nOptimization (DRPO). Our approach leverages a search-based optimization\nframework that allows LLMs to iteratively self-improve and craft the optimal\nalignment instructions, all without additional training or human intervention.\nThe core of DRPO is a dynamic rewarding mechanism, which identifies and\nrectifies model-specific alignment weaknesses, allowing LLMs to adapt\nefficiently to diverse alignment challenges. Empirical evaluations on eight\nrecent LLMs, both open- and closed-sourced, demonstrate that DRPO significantly\nenhances alignment performance, with base models outperforming their\nSFT/RLHF-tuned counterparts. Moreover, the prompts automatically optimized by\nDRPO surpass those curated by human experts, further validating the\neffectiveness of our approach. Our findings highlight the great potential of\ncurrent LLMs to achieve adaptive self-alignment through inference-time\noptimization, complementing tuning-based alignment methods."
                },
                "authors": [
                    {
                        "name": "Somanshu Singla"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Tianyang Liu"
                    },
                    {
                        "name": "Abdullah Ashfaq"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Eric P. Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric P. Xing"
                },
                "author": "Eric P. Xing",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.09417v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.09417v3",
                "updated": "2024-11-14T02:00:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    2,
                    0,
                    33,
                    3,
                    319,
                    0
                ],
                "published": "2024-01-17T18:56:18Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    18,
                    56,
                    18,
                    2,
                    17,
                    0
                ],
                "title": "Vision Mamba: Efficient Visual Representation Learning with\n  Bidirectional State Space Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Mamba: Efficient Visual Representation Learning with\n  Bidirectional State Space Model"
                },
                "summary": "Recently the state space models (SSMs) with efficient hardware-aware designs,\ni.e., the Mamba deep learning model, have shown great potential for long\nsequence modeling. Meanwhile building efficient and generic vision backbones\npurely upon SSMs is an appealing direction. However, representing visual data\nis challenging for SSMs due to the position-sensitivity of visual data and the\nrequirement of global context for visual understanding. In this paper, we show\nthat the reliance on self-attention for visual representation learning is not\nnecessary and propose a new generic vision backbone with bidirectional Mamba\nblocks (Vim), which marks the image sequences with position embeddings and\ncompresses the visual representation with bidirectional state space models. On\nImageNet classification, COCO object detection, and ADE20k semantic\nsegmentation tasks, Vim achieves higher performance compared to\nwell-established vision transformers like DeiT, while also demonstrating\nsignificantly improved computation & memory efficiency. For example, Vim is\n2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch\ninference to extract features on images with a resolution of 1248$\\times$1248.\nThe results demonstrate that Vim is capable of overcoming the computation &\nmemory constraints on performing Transformer-style understanding for\nhigh-resolution images and it has great potential to be the next-generation\nbackbone for vision foundation models. Code is available at\nhttps://github.com/hustvl/Vim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the state space models (SSMs) with efficient hardware-aware designs,\ni.e., the Mamba deep learning model, have shown great potential for long\nsequence modeling. Meanwhile building efficient and generic vision backbones\npurely upon SSMs is an appealing direction. However, representing visual data\nis challenging for SSMs due to the position-sensitivity of visual data and the\nrequirement of global context for visual understanding. In this paper, we show\nthat the reliance on self-attention for visual representation learning is not\nnecessary and propose a new generic vision backbone with bidirectional Mamba\nblocks (Vim), which marks the image sequences with position embeddings and\ncompresses the visual representation with bidirectional state space models. On\nImageNet classification, COCO object detection, and ADE20k semantic\nsegmentation tasks, Vim achieves higher performance compared to\nwell-established vision transformers like DeiT, while also demonstrating\nsignificantly improved computation & memory efficiency. For example, Vim is\n2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch\ninference to extract features on images with a resolution of 1248$\\times$1248.\nThe results demonstrate that Vim is capable of overcoming the computation &\nmemory constraints on performing Transformer-style understanding for\nhigh-resolution images and it has great potential to be the next-generation\nbackbone for vision foundation models. Code is available at\nhttps://github.com/hustvl/Vim."
                },
                "authors": [
                    {
                        "name": "Lianghui Zhu"
                    },
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Vision Mamba (Vim) is accepted by ICML 2024. Code is available at\n  https://github.com/hustvl/Vim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.09417v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.09417v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09125v1",
                "updated": "2024-11-14T01:48:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    48,
                    8,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T01:48:08Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    48,
                    8,
                    3,
                    319,
                    0
                ],
                "title": "DROJ: A Prompt-Driven Attack against Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DROJ: A Prompt-Driven Attack against Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various natural language processing tasks. Due to their training on\ninternet-sourced datasets, LLMs can sometimes generate objectionable content,\nnecessitating extensive alignment with human feedback to avoid such outputs.\nDespite massive alignment efforts, LLMs remain susceptible to adversarial\njailbreak attacks, which usually are manipulated prompts designed to circumvent\nsafety mechanisms and elicit harmful responses. Here, we introduce a novel\napproach, Directed Rrepresentation Optimization Jailbreak (DROJ), which\noptimizes jailbreak prompts at the embedding level to shift the hidden\nrepresentations of harmful queries towards directions that are more likely to\nelicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat\nmodel show that DROJ achieves a 100\\% keyword-based Attack Success Rate (ASR),\neffectively preventing direct refusals. However, the model occasionally\nproduces repetitive and non-informative responses. To mitigate this, we\nintroduce a helpfulness system prompt that enhances the utility of the model's\nresponses. Our code is available at\nhttps://github.com/Leon-Leyang/LLM-Safeguard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various natural language processing tasks. Due to their training on\ninternet-sourced datasets, LLMs can sometimes generate objectionable content,\nnecessitating extensive alignment with human feedback to avoid such outputs.\nDespite massive alignment efforts, LLMs remain susceptible to adversarial\njailbreak attacks, which usually are manipulated prompts designed to circumvent\nsafety mechanisms and elicit harmful responses. Here, we introduce a novel\napproach, Directed Rrepresentation Optimization Jailbreak (DROJ), which\noptimizes jailbreak prompts at the embedding level to shift the hidden\nrepresentations of harmful queries towards directions that are more likely to\nelicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat\nmodel show that DROJ achieves a 100\\% keyword-based Attack Success Rate (ASR),\neffectively preventing direct refusals. However, the model occasionally\nproduces repetitive and non-informative responses. To mitigate this, we\nintroduce a helpfulness system prompt that enhances the utility of the model's\nresponses. Our code is available at\nhttps://github.com/Leon-Leyang/LLM-Safeguard."
                },
                "authors": [
                    {
                        "name": "Leyang Hu"
                    },
                    {
                        "name": "Boran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Boran Wang"
                },
                "author": "Boran Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18373v2",
                "updated": "2024-11-14T01:41:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    41,
                    33,
                    3,
                    319,
                    0
                ],
                "published": "2024-04-29T02:23:53Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    2,
                    23,
                    53,
                    0,
                    120,
                    0
                ],
                "title": "6G comprehensive intelligence: network operations and optimization based\n  on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G comprehensive intelligence: network operations and optimization based\n  on Large Language Models"
                },
                "summary": "The sixth generation mobile communication standard (6G) can promote the\ndevelopment of Industrial Internet and Internet of Things (IoT). To achieve\ncomprehensive intelligent development of the network and provide customers with\nhigher quality personalized services. This paper proposes a network performance\noptimization and intelligent operation network architecture based on Large\nLanguage Model (LLM), aiming to build a comprehensive intelligent 6G network\nsystem. The Large Language Model, with more parameters and stronger learning\nability, can more accurately capture patterns and features in data, which can\nachieve more accurate content output and high intelligence and provide strong\nsupport for related research such as network data security, privacy protection,\nand health assessment. This paper also presents the design framework of a\nnetwork health assessment system based on LLM and focuses on its potential\napplication value, through the case of network health management system, it is\nfully demonstrated that the 6G intelligent network system based on LLM has\nimportant practical significance for the comprehensive realization of\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sixth generation mobile communication standard (6G) can promote the\ndevelopment of Industrial Internet and Internet of Things (IoT). To achieve\ncomprehensive intelligent development of the network and provide customers with\nhigher quality personalized services. This paper proposes a network performance\noptimization and intelligent operation network architecture based on Large\nLanguage Model (LLM), aiming to build a comprehensive intelligent 6G network\nsystem. The Large Language Model, with more parameters and stronger learning\nability, can more accurately capture patterns and features in data, which can\nachieve more accurate content output and high intelligence and provide strong\nsupport for related research such as network data security, privacy protection,\nand health assessment. This paper also presents the design framework of a\nnetwork health assessment system based on LLM and focuses on its potential\napplication value, through the case of network health management system, it is\nfully demonstrated that the 6G intelligent network system based on LLM has\nimportant practical significance for the comprehensive realization of\nintelligence."
                },
                "authors": [
                    {
                        "name": "Sifan Long"
                    },
                    {
                        "name": "Fengxiao Tang"
                    },
                    {
                        "name": "Yangfan Li"
                    },
                    {
                        "name": "Tiao Tan"
                    },
                    {
                        "name": "Zhengjie Jin"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "arxiv_comment": "8 pages, 5 figures, 15 preferences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04997v2",
                "updated": "2024-11-14T01:36:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    36,
                    12,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-07T18:59:16Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    16,
                    3,
                    312,
                    0
                ],
                "title": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation"
                },
                "summary": "CLIP is one of the most important multimodal foundational models today. What\npowers CLIP's capabilities? The rich supervision signals provided by natural\nlanguage, the carrier of human knowledge, shape a powerful cross-modal\nrepresentation space. However, with the rapid advancements in large language\nmodels LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and\ngeneration are continually being pushed. This raises an intriguing question:\ncan the capabilities of LLMs be harnessed to further improve multimodal\nrepresentation learning? The potential benefits of incorporating LLMs into CLIP\nare clear. LLMs' strong textual understanding can fundamentally improve CLIP's\nability to handle image captions, drastically enhancing its ability to process\nlong and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs\nare trained on a vast corpus of text, possessing open-world knowledge. This\nallows them to expand on caption information during training, increasing the\nefficiency of the learning process. In this paper, we propose LLM2CLIP, a novel\napproach that embraces the power of LLMs to unlock CLIP's potential. By\nfine-tuning the LLM in the caption space with contrastive learning, we extract\nits textual capabilities into the output embeddings, significantly improving\nthe output layer's textual discriminability. We then design an efficient\ntraining process where the fine-tuned LLM acts as a powerful teacher for CLIP's\nvisual encoder. Thanks to the LLM's presence, we can now incorporate longer and\nmore complex captions without being restricted by vanilla CLIP's text encoder's\ncontext window and ability limitations. Our experiments demonstrate that this\napproach brings substantial improvements in cross-modal tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP is one of the most important multimodal foundational models today. What\npowers CLIP's capabilities? The rich supervision signals provided by natural\nlanguage, the carrier of human knowledge, shape a powerful cross-modal\nrepresentation space. However, with the rapid advancements in large language\nmodels LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and\ngeneration are continually being pushed. This raises an intriguing question:\ncan the capabilities of LLMs be harnessed to further improve multimodal\nrepresentation learning? The potential benefits of incorporating LLMs into CLIP\nare clear. LLMs' strong textual understanding can fundamentally improve CLIP's\nability to handle image captions, drastically enhancing its ability to process\nlong and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs\nare trained on a vast corpus of text, possessing open-world knowledge. This\nallows them to expand on caption information during training, increasing the\nefficiency of the learning process. In this paper, we propose LLM2CLIP, a novel\napproach that embraces the power of LLMs to unlock CLIP's potential. By\nfine-tuning the LLM in the caption space with contrastive learning, we extract\nits textual capabilities into the output embeddings, significantly improving\nthe output layer's textual discriminability. We then design an efficient\ntraining process where the fine-tuned LLM acts as a powerful teacher for CLIP's\nvisual encoder. Thanks to the LLM's presence, we can now incorporate longer and\nmore complex captions without being restricted by vanilla CLIP's text encoder's\ncontext window and ability limitations. Our experiments demonstrate that this\napproach brings substantial improvements in cross-modal tasks."
                },
                "authors": [
                    {
                        "name": "Weiquan Huang"
                    },
                    {
                        "name": "Aoqi Wu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Liang Hu"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Xiyang Dai"
                    },
                    {
                        "name": "Dongdong Chen"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.09689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09689v1",
                "updated": "2024-11-14T18:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    55,
                    26,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    55,
                    26,
                    3,
                    319,
                    0
                ],
                "title": "LLM Hallucination Reasoning with Zero-shot Knowledge Test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Hallucination Reasoning with Zero-shot Knowledge Test"
                },
                "summary": "LLM hallucination, where LLMs occasionally generate unfaithful text, poses\nsignificant challenges for their practical applications. Most existing\ndetection methods rely on external knowledge, LLM fine-tuning, or\nhallucination-labeled datasets, and they do not distinguish between different\ntypes of hallucinations, which are crucial for improving detection performance.\nWe introduce a new task, Hallucination Reasoning, which classifies\nLLM-generated text into one of three categories: aligned, misaligned, and\nfabricated. Our novel zero-shot method assesses whether LLM has enough\nknowledge about a given prompt and text. Our experiments conducted on new\ndatasets demonstrate the effectiveness of our method in hallucination reasoning\nand underscore its importance for enhancing detection performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM hallucination, where LLMs occasionally generate unfaithful text, poses\nsignificant challenges for their practical applications. Most existing\ndetection methods rely on external knowledge, LLM fine-tuning, or\nhallucination-labeled datasets, and they do not distinguish between different\ntypes of hallucinations, which are crucial for improving detection performance.\nWe introduce a new task, Hallucination Reasoning, which classifies\nLLM-generated text into one of three categories: aligned, misaligned, and\nfabricated. Our novel zero-shot method assesses whether LLM has enough\nknowledge about a given prompt and text. Our experiments conducted on new\ndatasets demonstrate the effectiveness of our method in hallucination reasoning\nand underscore its importance for enhancing detection performance."
                },
                "authors": [
                    {
                        "name": "Seongmin Lee"
                    },
                    {
                        "name": "Hsiang Hsu"
                    },
                    {
                        "name": "Chun-Fu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Fu Chen"
                },
                "author": "Chun-Fu Chen",
                "arxiv_comment": "12 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v1",
                "updated": "2024-11-14T18:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05777v2",
                "updated": "2024-11-14T18:35:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    35,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-08T18:43:15Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    43,
                    15,
                    4,
                    313,
                    0
                ],
                "title": "Quantitative Assessment of Intersectional Empathetic Bias and\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative Assessment of Intersectional Empathetic Bias and\n  Understanding"
                },
                "summary": "A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results."
                },
                "authors": [
                    {
                        "name": "Vojtech Formanek"
                    },
                    {
                        "name": "Ondrej Sotolar"
                    }
                ],
                "author_detail": {
                    "name": "Ondrej Sotolar"
                },
                "author": "Ondrej Sotolar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09660v1",
                "updated": "2024-11-14T18:31:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    31,
                    16,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:31:16Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    31,
                    16,
                    3,
                    319,
                    0
                ],
                "title": "Capacity and Power Consumption of Multi-Layer 6G Networks Using the\n  Upper Mid-Band",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capacity and Power Consumption of Multi-Layer 6G Networks Using the\n  Upper Mid-Band"
                },
                "summary": "This paper presents a new system model to evaluate the capacity and power\nconsumption of multi-layer 6G networks utilising the upper mid-band (FR3). The\nmodel captures heterogeneous 4G, 5G, and 6G deployments, analyzing their\nperformance under different deployment strategies. Our results show that\nstrategic 6G deployments, non-co-located with existing 5G sites, significantly\nenhance throughput, with median and peak user rates of 300 Mbps and exceeding 1\nGbps, respectively. We also emphasize the importance of priority-based cell\nreselection and beam configuration to fully leverage 6G capabilities. While 6G\nimplementation increases power consumption by 33%, non-colocated deployments\nstrike a balance between performance and power consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new system model to evaluate the capacity and power\nconsumption of multi-layer 6G networks utilising the upper mid-band (FR3). The\nmodel captures heterogeneous 4G, 5G, and 6G deployments, analyzing their\nperformance under different deployment strategies. Our results show that\nstrategic 6G deployments, non-co-located with existing 5G sites, significantly\nenhance throughput, with median and peak user rates of 300 Mbps and exceeding 1\nGbps, respectively. We also emphasize the importance of priority-based cell\nreselection and beam configuration to fully leverage 6G capabilities. While 6G\nimplementation increases power consumption by 33%, non-colocated deployments\nstrike a balance between performance and power consumption."
                },
                "authors": [
                    {
                        "name": "David López-Pérez"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Giovanni Geraci"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Geraci"
                },
                "author": "Giovanni Geraci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03862v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03862v3",
                "updated": "2024-11-14T18:27:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    27,
                    39,
                    3,
                    319,
                    0
                ],
                "published": "2024-04-05T02:27:09Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    2,
                    27,
                    9,
                    4,
                    96,
                    0
                ],
                "title": "Verifiable by Design: Aligning Language Models to Quote from\n  Pre-Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifiable by Design: Aligning Language Models to Quote from\n  Pre-Training Data"
                },
                "summary": "To trust the fluent generations of large language models (LLMs), humans must\nbe able to verify their correctness against trusted, external sources. Recent\nefforts, such as providing citations via retrieved documents or post-hoc\nprovenance, enhance verifiability but provide no guarantees on their\ncorrectness. To address these limitations, we tackle the verifiability goal\nwith a different philosophy: trivializing the verification process by\ndeveloping models that quote verbatim statements from trusted sources in their\npre-training data. We propose Quote-Tuning, which demonstrates the feasibility\nof aligning models to quote. The core of Quote-Tuning is a fast membership\ninference function that efficiently verifies text against trusted corpora. We\nleverage this tool to design a reward function to quantify quotes in model\nresponses, and curate datasets for preference learning. Experiments show that\nQuote-Tuning significantly increases verbatim quotes from high-quality\ndocuments by up to 130% relative to base models while maintaining response\nquality. Quote-Tuning is applicable in different tasks, generalizes to\nout-of-domain data and diverse model families, and provides additional benefits\nto truthfulness. Our method not only serves as a hassle-free method to increase\nquoting but also opens up avenues for improving LLM trustworthiness through\nbetter verifiability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To trust the fluent generations of large language models (LLMs), humans must\nbe able to verify their correctness against trusted, external sources. Recent\nefforts, such as providing citations via retrieved documents or post-hoc\nprovenance, enhance verifiability but provide no guarantees on their\ncorrectness. To address these limitations, we tackle the verifiability goal\nwith a different philosophy: trivializing the verification process by\ndeveloping models that quote verbatim statements from trusted sources in their\npre-training data. We propose Quote-Tuning, which demonstrates the feasibility\nof aligning models to quote. The core of Quote-Tuning is a fast membership\ninference function that efficiently verifies text against trusted corpora. We\nleverage this tool to design a reward function to quantify quotes in model\nresponses, and curate datasets for preference learning. Experiments show that\nQuote-Tuning significantly increases verbatim quotes from high-quality\ndocuments by up to 130% relative to base models while maintaining response\nquality. Quote-Tuning is applicable in different tasks, generalizes to\nout-of-domain data and diverse model families, and provides additional benefits\nto truthfulness. Our method not only serves as a hassle-free method to increase\nquoting but also opens up avenues for improving LLM trustworthiness through\nbetter verifiability."
                },
                "authors": [
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Marc Marone"
                    },
                    {
                        "name": "Tianjian Li"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03862v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03862v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09648v1",
                "updated": "2024-11-14T18:17:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    17,
                    30,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:17:30Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    17,
                    30,
                    3,
                    319,
                    0
                ],
                "title": "Med-Bot: An AI-Powered Assistant to Provide Accurate and Reliable\n  Medical Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-Bot: An AI-Powered Assistant to Provide Accurate and Reliable\n  Medical Information"
                },
                "summary": "This paper introduces Med-Bot, an AI-powered chatbot designed to provide\nusers with accurate and reliable medical information. Utilizing advanced\nlibraries and frameworks such as PyTorch, Chromadb, Langchain and Autogptq,\nMed-Bot is built to handle the complexities of natural language understanding\nin a healthcare context. The integration of llamaassisted data processing and\nAutoGPT-Q provides enhanced performance in processing and responding to queries\nbased on PDFs of medical literature, ensuring that users receive precise and\ntrustworthy information. This research details the methodologies employed in\ndeveloping Med-Bot and evaluates its effectiveness in disseminating healthcare\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Med-Bot, an AI-powered chatbot designed to provide\nusers with accurate and reliable medical information. Utilizing advanced\nlibraries and frameworks such as PyTorch, Chromadb, Langchain and Autogptq,\nMed-Bot is built to handle the complexities of natural language understanding\nin a healthcare context. The integration of llamaassisted data processing and\nAutoGPT-Q provides enhanced performance in processing and responding to queries\nbased on PDFs of medical literature, ensuring that users receive precise and\ntrustworthy information. This research details the methodologies employed in\ndeveloping Med-Bot and evaluates its effectiveness in disseminating healthcare\ninformation."
                },
                "authors": [
                    {
                        "name": "Ahan Bhatt"
                    },
                    {
                        "name": "Nandan Vaghela"
                    }
                ],
                "author_detail": {
                    "name": "Nandan Vaghela"
                },
                "author": "Nandan Vaghela",
                "arxiv_comment": "3 figures, 5 pages Keywords-LLM, AI-powered healthcare, Medical\n  chatbot, Context-based interaction, Llama-assisted data processing,\n  AutoGPT-Q, PyTorch, TensorFlow, Reliable medical information, Machine\n  learning in healthcare, Conversational AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09645v1",
                "updated": "2024-11-14T18:14:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    14,
                    32,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:14:32Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    14,
                    32,
                    3,
                    319,
                    0
                ],
                "title": "How do Machine Learning Models Change?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How do Machine Learning Models Change?"
                },
                "summary": "The proliferation of Machine Learning (ML) models and their open-source\nimplementations has transformed Artificial Intelligence research and\napplications. Platforms like Hugging Face (HF) enable the development, sharing,\nand deployment of these models, fostering an evolving ecosystem. While previous\nstudies have examined aspects of models hosted on platforms like HF, a\ncomprehensive longitudinal study of how these models change remains\nunderexplored. This study addresses this gap by utilizing both repository\nmining and longitudinal analysis methods to examine over 200,000 commits and\n1,200 releases from over 50,000 models on HF. We replicate and extend an ML\nchange taxonomy for classifying commits and utilize Bayesian networks to\nuncover patterns in commit and release activities over time. Our findings\nindicate that commit activities align with established data science\nmethodologies, such as CRISP-DM, emphasizing iterative refinement and\ncontinuous improvement. Additionally, release patterns tend to consolidate\nsignificant updates, particularly in documentation, distinguishing between\ngranular changes and milestone-based releases. Furthermore, projects with\nhigher popularity prioritize infrastructure enhancements early in their\nlifecycle, and those with intensive collaboration practices exhibit improved\ndocumentation standards. These and other insights enhance the understanding of\nmodel changes on community platforms and provide valuable guidance for best\npractices in model maintenance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Machine Learning (ML) models and their open-source\nimplementations has transformed Artificial Intelligence research and\napplications. Platforms like Hugging Face (HF) enable the development, sharing,\nand deployment of these models, fostering an evolving ecosystem. While previous\nstudies have examined aspects of models hosted on platforms like HF, a\ncomprehensive longitudinal study of how these models change remains\nunderexplored. This study addresses this gap by utilizing both repository\nmining and longitudinal analysis methods to examine over 200,000 commits and\n1,200 releases from over 50,000 models on HF. We replicate and extend an ML\nchange taxonomy for classifying commits and utilize Bayesian networks to\nuncover patterns in commit and release activities over time. Our findings\nindicate that commit activities align with established data science\nmethodologies, such as CRISP-DM, emphasizing iterative refinement and\ncontinuous improvement. Additionally, release patterns tend to consolidate\nsignificant updates, particularly in documentation, distinguishing between\ngranular changes and milestone-based releases. Furthermore, projects with\nhigher popularity prioritize infrastructure enhancements early in their\nlifecycle, and those with intensive collaboration practices exhibit improved\ndocumentation standards. These and other insights enhance the understanding of\nmodel changes on community platforms and provide valuable guidance for best\npractices in model maintenance."
                },
                "authors": [
                    {
                        "name": "Joel Castaño"
                    },
                    {
                        "name": "Rafael Cabañas"
                    },
                    {
                        "name": "Antonio Salmerón"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Silverio Martínez-Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Silverio Martínez-Fernández"
                },
                "author": "Silverio Martínez-Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04783v2",
                "updated": "2024-11-14T18:14:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    14,
                    0,
                    3,
                    319,
                    0
                ],
                "published": "2024-03-02T16:52:22Z",
                "published_parsed": [
                    2024,
                    3,
                    2,
                    16,
                    52,
                    22,
                    5,
                    62,
                    0
                ],
                "title": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks"
                },
                "summary": "Despite extensive pre-training in moral alignment to prevent generating\nharmful information, large language models (LLMs) remain vulnerable to\njailbreak attacks. In this paper, we propose AutoDefense, a multi-agent defense\nframework that filters harmful responses from LLMs. With the response-filtering\nmechanism, our framework is robust against different jailbreak attack prompts,\nand can be used to defend different victim models. AutoDefense assigns\ndifferent roles to LLM agents and employs them to complete the defense task\ncollaboratively. The division in tasks enhances the overall\ninstruction-following of LLMs and enables the integration of other defense\ncomponents as tools. With AutoDefense, small open-source LMs can serve as\nagents and defend larger models against jailbreak attacks. Our experiments show\nthat AutoDefense can effectively defense against different jailbreak attacks,\nwhile maintaining the performance at normal user request. For example, we\nreduce the attack success rate on GPT-3.5 from 55.74% to 7.95% using\nLLaMA-2-13b with a 3-agent system. Our code and data are publicly available at\nhttps://github.com/XHMY/AutoDefense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite extensive pre-training in moral alignment to prevent generating\nharmful information, large language models (LLMs) remain vulnerable to\njailbreak attacks. In this paper, we propose AutoDefense, a multi-agent defense\nframework that filters harmful responses from LLMs. With the response-filtering\nmechanism, our framework is robust against different jailbreak attack prompts,\nand can be used to defend different victim models. AutoDefense assigns\ndifferent roles to LLM agents and employs them to complete the defense task\ncollaboratively. The division in tasks enhances the overall\ninstruction-following of LLMs and enables the integration of other defense\ncomponents as tools. With AutoDefense, small open-source LMs can serve as\nagents and defend larger models against jailbreak attacks. Our experiments show\nthat AutoDefense can effectively defense against different jailbreak attacks,\nwhile maintaining the performance at normal user request. For example, we\nreduce the attack success rate on GPT-3.5 from 55.74% to 7.95% using\nLLaMA-2-13b with a 3-agent system. Our code and data are publicly available at\nhttps://github.com/XHMY/AutoDefense."
                },
                "authors": [
                    {
                        "name": "Yifan Zeng"
                    },
                    {
                        "name": "Yiran Wu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Huazheng Wang"
                    },
                    {
                        "name": "Qingyun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyun Wu"
                },
                "author": "Qingyun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04573v2",
                "updated": "2024-11-14T18:01:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    1,
                    10,
                    3,
                    319,
                    0
                ],
                "published": "2024-07-05T15:08:44Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    15,
                    8,
                    44,
                    4,
                    187,
                    0
                ],
                "title": "VRSD: Rethinking Similarity and Diversity for Retrieval in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VRSD: Rethinking Similarity and Diversity for Retrieval in Large\n  Language Models"
                },
                "summary": "Vector retrieval algorithms are essential for semantic queries within the\nrapidly evolving landscape of Large Language Models (LLMs). The ability to\nretrieve vectors that satisfy both similarity and diversity criteria\nsubstantially enhances the performance of LLMs. Although Maximal Marginal\nRelevance (MMR) is widely employed in retrieval scenarios requiring relevance\nand diversity, variations in the parameter $\\lambda$ lead to fluctuations that\ncomplicate the optimization trajectory in vector spaces. This obscures the\ndirection of improvement and highlights the lack of a robust theoretical\nanalysis regarding similarity and diversity constraints in retrieval processes.\nTo address these challenges, this paper introduces a novel approach that\ncharacterizes both constraints through the relationship between the sum vector\nand the query vector. The proximity of these vectors ensures the similarity\nconstraint, while requiring individual vectors within the sum vector to diverge\nin their alignment with the query vector satisfies the diversity constraint. We\nfirst formulate a new combinatorial optimization problem, selecting k vectors\nfrom a candidate set such that their sum vector maximally aligns with the query\nvector, and demonstrate that this problem is NP-complete. This result\nunderscores the inherent difficulty of simultaneously achieving similarity and\ndiversity in vector retrieval, thereby providing a theoretical foundation for\nfuture research. Subsequently, we present the heuristic algorithm Vectors\nRetrieval with Similarity and Diversity, VRSD, which features a clear\noptimization objective and eliminates the need for preset parameters. VRSD also\nachieves a modest reduction in time complexity compared to MMR. Empirical\nvalidation confirms that VRSD significantly outperforms MMR across various\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector retrieval algorithms are essential for semantic queries within the\nrapidly evolving landscape of Large Language Models (LLMs). The ability to\nretrieve vectors that satisfy both similarity and diversity criteria\nsubstantially enhances the performance of LLMs. Although Maximal Marginal\nRelevance (MMR) is widely employed in retrieval scenarios requiring relevance\nand diversity, variations in the parameter $\\lambda$ lead to fluctuations that\ncomplicate the optimization trajectory in vector spaces. This obscures the\ndirection of improvement and highlights the lack of a robust theoretical\nanalysis regarding similarity and diversity constraints in retrieval processes.\nTo address these challenges, this paper introduces a novel approach that\ncharacterizes both constraints through the relationship between the sum vector\nand the query vector. The proximity of these vectors ensures the similarity\nconstraint, while requiring individual vectors within the sum vector to diverge\nin their alignment with the query vector satisfies the diversity constraint. We\nfirst formulate a new combinatorial optimization problem, selecting k vectors\nfrom a candidate set such that their sum vector maximally aligns with the query\nvector, and demonstrate that this problem is NP-complete. This result\nunderscores the inherent difficulty of simultaneously achieving similarity and\ndiversity in vector retrieval, thereby providing a theoretical foundation for\nfuture research. Subsequently, we present the heuristic algorithm Vectors\nRetrieval with Similarity and Diversity, VRSD, which features a clear\noptimization objective and eliminates the need for preset parameters. VRSD also\nachieves a modest reduction in time complexity compared to MMR. Empirical\nvalidation confirms that VRSD significantly outperforms MMR across various\ndatasets."
                },
                "authors": [
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09625v1",
                "updated": "2024-11-14T17:49:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    49,
                    27,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:49:27Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    49,
                    27,
                    3,
                    319,
                    0
                ],
                "title": "Local deployment of large-scale music AI models on commodity hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local deployment of large-scale music AI models on commodity hardware"
                },
                "summary": "We present the MIDInfinite, a web application capable of generating symbolic\nmusic using a large-scale generative AI model locally on commodity hardware.\nCreating this demo involved porting the Anticipatory Music Transformer, a large\nlanguage model (LLM) pre-trained on the Lakh MIDI dataset, to the Machine\nLearning Compilation (MLC) framework. Once the model is ported, MLC facilitates\ninference on a variety of runtimes including C++, mobile, and the browser. We\nenvision that MLC has the potential to bridge the gap between the landscape of\nincreasingly capable music AI models and technology more familiar to music\nsoftware developers. As a proof of concept, we build a web application that\nallows users to generate endless streams of multi-instrumental MIDI in the\nbrowser, either from scratch or conditioned on a prompt. On commodity hardware\n(an M3 Macbook Pro), our demo can generate 51 notes per second, which is faster\nthan real-time playback for 72.9% of generations, and increases to 86.3% with 2\nseconds of upfront buffering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the MIDInfinite, a web application capable of generating symbolic\nmusic using a large-scale generative AI model locally on commodity hardware.\nCreating this demo involved porting the Anticipatory Music Transformer, a large\nlanguage model (LLM) pre-trained on the Lakh MIDI dataset, to the Machine\nLearning Compilation (MLC) framework. Once the model is ported, MLC facilitates\ninference on a variety of runtimes including C++, mobile, and the browser. We\nenvision that MLC has the potential to bridge the gap between the landscape of\nincreasingly capable music AI models and technology more familiar to music\nsoftware developers. As a proof of concept, we build a web application that\nallows users to generate endless streams of multi-instrumental MIDI in the\nbrowser, either from scratch or conditioned on a prompt. On commodity hardware\n(an M3 Macbook Pro), our demo can generate 51 notes per second, which is faster\nthan real-time playback for 72.9% of generations, and increases to 86.3% with 2\nseconds of upfront buffering."
                },
                "authors": [
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Charlie Ruan"
                    },
                    {
                        "name": "Zihe Zhao"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Chris Donahue"
                    }
                ],
                "author_detail": {
                    "name": "Chris Donahue"
                },
                "author": "Chris Donahue",
                "arxiv_comment": "2 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09613v1",
                "updated": "2024-11-14T17:33:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    33,
                    36,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:33:36Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    33,
                    36,
                    3,
                    319,
                    0
                ],
                "title": "PTR: Precision-Driven Tool Recommendation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PTR: Precision-Driven Tool Recommendation for Large Language Models"
                },
                "summary": "By augmenting Large Language Models (LLMs) with external tools, their\ncapacity to solve complex problems has been significantly enhanced. However,\ndespite ongoing advancements in the parsing capabilities of LLMs, incorporating\nall available tools simultaneously in the prompt remains impractical due to the\nvast number of external tools. Consequently, it is essential to provide LLMs\nwith a precise set of tools tailored to the specific task, considering both\nquantity and quality. Current tool retrieval methods primarily focus on\nrefining the ranking list of tools and directly packaging a fixed number of\ntop-ranked tools as the tool set. However, these approaches often fail to equip\nLLMs with the optimal set of tools prior to execution, since the optimal number\nof tools for different tasks could be different, resulting in inefficiencies\nsuch as redundant or unsuitable tools, which impede immediate access to the\nmost relevant tools. This paper addresses the challenge of recommending precise\ntoolsets for LLMs. We introduce the problem of tool recommendation, define its\nscope, and propose a novel Precision-driven Tool Recommendation (PTR) approach.\nPTR captures an initial, concise set of tools by leveraging historical tool\nbundle usage and dynamically adjusts the tool set by performing tool matching,\nculminating in a multi-view-based tool addition. Additionally, we present a new\ndataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness\nof tool recommendation for LLMs. We further validate our design choices through\ncomprehensive experiments, demonstrating promising accuracy across two open\nbenchmarks and our RecTools dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By augmenting Large Language Models (LLMs) with external tools, their\ncapacity to solve complex problems has been significantly enhanced. However,\ndespite ongoing advancements in the parsing capabilities of LLMs, incorporating\nall available tools simultaneously in the prompt remains impractical due to the\nvast number of external tools. Consequently, it is essential to provide LLMs\nwith a precise set of tools tailored to the specific task, considering both\nquantity and quality. Current tool retrieval methods primarily focus on\nrefining the ranking list of tools and directly packaging a fixed number of\ntop-ranked tools as the tool set. However, these approaches often fail to equip\nLLMs with the optimal set of tools prior to execution, since the optimal number\nof tools for different tasks could be different, resulting in inefficiencies\nsuch as redundant or unsuitable tools, which impede immediate access to the\nmost relevant tools. This paper addresses the challenge of recommending precise\ntoolsets for LLMs. We introduce the problem of tool recommendation, define its\nscope, and propose a novel Precision-driven Tool Recommendation (PTR) approach.\nPTR captures an initial, concise set of tools by leveraging historical tool\nbundle usage and dynamically adjusts the tool set by performing tool matching,\nculminating in a multi-view-based tool addition. Additionally, we present a new\ndataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness\nof tool recommendation for LLMs. We further validate our design choices through\ncomprehensive experiments, demonstrating promising accuracy across two open\nbenchmarks and our RecTools dataset."
                },
                "authors": [
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09601v1",
                "updated": "2024-11-14T17:21:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    21,
                    2,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:21:02Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    21,
                    2,
                    3,
                    319,
                    0
                ],
                "title": "Accelerating Knowledge Graph and Ontology Engineering with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Knowledge Graph and Ontology Engineering with Large\n  Language Models"
                },
                "summary": "Large Language Models bear the promise of significant acceleration of key\nKnowledge Graph and Ontology Engineering tasks, including ontology modeling,\nextension, modification, population, alignment, as well as entity\ndisambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering\nas a new and coming area of research, and argue that modular approaches to\nontologies will be of central importance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models bear the promise of significant acceleration of key\nKnowledge Graph and Ontology Engineering tasks, including ontology modeling,\nextension, modification, population, alignment, as well as entity\ndisambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering\nas a new and coming area of research, and argue that modular approaches to\nontologies will be of central importance."
                },
                "authors": [
                    {
                        "name": "Cogan Shimizu"
                    },
                    {
                        "name": "Pascal Hitzler"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Hitzler"
                },
                "author": "Pascal Hitzler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09595v1",
                "updated": "2024-11-14T17:08:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    8,
                    23,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:08:23Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    8,
                    23,
                    3,
                    319,
                    0
                ],
                "title": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models"
                },
                "summary": "This work explores expanding the capabilities of large language models (LLMs)\npretrained on text to generate 3D meshes within a unified model. This offers\nkey advantages of (1) leveraging spatial knowledge already embedded in LLMs,\nderived from textual sources like 3D tutorials, and (2) enabling conversational\n3D generation and mesh understanding. A primary challenge is effectively\ntokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly.\nTo address this, we introduce LLaMA-Mesh, a novel approach that represents the\nvertex coordinates and face definitions of 3D meshes as plain text, allowing\ndirect integration with LLMs without expanding the vocabulary. We construct a\nsupervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate\n3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs\nas required, and (3) understand and interpret 3D meshes. Our work is the first\nto demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge\nfor 3D mesh generation in a text-based format, effectively unifying the 3D and\ntext modalities. LLaMA-Mesh achieves mesh generation quality on par with models\ntrained from scratch while maintaining strong text generation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores expanding the capabilities of large language models (LLMs)\npretrained on text to generate 3D meshes within a unified model. This offers\nkey advantages of (1) leveraging spatial knowledge already embedded in LLMs,\nderived from textual sources like 3D tutorials, and (2) enabling conversational\n3D generation and mesh understanding. A primary challenge is effectively\ntokenizing 3D mesh data into discrete tokens that LLMs can process seamlessly.\nTo address this, we introduce LLaMA-Mesh, a novel approach that represents the\nvertex coordinates and face definitions of 3D meshes as plain text, allowing\ndirect integration with LLMs without expanding the vocabulary. We construct a\nsupervised fine-tuning (SFT) dataset enabling pretrained LLMs to (1) generate\n3D meshes from text prompts, (2) produce interleaved text and 3D mesh outputs\nas required, and (3) understand and interpret 3D meshes. Our work is the first\nto demonstrate that LLMs can be fine-tuned to acquire complex spatial knowledge\nfor 3D mesh generation in a text-based format, effectively unifying the 3D and\ntext modalities. LLaMA-Mesh achieves mesh generation quality on par with models\ntrained from scratch while maintaining strong text generation performance."
                },
                "authors": [
                    {
                        "name": "Zhengyi Wang"
                    },
                    {
                        "name": "Jonathan Lorraine"
                    },
                    {
                        "name": "Yikai Wang"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Xiaohui Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohui Zeng"
                },
                "author": "Xiaohui Zeng",
                "arxiv_comment": "See the project website at\n  https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5; I.2.10; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09590v1",
                "updated": "2024-11-14T17:01:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    1,
                    24,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T17:01:24Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    1,
                    24,
                    3,
                    319,
                    0
                ],
                "title": "Adopting RAG for LLM-Aided Future Vehicle Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adopting RAG for LLM-Aided Future Vehicle Design"
                },
                "summary": "In this paper, we explore the integration of Large Language Models (LLMs)\nwith Retrieval-Augmented Generation (RAG) to enhance automated design and\nsoftware development in the automotive industry. We present two case studies: a\nstandardization compliance chatbot and a design copilot, both utilizing RAG to\nprovide accurate, context-aware responses. We evaluate four LLMs-GPT-4o,\nLLAMA3, Mistral, and Mixtral -- comparing their answering accuracy and\nexecution time. Our results demonstrate that while GPT-4 offers superior\nperformance, LLAMA3 and Mistral also show promising capabilities for local\ndeployment, addressing data privacy concerns in automotive applications. This\nstudy highlights the potential of RAG-augmented LLMs in improving design\nworkflows and compliance in automotive engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the integration of Large Language Models (LLMs)\nwith Retrieval-Augmented Generation (RAG) to enhance automated design and\nsoftware development in the automotive industry. We present two case studies: a\nstandardization compliance chatbot and a design copilot, both utilizing RAG to\nprovide accurate, context-aware responses. We evaluate four LLMs-GPT-4o,\nLLAMA3, Mistral, and Mixtral -- comparing their answering accuracy and\nexecution time. Our results demonstrate that while GPT-4 offers superior\nperformance, LLAMA3 and Mistral also show promising capabilities for local\ndeployment, addressing data privacy concerns in automotive applications. This\nstudy highlights the potential of RAG-augmented LLMs in improving design\nworkflows and compliance in automotive engineering."
                },
                "authors": [
                    {
                        "name": "Vahid Zolfaghari"
                    },
                    {
                        "name": "Nenad Petrovic"
                    },
                    {
                        "name": "Fengjunjie Pan"
                    },
                    {
                        "name": "Krzysztof Lebioda"
                    },
                    {
                        "name": "Alois Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois Knoll"
                },
                "author": "Alois Knoll",
                "arxiv_comment": "Conference paper accepted in IEEE FLLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09439v1",
                "updated": "2024-11-14T16:58:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    58,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:58:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    58,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Spider: Any-to-Many Multimodal LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spider: Any-to-Many Multimodal LLM"
                },
                "summary": "Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models\n(LLMs), enabling the integration of various modalities. However, Any-to-Any\nMLLMs are limited to generating pairwise modalities 'Text + X' within a single\nresponse, such as Text + {Image or Audio or Video}. To address this limitation,\nwe introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG)\nframework, which can generate an arbitrary combination of modalities 'Text +\nXs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our\nSpider integrates three core components: a Base Model for basic X-to-X (i.e.,\nAny-to-Any) modality processing, a novel Efficient Decoders-Controller for\ncontrolling multimodal Decoders to generate Xs (many-modal) contents, and an\nAny-to-Many Instruction Template designed for producing Xs signal prompts. To\ntrain Spider, we constructed a novel Text-formatted Many-Modal (TMM) dataset,\nwhich facilitates the learning of the X-to-Xs (i.e., Any-to-Many) capability\nnecessary for AMMG. Ultimately, the well-trained Spider generates a pseudo\nX-to-Xs dataset, the first-ever X-to-Xs many-modal dataset, enhancing the\npotential for AMMG task in future research. Overall, this work not only pushes\nthe boundary of multimodal interaction but also provides rich data support for\nadvancing the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs (MLLMs) have emerged as an extension of Large Language Models\n(LLMs), enabling the integration of various modalities. However, Any-to-Any\nMLLMs are limited to generating pairwise modalities 'Text + X' within a single\nresponse, such as Text + {Image or Audio or Video}. To address this limitation,\nwe introduce Spider, a novel efficient Any-to-Many Modalities Generation (AMMG)\nframework, which can generate an arbitrary combination of modalities 'Text +\nXs', such as Text + {Image and Audio and Video}. To achieve efficient AMMG, our\nSpider integrates three core components: a Base Model for basic X-to-X (i.e.,\nAny-to-Any) modality processing, a novel Efficient Decoders-Controller for\ncontrolling multimodal Decoders to generate Xs (many-modal) contents, and an\nAny-to-Many Instruction Template designed for producing Xs signal prompts. To\ntrain Spider, we constructed a novel Text-formatted Many-Modal (TMM) dataset,\nwhich facilitates the learning of the X-to-Xs (i.e., Any-to-Many) capability\nnecessary for AMMG. Ultimately, the well-trained Spider generates a pseudo\nX-to-Xs dataset, the first-ever X-to-Xs many-modal dataset, enhancing the\npotential for AMMG task in future research. Overall, this work not only pushes\nthe boundary of multimodal interaction but also provides rich data support for\nadvancing the field."
                },
                "authors": [
                    {
                        "name": "Jinxiang Lai"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Xiaocheng Lu"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09580v1",
                "updated": "2024-11-14T16:42:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    42,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:42:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    42,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Software Performance Engineering for Foundation Model-Powered Software\n  (FMware)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Performance Engineering for Foundation Model-Powered Software\n  (FMware)"
                },
                "summary": "The rise of Foundation Models (FMs) like Large Language Models (LLMs) is\nrevolutionizing software development. Despite the impressive prototypes,\ntransforming FMware into production-ready products demands complex engineering\nacross various domains. A critical but overlooked aspect is performance\nengineering, which aims at ensuring FMware meets performance goals such as\nthroughput and latency to avoid user dissatisfaction and financial loss. Often,\nperformance considerations are an afterthought, leading to costly optimization\nefforts post-deployment. FMware's high computational resource demands highlight\nthe need for efficient hardware use. Continuous performance engineering is\nessential to prevent degradation. This paper highlights the significance of\nSoftware Performance Engineering (SPE) in FMware, identifying four key\nchallenges: cognitive architecture design, communication protocols, tuning and\noptimization, and deployment. These challenges are based on literature surveys\nand experiences from developing an in-house FMware system. We discuss problems,\ncurrent practices, and innovative paths for the software engineering community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Foundation Models (FMs) like Large Language Models (LLMs) is\nrevolutionizing software development. Despite the impressive prototypes,\ntransforming FMware into production-ready products demands complex engineering\nacross various domains. A critical but overlooked aspect is performance\nengineering, which aims at ensuring FMware meets performance goals such as\nthroughput and latency to avoid user dissatisfaction and financial loss. Often,\nperformance considerations are an afterthought, leading to costly optimization\nefforts post-deployment. FMware's high computational resource demands highlight\nthe need for efficient hardware use. Continuous performance engineering is\nessential to prevent degradation. This paper highlights the significance of\nSoftware Performance Engineering (SPE) in FMware, identifying four key\nchallenges: cognitive architecture design, communication protocols, tuning and\noptimization, and deployment. These challenges are based on literature surveys\nand experiences from developing an in-house FMware system. We discuss problems,\ncurrent practices, and innovative paths for the software engineering community."
                },
                "authors": [
                    {
                        "name": "Haoxiang Zhang"
                    },
                    {
                        "name": "Shi Chang"
                    },
                    {
                        "name": "Arthur Leung"
                    },
                    {
                        "name": "Kishanthan Thangarajah"
                    },
                    {
                        "name": "Boyuan Chen"
                    },
                    {
                        "name": "Hanan Lutfiyya"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22980v2",
                "updated": "2024-11-14T16:40:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    40,
                    0,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-30T12:45:12Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    12,
                    45,
                    12,
                    2,
                    304,
                    0
                ],
                "title": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient End-to-End 6-Dof Grasp Detection Framework for Edge Devices\n  with Hierarchical Heatmaps and Feature Propagation"
                },
                "summary": "6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments. Benefiting from our end-to-end\nmethodology and efficient network design, our approach surpasses previous\nmethods in model inference efficiency and achieves real-time 6-Dof grasp\ndetection on edge devices. Furthermore, real-world experiments validate the\neffectiveness of our method, achieving a satisfactory 94% object grasping\nsuccess rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6-DoF grasp detection is critically important for the advancement of\nintelligent embodied systems, as it provides feasible robot poses for object\ngrasping. Various methods have been proposed to detect 6-DoF grasps through the\nextraction of 3D geometric features from RGBD or point cloud data. However,\nmost of these approaches encounter challenges during real robot deployment due\nto their significant computational demands, which can be particularly\nproblematic for mobile robot platforms, especially those reliant on edge\ncomputing devices. This paper presents an Efficient End-to-End Grasp Detection\nNetwork (E3GNet) for 6-DoF grasp detection utilizing hierarchical heatmap\nrepresentations. E3GNet effectively identifies high-quality and diverse grasps\nin cluttered real-world environments. Benefiting from our end-to-end\nmethodology and efficient network design, our approach surpasses previous\nmethods in model inference efficiency and achieves real-time 6-Dof grasp\ndetection on edge devices. Furthermore, real-world experiments validate the\neffectiveness of our method, achieving a satisfactory 94% object grasping\nsuccess rate."
                },
                "authors": [
                    {
                        "name": "Kaiqin Yang"
                    },
                    {
                        "name": "Yixiang Dai"
                    },
                    {
                        "name": "Guijin Wang"
                    },
                    {
                        "name": "Siang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siang Chen"
                },
                "author": "Siang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09553v1",
                "updated": "2024-11-14T16:06:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    6,
                    30,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:06:30Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    6,
                    30,
                    3,
                    319,
                    0
                ],
                "title": "OOD-SEG: Out-Of-Distribution detection for image SEGmentation with\n  sparse multi-class positive-only annotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OOD-SEG: Out-Of-Distribution detection for image SEGmentation with\n  sparse multi-class positive-only annotations"
                },
                "summary": "Despite significant advancements, segmentation based on deep neural networks\nin medical and surgical imaging faces several challenges, two of which we aim\nto address in this work. First, acquiring complete pixel-level segmentation\nlabels for medical images is time-consuming and requires domain expertise.\nSecond, typical segmentation pipelines cannot detect out-of-distribution (OOD)\npixels, leaving them prone to spurious outputs during deployment. In this work,\nwe propose a novel segmentation approach exploiting OOD detection that learns\nonly from sparsely annotated pixels from multiple positive-only classes. %but\n\\emph{no background class} annotation. These multi-class positive annotations\nnaturally fall within the in-distribution (ID) set. Unlabelled pixels may\ncontain positive classes but also negative ones, including what is typically\nreferred to as \\emph{background} in standard segmentation formulations. Here,\nwe forgo the need for background annotation and consider these together with\nany other unseen classes as part of the OOD set. Our framework can integrate,\nat a pixel-level, any OOD detection approaches designed for classification\ntasks. To address the lack of existing OOD datasets and established evaluation\nmetric for medical image segmentation, we propose a cross-validation strategy\nthat treats held-out labelled classes as OOD. Extensive experiments on both\nmulti-class hyperspectral and RGB surgical imaging datasets demonstrate the\nrobustness and generalisation capability of our proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements, segmentation based on deep neural networks\nin medical and surgical imaging faces several challenges, two of which we aim\nto address in this work. First, acquiring complete pixel-level segmentation\nlabels for medical images is time-consuming and requires domain expertise.\nSecond, typical segmentation pipelines cannot detect out-of-distribution (OOD)\npixels, leaving them prone to spurious outputs during deployment. In this work,\nwe propose a novel segmentation approach exploiting OOD detection that learns\nonly from sparsely annotated pixels from multiple positive-only classes. %but\n\\emph{no background class} annotation. These multi-class positive annotations\nnaturally fall within the in-distribution (ID) set. Unlabelled pixels may\ncontain positive classes but also negative ones, including what is typically\nreferred to as \\emph{background} in standard segmentation formulations. Here,\nwe forgo the need for background annotation and consider these together with\nany other unseen classes as part of the OOD set. Our framework can integrate,\nat a pixel-level, any OOD detection approaches designed for classification\ntasks. To address the lack of existing OOD datasets and established evaluation\nmetric for medical image segmentation, we propose a cross-validation strategy\nthat treats held-out labelled classes as OOD. Extensive experiments on both\nmulti-class hyperspectral and RGB surgical imaging datasets demonstrate the\nrobustness and generalisation capability of our proposed framework."
                },
                "authors": [
                    {
                        "name": "Junwen Wang"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Oscar MacCormac"
                    },
                    {
                        "name": "Jonathan Shapey"
                    },
                    {
                        "name": "Tom Vercauteren"
                    }
                ],
                "author_detail": {
                    "name": "Tom Vercauteren"
                },
                "author": "Tom Vercauteren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09539v1",
                "updated": "2024-11-14T15:55:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    55,
                    37,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T15:55:37Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    55,
                    37,
                    3,
                    319,
                    0
                ],
                "title": "A Practical Guide to Fine-tuning Language Models with Limited Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Guide to Fine-tuning Language Models with Limited Data"
                },
                "summary": "Employing pre-trained Large Language Models (LLMs) has become the de facto\nstandard in Natural Language Processing (NLP) despite their extensive data\nrequirements. Motivated by the recent surge in research focused on training\nLLMs with limited data, particularly in low-resource domains and languages,\nthis paper surveys recent transfer learning approaches to optimize model\nperformance in downstream tasks where data is scarce. We first address initial\nand continued pre-training strategies to better leverage prior knowledge in\nunseen domains and languages. We then examine how to maximize the utility of\nlimited data during fine-tuning and few-shot learning. The final section takes\na task-specific perspective, reviewing models and methods suited for different\nlevels of data scarcity. Our goal is to provide practitioners with practical\nguidelines for overcoming the challenges posed by constrained data while also\nhighlighting promising directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing pre-trained Large Language Models (LLMs) has become the de facto\nstandard in Natural Language Processing (NLP) despite their extensive data\nrequirements. Motivated by the recent surge in research focused on training\nLLMs with limited data, particularly in low-resource domains and languages,\nthis paper surveys recent transfer learning approaches to optimize model\nperformance in downstream tasks where data is scarce. We first address initial\nand continued pre-training strategies to better leverage prior knowledge in\nunseen domains and languages. We then examine how to maximize the utility of\nlimited data during fine-tuning and few-shot learning. The final section takes\na task-specific perspective, reviewing models and methods suited for different\nlevels of data scarcity. Our goal is to provide practitioners with practical\nguidelines for overcoming the challenges posed by constrained data while also\nhighlighting promising directions for future research."
                },
                "authors": [
                    {
                        "name": "Márton Szép"
                    },
                    {
                        "name": "Daniel Rueckert"
                    },
                    {
                        "name": "Rüdiger von Eisenhart-Rothe"
                    },
                    {
                        "name": "Florian Hinterwimmer"
                    }
                ],
                "author_detail": {
                    "name": "Florian Hinterwimmer"
                },
                "author": "Florian Hinterwimmer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09534v1",
                "updated": "2024-11-14T15:52:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    52,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T15:52:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    52,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Enabling Efficient Wearables: An Analysis of Low-Power Microcontrollers\n  for Biomedical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Wearables: An Analysis of Low-Power Microcontrollers\n  for Biomedical Applications"
                },
                "summary": "Breakthroughs in ultra-low-power chip technology are transforming biomedical\nwearables, making it possible to monitor patients in real time with devices\noperating on mere {\\mu}W. Although many studies have examined the power\nperformance of commercial microcontrollers, it remains unclear which ones\nperform best across diverse application profiles and which hardware features\nare most crucial for minimizing energy consumption under varying computational\nloads. Identifying these features for typical wearable applications and\nunderstanding their effects on performance and energy efficiency are essential\nfor optimizing deployment strategies and informing future hardware designs. In\nthis work, we conduct an in-depth study of state-of-the-art (SoA)\nmicro-controller units(MCUs) in terms of processing capability and energy\nefficiency using representative end-to-end SoA wearable applications. We\nsystematically benchmark each platform across three primary application phases:\nidle, data acquisition, and processing, allowing a holistic assessment of the\nplatform processing capability and overall energy efficiency across varying\npatient-monitoring application profiles. Our detailed analysis of performance\nand energy discrepancies across different platforms reveals key strengths and\nlimitations of the current low-power hardware design and pinpoints the\nstrengths and weaknesses of SoA MCUs. We conclude with actionable insights for\nwearable application designers and hardware engineers, aiming to inform future\nhardware design improvements and support optimal platform selection for\nenergy-constrained biomedical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breakthroughs in ultra-low-power chip technology are transforming biomedical\nwearables, making it possible to monitor patients in real time with devices\noperating on mere {\\mu}W. Although many studies have examined the power\nperformance of commercial microcontrollers, it remains unclear which ones\nperform best across diverse application profiles and which hardware features\nare most crucial for minimizing energy consumption under varying computational\nloads. Identifying these features for typical wearable applications and\nunderstanding their effects on performance and energy efficiency are essential\nfor optimizing deployment strategies and informing future hardware designs. In\nthis work, we conduct an in-depth study of state-of-the-art (SoA)\nmicro-controller units(MCUs) in terms of processing capability and energy\nefficiency using representative end-to-end SoA wearable applications. We\nsystematically benchmark each platform across three primary application phases:\nidle, data acquisition, and processing, allowing a holistic assessment of the\nplatform processing capability and overall energy efficiency across varying\npatient-monitoring application profiles. Our detailed analysis of performance\nand energy discrepancies across different platforms reveals key strengths and\nlimitations of the current low-power hardware design and pinpoints the\nstrengths and weaknesses of SoA MCUs. We conclude with actionable insights for\nwearable application designers and hardware engineers, aiming to inform future\nhardware design improvements and support optimal platform selection for\nenergy-constrained biomedical applications."
                },
                "authors": [
                    {
                        "name": "Dimitrios Samakovlis"
                    },
                    {
                        "name": "Stefano Albini"
                    },
                    {
                        "name": "Rubén Rodríguez Álvarez"
                    },
                    {
                        "name": "Denisa-Andreea Constantinescu"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Miguel Peón-Quirós"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "arxiv_comment": "21 pages, 6 Figures, 6 Tables, Submitted to ACM TECS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08278v2",
                "updated": "2024-11-14T15:49:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    49,
                    46,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-13T01:33:05Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    1,
                    33,
                    5,
                    2,
                    318,
                    0
                ],
                "title": "Knowledge Bases in Support of Large Language Models for Processing Web\n  News",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Bases in Support of Large Language Models for Processing Web\n  News"
                },
                "summary": "Large Language Models (LLMs) have received considerable interest in wide\napplications lately. During pre-training via massive datasets, such a model\nimplicitly memorizes the factual knowledge of trained datasets in its hidden\nparameters. However, knowledge held implicitly in parameters often makes its\nuse by downstream applications ineffective due to the lack of common-sense\nreasoning. In this article, we introduce a general framework that permits to\nbuild knowledge bases with an aid of LLMs, tailored for processing Web news.\nThe framework applies a rule-based News Information Extractor (NewsIE) to news\nitems for extracting their relational tuples, referred to as knowledge bases,\nwhich are then graph-convoluted with the implicit knowledge facts of news items\nobtained by LLMs, for their classification. It involves two lightweight\ncomponents: 1) NewsIE: for extracting the structural information of every news\nitem, in the form of relational tuples; 2) BERTGraph: for graph convoluting the\nimplicit knowledge facts with relational tuples extracted by NewsIE. We have\nevaluated our framework under different news-related datasets for news category\nclassification, with promising experimental results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have received considerable interest in wide\napplications lately. During pre-training via massive datasets, such a model\nimplicitly memorizes the factual knowledge of trained datasets in its hidden\nparameters. However, knowledge held implicitly in parameters often makes its\nuse by downstream applications ineffective due to the lack of common-sense\nreasoning. In this article, we introduce a general framework that permits to\nbuild knowledge bases with an aid of LLMs, tailored for processing Web news.\nThe framework applies a rule-based News Information Extractor (NewsIE) to news\nitems for extracting their relational tuples, referred to as knowledge bases,\nwhich are then graph-convoluted with the implicit knowledge facts of news items\nobtained by LLMs, for their classification. It involves two lightweight\ncomponents: 1) NewsIE: for extracting the structural information of every news\nitem, in the form of relational tuples; 2) BERTGraph: for graph convoluting the\nimplicit knowledge facts with relational tuples extracted by NewsIE. We have\nevaluated our framework under different news-related datasets for news category\nclassification, with promising experimental results."
                },
                "authors": [
                    {
                        "name": "Yihe Zhang"
                    },
                    {
                        "name": "Nabin Pakka"
                    },
                    {
                        "name": "Nian-Feng Tzeng"
                    }
                ],
                "author_detail": {
                    "name": "Nian-Feng Tzeng"
                },
                "author": "Nian-Feng Tzeng",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09523v1",
                "updated": "2024-11-14T15:40:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    40,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T15:40:04Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    40,
                    4,
                    3,
                    319,
                    0
                ],
                "title": "Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats\n  in LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats\n  in LLM-Based Agents"
                },
                "summary": "With the continuous development of large language models (LLMs),\ntransformer-based models have made groundbreaking advances in numerous natural\nlanguage processing (NLP) tasks, leading to the emergence of a series of agents\nthat use LLMs as their control hub. While LLMs have achieved success in various\ntasks, they face numerous security and privacy threats, which become even more\nsevere in the agent scenarios. To enhance the reliability of LLM-based\napplications, a range of research has emerged to assess and mitigate these\nrisks from different perspectives.\n  To help researchers gain a comprehensive understanding of various risks, this\nsurvey collects and analyzes the different threats faced by these agents. To\naddress the challenges posed by previous taxonomies in handling cross-module\nand cross-stage threats, we propose a novel taxonomy framework based on the\nsources and impacts. Additionally, we identify six key features of LLM-based\nagents, based on which we summarize the current research progress and analyze\ntheir limitations. Subsequently, we select four representative agents as case\nstudies to analyze the risks they may face in practical use. Finally, based on\nthe aforementioned analyses, we propose future research directions from the\nperspectives of data, methodology, and policy, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous development of large language models (LLMs),\ntransformer-based models have made groundbreaking advances in numerous natural\nlanguage processing (NLP) tasks, leading to the emergence of a series of agents\nthat use LLMs as their control hub. While LLMs have achieved success in various\ntasks, they face numerous security and privacy threats, which become even more\nsevere in the agent scenarios. To enhance the reliability of LLM-based\napplications, a range of research has emerged to assess and mitigate these\nrisks from different perspectives.\n  To help researchers gain a comprehensive understanding of various risks, this\nsurvey collects and analyzes the different threats faced by these agents. To\naddress the challenges posed by previous taxonomies in handling cross-module\nand cross-stage threats, we propose a novel taxonomy framework based on the\nsources and impacts. Additionally, we identify six key features of LLM-based\nagents, based on which we summarize the current research progress and analyze\ntheir limitations. Subsequently, we select four representative agents as case\nstudies to analyze the risks they may face in practical use. Finally, based on\nthe aforementioned analyses, we propose future research directions from the\nperspectives of data, methodology, and policy, respectively."
                },
                "authors": [
                    {
                        "name": "Yuyou Gan"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Zhe Ma"
                    },
                    {
                        "name": "Ping He"
                    },
                    {
                        "name": "Rui Zeng"
                    },
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Qingming Li"
                    },
                    {
                        "name": "Chunyi Zhou"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Ting Wang"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Yingcai Wu"
                    },
                    {
                        "name": "Shouling Ji"
                    }
                ],
                "author_detail": {
                    "name": "Shouling Ji"
                },
                "author": "Shouling Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09510v1",
                "updated": "2024-11-14T15:19:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    19,
                    1,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T15:19:01Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    19,
                    1,
                    3,
                    319,
                    0
                ],
                "title": "Communication Compression for Tensor Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication Compression for Tensor Parallel LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have pushed the frontier of artificial\nintelligence but are comprised of hundreds of billions of parameters and\noperations. For faster inference latency, LLMs are deployed on multiple\nhardware accelerators through various Model Parallelism strategies. Our paper\nlooks into the details on one such strategy - Tensor Parallel - and proposes to\nreduce latency by compressing inter-accelerator communication. We leverage fine\ngrained quantization techniques to compress selected activations by 3.5 - 4.5x.\nOur proposed method leads up to 2x reduction of time-to-first-token (TTFT) with\nnegligible model performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have pushed the frontier of artificial\nintelligence but are comprised of hundreds of billions of parameters and\noperations. For faster inference latency, LLMs are deployed on multiple\nhardware accelerators through various Model Parallelism strategies. Our paper\nlooks into the details on one such strategy - Tensor Parallel - and proposes to\nreduce latency by compressing inter-accelerator communication. We leverage fine\ngrained quantization techniques to compress selected activations by 3.5 - 4.5x.\nOur proposed method leads up to 2x reduction of time-to-first-token (TTFT) with\nnegligible model performance degradation."
                },
                "authors": [
                    {
                        "name": "Jan Hansen-Palmus"
                    },
                    {
                        "name": "Michael Truong-Le"
                    },
                    {
                        "name": "Oliver Hausdörfer"
                    },
                    {
                        "name": "Alok Verma"
                    }
                ],
                "author_detail": {
                    "name": "Alok Verma"
                },
                "author": "Alok Verma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09507v1",
                "updated": "2024-11-14T15:17:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    17,
                    50,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T15:17:50Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    15,
                    17,
                    50,
                    3,
                    319,
                    0
                ],
                "title": "Toward a Cohesive AI and Simulation Software Ecosystem for Scientific\n  Innovation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Cohesive AI and Simulation Software Ecosystem for Scientific\n  Innovation"
                },
                "summary": "In this paper, we discuss the need for an integrated software stack that\nunites artificial intelligence (AI) and modeling and simulation (ModSim) tools\nto advance scientific discovery. The authors advocate for a unified AI/ModSim\nsoftware ecosystem that ensures compatibility across a wide range of software\non diverse high-performance computing systems, promoting ease of deployment,\nversion management, and binary distribution. Key challenges highlighted include\nbalancing the distinct needs of AI and ModSim, especially in terms of software\nbuild practices, dependency management, and compatibility. The document\nunderscores the importance of continuous integration, community-driven\nstewardship, and collaboration with the Department of Energy (DOE) to develop a\nportable and cohesive scientific software ecosystem. Recommendations focus on\nsupporting standardized environments through initiatives like the Extreme-scale\nScientific Software Stack (E4S) and Spack to foster interdisciplinary\ninnovation and facilitate new scientific advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we discuss the need for an integrated software stack that\nunites artificial intelligence (AI) and modeling and simulation (ModSim) tools\nto advance scientific discovery. The authors advocate for a unified AI/ModSim\nsoftware ecosystem that ensures compatibility across a wide range of software\non diverse high-performance computing systems, promoting ease of deployment,\nversion management, and binary distribution. Key challenges highlighted include\nbalancing the distinct needs of AI and ModSim, especially in terms of software\nbuild practices, dependency management, and compatibility. The document\nunderscores the importance of continuous integration, community-driven\nstewardship, and collaboration with the Department of Energy (DOE) to develop a\nportable and cohesive scientific software ecosystem. Recommendations focus on\nsupporting standardized environments through initiatives like the Extreme-scale\nScientific Software Stack (E4S) and Spack to foster interdisciplinary\ninnovation and facilitate new scientific advancements."
                },
                "authors": [
                    {
                        "name": "Michael A. Heroux"
                    },
                    {
                        "name": "Sameer Shende"
                    },
                    {
                        "name": "Lois Curfman McInnes"
                    },
                    {
                        "name": "Todd Gamblin"
                    },
                    {
                        "name": "James M. Willenbring"
                    }
                ],
                "author_detail": {
                    "name": "James M. Willenbring"
                },
                "author": "James M. Willenbring",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09492v1",
                "updated": "2024-11-14T14:58:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    58,
                    38,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:58:38Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    58,
                    38,
                    3,
                    319,
                    0
                ],
                "title": "MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in\n  LLMs"
                },
                "summary": "Large language models (LLMs) excel in high-resource languages but face\nnotable challenges in low-resource languages like Mongolian. This paper\naddresses these challenges by categorizing capabilities into language abilities\n(syntax and semantics) and cognitive abilities (knowledge and reasoning). To\nsystematically evaluate these areas, we developed MM-Eval, a specialized\ndataset based on Modern Mongolian Language Textbook I and enriched with WebQSP\nand MGSM datasets.\n  Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat,\nLlama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all models\nperformed better on syntactic tasks than semantic tasks, highlighting a gap in\ndeeper language understanding; and 2) knowledge tasks showed a moderate\ndecline, suggesting that models can transfer general knowledge from\nhigh-resource to low-resource contexts.\n  The release of MM-Eval, comprising 569 syntax, 677 semantics, 344 knowledge,\nand 250 reasoning tasks, offers valuable insights for advancing NLP and LLMs in\nlow-resource languages like Mongolian. The dataset is available at\nhttps://github.com/joenahm/MM-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in high-resource languages but face\nnotable challenges in low-resource languages like Mongolian. This paper\naddresses these challenges by categorizing capabilities into language abilities\n(syntax and semantics) and cognitive abilities (knowledge and reasoning). To\nsystematically evaluate these areas, we developed MM-Eval, a specialized\ndataset based on Modern Mongolian Language Textbook I and enriched with WebQSP\nand MGSM datasets.\n  Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat,\nLlama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all models\nperformed better on syntactic tasks than semantic tasks, highlighting a gap in\ndeeper language understanding; and 2) knowledge tasks showed a moderate\ndecline, suggesting that models can transfer general knowledge from\nhigh-resource to low-resource contexts.\n  The release of MM-Eval, comprising 569 syntax, 677 semantics, 344 knowledge,\nand 250 reasoning tasks, offers valuable insights for advancing NLP and LLMs in\nlow-resource languages like Mongolian. The dataset is available at\nhttps://github.com/joenahm/MM-Eval."
                },
                "authors": [
                    {
                        "name": "Mengyuan Zhang"
                    },
                    {
                        "name": "Ruihui Wang"
                    },
                    {
                        "name": "Bo Xia"
                    },
                    {
                        "name": "Yuan Sun"
                    },
                    {
                        "name": "Xiaobing Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaobing Zhao"
                },
                "author": "Xiaobing Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10873v2",
                "updated": "2024-11-14T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    52,
                    36,
                    3,
                    319,
                    0
                ],
                "published": "2024-03-16T09:52:21Z",
                "published_parsed": [
                    2024,
                    3,
                    16,
                    9,
                    52,
                    21,
                    5,
                    76,
                    0
                ],
                "title": "CSI Transfer From Sub-6G to mmWave: Reduced-Overhead Multi-User Hybrid\n  Beamforming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSI Transfer From Sub-6G to mmWave: Reduced-Overhead Multi-User Hybrid\n  Beamforming"
                },
                "summary": "Hybrid beamforming is vital in modern wireless systems, especially for\nmassive MIMO and millimeter-wave (mmWave) deployments, offering efficient\ndirectional transmission with reduced hardware complexity. However, effective\nbeamforming in multi-user scenarios relies heavily on accurate channel state\ninformation, the acquisition of which often requires significant pilot\noverhead, degrading system performance. To address this and inspired by the\nspatial congruence between sub-6GHz (sub-6G) and mmWave channels, we propose a\nSub-6G information Aided Multi-User Hybrid Beamforming (SA-MUHBF) framework,\navoiding excessive use of pilots at mmWave. SA-MUHBF employs a convolutional\nneural network to predict mmWave beamspace from sub-6G channel estimate,\nfollowed by a novel multi-layer graph neural network for analog beam selection\nand a linear minimum mean-square error algorithm for digital beamforming.\nNumerical results demonstrate that SA-MUHBF efficiently predicts the mmWave\nbeamspace representation and achieves superior spectrum efficiency over\nstate-of-the-art benchmarks. Moreover, SA-MUHBF demonstrates robust performance\nacross varied sub-6G system configurations and exhibits strong generalization\nto unseen scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid beamforming is vital in modern wireless systems, especially for\nmassive MIMO and millimeter-wave (mmWave) deployments, offering efficient\ndirectional transmission with reduced hardware complexity. However, effective\nbeamforming in multi-user scenarios relies heavily on accurate channel state\ninformation, the acquisition of which often requires significant pilot\noverhead, degrading system performance. To address this and inspired by the\nspatial congruence between sub-6GHz (sub-6G) and mmWave channels, we propose a\nSub-6G information Aided Multi-User Hybrid Beamforming (SA-MUHBF) framework,\navoiding excessive use of pilots at mmWave. SA-MUHBF employs a convolutional\nneural network to predict mmWave beamspace from sub-6G channel estimate,\nfollowed by a novel multi-layer graph neural network for analog beam selection\nand a linear minimum mean-square error algorithm for digital beamforming.\nNumerical results demonstrate that SA-MUHBF efficiently predicts the mmWave\nbeamspace representation and achieves superior spectrum efficiency over\nstate-of-the-art benchmarks. Moreover, SA-MUHBF demonstrates robust performance\nacross varied sub-6G system configurations and exhibits strong generalization\nto unseen scenarios."
                },
                "authors": [
                    {
                        "name": "Weicao Deng"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Ming-Min Zhao"
                    },
                    {
                        "name": "Min-Jian Zhao"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "Accepted by IEEE JSAC NGAT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.10873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06900v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06900v5",
                "updated": "2024-11-14T14:28:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    58,
                    3,
                    319,
                    0
                ],
                "published": "2024-02-10T07:55:27Z",
                "published_parsed": [
                    2024,
                    2,
                    10,
                    7,
                    55,
                    27,
                    5,
                    41,
                    0
                ],
                "title": "Can LLMs Recognize Toxicity? A Structured Investigation Framework and\n  Toxicity Metric",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Recognize Toxicity? A Structured Investigation Framework and\n  Toxicity Metric"
                },
                "summary": "In the pursuit of developing Large Language Models (LLMs) that adhere to\nsocietal standards, it is imperative to detect the toxicity in the generated\ntext. The majority of existing toxicity metrics rely on encoder models trained\non specific toxicity datasets, which are susceptible to out-of-distribution\n(OOD) problems and depend on the dataset's definition of toxicity. In this\npaper, we introduce a robust metric grounded on LLMs to flexibly measure\ntoxicity according to the given definition. We first analyze the toxicity\nfactors, followed by an examination of the intrinsic toxic attributes of LLMs\nto ascertain their suitability as evaluators. Finally, we evaluate the\nperformance of our metric with detailed analysis. Our empirical results\ndemonstrate outstanding performance in measuring toxicity within verified\nfactors, improving on conventional metrics by 12 points in the F1 score. Our\nfindings also indicate that upstream toxicity significantly influences\ndownstream metrics, suggesting that LLMs are unsuitable for toxicity\nevaluations within unverified factors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the pursuit of developing Large Language Models (LLMs) that adhere to\nsocietal standards, it is imperative to detect the toxicity in the generated\ntext. The majority of existing toxicity metrics rely on encoder models trained\non specific toxicity datasets, which are susceptible to out-of-distribution\n(OOD) problems and depend on the dataset's definition of toxicity. In this\npaper, we introduce a robust metric grounded on LLMs to flexibly measure\ntoxicity according to the given definition. We first analyze the toxicity\nfactors, followed by an examination of the intrinsic toxic attributes of LLMs\nto ascertain their suitability as evaluators. Finally, we evaluate the\nperformance of our metric with detailed analysis. Our empirical results\ndemonstrate outstanding performance in measuring toxicity within verified\nfactors, improving on conventional metrics by 12 points in the F1 score. Our\nfindings also indicate that upstream toxicity significantly influences\ndownstream metrics, suggesting that LLMs are unsuitable for toxicity\nevaluations within unverified factors."
                },
                "authors": [
                    {
                        "name": "Hyukhun Koh"
                    },
                    {
                        "name": "Dohyung Kim"
                    },
                    {
                        "name": "Minwoo Lee"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "arxiv_comment": "8 page long",
                "arxiv_journal_ref": "EMNLP2024 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06900v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06900v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07668v2",
                "updated": "2024-11-14T14:28:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    24,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-12T09:35:23Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    9,
                    35,
                    23,
                    1,
                    317,
                    0
                ],
                "title": "Towards Evaluation Guidelines for Empirical Studies involving LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Evaluation Guidelines for Empirical Studies involving LLMs"
                },
                "summary": "In the short period since the release of ChatGPT in November 2022, large\nlanguage models (LLMs) have changed the software engineering research\nlandscape. While there are numerous opportunities to use LLMs for supporting\nresearch or software engineering tasks, solid science needs rigorous empirical\nevaluations. However, so far, there are no specific guidelines for conducting\nand assessing studies involving LLMs in software engineering research. Our\nfocus is on empirical studies that either use LLMs as part of the research\nprocess (e.g., for data annotation) or studies that evaluate existing or new\ntools that are based on LLMs. This paper contributes the first set of\nguidelines for such studies. Our goal is to start a discussion in the software\nengineering research community to reach a common understanding of what our\ncommunity standards are for high-quality empirical studies involving LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the short period since the release of ChatGPT in November 2022, large\nlanguage models (LLMs) have changed the software engineering research\nlandscape. While there are numerous opportunities to use LLMs for supporting\nresearch or software engineering tasks, solid science needs rigorous empirical\nevaluations. However, so far, there are no specific guidelines for conducting\nand assessing studies involving LLMs in software engineering research. Our\nfocus is on empirical studies that either use LLMs as part of the research\nprocess (e.g., for data annotation) or studies that evaluate existing or new\ntools that are based on LLMs. This paper contributes the first set of\nguidelines for such studies. Our goal is to start a discussion in the software\nengineering research community to reach a common understanding of what our\ncommunity standards are for high-quality empirical studies involving LLMs."
                },
                "authors": [
                    {
                        "name": "Stefan Wagner"
                    },
                    {
                        "name": "Marvin Muñoz Barón"
                    },
                    {
                        "name": "Davide Falessi"
                    },
                    {
                        "name": "Sebastian Baltes"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Baltes"
                },
                "author": "Sebastian Baltes",
                "arxiv_comment": "4 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08586v2",
                "updated": "2024-11-14T14:07:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    7,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-13T13:09:14Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    13,
                    9,
                    14,
                    2,
                    318,
                    0
                ],
                "title": "Optimizing Automatic Summarization of Long Clinical Records Using\n  Dynamic Context Extension:Testing and Evaluation of the NBCE Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Automatic Summarization of Long Clinical Records Using\n  Dynamic Context Extension:Testing and Evaluation of the NBCE Method"
                },
                "summary": "Summarizing patient clinical notes is vital for reducing documentation\nburdens. Current manual summarization makes medical staff struggle. We propose\nan automatic method using LLMs, but long inputs cause LLMs to lose context,\nreducing output quality especially in small size model. We used a 7B model,\nopen-calm-7b, enhanced with Native Bayes Context Extend and a redesigned\ndecoding mechanism to reference one sentence at a time, keeping inputs within\ncontext windows, 2048 tokens. Our improved model achieved near parity with\nGoogle's over 175B Gemini on ROUGE-L metrics with 200 samples, indicating\nstrong performance using less resources, enhancing automated EMR summarization\nfeasibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summarizing patient clinical notes is vital for reducing documentation\nburdens. Current manual summarization makes medical staff struggle. We propose\nan automatic method using LLMs, but long inputs cause LLMs to lose context,\nreducing output quality especially in small size model. We used a 7B model,\nopen-calm-7b, enhanced with Native Bayes Context Extend and a redesigned\ndecoding mechanism to reference one sentence at a time, keeping inputs within\ncontext windows, 2048 tokens. Our improved model achieved near parity with\nGoogle's over 175B Gemini on ROUGE-L metrics with 200 samples, indicating\nstrong performance using less resources, enhancing automated EMR summarization\nfeasibility."
                },
                "authors": [
                    {
                        "name": "Guoqing Zhang"
                    },
                    {
                        "name": "Keita Fukuyama"
                    },
                    {
                        "name": "Kazumasa Kishimoto"
                    },
                    {
                        "name": "Tomohiro Kuroda"
                    }
                ],
                "author_detail": {
                    "name": "Tomohiro Kuroda"
                },
                "author": "Tomohiro Kuroda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15933v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15933v2",
                "updated": "2024-11-14T13:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    59,
                    15,
                    3,
                    319,
                    0
                ],
                "published": "2024-09-24T09:57:25Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    9,
                    57,
                    25,
                    1,
                    268,
                    0
                ],
                "title": "SLIMER-IT: Zero-Shot NER on Italian Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLIMER-IT: Zero-Shot NER on Italian Language"
                },
                "summary": "Traditional approaches to Named Entity Recognition (NER) frame the task into\na BIO sequence labeling problem. Although these systems often excel in the\ndownstream task at hand, they require extensive annotated data and struggle to\ngeneralize to out-of-distribution input domains and unseen entity types. On the\ncontrary, Large Language Models (LLMs) have demonstrated strong zero-shot\ncapabilities. While several works address Zero-Shot NER in English, little has\nbeen done in other languages. In this paper, we define an evaluation framework\nfor Zero-Shot NER, applying it to the Italian language. Furthermore, we\nintroduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning\napproach for zero-shot NER leveraging prompts enriched with definition and\nguidelines. Comparisons with other state-of-the-art models, demonstrate the\nsuperiority of SLIMER-IT on never-seen-before entity tags.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional approaches to Named Entity Recognition (NER) frame the task into\na BIO sequence labeling problem. Although these systems often excel in the\ndownstream task at hand, they require extensive annotated data and struggle to\ngeneralize to out-of-distribution input domains and unseen entity types. On the\ncontrary, Large Language Models (LLMs) have demonstrated strong zero-shot\ncapabilities. While several works address Zero-Shot NER in English, little has\nbeen done in other languages. In this paper, we define an evaluation framework\nfor Zero-Shot NER, applying it to the Italian language. Furthermore, we\nintroduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning\napproach for zero-shot NER leveraging prompts enriched with definition and\nguidelines. Comparisons with other state-of-the-art models, demonstrate the\nsuperiority of SLIMER-IT on never-seen-before entity tags."
                },
                "authors": [
                    {
                        "name": "Andrew Zamai"
                    },
                    {
                        "name": "Leonardo Rigutini"
                    },
                    {
                        "name": "Marco Maggini"
                    },
                    {
                        "name": "Andrea Zugarini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zugarini"
                },
                "author": "Andrea Zugarini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15933v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15933v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15696v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15696v2",
                "updated": "2024-11-14T13:57:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    57,
                    37,
                    3,
                    319,
                    0
                ],
                "published": "2024-08-28T10:51:18Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    10,
                    51,
                    18,
                    2,
                    241,
                    0
                ],
                "title": "Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: a case study on Baidu, Ernie and Qwen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: a case study on Baidu, Ernie and Qwen"
                },
                "summary": "Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective."
                },
                "authors": [
                    {
                        "name": "Geng Liu"
                    },
                    {
                        "name": "Carlo Alberto Bono"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15696v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15696v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17586v2",
                "updated": "2024-11-14T13:27:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    27,
                    45,
                    3,
                    319,
                    0
                ],
                "published": "2024-06-25T14:28:21Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    28,
                    21,
                    1,
                    177,
                    0
                ],
                "title": "Benchmarking SLAM Algorithms in the Cloud: The SLAM Hive Benchmarking\n  Suite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking SLAM Algorithms in the Cloud: The SLAM Hive Benchmarking\n  Suite"
                },
                "summary": "Evaluating the performance of Simultaneous Localization and Mapping (SLAM)\nalgorithms is essential for scientists and users of robotic systems alike. But\nthere are a multitude of different permutations of possible options of hardware\nsetups and algorithm configurations, as well as different datasets and\nalgorithms, such that it was previously infeasible to thoroughly compare SLAM\nsystems against the full state of the art. To solve that we present the SLAM\nHive Benchmarking Suite, which is able to analyze SLAM algorithms in 1000's of\nmapping runs, through its utilization of container technology and deployment in\nthe cloud. This paper presents the architecture and open source implementation\nof SLAM Hive and compares it to existing efforts on SLAM evaluation. We perform\nmapping runs with popular visual, RGBD and LiDAR based SLAM algorithms against\ncommonly used datasets and show how SLAM Hive can be used to conveniently\nanalyze the results against various aspects. Through this we envision that SLAM\nHive can become an essential tool for proper comparisons and evaluations of\nSLAM algorithms and thus drive the scientific development in the research on\nSLAM. The open source software as well as a demo to show the live analysis of\n1000's of mapping runs can be found on our SLAM Hive website.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the performance of Simultaneous Localization and Mapping (SLAM)\nalgorithms is essential for scientists and users of robotic systems alike. But\nthere are a multitude of different permutations of possible options of hardware\nsetups and algorithm configurations, as well as different datasets and\nalgorithms, such that it was previously infeasible to thoroughly compare SLAM\nsystems against the full state of the art. To solve that we present the SLAM\nHive Benchmarking Suite, which is able to analyze SLAM algorithms in 1000's of\nmapping runs, through its utilization of container technology and deployment in\nthe cloud. This paper presents the architecture and open source implementation\nof SLAM Hive and compares it to existing efforts on SLAM evaluation. We perform\nmapping runs with popular visual, RGBD and LiDAR based SLAM algorithms against\ncommonly used datasets and show how SLAM Hive can be used to conveniently\nanalyze the results against various aspects. Through this we envision that SLAM\nHive can become an essential tool for proper comparisons and evaluations of\nSLAM algorithms and thus drive the scientific development in the research on\nSLAM. The open source software as well as a demo to show the live analysis of\n1000's of mapping runs can be found on our SLAM Hive website."
                },
                "authors": [
                    {
                        "name": "Xinzhe Liu"
                    },
                    {
                        "name": "Yuanyuan Yang"
                    },
                    {
                        "name": "Bowen Xu"
                    },
                    {
                        "name": "Delin Feng"
                    },
                    {
                        "name": "Sören Schwertfeger"
                    }
                ],
                "author_detail": {
                    "name": "Sören Schwertfeger"
                },
                "author": "Sören Schwertfeger",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2303.11854",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09413v1",
                "updated": "2024-11-14T13:07:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    7,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:07:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    7,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Script-centric behavior understanding for assisted autism spectrum\n  disorder diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Script-centric behavior understanding for assisted autism spectrum\n  disorder diagnosis"
                },
                "summary": "Observing and analyzing children's social behaviors is crucial for the early\ndiagnosis of Autism Spectrum Disorders (ASD). This work focuses on\nautomatically detecting ASD using computer vision techniques and large language\nmodels (LLMs). Existing methods typically rely on supervised learning. However,\nthe scarcity of ASD diagnostic datasets and the lack of interpretability in\ndiagnostic results significantly limits its clinical application. To address\nthese challenges, we introduce a novel unsupervised approach based on\nscript-centric behavior understanding. Our pipeline converts video content into\nscripts that describe the behavior of characters, leveraging the\ngeneralizability of large language models to detect ASD in a zero-shot or\nfew-shot manner. Specifically, we propose a scripts transcription module for\nmultimodal behavior data textualization and a domain prompts module to bridge\nLLMs. Our method achieves an accuracy of 92.00\\% in diagnosing ASD in children\nwith an average age of 24 months, surpassing the performance of supervised\nlearning methods by 3.58\\% absolutely. Extensive experiments confirm the\neffectiveness of our approach and suggest its potential for advancing ASD\nresearch through LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observing and analyzing children's social behaviors is crucial for the early\ndiagnosis of Autism Spectrum Disorders (ASD). This work focuses on\nautomatically detecting ASD using computer vision techniques and large language\nmodels (LLMs). Existing methods typically rely on supervised learning. However,\nthe scarcity of ASD diagnostic datasets and the lack of interpretability in\ndiagnostic results significantly limits its clinical application. To address\nthese challenges, we introduce a novel unsupervised approach based on\nscript-centric behavior understanding. Our pipeline converts video content into\nscripts that describe the behavior of characters, leveraging the\ngeneralizability of large language models to detect ASD in a zero-shot or\nfew-shot manner. Specifically, we propose a scripts transcription module for\nmultimodal behavior data textualization and a domain prompts module to bridge\nLLMs. Our method achieves an accuracy of 92.00\\% in diagnosing ASD in children\nwith an average age of 24 months, surpassing the performance of supervised\nlearning methods by 3.58\\% absolutely. Extensive experiments confirm the\neffectiveness of our approach and suggest its potential for advancing ASD\nresearch through LLMs."
                },
                "authors": [
                    {
                        "name": "Wenxing Liu"
                    },
                    {
                        "name": "Yueran Pan"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "5 pages, 4 figures, submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09410v1",
                "updated": "2024-11-14T13:00:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    0,
                    23,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:00:23Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    0,
                    23,
                    3,
                    319,
                    0
                ],
                "title": "LLM-assisted Explicit and Implicit Multi-interest Learning Framework for\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-assisted Explicit and Implicit Multi-interest Learning Framework for\n  Sequential Recommendation"
                },
                "summary": "Multi-interest modeling in current recommender systems (RS) is mainly based\non user behavioral data, capturing user interest preferences from multiple\ndimensions. However, since behavioral data is implicit and often highly sparse,\nit is challenging to understand users' complex and diverse interests. Recent\nstudies have shown that the rich semantic information in the text can\neffectively supplement the deficiencies of behavioral data. Despite this, it is\nstill difficult for small models to directly extract semantic features\nassociated with users' deep interests. That is, how to effectively align\nsemantics with behavioral information to form a more comprehensive and accurate\nunderstanding of user interests has become a critical research problem.To\naddress this, we propose an LLM-assisted explicit and implicit multi-interest\nlearning framework (named EIMF) to model user interests on two levels: behavior\nand semantics. The framework consists of two parts: Implicit Behavioral\nInterest Module (IBIM) and Explicit Semantic Interest Module (ESIM). The\ntraditional multi-interest RS model in IBIM can learn users' implicit\nbehavioral interests from interactions with items. In ESIM, we first adopt a\nclustering algorithm to select typical samples and design a prompting strategy\non LLM to obtain explicit semantic interests. Furthermore, in the training\nphase, the semantic interests of typical samples can enhance the representation\nlearning of behavioral interests based on the multi-task learning on semantic\nprediction and modality alignment. Therefore, in the inference stage, accurate\nrecommendations can be achieved with only the user's behavioral data. Extensive\nexperiments on real-world datasets demonstrate the effectiveness of the\nproposed EIMF framework, which effectively and efficiently combines small\nmodels with LLM to improve the accuracy of multi-interest modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-interest modeling in current recommender systems (RS) is mainly based\non user behavioral data, capturing user interest preferences from multiple\ndimensions. However, since behavioral data is implicit and often highly sparse,\nit is challenging to understand users' complex and diverse interests. Recent\nstudies have shown that the rich semantic information in the text can\neffectively supplement the deficiencies of behavioral data. Despite this, it is\nstill difficult for small models to directly extract semantic features\nassociated with users' deep interests. That is, how to effectively align\nsemantics with behavioral information to form a more comprehensive and accurate\nunderstanding of user interests has become a critical research problem.To\naddress this, we propose an LLM-assisted explicit and implicit multi-interest\nlearning framework (named EIMF) to model user interests on two levels: behavior\nand semantics. The framework consists of two parts: Implicit Behavioral\nInterest Module (IBIM) and Explicit Semantic Interest Module (ESIM). The\ntraditional multi-interest RS model in IBIM can learn users' implicit\nbehavioral interests from interactions with items. In ESIM, we first adopt a\nclustering algorithm to select typical samples and design a prompting strategy\non LLM to obtain explicit semantic interests. Furthermore, in the training\nphase, the semantic interests of typical samples can enhance the representation\nlearning of behavioral interests based on the multi-task learning on semantic\nprediction and modality alignment. Therefore, in the inference stage, accurate\nrecommendations can be achieved with only the user's behavioral data. Extensive\nexperiments on real-world datasets demonstrate the effectiveness of the\nproposed EIMF framework, which effectively and efficiently combines small\nmodels with LLM to improve the accuracy of multi-interest modeling."
                },
                "authors": [
                    {
                        "name": "Shutong Qiao"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12514v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12514v4",
                "updated": "2024-11-14T12:03:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    12,
                    3,
                    37,
                    3,
                    319,
                    0
                ],
                "published": "2024-09-19T07:10:18Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    7,
                    10,
                    18,
                    3,
                    263,
                    0
                ],
                "title": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for\n  Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have shown remarkable potential in\nvisuomotor control and instruction comprehension through end-to-end learning\nprocesses. However, current VLA models face significant challenges: they are\nslow during inference and require extensive pre-training on large amounts of\nrobotic data, making real-world deployment difficult. In this paper, we\nintroduce a new family of compact vision-language-action models, called\nTinyVLA, which offers two key advantages over existing VLA models: (1) faster\ninference speeds, and (2) improved data efficiency, eliminating the need for\npre-training stage. Our framework incorporates two essential components to\nbuild TinyVLA: (1) initializing the policy backbone with robust, high-speed\nmultimodal models, and (2) integrating a diffusion policy decoder during\nfine-tuning to enable precise robot actions. We conducted extensive evaluations\nof TinyVLA in both simulation and on real robots, demonstrating that our\napproach significantly outperforms the state-of-the-art VLA model, OpenVLA, in\nterms of speed and data efficiency, while delivering comparable or superior\nperformance. Additionally, TinyVLA exhibits strong generalization capabilities\nacross various dimensions, including language instructions, novel objects,\nunseen positions, changes in object appearance, background variations, and\nenvironmental shifts, often matching or exceeding the performance of OpenVLA.\nWe believe that \\methodname offers an interesting perspective on utilizing\npre-trained multimodal models for policy learning. Our project is at\nhttps://tiny-vla.github.io."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Jinming Li"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Zhiyuan Xu"
                    },
                    {
                        "name": "Ning Liu"
                    },
                    {
                        "name": "Ran Cheng"
                    },
                    {
                        "name": "Chaomin Shen"
                    },
                    {
                        "name": "Yaxin Peng"
                    },
                    {
                        "name": "Feifei Feng"
                    },
                    {
                        "name": "Jian Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Tang"
                },
                "author": "Jian Tang",
                "arxiv_comment": "add more citations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12514v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12514v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07940v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07940v3",
                "updated": "2024-11-14T11:51:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    11,
                    51,
                    0,
                    3,
                    319,
                    0
                ],
                "published": "2024-03-11T02:06:30Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    2,
                    6,
                    30,
                    0,
                    71,
                    0
                ],
                "title": "InfiBench: Evaluating the Question-Answering Capabilities of Code Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiBench: Evaluating the Question-Answering Capabilities of Code Large\n  Language Models"
                },
                "summary": "Large Language Models for code (code LLMs) have witnessed tremendous progress\nin recent years. With the rapid development of code LLMs, many popular\nevaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to\nmeasure the performance of code LLMs with a particular focus on code generation\ntasks. However, they are insufficient to cover the full range of expected\ncapabilities of code LLMs, which span beyond code generation to answering\ndiverse coding-related questions. To fill this gap, we propose InfiBench, the\nfirst large-scale freeform question-answering (QA) benchmark for code to our\nknowledge, comprising 234 carefully selected high-quality Stack Overflow\nquestions that span across 15 programming languages. InfiBench uses four types\nof model-free automatic metrics to evaluate response correctness where domain\nexperts carefully concretize the criterion for each question. We conduct a\nsystematic evaluation for over 100 latest code LLMs on InfiBench, leading to a\nseries of novel and insightful findings. Our detailed analyses showcase\npotential directions for further advancement of code LLMs. InfiBench is fully\nopen source at https://infi-coder.github.io/infibench and continuously\nexpanding to foster more scientific and systematic practices for code LLM\nevaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for code (code LLMs) have witnessed tremendous progress\nin recent years. With the rapid development of code LLMs, many popular\nevaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to\nmeasure the performance of code LLMs with a particular focus on code generation\ntasks. However, they are insufficient to cover the full range of expected\ncapabilities of code LLMs, which span beyond code generation to answering\ndiverse coding-related questions. To fill this gap, we propose InfiBench, the\nfirst large-scale freeform question-answering (QA) benchmark for code to our\nknowledge, comprising 234 carefully selected high-quality Stack Overflow\nquestions that span across 15 programming languages. InfiBench uses four types\nof model-free automatic metrics to evaluate response correctness where domain\nexperts carefully concretize the criterion for each question. We conduct a\nsystematic evaluation for over 100 latest code LLMs on InfiBench, leading to a\nseries of novel and insightful findings. Our detailed analyses showcase\npotential directions for further advancement of code LLMs. InfiBench is fully\nopen source at https://infi-coder.github.io/infibench and continuously\nexpanding to foster more scientific and systematic practices for code LLM\nevaluation."
                },
                "authors": [
                    {
                        "name": "Linyi Li"
                    },
                    {
                        "name": "Shijie Geng"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Yibo He"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Ziyue Hua"
                    },
                    {
                        "name": "Guanghan Ning"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "arxiv_comment": "31 pages. Appear at NeurIPS 2024 Datasets and Benchmarks track.\n  Project website: https://infi-coder.github.io/infibench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07940v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07940v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09380v1",
                "updated": "2024-11-14T11:48:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    11,
                    48,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T11:48:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    11,
                    48,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Connecting the Unconnected: A DT Case Study of Nomadic Nodes Deployment\n  in Nepal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connecting the Unconnected: A DT Case Study of Nomadic Nodes Deployment\n  in Nepal"
                },
                "summary": "This paper addresses the challenge of robust cellular connectivity in dense,\nunderdeveloped urban environments, specifically focusing on Kathmandu, Nepal.\nAs cities grow, existing cellular infrastructure struggles to meet the demand\nfor reliable, high-throughput, and low-latency communication services. The lack\nof investment in new technologies and the intricacies of the cities' landscape\npose even more difficulties for robust connectivity. This work addresses the\nabove challenges in a cost-effective and flexible way. We investigate the\ndeployment of LTE Nomadic Nodes (NNs) at scale in order to enhance network\ncapacity and coverage. Utilising a Digital Twin (DT), we simulate and optimise\nNN placement, considering Kathmandu's physical and environmental\ncharacteristics. Our approach leverages the DRIVE DT framework, which enables\nthe systemic evaluation of various network configurations and user mobility\nscenarios. The results demonstrate that NNs significantly improve signal\nstrength and expected user datarates, presenting a viable solution for\nenhancing urban cellular connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of robust cellular connectivity in dense,\nunderdeveloped urban environments, specifically focusing on Kathmandu, Nepal.\nAs cities grow, existing cellular infrastructure struggles to meet the demand\nfor reliable, high-throughput, and low-latency communication services. The lack\nof investment in new technologies and the intricacies of the cities' landscape\npose even more difficulties for robust connectivity. This work addresses the\nabove challenges in a cost-effective and flexible way. We investigate the\ndeployment of LTE Nomadic Nodes (NNs) at scale in order to enhance network\ncapacity and coverage. Utilising a Digital Twin (DT), we simulate and optimise\nNN placement, considering Kathmandu's physical and environmental\ncharacteristics. Our approach leverages the DRIVE DT framework, which enables\nthe systemic evaluation of various network configurations and user mobility\nscenarios. The results demonstrate that NNs significantly improve signal\nstrength and expected user datarates, presenting a viable solution for\nenhancing urban cellular connectivity."
                },
                "authors": [
                    {
                        "name": "Ioannis Mavromatis"
                    },
                    {
                        "name": "Angeliki Katsenou"
                    },
                    {
                        "name": "Klodian Bardhi"
                    },
                    {
                        "name": "Evangelos Xenos"
                    },
                    {
                        "name": "Dimitra Simeonidou"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Simeonidou"
                },
                "author": "Dimitra Simeonidou",
                "arxiv_comment": "Accepted for publication at IEEE CCNC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18406v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18406v2",
                "updated": "2024-11-14T10:55:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    55,
                    14,
                    3,
                    319,
                    0
                ],
                "published": "2024-06-26T14:57:38Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    14,
                    57,
                    38,
                    2,
                    178,
                    0
                ],
                "title": "IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying\n  and Reweighting Context-Aware Neurons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRCAN: Mitigating Knowledge Conflicts in LLM Generation via Identifying\n  and Reweighting Context-Aware Neurons"
                },
                "summary": "It is widely acknowledged that large language models (LLMs) encode a vast\nreservoir of knowledge after being trained on mass data. Recent studies\ndisclose knowledge conflicts in LLM generation, wherein outdated or incorrect\nparametric knowledge (i.e., encoded knowledge) contradicts new knowledge\nprovided in the context. To mitigate such knowledge conflicts, we propose a\nnovel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to\ncapitalize on neurons that are crucial in processing contextual cues.\nSpecifically, IRCAN first identifies neurons that significantly contribute to\ncontext processing, utilizing a context-aware attribution score derived from\nintegrated gradients. Subsequently, the identified context-aware neurons are\nstrengthened via reweighting. In doing so, we steer LLMs to generate\ncontext-sensitive outputs with respect to the new knowledge provided in the\ncontext. Extensive experiments conducted across a variety of models and tasks\ndemonstrate that IRCAN not only achieves remarkable improvements in handling\nknowledge conflicts but also offers a scalable, plug-and-play solution that can\nbe integrated seamlessly with existing models. Our codes are released at\nhttps://github.com/danshi777/IRCAN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is widely acknowledged that large language models (LLMs) encode a vast\nreservoir of knowledge after being trained on mass data. Recent studies\ndisclose knowledge conflicts in LLM generation, wherein outdated or incorrect\nparametric knowledge (i.e., encoded knowledge) contradicts new knowledge\nprovided in the context. To mitigate such knowledge conflicts, we propose a\nnovel framework, IRCAN (Identifying and Reweighting Context-Aware Neurons) to\ncapitalize on neurons that are crucial in processing contextual cues.\nSpecifically, IRCAN first identifies neurons that significantly contribute to\ncontext processing, utilizing a context-aware attribution score derived from\nintegrated gradients. Subsequently, the identified context-aware neurons are\nstrengthened via reweighting. In doing so, we steer LLMs to generate\ncontext-sensitive outputs with respect to the new knowledge provided in the\ncontext. Extensive experiments conducted across a variety of models and tasks\ndemonstrate that IRCAN not only achieves remarkable improvements in handling\nknowledge conflicts but also offers a scalable, plug-and-play solution that can\nbe integrated seamlessly with existing models. Our codes are released at\nhttps://github.com/danshi777/IRCAN."
                },
                "authors": [
                    {
                        "name": "Dan Shi"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Tianhao Shen"
                    },
                    {
                        "name": "Weilong Dong"
                    },
                    {
                        "name": "Xinwei Wu"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18406v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18406v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09341v1",
                "updated": "2024-11-14T10:37:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    37,
                    34,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T10:37:34Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    37,
                    34,
                    3,
                    319,
                    0
                ],
                "title": "Approximated Variational Bayesian Inverse Reinforcement Learning for\n  Large Language Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximated Variational Bayesian Inverse Reinforcement Learning for\n  Large Language Model Alignment"
                },
                "summary": "The alignment of large language models (LLMs) is crucial for generating\nhelpful and harmless content. Existing approaches leverage preference-based\nhuman feedback data to learn the reward function and align the LLM with the\nfeedback data. However, these approaches focus on modeling the reward\ndifference between the chosen and rejected demonstrations, rather than directly\nmodeling the true reward from each demonstration. Moreover, these approaches\nassume that the reward is only obtained at the end of the sentence, which\noverlooks the modeling of intermediate rewards. These issues lead to\ninsufficient use of training signals in the feedback data, limiting the\nrepresentation and generalization ability of the reward and potentially\nresulting in reward hacking. In this paper, we formulate LLM alignment as a\nBayesian Inverse Reinforcement Learning (BIRL) problem and propose a novel\ntraining objective, Approximated Variational Alignment (AVA), to perform LLM\nalignment through Approximated Variational Reward Imitation Learning (AVRIL).\nThe BIRL formulation facilitates intermediate reward modeling and direct reward\nmodeling on each single demonstration, which enhances the utilization of\ntraining signals in the feedback data. Experiments show that AVA outperforms\nexisting LLM alignment approaches in reward modeling, RL fine-tuning, and\ndirect optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of large language models (LLMs) is crucial for generating\nhelpful and harmless content. Existing approaches leverage preference-based\nhuman feedback data to learn the reward function and align the LLM with the\nfeedback data. However, these approaches focus on modeling the reward\ndifference between the chosen and rejected demonstrations, rather than directly\nmodeling the true reward from each demonstration. Moreover, these approaches\nassume that the reward is only obtained at the end of the sentence, which\noverlooks the modeling of intermediate rewards. These issues lead to\ninsufficient use of training signals in the feedback data, limiting the\nrepresentation and generalization ability of the reward and potentially\nresulting in reward hacking. In this paper, we formulate LLM alignment as a\nBayesian Inverse Reinforcement Learning (BIRL) problem and propose a novel\ntraining objective, Approximated Variational Alignment (AVA), to perform LLM\nalignment through Approximated Variational Reward Imitation Learning (AVRIL).\nThe BIRL formulation facilitates intermediate reward modeling and direct reward\nmodeling on each single demonstration, which enhances the utilization of\ntraining signals in the feedback data. Experiments show that AVA outperforms\nexisting LLM alignment approaches in reward modeling, RL fine-tuning, and\ndirect optimization."
                },
                "authors": [
                    {
                        "name": "Yuang Cai"
                    },
                    {
                        "name": "Yuyu Yuan"
                    },
                    {
                        "name": "Jinsheng Shi"
                    },
                    {
                        "name": "Qinhong Lin"
                    }
                ],
                "author_detail": {
                    "name": "Qinhong Lin"
                },
                "author": "Qinhong Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09339v1",
                "updated": "2024-11-14T10:36:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    36,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T10:36:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    36,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Re-Parameterization of Lightweight Transformer for On-Device Speech\n  Emotion Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-Parameterization of Lightweight Transformer for On-Device Speech\n  Emotion Recognition"
                },
                "summary": "With the increasing implementation of machine learning models on edge or\nInternet-of-Things (IoT) devices, deploying advanced models on\nresource-constrained IoT devices remains challenging. Transformer models, a\ncurrently dominant neural architecture, have achieved great success in broad\ndomains but their complexity hinders its deployment on IoT devices with limited\ncomputation capability and storage size. Although many model compression\napproaches have been explored, they often suffer from notorious performance\ndegradation. To address this issue, we introduce a new method, namely\nTransformer Re-parameterization, to boost the performance of lightweight\nTransformer models. It consists of two processes: the High-Rank Factorization\n(HRF) process in the training stage and the deHigh-Rank Factorization (deHRF)\nprocess in the inference stage. In the former process, we insert an additional\nlinear layer before the Feed-Forward Network (FFN) of the lightweight\nTransformer. It is supposed that the inserted HRF layers can enhance the model\nlearning capability. In the later process, the auxiliary HRF layer will be\nmerged together with the following FFN layer into one linear layer and thus\nrecover the original structure of the lightweight model. To examine the\neffectiveness of the proposed method, we evaluate it on three widely used\nTransformer variants, i.e., ConvTransformer, Conformer, and SpeechFormer\nnetworks, in the application of speech emotion recognition on the IEMOCAP, M3ED\nand DAIC-WOZ datasets. Experimental results show that our proposed method\nconsistently improves the performance of lightweight Transformers, even making\nthem comparable to large models. The proposed re-parameterization approach\nenables advanced Transformer models to be deployed on resource-constrained IoT\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing implementation of machine learning models on edge or\nInternet-of-Things (IoT) devices, deploying advanced models on\nresource-constrained IoT devices remains challenging. Transformer models, a\ncurrently dominant neural architecture, have achieved great success in broad\ndomains but their complexity hinders its deployment on IoT devices with limited\ncomputation capability and storage size. Although many model compression\napproaches have been explored, they often suffer from notorious performance\ndegradation. To address this issue, we introduce a new method, namely\nTransformer Re-parameterization, to boost the performance of lightweight\nTransformer models. It consists of two processes: the High-Rank Factorization\n(HRF) process in the training stage and the deHigh-Rank Factorization (deHRF)\nprocess in the inference stage. In the former process, we insert an additional\nlinear layer before the Feed-Forward Network (FFN) of the lightweight\nTransformer. It is supposed that the inserted HRF layers can enhance the model\nlearning capability. In the later process, the auxiliary HRF layer will be\nmerged together with the following FFN layer into one linear layer and thus\nrecover the original structure of the lightweight model. To examine the\neffectiveness of the proposed method, we evaluate it on three widely used\nTransformer variants, i.e., ConvTransformer, Conformer, and SpeechFormer\nnetworks, in the application of speech emotion recognition on the IEMOCAP, M3ED\nand DAIC-WOZ datasets. Experimental results show that our proposed method\nconsistently improves the performance of lightweight Transformers, even making\nthem comparable to large models. The proposed re-parameterization approach\nenables advanced Transformer models to be deployed on resource-constrained IoT\ndevices."
                },
                "authors": [
                    {
                        "name": "Zixing Zhang"
                    },
                    {
                        "name": "Zhongren Dong"
                    },
                    {
                        "name": "Weixiang Xu"
                    },
                    {
                        "name": "Jing Han"
                    }
                ],
                "author_detail": {
                    "name": "Jing Han"
                },
                "author": "Jing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09318v1",
                "updated": "2024-11-14T10:00:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    0,
                    33,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T10:00:33Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    10,
                    0,
                    33,
                    3,
                    319,
                    0
                ],
                "title": "DriveThru: a Document Extraction Platform and Benchmark Datasets for\n  Indonesian Local Language Archives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriveThru: a Document Extraction Platform and Benchmark Datasets for\n  Indonesian Local Language Archives"
                },
                "summary": "Indonesia is one of the most diverse countries linguistically. However,\ndespite this linguistic diversity, Indonesian languages remain underrepresented\nin Natural Language Processing (NLP) research and technologies. In the past two\nyears, several efforts have been conducted to construct NLP resources for\nIndonesian languages. However, most of these efforts have been focused on\ncreating manual resources thus difficult to scale to more languages. Although\nmany Indonesian languages do not have a web presence, locally there are\nresources that document these languages well in printed forms such as books,\nmagazines, and newspapers. Digitizing these existing resources will enable\nscaling of Indonesian language resource construction to many more languages. In\nthis paper, we propose an alternative method of creating datasets by digitizing\ndocuments, which have not previously been used to build digital language\nresources in Indonesia. DriveThru is a platform for extracting document content\nutilizing Optical Character Recognition (OCR) techniques in its system to\nprovide language resource building with less manual effort and cost. This paper\nalso studies the utility of current state-of-the-art LLM for post-OCR\ncorrection to show the capability of increasing the character accuracy rate\n(CAR) and word accuracy rate (WAR) compared to off-the-shelf OCR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indonesia is one of the most diverse countries linguistically. However,\ndespite this linguistic diversity, Indonesian languages remain underrepresented\nin Natural Language Processing (NLP) research and technologies. In the past two\nyears, several efforts have been conducted to construct NLP resources for\nIndonesian languages. However, most of these efforts have been focused on\ncreating manual resources thus difficult to scale to more languages. Although\nmany Indonesian languages do not have a web presence, locally there are\nresources that document these languages well in printed forms such as books,\nmagazines, and newspapers. Digitizing these existing resources will enable\nscaling of Indonesian language resource construction to many more languages. In\nthis paper, we propose an alternative method of creating datasets by digitizing\ndocuments, which have not previously been used to build digital language\nresources in Indonesia. DriveThru is a platform for extracting document content\nutilizing Optical Character Recognition (OCR) techniques in its system to\nprovide language resource building with less manual effort and cost. This paper\nalso studies the utility of current state-of-the-art LLM for post-OCR\ncorrection to show the capability of increasing the character accuracy rate\n(CAR) and word accuracy rate (WAR) compared to off-the-shelf OCR."
                },
                "authors": [
                    {
                        "name": "MohammadRifqi Farhansyah"
                    },
                    {
                        "name": "Muhammad Zuhdi Fikri Johari"
                    },
                    {
                        "name": "Afinzaki Amiral"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Kumara Ari Yuana"
                    },
                    {
                        "name": "Derry Tanti Wijaya"
                    }
                ],
                "author_detail": {
                    "name": "Derry Tanti Wijaya"
                },
                "author": "Derry Tanti Wijaya",
                "arxiv_comment": "12 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05521v2",
                "updated": "2024-11-14T09:28:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    28,
                    49,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-08T12:27:13Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    12,
                    27,
                    13,
                    4,
                    313,
                    0
                ],
                "title": "SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark"
                },
                "summary": "Electronic health records (EHRs) are stored in various database systems with\ndifferent database models on heterogeneous storage architectures, such as\nrelational databases, document stores, or graph databases. These different\ndatabase models have a big impact on query complexity and performance. While\nthis has been a known fact in database research, its implications for the\ngrowing number of Text-to-Query systems have surprisingly not been investigated\nso far. In this paper, we present SM3-Text-to-Query, the first multi-model\nmedical Text-to-Query benchmark based on synthetic patient data from Synthea,\nfollowing the SNOMED-CT taxonomy -- a widely used knowledge graph ontology\ncovering medical terminology. SM3-Text-to-Query provides data representations\nfor relational databases (PostgreSQL), document stores (MongoDB), and graph\ndatabases (Neo4j and GraphDB (RDF)), allowing the evaluation across four\npopular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically\nand manually develop 408 template questions, which we augment to construct a\nbenchmark of 10K diverse natural language question/query pairs for these four\nquery languages (40K pairs overall). On our dataset, we evaluate several common\nin-context-learning (ICL) approaches for a set of representative closed and\nopen-source LLMs. Our evaluation sheds light on the trade-offs between database\nmodels and query languages for different ICL strategies and LLMs. Last,\nSM3-Text-to-Query is easily extendable to additional query languages or real,\nstandard-based patient databases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic health records (EHRs) are stored in various database systems with\ndifferent database models on heterogeneous storage architectures, such as\nrelational databases, document stores, or graph databases. These different\ndatabase models have a big impact on query complexity and performance. While\nthis has been a known fact in database research, its implications for the\ngrowing number of Text-to-Query systems have surprisingly not been investigated\nso far. In this paper, we present SM3-Text-to-Query, the first multi-model\nmedical Text-to-Query benchmark based on synthetic patient data from Synthea,\nfollowing the SNOMED-CT taxonomy -- a widely used knowledge graph ontology\ncovering medical terminology. SM3-Text-to-Query provides data representations\nfor relational databases (PostgreSQL), document stores (MongoDB), and graph\ndatabases (Neo4j and GraphDB (RDF)), allowing the evaluation across four\npopular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically\nand manually develop 408 template questions, which we augment to construct a\nbenchmark of 10K diverse natural language question/query pairs for these four\nquery languages (40K pairs overall). On our dataset, we evaluate several common\nin-context-learning (ICL) approaches for a set of representative closed and\nopen-source LLMs. Our evaluation sheds light on the trade-offs between database\nmodels and query languages for different ICL strategies and LLMs. Last,\nSM3-Text-to-Query is easily extendable to additional query languages or real,\nstandard-based patient databases."
                },
                "authors": [
                    {
                        "name": "Sithursan Sivasubramaniam"
                    },
                    {
                        "name": "Cedric Osei-Akoto"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Kurt Stockinger"
                    },
                    {
                        "name": "Jonathan Fuerst"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Fuerst"
                },
                "author": "Jonathan Fuerst",
                "arxiv_comment": "NeurIPS 2024 Track Datasets and Benchmarks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14979v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14979v4",
                "updated": "2024-11-14T09:17:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    17,
                    48,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-19T05:01:56Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    5,
                    1,
                    56,
                    5,
                    293,
                    0
                ],
                "title": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration From Cognitive Psychology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration From Cognitive Psychology"
                },
                "summary": "The cognitive mechanism by which Large Language Models (LLMs) solve\nmathematical problems remains a widely debated and unresolved issue. Currently,\nthere is little interpretable experimental evidence that connects LLMs'\nproblem-solving with human cognitive psychology.To determine if LLMs possess\nhuman-like mathematical reasoning, we modified the problems used in the human\nCognitive Reflection Test (CRT). Our results show that, even with the use of\nChains of Thought (CoT) prompts, mainstream LLMs, including the latest o1 model\n(noted for its reasoning capabilities), have a high error rate when solving\nthese modified CRT problems. Specifically, the average accuracy rate dropped by\nup to 50% compared to the original questions.Further analysis of LLMs'\nincorrect answers suggests that they primarily rely on pattern matching from\ntheir training data, which aligns more with human intuition (System 1 thinking)\nrather than with human-like reasoning (System 2 thinking). This finding\nchallenges the belief that LLMs have genuine mathematical reasoning abilities\ncomparable to humans. As a result, this work may adjust overly optimistic views\non LLMs' progress towards artificial general intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cognitive mechanism by which Large Language Models (LLMs) solve\nmathematical problems remains a widely debated and unresolved issue. Currently,\nthere is little interpretable experimental evidence that connects LLMs'\nproblem-solving with human cognitive psychology.To determine if LLMs possess\nhuman-like mathematical reasoning, we modified the problems used in the human\nCognitive Reflection Test (CRT). Our results show that, even with the use of\nChains of Thought (CoT) prompts, mainstream LLMs, including the latest o1 model\n(noted for its reasoning capabilities), have a high error rate when solving\nthese modified CRT problems. Specifically, the average accuracy rate dropped by\nup to 50% compared to the original questions.Further analysis of LLMs'\nincorrect answers suggests that they primarily rely on pattern matching from\ntheir training data, which aligns more with human intuition (System 1 thinking)\nrather than with human-like reasoning (System 2 thinking). This finding\nchallenges the belief that LLMs have genuine mathematical reasoning abilities\ncomparable to humans. As a result, this work may adjust overly optimistic views\non LLMs' progress towards artificial general intelligence."
                },
                "authors": [
                    {
                        "name": "Wei Xie"
                    },
                    {
                        "name": "Shuoyoucheng Ma"
                    },
                    {
                        "name": "Zhenhua Wang"
                    },
                    {
                        "name": "Enze Wang"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Xiaobing Sun"
                    },
                    {
                        "name": "Baosheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Baosheng Wang"
                },
                "author": "Baosheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14979v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14979v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09297v1",
                "updated": "2024-11-14T09:16:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    16,
                    48,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:16:48Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    16,
                    48,
                    3,
                    319,
                    0
                ],
                "title": "DTELS: Towards Dynamic Granularity of Timeline Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DTELS: Towards Dynamic Granularity of Timeline Summarization"
                },
                "summary": "The rapid proliferation of online news has posed significant challenges in\ntracking the continuous development of news topics. Traditional timeline\nsummarization constructs a chronological summary of the events but often lacks\nthe flexibility to meet the diverse granularity needs. To overcome this\nlimitation, we introduce a new paradigm, Dynamic-granularity TimELine\nSummarization, (DTELS), which aims to construct adaptive timelines based on\nuser instructions or requirements. This paper establishes a comprehensive\nbenchmark for DTLES that includes: (1) an evaluation framework grounded in\njournalistic standards to assess the timeline quality across four dimensions:\nInformativeness, Granular Consistency, Factuality, and Coherence; (2) a\nlarge-scale, multi-source dataset with multiple granularity timeline\nannotations based on a consensus process to facilitate authority; (3) extensive\nexperiments and analysis with two proposed solutions based on Large Language\nModels (LLMs) and existing state-of-the-art TLS methods. The experimental\nresults demonstrate the effectiveness of LLM-based solutions. However, even the\nmost advanced LLMs struggle to consistently generate timelines that are both\ninformative and granularly consistent, highlighting the challenges of the DTELS\ntask.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of online news has posed significant challenges in\ntracking the continuous development of news topics. Traditional timeline\nsummarization constructs a chronological summary of the events but often lacks\nthe flexibility to meet the diverse granularity needs. To overcome this\nlimitation, we introduce a new paradigm, Dynamic-granularity TimELine\nSummarization, (DTELS), which aims to construct adaptive timelines based on\nuser instructions or requirements. This paper establishes a comprehensive\nbenchmark for DTLES that includes: (1) an evaluation framework grounded in\njournalistic standards to assess the timeline quality across four dimensions:\nInformativeness, Granular Consistency, Factuality, and Coherence; (2) a\nlarge-scale, multi-source dataset with multiple granularity timeline\nannotations based on a consensus process to facilitate authority; (3) extensive\nexperiments and analysis with two proposed solutions based on Large Language\nModels (LLMs) and existing state-of-the-art TLS methods. The experimental\nresults demonstrate the effectiveness of LLM-based solutions. However, even the\nmost advanced LLMs struggle to consistently generate timelines that are both\ninformative and granularly consistent, highlighting the challenges of the DTELS\ntask."
                },
                "authors": [
                    {
                        "name": "Chenlong Zhang"
                    },
                    {
                        "name": "Tong Zhou"
                    },
                    {
                        "name": "Pengfei Cao"
                    },
                    {
                        "name": "Zhuoran Jin"
                    },
                    {
                        "name": "Yubo Chen"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09289v1",
                "updated": "2024-11-14T09:03:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    3,
                    54,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:03:54Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    3,
                    54,
                    3,
                    319,
                    0
                ],
                "title": "StreamAdapter: Efficient Test Time Adaptation from Contextual Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAdapter: Efficient Test Time Adaptation from Contextual Streams"
                },
                "summary": "In-context learning (ICL) allows large language models (LLMs) to adapt to new\ntasks directly from the given demonstrations without requiring gradient\nupdates. While recent advances have expanded context windows to accommodate\nmore demonstrations, this approach increases inference costs without\nnecessarily improving performance. To mitigate these issues, We propose\nStreamAdapter, a novel approach that directly updates model parameters from\ncontext at test time, eliminating the need for explicit in-context\ndemonstrations. StreamAdapter employs context mapping and weight absorption\nmechanisms to dynamically transform ICL demonstrations into parameter updates\nwith minimal additional parameters. By reducing reliance on numerous in-context\nexamples, StreamAdapter significantly reduce inference costs and allows for\nefficient inference with constant time complexity, regardless of demonstration\ncount. Extensive experiments across diverse tasks and model architectures\ndemonstrate that StreamAdapter achieves comparable or superior adaptation\ncapability to ICL while requiring significantly fewer demonstrations. The\nsuperior task adaptation and context encoding capabilities of StreamAdapter on\nboth language understanding and generation tasks provides a new perspective for\nadapting LLMs at test time using context, allowing for more efficient\nadaptation across scenarios and more cost-effective inference",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) allows large language models (LLMs) to adapt to new\ntasks directly from the given demonstrations without requiring gradient\nupdates. While recent advances have expanded context windows to accommodate\nmore demonstrations, this approach increases inference costs without\nnecessarily improving performance. To mitigate these issues, We propose\nStreamAdapter, a novel approach that directly updates model parameters from\ncontext at test time, eliminating the need for explicit in-context\ndemonstrations. StreamAdapter employs context mapping and weight absorption\nmechanisms to dynamically transform ICL demonstrations into parameter updates\nwith minimal additional parameters. By reducing reliance on numerous in-context\nexamples, StreamAdapter significantly reduce inference costs and allows for\nefficient inference with constant time complexity, regardless of demonstration\ncount. Extensive experiments across diverse tasks and model architectures\ndemonstrate that StreamAdapter achieves comparable or superior adaptation\ncapability to ICL while requiring significantly fewer demonstrations. The\nsuperior task adaptation and context encoding capabilities of StreamAdapter on\nboth language understanding and generation tasks provides a new perspective for\nadapting LLMs at test time using context, allowing for more efficient\nadaptation across scenarios and more cost-effective inference"
                },
                "authors": [
                    {
                        "name": "Dilxat Muhtar"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Yaming Yang"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Yadong Lu"
                    },
                    {
                        "name": "Jianfeng Liu"
                    },
                    {
                        "name": "Yuefeng Zhan"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Weiwei Deng"
                    },
                    {
                        "name": "Feng Sun"
                    },
                    {
                        "name": "Xueliang Zhang"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "22 Pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09269v1",
                "updated": "2024-11-14T08:12:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    12,
                    36,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:12:36Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    12,
                    36,
                    3,
                    319,
                    0
                ],
                "title": "Harnessing multiple LLMs for Information Retrieval: A case study on Deep\n  Learning methodologies in Biodiversity publications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing multiple LLMs for Information Retrieval: A case study on Deep\n  Learning methodologies in Biodiversity publications"
                },
                "summary": "Deep Learning (DL) techniques are increasingly applied in scientific studies\nacross various domains to address complex research questions. However, the\nmethodological details of these DL models are often hidden in the unstructured\ntext. As a result, critical information about how these models are designed,\ntrained, and evaluated is challenging to access and comprehend. To address this\nissue, in this work, we use five different open-source Large Language Models\n(LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B,\nand Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG)\napproach to extract and process DL methodological details from scientific\npublications automatically. We built a voting classifier from the outputs of\nfive LLMs to accurately report DL methodological information. We tested our\napproach using biodiversity publications, building upon our previous research.\nTo validate our pipeline, we employed two datasets of DL-related biodiversity\npublications: a curated set of 100 publications from our prior work and a set\nof 364 publications from the Ecological Informatics journal. Our results\ndemonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of\nDL methodological information, achieving an accuracy of 69.5% (417 out of 600\ncomparisons) based solely on textual content from publications. This\nperformance was assessed against human annotators who had access to code,\nfigures, tables, and other supplementary information. Although demonstrated in\nbiodiversity, our methodology is not limited to this field; it can be applied\nacross other scientific domains where detailed methodological reporting is\nessential for advancing knowledge and ensuring reproducibility. This study\npresents a scalable and reliable approach for automating information\nextraction, facilitating better reproducibility and knowledge transfer across\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning (DL) techniques are increasingly applied in scientific studies\nacross various domains to address complex research questions. However, the\nmethodological details of these DL models are often hidden in the unstructured\ntext. As a result, critical information about how these models are designed,\ntrained, and evaluated is challenging to access and comprehend. To address this\nissue, in this work, we use five different open-source Large Language Models\n(LLMs): Llama-3 70B, Llama-3.1 70B, Mixtral-8x22B-Instruct-v0.1, Mixtral 8x7B,\nand Gemma 2 9B in combination with Retrieval-Augmented Generation (RAG)\napproach to extract and process DL methodological details from scientific\npublications automatically. We built a voting classifier from the outputs of\nfive LLMs to accurately report DL methodological information. We tested our\napproach using biodiversity publications, building upon our previous research.\nTo validate our pipeline, we employed two datasets of DL-related biodiversity\npublications: a curated set of 100 publications from our prior work and a set\nof 364 publications from the Ecological Informatics journal. Our results\ndemonstrate that the multi-LLM, RAG-assisted pipeline enhances the retrieval of\nDL methodological information, achieving an accuracy of 69.5% (417 out of 600\ncomparisons) based solely on textual content from publications. This\nperformance was assessed against human annotators who had access to code,\nfigures, tables, and other supplementary information. Although demonstrated in\nbiodiversity, our methodology is not limited to this field; it can be applied\nacross other scientific domains where detailed methodological reporting is\nessential for advancing knowledge and ensuring reproducibility. This study\npresents a scalable and reliable approach for automating information\nextraction, facilitating better reproducibility and knowledge transfer across\nstudies."
                },
                "authors": [
                    {
                        "name": "Vamsi Krishna Kommineni"
                    },
                    {
                        "name": "Birgitta König-Ries"
                    },
                    {
                        "name": "Sheeba Samuel"
                    }
                ],
                "author_detail": {
                    "name": "Sheeba Samuel"
                },
                "author": "Sheeba Samuel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09267v1",
                "updated": "2024-11-14T08:08:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    8,
                    25,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:08:25Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    8,
                    25,
                    3,
                    319,
                    0
                ],
                "title": "Towards efficient compression and communication for prototype-based\n  decentralized learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards efficient compression and communication for prototype-based\n  decentralized learning"
                },
                "summary": "In prototype-based federated learning, the exchange of model parameters\nbetween clients and the master server is replaced by transmission of prototypes\nor quantized versions of the data samples to the aggregation server. A fully\ndecentralized deployment of prototype-based learning, without a central\nagregartor of prototypes, is more robust upon network failures and reacts\nfaster to changes in the statistical distribution of the data, suggesting\npotential advantages and quick adaptation in dynamic learning tasks, e.g., when\nthe data sources are IoT devices or when data is non-iid. In this paper, we\nconsider the problem of designing a communication-efficient decentralized\nlearning system based on prototypes. We address the challenge of prototype\nredundancy by leveraging on a twofold data compression technique, i.e., sending\nonly update messages if the prototypes are informationtheoretically useful (via\nthe Jensen-Shannon distance), and using clustering on the prototypes to\ncompress the update messages used in the gossip protocol. We also use parallel\ninstead of sequential gossiping, and present an analysis of its\nage-of-information (AoI). Our experimental results show that, with these\nimprovements, the communications load can be substantially reduced without\ndecreasing the convergence rate of the learning algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In prototype-based federated learning, the exchange of model parameters\nbetween clients and the master server is replaced by transmission of prototypes\nor quantized versions of the data samples to the aggregation server. A fully\ndecentralized deployment of prototype-based learning, without a central\nagregartor of prototypes, is more robust upon network failures and reacts\nfaster to changes in the statistical distribution of the data, suggesting\npotential advantages and quick adaptation in dynamic learning tasks, e.g., when\nthe data sources are IoT devices or when data is non-iid. In this paper, we\nconsider the problem of designing a communication-efficient decentralized\nlearning system based on prototypes. We address the challenge of prototype\nredundancy by leveraging on a twofold data compression technique, i.e., sending\nonly update messages if the prototypes are informationtheoretically useful (via\nthe Jensen-Shannon distance), and using clustering on the prototypes to\ncompress the update messages used in the gossip protocol. We also use parallel\ninstead of sequential gossiping, and present an analysis of its\nage-of-information (AoI). Our experimental results show that, with these\nimprovements, the communications load can be substantially reduced without\ndecreasing the convergence rate of the learning algorithm."
                },
                "authors": [
                    {
                        "name": "Pablo Fernández-Piñeiro"
                    },
                    {
                        "name": "Manuel Ferández-Veiga"
                    },
                    {
                        "name": "Rebeca P. Díaz-Redondo"
                    },
                    {
                        "name": "Ana Fernández-Vilas"
                    },
                    {
                        "name": "Martín González-Soto"
                    }
                ],
                "author_detail": {
                    "name": "Martín González-Soto"
                },
                "author": "Martín González-Soto",
                "arxiv_comment": "15 pages, 2 tables, 7 figures, 6 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09266v1",
                "updated": "2024-11-14T08:07:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    7,
                    2,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:07:02Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    7,
                    2,
                    3,
                    319,
                    0
                ],
                "title": "How Good is ChatGPT at Audiovisual Deepfake Detection: A Comparative\n  Study of ChatGPT, AI Models and Human Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Good is ChatGPT at Audiovisual Deepfake Detection: A Comparative\n  Study of ChatGPT, AI Models and Human Perception"
                },
                "summary": "Multimodal deepfakes involving audiovisual manipulations are a growing threat\nbecause they are difficult to detect with the naked eye or using unimodal deep\nlearningbased forgery detection methods. Audiovisual forensic models, while\nmore capable than unimodal models, require large training datasets and are\ncomputationally expensive for training and inference. Furthermore, these models\nlack interpretability and often do not generalize well to unseen manipulations.\nIn this study, we examine the detection capabilities of a large language model\n(LLM) (i.e., ChatGPT) to identify and account for any possible visual and\nauditory artifacts and manipulations in audiovisual deepfake content. Extensive\nexperiments are conducted on videos from a benchmark multimodal deepfake\ndataset to evaluate the detection performance of ChatGPT and compare it with\nthe detection capabilities of state-of-the-art multimodal forensic models and\nhumans. Experimental results demonstrate the importance of domain knowledge and\nprompt engineering for video forgery detection tasks using LLMs. Unlike\napproaches based on end-to-end learning, ChatGPT can account for spatial and\nspatiotemporal artifacts and inconsistencies that may exist within or across\nmodalities. Additionally, we discuss the limitations of ChatGPT for multimedia\nforensic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal deepfakes involving audiovisual manipulations are a growing threat\nbecause they are difficult to detect with the naked eye or using unimodal deep\nlearningbased forgery detection methods. Audiovisual forensic models, while\nmore capable than unimodal models, require large training datasets and are\ncomputationally expensive for training and inference. Furthermore, these models\nlack interpretability and often do not generalize well to unseen manipulations.\nIn this study, we examine the detection capabilities of a large language model\n(LLM) (i.e., ChatGPT) to identify and account for any possible visual and\nauditory artifacts and manipulations in audiovisual deepfake content. Extensive\nexperiments are conducted on videos from a benchmark multimodal deepfake\ndataset to evaluate the detection performance of ChatGPT and compare it with\nthe detection capabilities of state-of-the-art multimodal forensic models and\nhumans. Experimental results demonstrate the importance of domain knowledge and\nprompt engineering for video forgery detection tasks using LLMs. Unlike\napproaches based on end-to-end learning, ChatGPT can account for spatial and\nspatiotemporal artifacts and inconsistencies that may exist within or across\nmodalities. Additionally, we discuss the limitations of ChatGPT for multimedia\nforensic tasks."
                },
                "authors": [
                    {
                        "name": "Sahibzada Adil Shahzad"
                    },
                    {
                        "name": "Ammarah Hashmi"
                    },
                    {
                        "name": "Yan-Tsung Peng"
                    },
                    {
                        "name": "Yu Tsao"
                    },
                    {
                        "name": "Hsin-Min Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hsin-Min Wang"
                },
                "author": "Hsin-Min Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09261v1",
                "updated": "2024-11-14T07:58:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    58,
                    44,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T07:58:44Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    58,
                    44,
                    3,
                    319,
                    0
                ],
                "title": "Automating Autograding: Large Language Models as Test Suite Generators\n  for Introductory Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Autograding: Large Language Models as Test Suite Generators\n  for Introductory Programming"
                },
                "summary": "Automatically graded programming assignments provide instant feedback to\nstudents and significantly reduce manual grading time for instructors. However,\ncreating comprehensive suites of test cases for programming problems within\nautomatic graders can be time-consuming and complex. The effort needed to\ndefine test suites may deter some instructors from creating additional problems\nor lead to inadequate test coverage, potentially resulting in misleading\nfeedback on student solutions. Such limitations may reduce student access to\nthe well-documented benefits of timely feedback when learning programming.\n  In this work, we evaluate the effectiveness of using Large Language Models\n(LLMs), as part of a larger workflow, to automatically generate test suites for\nCS1-level programming problems. Each problem's statement and reference solution\nare provided to GPT-4 to produce a test suite that can be used by an\nautograder. We evaluate our proposed approach using a sample of 26 problems,\nand more than 25,000 attempted solutions to those problems, submitted by\nstudents in an introductory programming course. We compare the performance of\nthe LLM-generated test suites against the instructor-created test suites for\neach problem. Our findings reveal that LLM-generated test suites can correctly\nidentify most valid solutions, and for most problems are at least as\ncomprehensive as the instructor test suites. Additionally, the LLM-generated\ntest suites exposed ambiguities in some problem statements, underscoring their\npotential to improve both autograding and instructional design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically graded programming assignments provide instant feedback to\nstudents and significantly reduce manual grading time for instructors. However,\ncreating comprehensive suites of test cases for programming problems within\nautomatic graders can be time-consuming and complex. The effort needed to\ndefine test suites may deter some instructors from creating additional problems\nor lead to inadequate test coverage, potentially resulting in misleading\nfeedback on student solutions. Such limitations may reduce student access to\nthe well-documented benefits of timely feedback when learning programming.\n  In this work, we evaluate the effectiveness of using Large Language Models\n(LLMs), as part of a larger workflow, to automatically generate test suites for\nCS1-level programming problems. Each problem's statement and reference solution\nare provided to GPT-4 to produce a test suite that can be used by an\nautograder. We evaluate our proposed approach using a sample of 26 problems,\nand more than 25,000 attempted solutions to those problems, submitted by\nstudents in an introductory programming course. We compare the performance of\nthe LLM-generated test suites against the instructor-created test suites for\neach problem. Our findings reveal that LLM-generated test suites can correctly\nidentify most valid solutions, and for most problems are at least as\ncomprehensive as the instructor test suites. Additionally, the LLM-generated\ntest suites exposed ambiguities in some problem statements, underscoring their\npotential to improve both autograding and instructional design."
                },
                "authors": [
                    {
                        "name": "Umar Alkafaween"
                    },
                    {
                        "name": "Ibrahim Albluwi"
                    },
                    {
                        "name": "Paul Denny"
                    }
                ],
                "author_detail": {
                    "name": "Paul Denny"
                },
                "author": "Paul Denny",
                "arxiv_comment": "Submitted to Journal of Computer Assisted Learning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.3.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09259v1",
                "updated": "2024-11-14T07:51:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    51,
                    51,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T07:51:51Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    51,
                    51,
                    3,
                    319,
                    0
                ],
                "title": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak Attacks and Defenses against Multimodal Generative Models: A\n  Survey"
                },
                "summary": "The rapid evolution of multimodal foundation models has led to significant\nadvancements in cross-modal understanding and generation across diverse\nmodalities, including text, images, audio, and video. However, these models\nremain susceptible to jailbreak attacks, which can bypass built-in safety\nmechanisms and induce the production of potentially harmful content.\nConsequently, understanding the methods of jailbreak attacks and existing\ndefense mechanisms is essential to ensure the safe deployment of multimodal\ngenerative models in real-world scenarios, particularly in security-sensitive\napplications. To provide comprehensive insight into this topic, this survey\nreviews jailbreak and defense in multimodal generative models. First, given the\ngeneralized lifecycle of multimodal jailbreak, we systematically explore\nattacks and corresponding defense strategies across four levels: input,\nencoder, generator, and output. Based on this analysis, we present a detailed\ntaxonomy of attack methods, defense mechanisms, and evaluation frameworks\nspecific to multimodal generative models. Additionally, we cover a wide range\nof input-output configurations, including modalities such as Any-to-Text,\nAny-to-Vision, and Any-to-Any within generative systems. Finally, we highlight\ncurrent research challenges and propose potential directions for future\nresearch.The open-source repository corresponding to this work can be found at\nhttps://github.com/liuxuannan/Awesome-Multimodal-Jailbreak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of multimodal foundation models has led to significant\nadvancements in cross-modal understanding and generation across diverse\nmodalities, including text, images, audio, and video. However, these models\nremain susceptible to jailbreak attacks, which can bypass built-in safety\nmechanisms and induce the production of potentially harmful content.\nConsequently, understanding the methods of jailbreak attacks and existing\ndefense mechanisms is essential to ensure the safe deployment of multimodal\ngenerative models in real-world scenarios, particularly in security-sensitive\napplications. To provide comprehensive insight into this topic, this survey\nreviews jailbreak and defense in multimodal generative models. First, given the\ngeneralized lifecycle of multimodal jailbreak, we systematically explore\nattacks and corresponding defense strategies across four levels: input,\nencoder, generator, and output. Based on this analysis, we present a detailed\ntaxonomy of attack methods, defense mechanisms, and evaluation frameworks\nspecific to multimodal generative models. Additionally, we cover a wide range\nof input-output configurations, including modalities such as Any-to-Text,\nAny-to-Vision, and Any-to-Any within generative systems. Finally, we highlight\ncurrent research challenges and propose potential directions for future\nresearch.The open-source repository corresponding to this work can be found at\nhttps://github.com/liuxuannan/Awesome-Multimodal-Jailbreak."
                },
                "authors": [
                    {
                        "name": "Xuannan Liu"
                    },
                    {
                        "name": "Xing Cui"
                    },
                    {
                        "name": "Peipei Li"
                    },
                    {
                        "name": "Zekun Li"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Shuhan Xia"
                    },
                    {
                        "name": "Miaoxuan Zhang"
                    },
                    {
                        "name": "Yueying Zou"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "ongoing work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09255v1",
                "updated": "2024-11-14T07:41:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    41,
                    34,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T07:41:34Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    41,
                    34,
                    3,
                    319,
                    0
                ],
                "title": "DAHL: Domain-specific Automated Hallucination Evaluation of Long-Form\n  Text through a Benchmark Dataset in Biomedicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAHL: Domain-specific Automated Hallucination Evaluation of Long-Form\n  Text through a Benchmark Dataset in Biomedicine"
                },
                "summary": "We introduce DAHL, a benchmark dataset and automated evaluation system\ndesigned to assess hallucination in long-form text generation, specifically\nwithin the biomedical domain. Our benchmark dataset, meticulously curated from\nbiomedical research papers, consists of 8,573 questions across 29 categories.\nDAHL evaluates fact-conflicting hallucinations in Large Language Models (LLMs)\nby deconstructing responses into atomic units, each representing a single piece\nof information. The accuracy of these responses is averaged to produce the DAHL\nScore, offering a more in-depth evaluation of hallucinations compared to\nprevious methods that rely on multiple-choice tasks. We conduct experiments\nwith 8 different models, finding that larger models tend to hallucinate less;\nhowever, beyond a model size of 7 to 8 billion parameters, further scaling does\nnot significantly improve factual accuracy. The DAHL Score holds potential as\nan efficient alternative to human-annotated preference labels, being able to be\nexpanded to other specialized domains. We release the dataset and code in\npublic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DAHL, a benchmark dataset and automated evaluation system\ndesigned to assess hallucination in long-form text generation, specifically\nwithin the biomedical domain. Our benchmark dataset, meticulously curated from\nbiomedical research papers, consists of 8,573 questions across 29 categories.\nDAHL evaluates fact-conflicting hallucinations in Large Language Models (LLMs)\nby deconstructing responses into atomic units, each representing a single piece\nof information. The accuracy of these responses is averaged to produce the DAHL\nScore, offering a more in-depth evaluation of hallucinations compared to\nprevious methods that rely on multiple-choice tasks. We conduct experiments\nwith 8 different models, finding that larger models tend to hallucinate less;\nhowever, beyond a model size of 7 to 8 billion parameters, further scaling does\nnot significantly improve factual accuracy. The DAHL Score holds potential as\nan efficient alternative to human-annotated preference labels, being able to be\nexpanded to other specialized domains. We release the dataset and code in\npublic."
                },
                "authors": [
                    {
                        "name": "Jean Seo"
                    },
                    {
                        "name": "Jongwon Lim"
                    },
                    {
                        "name": "Dongjun Jang"
                    },
                    {
                        "name": "Hyopil Shin"
                    }
                ],
                "author_detail": {
                    "name": "Hyopil Shin"
                },
                "author": "Hyopil Shin",
                "arxiv_comment": "EMNLP2024/FEVER",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09249v1",
                "updated": "2024-11-14T07:28:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    28,
                    9,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T07:28:09Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    28,
                    9,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Financial Domain Adaptation of Language Models via Model\n  Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Financial Domain Adaptation of Language Models via Model\n  Augmentation"
                },
                "summary": "The domain adaptation of language models, including large language models\n(LLMs), has become increasingly important as the use of such models continues\nto expand. This study demonstrates the effectiveness of Composition to Augment\nLanguage Models (CALM) in adapting to the financial domain. CALM is a model to\nextend the capabilities of existing models by introducing cross-attention\nbetween two LLMs with different functions. In our experiments, we developed a\nCALM to enhance the financial performance of an LLM with strong response\ncapabilities by leveraging a financial-specialized LLM. Notably, the CALM was\ntrained using a financial dataset different from the one used to train the\nfinancial-specialized LLM, confirming CALM's ability to adapt to various\ndatasets. The models were evaluated through quantitative Japanese financial\nbenchmarks and qualitative response comparisons, demonstrating that CALM\nenables superior responses with higher scores than the original models and\nbaselines. Additionally, comparative experiments on connection points revealed\nthat connecting the middle layers of the models is most effective in\nfacilitating adaptation to the financial domain. These findings confirm that\nCALM is a practical approach for adapting LLMs to the financial domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The domain adaptation of language models, including large language models\n(LLMs), has become increasingly important as the use of such models continues\nto expand. This study demonstrates the effectiveness of Composition to Augment\nLanguage Models (CALM) in adapting to the financial domain. CALM is a model to\nextend the capabilities of existing models by introducing cross-attention\nbetween two LLMs with different functions. In our experiments, we developed a\nCALM to enhance the financial performance of an LLM with strong response\ncapabilities by leveraging a financial-specialized LLM. Notably, the CALM was\ntrained using a financial dataset different from the one used to train the\nfinancial-specialized LLM, confirming CALM's ability to adapt to various\ndatasets. The models were evaluated through quantitative Japanese financial\nbenchmarks and qualitative response comparisons, demonstrating that CALM\nenables superior responses with higher scores than the original models and\nbaselines. Additionally, comparative experiments on connection points revealed\nthat connecting the middle layers of the models is most effective in\nfacilitating adaptation to the financial domain. These findings confirm that\nCALM is a practical approach for adapting LLMs to the financial domain."
                },
                "authors": [
                    {
                        "name": "Kota Tanabe"
                    },
                    {
                        "name": "Masanori Hirano"
                    },
                    {
                        "name": "Kazuki Matoya"
                    },
                    {
                        "name": "Kentaro Imajo"
                    },
                    {
                        "name": "Hiroki Sakaji"
                    },
                    {
                        "name": "Itsuki Noda"
                    }
                ],
                "author_detail": {
                    "name": "Itsuki Noda"
                },
                "author": "Itsuki Noda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09241v1",
                "updated": "2024-11-14T07:15:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    15,
                    24,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T07:15:24Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    15,
                    24,
                    3,
                    319,
                    0
                ],
                "title": "BlueME: Robust Underwater Robot-to-Robot Communication Using Compact\n  Magnetoelectric Antennas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlueME: Robust Underwater Robot-to-Robot Communication Using Compact\n  Magnetoelectric Antennas"
                },
                "summary": "We present the design, development, and experimental validation of BlueME, a\ncompact magnetoelectric (ME) antenna array system for underwater robot-to-robot\ncommunication. BlueME employs ME antennas operating at their natural mechanical\nresonance frequency to efficiently transmit and receive very-low-frequency\n(VLF) electromagnetic signals underwater. To evaluate its performance, we\ndeployed BlueME on an autonomous surface vehicle (ASV) and a remotely operated\nvehicle (ROV) in open-water field trials. Our tests demonstrate that BlueME\nmaintains reliable signal transmission at distances beyond 200 meters while\nconsuming only 1 watt of power. Field trials show that the system operates\neffectively in challenging underwater conditions such as turbidity, obstacles,\nand multipath interference -- that generally affect acoustics and optics. Our\nanalysis also examines the impact of complete submersion on system performance\nand identifies key deployment considerations. This work represents the first\npractical underwater deployment of ME antennas outside the laboratory and\nimplements the largest VLF ME array system to date. BlueME demonstrates\nsignificant potential for marine robotics and automation in multi-robot\ncooperative systems and remote sensor networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design, development, and experimental validation of BlueME, a\ncompact magnetoelectric (ME) antenna array system for underwater robot-to-robot\ncommunication. BlueME employs ME antennas operating at their natural mechanical\nresonance frequency to efficiently transmit and receive very-low-frequency\n(VLF) electromagnetic signals underwater. To evaluate its performance, we\ndeployed BlueME on an autonomous surface vehicle (ASV) and a remotely operated\nvehicle (ROV) in open-water field trials. Our tests demonstrate that BlueME\nmaintains reliable signal transmission at distances beyond 200 meters while\nconsuming only 1 watt of power. Field trials show that the system operates\neffectively in challenging underwater conditions such as turbidity, obstacles,\nand multipath interference -- that generally affect acoustics and optics. Our\nanalysis also examines the impact of complete submersion on system performance\nand identifies key deployment considerations. This work represents the first\npractical underwater deployment of ME antennas outside the laboratory and\nimplements the largest VLF ME array system to date. BlueME demonstrates\nsignificant potential for marine robotics and automation in multi-robot\ncooperative systems and remote sensor networks."
                },
                "authors": [
                    {
                        "name": "Mehron Talebi"
                    },
                    {
                        "name": "Sultan Mahmud"
                    },
                    {
                        "name": "Adam Khalifa"
                    },
                    {
                        "name": "Md Jahidul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Md Jahidul Islam"
                },
                "author": "Md Jahidul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10880v2",
                "updated": "2024-11-14T07:01:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    7,
                    1,
                    7,
                    3,
                    319,
                    0
                ],
                "published": "2024-06-16T10:04:19Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    10,
                    4,
                    19,
                    6,
                    168,
                    0
                ],
                "title": "Exploring the Potential of Multimodal LLM with Knowledge-Intensive\n  Multimodal ASR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Potential of Multimodal LLM with Knowledge-Intensive\n  Multimodal ASR"
                },
                "summary": "Recent advancements in multimodal large language models (MLLMs) have made\nsignificant progress in integrating information across various modalities, yet\nreal-world applications in educational and scientific domains remain\nchallenging. This paper introduces the Multimodal Scientific ASR (MS-ASR) task,\nwhich focuses on transcribing scientific conference videos by leveraging visual\ninformation from slides to enhance the accuracy of technical terminologies.\nRealized that traditional metrics like WER fall short in assessing performance\naccurately, prompting the proposal of severity-aware WER (SWER) that considers\nthe content type and severity of ASR errors. We propose the Scientific Vision\nAugmented ASR (SciVASR) framework as a baseline method, enabling MLLMs to\nimprove transcript quality through post-editing. Evaluations of\nstate-of-the-art MLLMs, including GPT-4o, show a 45% improvement over\nspeech-only baselines, highlighting the importance of multimodal information\nintegration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multimodal large language models (MLLMs) have made\nsignificant progress in integrating information across various modalities, yet\nreal-world applications in educational and scientific domains remain\nchallenging. This paper introduces the Multimodal Scientific ASR (MS-ASR) task,\nwhich focuses on transcribing scientific conference videos by leveraging visual\ninformation from slides to enhance the accuracy of technical terminologies.\nRealized that traditional metrics like WER fall short in assessing performance\naccurately, prompting the proposal of severity-aware WER (SWER) that considers\nthe content type and severity of ASR errors. We propose the Scientific Vision\nAugmented ASR (SciVASR) framework as a baseline method, enabling MLLMs to\nimprove transcript quality through post-editing. Evaluations of\nstate-of-the-art MLLMs, including GPT-4o, show a 45% improvement over\nspeech-only baselines, highlighting the importance of multimodal information\nintegration."
                },
                "authors": [
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "arxiv_comment": "Accepted to EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00996v2",
                "updated": "2024-11-14T06:55:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    55,
                    27,
                    3,
                    319,
                    0
                ],
                "published": "2024-07-01T06:22:38Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    6,
                    22,
                    38,
                    0,
                    183,
                    0
                ],
                "title": "Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?"
                },
                "summary": "Small Language Models (SLMs) are generally considered more compact versions\nof large language models (LLMs). This study investigates the ability of SLMs\nwith parameters between 1 and 3 billion to learn, retain, and subsequently\neliminate different types of noise present in the data. Four pre-trained SLMs\nwere utilized for this: Olmo 1B, Qwen1.5 1.8B, Gemma 2B, and Phi2 2.7B. The\nmodels were instruction-tuned on noise-free data and tested using in-context\nexamples to determine if they could learn noise through examples. Subsequently,\nnoise patterns were introduced in instruction tuning to evaluate the noise\nlearning, unlearning, and retention capabilities of the models. Olmo, the\nsmallest model, was highly sensitive to noise, quickly adapting to noisy\npatterns. Phi2 resisted learning character-level and transliteration noise,\nlikely due to its carefully curated, structured, and high-quality pretraining\ndata. Gemma excelled with transliteration noise, likely benefiting from its\nmultilingual pretraining. The findings can be used to develop robust training\nstrategies for SLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models (SLMs) are generally considered more compact versions\nof large language models (LLMs). This study investigates the ability of SLMs\nwith parameters between 1 and 3 billion to learn, retain, and subsequently\neliminate different types of noise present in the data. Four pre-trained SLMs\nwere utilized for this: Olmo 1B, Qwen1.5 1.8B, Gemma 2B, and Phi2 2.7B. The\nmodels were instruction-tuned on noise-free data and tested using in-context\nexamples to determine if they could learn noise through examples. Subsequently,\nnoise patterns were introduced in instruction tuning to evaluate the noise\nlearning, unlearning, and retention capabilities of the models. Olmo, the\nsmallest model, was highly sensitive to noise, quickly adapting to noisy\npatterns. Phi2 resisted learning character-level and transliteration noise,\nlikely due to its carefully curated, structured, and high-quality pretraining\ndata. Gemma excelled with transliteration noise, likely benefiting from its\nmultilingual pretraining. The findings can be used to develop robust training\nstrategies for SLMs."
                },
                "authors": [
                    {
                        "name": "Nicy Scaria"
                    },
                    {
                        "name": "Silvester John Joseph Kennedy"
                    },
                    {
                        "name": "Deepak Subramani"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Subramani"
                },
                "author": "Deepak Subramani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03735v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03735v4",
                "updated": "2024-11-14T06:42:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    42,
                    51,
                    3,
                    319,
                    0
                ],
                "published": "2024-01-08T08:54:22Z",
                "published_parsed": [
                    2024,
                    1,
                    8,
                    8,
                    54,
                    22,
                    0,
                    8,
                    0
                ],
                "title": "Language Models Encode the Value of Numbers Linearly",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Encode the Value of Numbers Linearly"
                },
                "summary": "Large language models (LLMs) have exhibited impressive competence in various\ntasks, but their internal mechanisms on mathematical problems are still\nunder-explored. In this paper, we study a fundamental question: how language\nmodels encode the value of numbers, a basic element in math. To study the\nquestion, we construct a synthetic dataset comprising addition problems and\nutilize linear probes to read out input numbers from the hidden states.\nExperimental results support the existence of encoded number values in LLMs on\ndifferent layers, and these values can be extracted via linear probes. Further\nexperiments show that LLMs store their calculation results in a similar manner,\nand we can intervene the output via simple vector additions, proving the causal\nconnection between encoded numbers and language model outputs. Our research\nprovides evidence that LLMs encode the value of numbers linearly, offering\ninsights for better exploring, designing, and utilizing numeric information in\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive competence in various\ntasks, but their internal mechanisms on mathematical problems are still\nunder-explored. In this paper, we study a fundamental question: how language\nmodels encode the value of numbers, a basic element in math. To study the\nquestion, we construct a synthetic dataset comprising addition problems and\nutilize linear probes to read out input numbers from the hidden states.\nExperimental results support the existence of encoded number values in LLMs on\ndifferent layers, and these values can be extracted via linear probes. Further\nexperiments show that LLMs store their calculation results in a similar manner,\nand we can intervene the output via simple vector additions, proving the causal\nconnection between encoded numbers and language model outputs. Our research\nprovides evidence that LLMs encode the value of numbers linearly, offering\ninsights for better exploring, designing, and utilizing numeric information in\nLLMs."
                },
                "authors": [
                    {
                        "name": "Fangwei Zhu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "The code and data are available at\n  https://github.com/solitaryzero/NumProbe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03735v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03735v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09224v1",
                "updated": "2024-11-14T06:40:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    40,
                    55,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T06:40:55Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    40,
                    55,
                    3,
                    319,
                    0
                ],
                "title": "Programming with AI: Evaluating ChatGPT, Gemini, AlphaCode, and GitHub\n  Copilot for Programmers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming with AI: Evaluating ChatGPT, Gemini, AlphaCode, and GitHub\n  Copilot for Programmers"
                },
                "summary": "Our everyday lives now heavily rely on artificial intelligence (AI) powered\nlarge language models (LLMs). Like regular users, programmers are also\nbenefiting from the newest large language models. In response to the critical\nrole that AI models play in modern software development, this study presents a\nthorough evaluation of leading programming assistants, including ChatGPT,\nGemini(Bard AI), AlphaCode, and GitHub Copilot. The evaluation is based on\ntasks like natural language processing and code generation accuracy in\ndifferent programming languages like Java, Python and C++. Based on the\nresults, it has emphasized their strengths and weaknesses and the importance of\nfurther modifications to increase the reliability and accuracy of the latest\npopular models. Although these AI assistants illustrate a high level of\nprogress in language understanding and code generation, along with ethical\nconsiderations and responsible usage, they provoke a necessity for discussion.\nWith time, developing more refined AI technology is essential for achieving\nadvanced solutions in various fields, especially with the knowledge of the\nfeature intricacies of these models and their implications. This study offers a\ncomparison of different LLMs and provides essential feedback on the rapidly\nchanging area of AI models. It also emphasizes the need for ethical\ndevelopmental practices to actualize AI models' full potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our everyday lives now heavily rely on artificial intelligence (AI) powered\nlarge language models (LLMs). Like regular users, programmers are also\nbenefiting from the newest large language models. In response to the critical\nrole that AI models play in modern software development, this study presents a\nthorough evaluation of leading programming assistants, including ChatGPT,\nGemini(Bard AI), AlphaCode, and GitHub Copilot. The evaluation is based on\ntasks like natural language processing and code generation accuracy in\ndifferent programming languages like Java, Python and C++. Based on the\nresults, it has emphasized their strengths and weaknesses and the importance of\nfurther modifications to increase the reliability and accuracy of the latest\npopular models. Although these AI assistants illustrate a high level of\nprogress in language understanding and code generation, along with ethical\nconsiderations and responsible usage, they provoke a necessity for discussion.\nWith time, developing more refined AI technology is essential for achieving\nadvanced solutions in various fields, especially with the knowledge of the\nfeature intricacies of these models and their implications. This study offers a\ncomparison of different LLMs and provides essential feedback on the rapidly\nchanging area of AI models. It also emphasizes the need for ethical\ndevelopmental practices to actualize AI models' full potential."
                },
                "authors": [
                    {
                        "name": "Md Kamrul Siam"
                    },
                    {
                        "name": "Huanying Gu"
                    },
                    {
                        "name": "Jerry Q. Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jerry Q. Cheng"
                },
                "author": "Jerry Q. Cheng",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09213v1",
                "updated": "2024-11-14T06:19:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    19,
                    18,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T06:19:18Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    19,
                    18,
                    3,
                    319,
                    0
                ],
                "title": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation\n  Systems for Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive and Practical Evaluation of Retrieval-Augmented Generation\n  Systems for Medical Question Answering"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a promising approach to\nenhance the performance of large language models (LLMs) in knowledge-intensive\ntasks such as those from medical domain. However, the sensitive nature of the\nmedical domain necessitates a completely accurate and trustworthy system. While\nexisting RAG benchmarks primarily focus on the standard retrieve-answer\nsetting, they overlook many practical scenarios that measure crucial aspects of\na reliable medical system. This paper addresses this gap by providing a\ncomprehensive evaluation framework for medical question-answering (QA) systems\nin a RAG setting for these situations, including sufficiency, integration, and\nrobustness. We introduce Medical Retrieval-Augmented Generation Benchmark\n(MedRGB) that provides various supplementary elements to four medical QA\ndatasets for testing LLMs' ability to handle these specific scenarios.\nUtilizing MedRGB, we conduct extensive evaluations of both state-of-the-art\ncommercial LLMs and open-source models across multiple retrieval conditions.\nOur experimental results reveals current models' limited ability to handle\nnoise and misinformation in the retrieved documents. We further analyze the\nLLMs' reasoning processes to provides valuable insights and future directions\nfor developing RAG systems in this critical medical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a promising approach to\nenhance the performance of large language models (LLMs) in knowledge-intensive\ntasks such as those from medical domain. However, the sensitive nature of the\nmedical domain necessitates a completely accurate and trustworthy system. While\nexisting RAG benchmarks primarily focus on the standard retrieve-answer\nsetting, they overlook many practical scenarios that measure crucial aspects of\na reliable medical system. This paper addresses this gap by providing a\ncomprehensive evaluation framework for medical question-answering (QA) systems\nin a RAG setting for these situations, including sufficiency, integration, and\nrobustness. We introduce Medical Retrieval-Augmented Generation Benchmark\n(MedRGB) that provides various supplementary elements to four medical QA\ndatasets for testing LLMs' ability to handle these specific scenarios.\nUtilizing MedRGB, we conduct extensive evaluations of both state-of-the-art\ncommercial LLMs and open-source models across multiple retrieval conditions.\nOur experimental results reveals current models' limited ability to handle\nnoise and misinformation in the retrieved documents. We further analyze the\nLLMs' reasoning processes to provides valuable insights and future directions\nfor developing RAG systems in this critical medical domain."
                },
                "authors": [
                    {
                        "name": "Nghia Trung Ngo"
                    },
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10570v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10570v6",
                "updated": "2024-11-14T06:09:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    9,
                    47,
                    3,
                    319,
                    0
                ],
                "published": "2023-10-16T16:45:12Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    16,
                    45,
                    12,
                    0,
                    289,
                    0
                ],
                "title": "On Context Utilization in Summarization with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Context Utilization in Summarization with Large Language Models"
                },
                "summary": "Large language models (LLMs) excel in abstractive summarization tasks,\ndelivering fluent and pertinent summaries. Recent advancements have extended\ntheir capabilities to handle long-input contexts, exceeding 100k tokens.\nHowever, in question answering, language models exhibit uneven utilization of\ntheir input context. They tend to favor the initial and final segments,\nresulting in a U-shaped performance pattern concerning where the answer is\nlocated within the input. This bias raises concerns, particularly in\nsummarization where crucial content may be dispersed throughout the source\ndocument(s). Besides, in summarization, mapping facts from the source to the\nsummary is not trivial as salient content is usually re-phrased. In this paper,\nwe conduct the first comprehensive study on context utilization and position\nbias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5\nevaluation metrics. We introduce a new evaluation benchmark called MiddleSum on\nthe which we benchmark two alternative inference methods to alleviate position\nbias: hierarchical summarization and incremental summarization. Our code and\ndata can be found here: https://github.com/ntunlp/MiddleSum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in abstractive summarization tasks,\ndelivering fluent and pertinent summaries. Recent advancements have extended\ntheir capabilities to handle long-input contexts, exceeding 100k tokens.\nHowever, in question answering, language models exhibit uneven utilization of\ntheir input context. They tend to favor the initial and final segments,\nresulting in a U-shaped performance pattern concerning where the answer is\nlocated within the input. This bias raises concerns, particularly in\nsummarization where crucial content may be dispersed throughout the source\ndocument(s). Besides, in summarization, mapping facts from the source to the\nsummary is not trivial as salient content is usually re-phrased. In this paper,\nwe conduct the first comprehensive study on context utilization and position\nbias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5\nevaluation metrics. We introduce a new evaluation benchmark called MiddleSum on\nthe which we benchmark two alternative inference methods to alleviate position\nbias: hierarchical summarization and incremental summarization. Our code and\ndata can be found here: https://github.com/ntunlp/MiddleSum."
                },
                "authors": [
                    {
                        "name": "Mathieu Ravaut"
                    },
                    {
                        "name": "Aixin Sun"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "ACL 2024. 9 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.10570v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10570v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00476v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00476v3",
                "updated": "2024-11-14T06:06:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    6,
                    9,
                    3,
                    319,
                    0
                ],
                "published": "2024-06-29T15:47:28Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    15,
                    47,
                    28,
                    5,
                    181,
                    0
                ],
                "title": "Large Language Models for Power Scheduling: A User-Centric Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Power Scheduling: A User-Centric Approach"
                },
                "summary": "While traditional optimization and scheduling schemes are designed to meet\nfixed, predefined system requirements, future systems are moving toward\nuser-driven approaches and personalized services, aiming to achieve high\nquality-of-experience (QoE) and flexibility. This challenge is particularly\npronounced in wireless and digitalized energy networks, where users'\nrequirements have largely not been taken into consideration due to the lack of\na common language between users and machines. The emergence of powerful large\nlanguage models (LLMs) marks a radical departure from traditional\nsystem-centric methods into more advanced user-centric approaches by providing\na natural communication interface between users and devices. In this paper, for\nthe first time, we introduce a novel architecture for resource scheduling\nproblems by constructing three LLM agents to convert an arbitrary user's voice\nrequest (VRQ) into a resource allocation vector. Specifically, we design an LLM\nintent recognition agent to translate the request into an optimization problem\n(OP), an LLM OP parameter identification agent, and an LLM OP solving agent. To\nevaluate system performance, we construct a database of typical VRQs in the\ncontext of electric vehicle (EV) charging. As a proof of concept, we primarily\nuse Llama 3 8B. Through testing with different prompt engineering scenarios,\nthe obtained results demonstrate the efficiency of the proposed architecture.\nThe conducted performance analysis allows key insights to be extracted. For\ninstance, having a larger set of candidate OPs to model the real-world problem\nmight degrade the final performance because of a higher recognition/OP\nclassification noise level. All results and codes are open source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While traditional optimization and scheduling schemes are designed to meet\nfixed, predefined system requirements, future systems are moving toward\nuser-driven approaches and personalized services, aiming to achieve high\nquality-of-experience (QoE) and flexibility. This challenge is particularly\npronounced in wireless and digitalized energy networks, where users'\nrequirements have largely not been taken into consideration due to the lack of\na common language between users and machines. The emergence of powerful large\nlanguage models (LLMs) marks a radical departure from traditional\nsystem-centric methods into more advanced user-centric approaches by providing\na natural communication interface between users and devices. In this paper, for\nthe first time, we introduce a novel architecture for resource scheduling\nproblems by constructing three LLM agents to convert an arbitrary user's voice\nrequest (VRQ) into a resource allocation vector. Specifically, we design an LLM\nintent recognition agent to translate the request into an optimization problem\n(OP), an LLM OP parameter identification agent, and an LLM OP solving agent. To\nevaluate system performance, we construct a database of typical VRQs in the\ncontext of electric vehicle (EV) charging. As a proof of concept, we primarily\nuse Llama 3 8B. Through testing with different prompt engineering scenarios,\nthe obtained results demonstrate the efficiency of the proposed architecture.\nThe conducted performance analysis allows key insights to be extracted. For\ninstance, having a larger set of candidate OPs to model the real-world problem\nmight degrade the final performance because of a higher recognition/OP\nclassification noise level. All results and codes are open source."
                },
                "authors": [
                    {
                        "name": "Thomas Mongaillard"
                    },
                    {
                        "name": "Samson Lasaulce"
                    },
                    {
                        "name": "Othman Hicheur"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Lina Bariah"
                    },
                    {
                        "name": "Vineeth S. Varma"
                    },
                    {
                        "name": "Hang Zou"
                    },
                    {
                        "name": "Qiyang Zhao"
                    },
                    {
                        "name": "Merouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Merouane Debbah"
                },
                "author": "Merouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00476v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00476v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08504v2",
                "updated": "2024-11-14T05:51:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    5,
                    51,
                    26,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-13T10:42:11Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    10,
                    42,
                    11,
                    2,
                    318,
                    0
                ],
                "title": "Towards Objective and Unbiased Decision Assessments with LLM-Enhanced\n  Hierarchical Attention Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Objective and Unbiased Decision Assessments with LLM-Enhanced\n  Hierarchical Attention Networks"
                },
                "summary": "How objective and unbiased are we while making decisions? This work\ninvestigates cognitive bias identification in high-stake decision making\nprocess by human experts, questioning its effectiveness in real-world settings,\nsuch as candidates assessments for university admission. We begin with a\nstatistical analysis assessing correlations among different decision points\namong in the current process, which discovers discrepancies that imply\ncognitive bias and inconsistency in decisions. This motivates our exploration\nof bias-aware AI-augmented workflow that surpass human judgment. We propose\nBGM-HAN, an enhanced Hierarchical Attention Network with Byte-Pair Encoding,\nGated Residual Connections and Multi-Head Attention. Using it as a backbone\nmodel, we further propose a Shortlist-Analyse-Recommend (SAR) agentic workflow,\nwhich simulate real-world decision-making. In our experiments, both the\nproposed model and the agentic workflow significantly improves on both human\njudgment and alternative models, validated with real-world data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How objective and unbiased are we while making decisions? This work\ninvestigates cognitive bias identification in high-stake decision making\nprocess by human experts, questioning its effectiveness in real-world settings,\nsuch as candidates assessments for university admission. We begin with a\nstatistical analysis assessing correlations among different decision points\namong in the current process, which discovers discrepancies that imply\ncognitive bias and inconsistency in decisions. This motivates our exploration\nof bias-aware AI-augmented workflow that surpass human judgment. We propose\nBGM-HAN, an enhanced Hierarchical Attention Network with Byte-Pair Encoding,\nGated Residual Connections and Multi-Head Attention. Using it as a backbone\nmodel, we further propose a Shortlist-Analyse-Recommend (SAR) agentic workflow,\nwhich simulate real-world decision-making. In our experiments, both the\nproposed model and the agentic workflow significantly improves on both human\njudgment and alternative models, validated with real-world data."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    }
                ],
                "author_detail": {
                    "name": "Roy Ka-Wei Lee"
                },
                "author": "Roy Ka-Wei Lee",
                "arxiv_comment": "Source code is available at: https://github.com/junhua/bgm-han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09200v1",
                "updated": "2024-11-14T05:45:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    5,
                    45,
                    55,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T05:45:55Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    5,
                    45,
                    55,
                    3,
                    319,
                    0
                ],
                "title": "Advancing Software Security and Reliability in Cloud Platforms through\n  AI-based Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Software Security and Reliability in Cloud Platforms through\n  AI-based Anomaly Detection"
                },
                "summary": "Continuous Integration/Continuous Deployment (CI/CD) is fundamental for\nadvanced software development, supporting faster and more efficient delivery of\ncode changes into cloud environments. However, security issues in the CI/CD\npipeline remain challenging, and incidents (e.g., DDoS, Bot, Log4j, etc.) are\nhappening over the cloud environments. While plenty of literature discusses\nstatic security testing and CI/CD practices, only a few deal with network\ntraffic pattern analysis to detect different cyberattacks. This research aims\nto enhance CI/CD pipeline security by implementing anomaly detection through AI\n(Artificial Intelligence) support. The goal is to identify unusual behaviour or\nvariations from network traffic patterns in pipeline and cloud platforms. The\nsystem shall integrate into the workflow to continuously monitor pipeline\nactivities and cloud infrastructure. Additionally, it aims to explore adaptive\nresponse mechanisms to mitigate the detected anomalies or security threats.\nThis research employed two popular network traffic datasets, CSE-CIC-IDS2018\nand CSE-CIC-IDS2017. We implemented a combination of Convolution Neural\nNetwork(CNN) and Long Short-Term Memory (LSTM) to detect unusual traffic\npatterns. We achieved an accuracy of 98.69% and 98.30% and generated log files\nin different CI/CD pipeline stages that resemble the network anomalies affected\nto address security challenges in modern DevOps practices, contributing to\nadvancing software security and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Integration/Continuous Deployment (CI/CD) is fundamental for\nadvanced software development, supporting faster and more efficient delivery of\ncode changes into cloud environments. However, security issues in the CI/CD\npipeline remain challenging, and incidents (e.g., DDoS, Bot, Log4j, etc.) are\nhappening over the cloud environments. While plenty of literature discusses\nstatic security testing and CI/CD practices, only a few deal with network\ntraffic pattern analysis to detect different cyberattacks. This research aims\nto enhance CI/CD pipeline security by implementing anomaly detection through AI\n(Artificial Intelligence) support. The goal is to identify unusual behaviour or\nvariations from network traffic patterns in pipeline and cloud platforms. The\nsystem shall integrate into the workflow to continuously monitor pipeline\nactivities and cloud infrastructure. Additionally, it aims to explore adaptive\nresponse mechanisms to mitigate the detected anomalies or security threats.\nThis research employed two popular network traffic datasets, CSE-CIC-IDS2018\nand CSE-CIC-IDS2017. We implemented a combination of Convolution Neural\nNetwork(CNN) and Long Short-Term Memory (LSTM) to detect unusual traffic\npatterns. We achieved an accuracy of 98.69% and 98.30% and generated log files\nin different CI/CD pipeline stages that resemble the network anomalies affected\nto address security challenges in modern DevOps practices, contributing to\nadvancing software security and reliability."
                },
                "authors": [
                    {
                        "name": "Sabbir M. Saleh"
                    },
                    {
                        "name": "Ibrahim Mohammed Sayem"
                    },
                    {
                        "name": "Nazim Madhavji"
                    },
                    {
                        "name": "John Steinbacher"
                    }
                ],
                "author_detail": {
                    "name": "John Steinbacher"
                },
                "author": "John Steinbacher",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06493v2",
                "updated": "2024-11-14T05:34:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    5,
                    34,
                    13,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-10T15:21:30Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    21,
                    30,
                    6,
                    315,
                    0
                ],
                "title": "LProtector: An LLM-driven Vulnerability Detection System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LProtector: An LLM-driven Vulnerability Detection System"
                },
                "summary": "This paper presents LProtector, an automated vulnerability detection system\nfor C/C++ codebases driven by the large language model (LLM) GPT-4o and\nRetrieval-Augmented Generation (RAG). As software complexity grows, traditional\nmethods face challenges in detecting vulnerabilities effectively. LProtector\nleverages GPT-4o's powerful code comprehension and generation capabilities to\nperform binary classification and identify vulnerabilities within target\ncodebases. We conducted experiments on the Big-Vul dataset, showing that\nLProtector outperforms two state-of-the-art baselines in terms of F1 score,\ndemonstrating the potential of integrating LLMs with vulnerability detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents LProtector, an automated vulnerability detection system\nfor C/C++ codebases driven by the large language model (LLM) GPT-4o and\nRetrieval-Augmented Generation (RAG). As software complexity grows, traditional\nmethods face challenges in detecting vulnerabilities effectively. LProtector\nleverages GPT-4o's powerful code comprehension and generation capabilities to\nperform binary classification and identify vulnerabilities within target\ncodebases. We conducted experiments on the Big-Vul dataset, showing that\nLProtector outperforms two state-of-the-art baselines in terms of F1 score,\ndemonstrating the potential of integrating LLMs with vulnerability detection."
                },
                "authors": [
                    {
                        "name": "Ze Sheng"
                    },
                    {
                        "name": "Fenghua Wu"
                    },
                    {
                        "name": "Xiangwu Zuo"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Yuxin Qiao"
                    },
                    {
                        "name": "Lei Hang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Hang"
                },
                "author": "Lei Hang",
                "arxiv_comment": "5 pages, 4 figures. This is a preprint version of the article. The\n  final version will be published in the proceedings of the IEEE conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05964v2",
                "updated": "2024-11-14T05:00:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    5,
                    0,
                    13,
                    3,
                    319,
                    0
                ],
                "published": "2024-06-10T01:46:42Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    1,
                    46,
                    42,
                    0,
                    162,
                    0
                ],
                "title": "Distributionally Robust Safe Sample Elimination under Covariate Shift",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributionally Robust Safe Sample Elimination under Covariate Shift"
                },
                "summary": "We consider a machine learning setup where one training dataset is used to\ntrain multiple models across slightly different data distributions. This occurs\nwhen customized models are needed for various deployment environments. To\nreduce storage and training costs, we propose the DRSSS method, which combines\ndistributionally robust (DR) optimization and safe sample screening (SSS). The\nkey benefit of this method is that models trained on the reduced dataset will\nperform the same as those trained on the full dataset for all possible\ndifferent environments. In this paper, we focus on covariate shift as a type of\ndata distribution change and demonstrate the effectiveness of our method\nthrough experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a machine learning setup where one training dataset is used to\ntrain multiple models across slightly different data distributions. This occurs\nwhen customized models are needed for various deployment environments. To\nreduce storage and training costs, we propose the DRSSS method, which combines\ndistributionally robust (DR) optimization and safe sample screening (SSS). The\nkey benefit of this method is that models trained on the reduced dataset will\nperform the same as those trained on the full dataset for all possible\ndifferent environments. In this paper, we focus on covariate shift as a type of\ndata distribution change and demonstrate the effectiveness of our method\nthrough experiments."
                },
                "authors": [
                    {
                        "name": "Hiroyuki Hanada"
                    },
                    {
                        "name": "Tatsuya Aoyama"
                    },
                    {
                        "name": "Satoshi Akahane"
                    },
                    {
                        "name": "Tomonari Tanaka"
                    },
                    {
                        "name": "Yoshito Okura"
                    },
                    {
                        "name": "Yu Inatsu"
                    },
                    {
                        "name": "Noriaki Hashimoto"
                    },
                    {
                        "name": "Shion Takeno"
                    },
                    {
                        "name": "Taro Murayama"
                    },
                    {
                        "name": "Hanju Lee"
                    },
                    {
                        "name": "Shinya Kojima"
                    },
                    {
                        "name": "Ichiro Takeuchi"
                    }
                ],
                "author_detail": {
                    "name": "Ichiro Takeuchi"
                },
                "author": "Ichiro Takeuchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11214v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11214v3",
                "updated": "2024-11-14T03:53:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    3,
                    53,
                    56,
                    3,
                    319,
                    0
                ],
                "published": "2024-06-17T05:13:25Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    5,
                    13,
                    25,
                    0,
                    169,
                    0
                ],
                "title": "Problematic Tokens: Tokenizer Bias in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problematic Tokens: Tokenizer Bias in Large Language Models"
                },
                "summary": "Recent advancements in large language models(LLMs), such as GPT-4 and GPT-4o,\nhave shown exceptional performance, especially in languages with abundant\nresources like English, thanks to extensive datasets that ensure robust\ntraining. Conversely, these models exhibit limitations when processing\nunder-resourced languages such as Chinese and Korean, where issues including\nhallucinatory responses remain prevalent. This paper traces the roots of these\ndisparities to the tokenization process inherent to these models. Specifically,\nit explores how the tokenizers vocabulary, often used to speed up the\ntokenization process and reduce tokens but constructed independently of the\nactual model training data, inadequately represents non-English languages. This\nmisrepresentation results in the propagation of under-trained or untrained\ntokens, which perpetuate biases and pose serious concerns related to data\nsecurity and ethical standards. We aim to dissect the tokenization mechanics of\nGPT-4o, illustrating how its simplified token-handling methods amplify these\nrisks and offer strategic solutions to mitigate associated security and ethical\nissues. Through this study, we emphasize the critical need to rethink\ntokenization frameworks to foster more equitable and secure AI technologies.\nThe code and data are available at https://github.com/yeyimilk/LLMGPT4o",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models(LLMs), such as GPT-4 and GPT-4o,\nhave shown exceptional performance, especially in languages with abundant\nresources like English, thanks to extensive datasets that ensure robust\ntraining. Conversely, these models exhibit limitations when processing\nunder-resourced languages such as Chinese and Korean, where issues including\nhallucinatory responses remain prevalent. This paper traces the roots of these\ndisparities to the tokenization process inherent to these models. Specifically,\nit explores how the tokenizers vocabulary, often used to speed up the\ntokenization process and reduce tokens but constructed independently of the\nactual model training data, inadequately represents non-English languages. This\nmisrepresentation results in the propagation of under-trained or untrained\ntokens, which perpetuate biases and pose serious concerns related to data\nsecurity and ethical standards. We aim to dissect the tokenization mechanics of\nGPT-4o, illustrating how its simplified token-handling methods amplify these\nrisks and offer strategic solutions to mitigate associated security and ethical\nissues. Through this study, we emphasize the critical need to rethink\ntokenization frameworks to foster more equitable and secure AI technologies.\nThe code and data are available at https://github.com/yeyimilk/LLMGPT4o"
                },
                "authors": [
                    {
                        "name": "Jin Yang"
                    },
                    {
                        "name": "Zhiqiang Wang"
                    },
                    {
                        "name": "Yanbin Lin"
                    },
                    {
                        "name": "Zunduo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zunduo Zhao"
                },
                "author": "Zunduo Zhao",
                "arxiv_comment": "11th IEEE Special session on Privacy and Security of Big Data (PSBD\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11214v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11214v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09159v1",
                "updated": "2024-11-14T03:27:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    3,
                    27,
                    43,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T03:27:43Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    3,
                    27,
                    43,
                    3,
                    319,
                    0
                ],
                "title": "PIMCOMP: An End-to-End DNN Compiler for Processing-In-Memory\n  Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIMCOMP: An End-to-End DNN Compiler for Processing-In-Memory\n  Accelerators"
                },
                "summary": "Various processing-in-memory (PIM) accelerators based on various devices,\nmicro-architectures, and interfaces have been proposed to accelerate deep\nneural networks (DNNs). How to deploy DNNs onto PIM-based accelerators is the\nkey to explore PIM's high performance and energy efficiency. The scale of DNN\nmodels, the diversity of PIM accelerators, and the complexity of deployment are\nfar beyond the human deployment capability. Hence, an automatic deployment\nmethodology is indispensable. In this work, we propose PIMCOMP, an end-to-end\nDNN compiler tailored for PIM accelerators, achieving efficient deployment of\nDNN models on PIM hardware. PIMCOMP can adapt to various PIM architectures by\nusing an abstract configurable PIM accelerator template with a set of\npseudo-instructions, which is a high-level abstraction of the hardware's\nfundamental functionalities. Through a generic multi-level optimization\nframework, PIMCOMP realizes an end-to-end conversion from a high-level DNN\ndescription to pseudo-instructions, which can be further converted to specific\nhardware intrinsics/primitives. The compilation addresses two critical issues\nin PIM-accelerated inference from a system perspective: resource utilization\nand dataflow scheduling. PIMCOMP adopts a flexible unfolding format to reshape\nand partition convolutional layers, adopts a weight-layout guided\ncomputation-storage-mapping approach to enhance resource utilization, and\nbalances the system's computation, memory access, and communication\ncharacteristics. For dataflow scheduling, we design two scheduling algorithms\nwith different inter-layer pipeline granularities to support varying\napplication scenarios while ensuring high computational parallelism.\nExperiments demonstrate that PIMCOMP improves throughput, latency, and energy\nefficiency across various architectures. PIMCOMP is open-sourced at\n\\url{https://github.com/sunxt99/PIMCOMP-NN}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various processing-in-memory (PIM) accelerators based on various devices,\nmicro-architectures, and interfaces have been proposed to accelerate deep\nneural networks (DNNs). How to deploy DNNs onto PIM-based accelerators is the\nkey to explore PIM's high performance and energy efficiency. The scale of DNN\nmodels, the diversity of PIM accelerators, and the complexity of deployment are\nfar beyond the human deployment capability. Hence, an automatic deployment\nmethodology is indispensable. In this work, we propose PIMCOMP, an end-to-end\nDNN compiler tailored for PIM accelerators, achieving efficient deployment of\nDNN models on PIM hardware. PIMCOMP can adapt to various PIM architectures by\nusing an abstract configurable PIM accelerator template with a set of\npseudo-instructions, which is a high-level abstraction of the hardware's\nfundamental functionalities. Through a generic multi-level optimization\nframework, PIMCOMP realizes an end-to-end conversion from a high-level DNN\ndescription to pseudo-instructions, which can be further converted to specific\nhardware intrinsics/primitives. The compilation addresses two critical issues\nin PIM-accelerated inference from a system perspective: resource utilization\nand dataflow scheduling. PIMCOMP adopts a flexible unfolding format to reshape\nand partition convolutional layers, adopts a weight-layout guided\ncomputation-storage-mapping approach to enhance resource utilization, and\nbalances the system's computation, memory access, and communication\ncharacteristics. For dataflow scheduling, we design two scheduling algorithms\nwith different inter-layer pipeline granularities to support varying\napplication scenarios while ensuring high computational parallelism.\nExperiments demonstrate that PIMCOMP improves throughput, latency, and energy\nefficiency across various architectures. PIMCOMP is open-sourced at\n\\url{https://github.com/sunxt99/PIMCOMP-NN}."
                },
                "authors": [
                    {
                        "name": "Xiaotian Sun"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Wanqian Li"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Xiaoming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Chen"
                },
                "author": "Xiaoming Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12382v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12382v3",
                "updated": "2024-11-14T03:10:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    3,
                    10,
                    45,
                    3,
                    319,
                    0
                ],
                "published": "2024-06-18T08:14:28Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    8,
                    14,
                    28,
                    1,
                    170,
                    0
                ],
                "title": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Instance Training to Instruction Learning: Task Adapters Generation\n  from Instructions"
                },
                "summary": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have acquired the ability to solve general tasks\nby utilizing instruction finetuning (IFT). However, IFT still relies heavily on\ninstance training of extensive task data, which greatly limits the adaptability\nof LLMs to real-world scenarios where labeled task instances are scarce and\nbroader task generalization becomes paramount. Contrary to LLMs, humans acquire\nskills and complete tasks not merely through repeated practice but also by\nunderstanding and following instructional guidelines. This paper is dedicated\nto simulating human learning to address the shortcomings of instance training,\nfocusing on instruction learning to enhance cross-task generalization. Within\nthis context, we introduce Task Adapters Generation from Instructions (TAGI),\nwhich automatically constructs the task-specific model in a parameter\ngeneration manner based on the given task instructions without retraining for\nunseen tasks. Specifically, we utilize knowledge distillation to enhance the\nconsistency between TAGI developed through Learning with Instruction and\ntask-specific models developed through Training with Instance, by aligning the\nlabels, output logits, and adapter parameters between them. TAGI is endowed\nwith cross-task generalization capabilities through a two-stage training\nprocess that includes hypernetwork pretraining and finetuning. We evaluate TAGI\non the Super-Natural Instructions and P3 datasets. The experimental results\ndemonstrate that TAGI can match or even outperform traditional meta-trained\nmodels and other hypernetwork models, while significantly reducing\ncomputational requirements."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Yuanzhe Zhang"
                    },
                    {
                        "name": "Yanchao Hao"
                    },
                    {
                        "name": "Shengping Liu"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12382v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12382v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09148v1",
                "updated": "2024-11-14T02:58:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    2,
                    58,
                    54,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T02:58:54Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    2,
                    58,
                    54,
                    3,
                    319,
                    0
                ],
                "title": "Toward Democratized Generative AI in Next-Generation Mobile Edge\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Democratized Generative AI in Next-Generation Mobile Edge\n  Networks"
                },
                "summary": "The rapid development of generative AI technologies, including large language\nmodels (LLMs), has brought transformative changes to various fields. However,\ndeploying such advanced models on mobile and edge devices remains challenging\ndue to their high computational, memory, communication, and energy\nrequirements. To address these challenges, we propose a model-centric framework\nfor democratizing generative AI deployment on mobile and edge networks. First,\nwe comprehensively review key compact model strategies, such as quantization,\nmodel pruning, and knowledge distillation, and present key performance metrics\nto optimize generative AI for mobile deployment. Next, we provide a focused\nreview of mobile and edge networks, emphasizing the specific challenges and\nrequirements of these environments. We further conduct a case study\ndemonstrating the effectiveness of these strategies by deploying LLMs on real\nmobile edge devices. Experimental results highlight the practicality of\ndemocratized LLMs, with significant improvements in generalization accuracy,\nhallucination rate, accessibility, and resource consumption. Finally, we\ndiscuss potential research directions to further advance the deployment of\ngenerative AI in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of generative AI technologies, including large language\nmodels (LLMs), has brought transformative changes to various fields. However,\ndeploying such advanced models on mobile and edge devices remains challenging\ndue to their high computational, memory, communication, and energy\nrequirements. To address these challenges, we propose a model-centric framework\nfor democratizing generative AI deployment on mobile and edge networks. First,\nwe comprehensively review key compact model strategies, such as quantization,\nmodel pruning, and knowledge distillation, and present key performance metrics\nto optimize generative AI for mobile deployment. Next, we provide a focused\nreview of mobile and edge networks, emphasizing the specific challenges and\nrequirements of these environments. We further conduct a case study\ndemonstrating the effectiveness of these strategies by deploying LLMs on real\nmobile edge devices. Experimental results highlight the practicality of\ndemocratized LLMs, with significant improvements in generalization accuracy,\nhallucination rate, accessibility, and resource consumption. Finally, we\ndiscuss potential research directions to further advance the deployment of\ngenerative AI in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Jiayi He"
                    },
                    {
                        "name": "Xiaofeng Luo"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Yonghui Li"
                    },
                    {
                        "name": "Biplab Sikdar"
                    }
                ],
                "author_detail": {
                    "name": "Biplab Sikdar"
                },
                "author": "Biplab Sikdar",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09146v1",
                "updated": "2024-11-14T02:57:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    2,
                    57,
                    23,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T02:57:23Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    2,
                    57,
                    23,
                    3,
                    319,
                    0
                ],
                "title": "Secrecy Energy Efficiency Maximization in IRS-Assisted VLC MISO Networks\n  with RSMA: A DS-PPO approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secrecy Energy Efficiency Maximization in IRS-Assisted VLC MISO Networks\n  with RSMA: A DS-PPO approach"
                },
                "summary": "This paper investigates intelligent reflecting surface (IRS)-assisted\nmultiple-input single-output (MISO) visible light communication (VLC) networks\nutilizing the rate-splitting multiple access (RSMA) scheme. {In these\nnetworks,} an eavesdropper (Eve) attempts to eavesdrop on communications\nintended for legitimate users (LUs). To enhance information security and energy\nefficiency simultaneously, we formulate a secrecy energy efficiency (SEE)\nmaximization problem. In the formulated problem, beamforming vectors, RSMA\ncommon rates, direct current (DC) bias, and IRS alignment matrices are jointly\noptimized subject to constraints on total power budget, quality of service\n(QoS) requirements, linear operating region of light emitting diodes (LEDs),\nand common information rate allocation. Due to the non-convex and NP-hard\nnature of the formulated problem, we propose a deep reinforcement learning\n(DRL)-based dual-sampling proximal policy optimization (DS-PPO) approach. {The\napproach leverages} dual sample strategies and generalized advantage estimation\n(GAE). In addition, to further simplify the design, we adopt the maximum ratio\ntransmission (MRT) and zero-forcing (ZF) as beamforming vectors in the action\nspace. Simulation results show that the proposed DS-PPO approach outperforms\ntraditional baseline approaches in terms of achievable SEE and significantly\nimproves convergence speed compared to the original PPO approach. Moreover,\nimplementing the RSMA scheme and IRS contributes to overall system performance,\n{achieving approximately $19.67\\%$ improvement over traditional multiple access\nschemes and $25.74\\%$ improvement over networks without IRS deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates intelligent reflecting surface (IRS)-assisted\nmultiple-input single-output (MISO) visible light communication (VLC) networks\nutilizing the rate-splitting multiple access (RSMA) scheme. {In these\nnetworks,} an eavesdropper (Eve) attempts to eavesdrop on communications\nintended for legitimate users (LUs). To enhance information security and energy\nefficiency simultaneously, we formulate a secrecy energy efficiency (SEE)\nmaximization problem. In the formulated problem, beamforming vectors, RSMA\ncommon rates, direct current (DC) bias, and IRS alignment matrices are jointly\noptimized subject to constraints on total power budget, quality of service\n(QoS) requirements, linear operating region of light emitting diodes (LEDs),\nand common information rate allocation. Due to the non-convex and NP-hard\nnature of the formulated problem, we propose a deep reinforcement learning\n(DRL)-based dual-sampling proximal policy optimization (DS-PPO) approach. {The\napproach leverages} dual sample strategies and generalized advantage estimation\n(GAE). In addition, to further simplify the design, we adopt the maximum ratio\ntransmission (MRT) and zero-forcing (ZF) as beamforming vectors in the action\nspace. Simulation results show that the proposed DS-PPO approach outperforms\ntraditional baseline approaches in terms of achievable SEE and significantly\nimproves convergence speed compared to the original PPO approach. Moreover,\nimplementing the RSMA scheme and IRS contributes to overall system performance,\n{achieving approximately $19.67\\%$ improvement over traditional multiple access\nschemes and $25.74\\%$ improvement over networks without IRS deployment."
                },
                "authors": [
                    {
                        "name": "Yangbo Guo"
                    },
                    {
                        "name": "Jianhui Fan"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Baofang Chang"
                    },
                    {
                        "name": "Derrick Wing Kwan Ng"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Dong In Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dong In Kim"
                },
                "author": "Dong In Kim",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08733v2",
                "updated": "2024-11-14T02:36:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    2,
                    36,
                    58,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-13T16:15:38Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    16,
                    15,
                    38,
                    2,
                    318,
                    0
                ],
                "title": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free\n  Self-Alignment of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Rewarding with Prompt Optimization Enables Tuning-free\n  Self-Alignment of Language Models"
                },
                "summary": "Aligning Large Language Models (LLMs) traditionally relies on costly training\nand human preference annotations. Self-alignment seeks to reduce these expenses\nby enabling models to align themselves. To further lower costs and achieve\nalignment without any expensive tuning or annotations, we introduce a new\ntuning-free approach for self-alignment, Dynamic Rewarding with Prompt\nOptimization (DRPO). Our approach leverages a search-based optimization\nframework that allows LLMs to iteratively self-improve and craft the optimal\nalignment instructions, all without additional training or human intervention.\nThe core of DRPO is a dynamic rewarding mechanism, which identifies and\nrectifies model-specific alignment weaknesses, allowing LLMs to adapt\nefficiently to diverse alignment challenges. Empirical evaluations on eight\nrecent LLMs, both open- and closed-sourced, demonstrate that DRPO significantly\nenhances alignment performance, with base models outperforming their\nSFT/RLHF-tuned counterparts. Moreover, the prompts automatically optimized by\nDRPO surpass those curated by human experts, further validating the\neffectiveness of our approach. Our findings highlight the great potential of\ncurrent LLMs to achieve adaptive self-alignment through inference-time\noptimization, complementing tuning-based alignment methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models (LLMs) traditionally relies on costly training\nand human preference annotations. Self-alignment seeks to reduce these expenses\nby enabling models to align themselves. To further lower costs and achieve\nalignment without any expensive tuning or annotations, we introduce a new\ntuning-free approach for self-alignment, Dynamic Rewarding with Prompt\nOptimization (DRPO). Our approach leverages a search-based optimization\nframework that allows LLMs to iteratively self-improve and craft the optimal\nalignment instructions, all without additional training or human intervention.\nThe core of DRPO is a dynamic rewarding mechanism, which identifies and\nrectifies model-specific alignment weaknesses, allowing LLMs to adapt\nefficiently to diverse alignment challenges. Empirical evaluations on eight\nrecent LLMs, both open- and closed-sourced, demonstrate that DRPO significantly\nenhances alignment performance, with base models outperforming their\nSFT/RLHF-tuned counterparts. Moreover, the prompts automatically optimized by\nDRPO surpass those curated by human experts, further validating the\neffectiveness of our approach. Our findings highlight the great potential of\ncurrent LLMs to achieve adaptive self-alignment through inference-time\noptimization, complementing tuning-based alignment methods."
                },
                "authors": [
                    {
                        "name": "Somanshu Singla"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Tianyang Liu"
                    },
                    {
                        "name": "Abdullah Ashfaq"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Eric P. Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric P. Xing"
                },
                "author": "Eric P. Xing",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09128v1",
                "updated": "2024-11-14T02:01:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    2,
                    1,
                    39,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T02:01:39Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    2,
                    1,
                    39,
                    3,
                    319,
                    0
                ],
                "title": "Performance Analysis of uRLLC in scalable Cell-free RAN System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of uRLLC in scalable Cell-free RAN System"
                },
                "summary": "As an essential part of mobile communication systems that beyond the fifth\ngeneration (B5G) and sixth generation (6G), ultra reliable low latency\ncommunication (uRLLC) places strict requirements on latency and reliability. In\nrecent years, with the improvement of mobile communication network performance,\ncentralized and distributed processing of cell-free mMIMO has been widely\nstudied, and wireless access networks (RAN) have also become a widely studied\ntopic in academia. This paper analyzes the performance of a novel scalable\ncell-free RAN (CF-RAN) architecture with multiple edge distributed units (EDUs)\nin the scenario of finite block length. The upper and lower bounds on its\nspectral efficiency (SE) performance are derived, and the complete set's\nformula and distributed processing can be used as their two exceptional cases,\nrespectively. Secondly, the paper further considers the distribution of users\nand large-scale fading models and studies the position distribution of remote\nradio units (RRUs). It is found that a uniform distribution of RRUs is\nbeneficial for improving the SE of finite block length under specific error\nrate performance, and RRUs need to be interwoven as much as possible under\nmultiple EDUs. This is different from traditional multi-node clustering\ncentralized collaborative processing. The paper compares the performance of\nMonte Carlo simulation and multi-RRU clustering group collaborative processing.\nAt the same time, this article verifies the accuracy of the space-time exchange\ntheory in the CF-RAN scenario. Through scalable EDU deployment, a trade-off\nbetween latency and reliability can be achieved in practical systems and\nexchanged with spatial degrees of freedom. This implementation can be seen as a\ndistributed and scalable implementation of the space-time exchange theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As an essential part of mobile communication systems that beyond the fifth\ngeneration (B5G) and sixth generation (6G), ultra reliable low latency\ncommunication (uRLLC) places strict requirements on latency and reliability. In\nrecent years, with the improvement of mobile communication network performance,\ncentralized and distributed processing of cell-free mMIMO has been widely\nstudied, and wireless access networks (RAN) have also become a widely studied\ntopic in academia. This paper analyzes the performance of a novel scalable\ncell-free RAN (CF-RAN) architecture with multiple edge distributed units (EDUs)\nin the scenario of finite block length. The upper and lower bounds on its\nspectral efficiency (SE) performance are derived, and the complete set's\nformula and distributed processing can be used as their two exceptional cases,\nrespectively. Secondly, the paper further considers the distribution of users\nand large-scale fading models and studies the position distribution of remote\nradio units (RRUs). It is found that a uniform distribution of RRUs is\nbeneficial for improving the SE of finite block length under specific error\nrate performance, and RRUs need to be interwoven as much as possible under\nmultiple EDUs. This is different from traditional multi-node clustering\ncentralized collaborative processing. The paper compares the performance of\nMonte Carlo simulation and multi-RRU clustering group collaborative processing.\nAt the same time, this article verifies the accuracy of the space-time exchange\ntheory in the CF-RAN scenario. Through scalable EDU deployment, a trade-off\nbetween latency and reliability can be achieved in practical systems and\nexchanged with spatial degrees of freedom. This implementation can be seen as a\ndistributed and scalable implementation of the space-time exchange theory."
                },
                "authors": [
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Dongming Wang"
                    },
                    {
                        "name": "Yunxiang Guo"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Xiaohu You"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu You"
                },
                "author": "Xiaohu You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09125v1",
                "updated": "2024-11-14T01:48:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    48,
                    8,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T01:48:08Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    48,
                    8,
                    3,
                    319,
                    0
                ],
                "title": "DROJ: A Prompt-Driven Attack against Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DROJ: A Prompt-Driven Attack against Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various natural language processing tasks. Due to their training on\ninternet-sourced datasets, LLMs can sometimes generate objectionable content,\nnecessitating extensive alignment with human feedback to avoid such outputs.\nDespite massive alignment efforts, LLMs remain susceptible to adversarial\njailbreak attacks, which usually are manipulated prompts designed to circumvent\nsafety mechanisms and elicit harmful responses. Here, we introduce a novel\napproach, Directed Rrepresentation Optimization Jailbreak (DROJ), which\noptimizes jailbreak prompts at the embedding level to shift the hidden\nrepresentations of harmful queries towards directions that are more likely to\nelicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat\nmodel show that DROJ achieves a 100\\% keyword-based Attack Success Rate (ASR),\neffectively preventing direct refusals. However, the model occasionally\nproduces repetitive and non-informative responses. To mitigate this, we\nintroduce a helpfulness system prompt that enhances the utility of the model's\nresponses. Our code is available at\nhttps://github.com/Leon-Leyang/LLM-Safeguard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various natural language processing tasks. Due to their training on\ninternet-sourced datasets, LLMs can sometimes generate objectionable content,\nnecessitating extensive alignment with human feedback to avoid such outputs.\nDespite massive alignment efforts, LLMs remain susceptible to adversarial\njailbreak attacks, which usually are manipulated prompts designed to circumvent\nsafety mechanisms and elicit harmful responses. Here, we introduce a novel\napproach, Directed Rrepresentation Optimization Jailbreak (DROJ), which\noptimizes jailbreak prompts at the embedding level to shift the hidden\nrepresentations of harmful queries towards directions that are more likely to\nelicit affirmative responses from the model. Our evaluations on LLaMA-2-7b-chat\nmodel show that DROJ achieves a 100\\% keyword-based Attack Success Rate (ASR),\neffectively preventing direct refusals. However, the model occasionally\nproduces repetitive and non-informative responses. To mitigate this, we\nintroduce a helpfulness system prompt that enhances the utility of the model's\nresponses. Our code is available at\nhttps://github.com/Leon-Leyang/LLM-Safeguard."
                },
                "authors": [
                    {
                        "name": "Leyang Hu"
                    },
                    {
                        "name": "Boran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Boran Wang"
                },
                "author": "Boran Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18373v2",
                "updated": "2024-11-14T01:41:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    41,
                    33,
                    3,
                    319,
                    0
                ],
                "published": "2024-04-29T02:23:53Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    2,
                    23,
                    53,
                    0,
                    120,
                    0
                ],
                "title": "6G comprehensive intelligence: network operations and optimization based\n  on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G comprehensive intelligence: network operations and optimization based\n  on Large Language Models"
                },
                "summary": "The sixth generation mobile communication standard (6G) can promote the\ndevelopment of Industrial Internet and Internet of Things (IoT). To achieve\ncomprehensive intelligent development of the network and provide customers with\nhigher quality personalized services. This paper proposes a network performance\noptimization and intelligent operation network architecture based on Large\nLanguage Model (LLM), aiming to build a comprehensive intelligent 6G network\nsystem. The Large Language Model, with more parameters and stronger learning\nability, can more accurately capture patterns and features in data, which can\nachieve more accurate content output and high intelligence and provide strong\nsupport for related research such as network data security, privacy protection,\nand health assessment. This paper also presents the design framework of a\nnetwork health assessment system based on LLM and focuses on its potential\napplication value, through the case of network health management system, it is\nfully demonstrated that the 6G intelligent network system based on LLM has\nimportant practical significance for the comprehensive realization of\nintelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sixth generation mobile communication standard (6G) can promote the\ndevelopment of Industrial Internet and Internet of Things (IoT). To achieve\ncomprehensive intelligent development of the network and provide customers with\nhigher quality personalized services. This paper proposes a network performance\noptimization and intelligent operation network architecture based on Large\nLanguage Model (LLM), aiming to build a comprehensive intelligent 6G network\nsystem. The Large Language Model, with more parameters and stronger learning\nability, can more accurately capture patterns and features in data, which can\nachieve more accurate content output and high intelligence and provide strong\nsupport for related research such as network data security, privacy protection,\nand health assessment. This paper also presents the design framework of a\nnetwork health assessment system based on LLM and focuses on its potential\napplication value, through the case of network health management system, it is\nfully demonstrated that the 6G intelligent network system based on LLM has\nimportant practical significance for the comprehensive realization of\nintelligence."
                },
                "authors": [
                    {
                        "name": "Sifan Long"
                    },
                    {
                        "name": "Fengxiao Tang"
                    },
                    {
                        "name": "Yangfan Li"
                    },
                    {
                        "name": "Tiao Tan"
                    },
                    {
                        "name": "Zhengjie Jin"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "arxiv_comment": "8 pages, 5 figures, 15 preferences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04997v2",
                "updated": "2024-11-14T01:36:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    36,
                    12,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-07T18:59:16Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    59,
                    16,
                    3,
                    312,
                    0
                ],
                "title": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation"
                },
                "summary": "CLIP is one of the most important multimodal foundational models today. What\npowers CLIP's capabilities? The rich supervision signals provided by natural\nlanguage, the carrier of human knowledge, shape a powerful cross-modal\nrepresentation space. However, with the rapid advancements in large language\nmodels LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and\ngeneration are continually being pushed. This raises an intriguing question:\ncan the capabilities of LLMs be harnessed to further improve multimodal\nrepresentation learning? The potential benefits of incorporating LLMs into CLIP\nare clear. LLMs' strong textual understanding can fundamentally improve CLIP's\nability to handle image captions, drastically enhancing its ability to process\nlong and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs\nare trained on a vast corpus of text, possessing open-world knowledge. This\nallows them to expand on caption information during training, increasing the\nefficiency of the learning process. In this paper, we propose LLM2CLIP, a novel\napproach that embraces the power of LLMs to unlock CLIP's potential. By\nfine-tuning the LLM in the caption space with contrastive learning, we extract\nits textual capabilities into the output embeddings, significantly improving\nthe output layer's textual discriminability. We then design an efficient\ntraining process where the fine-tuned LLM acts as a powerful teacher for CLIP's\nvisual encoder. Thanks to the LLM's presence, we can now incorporate longer and\nmore complex captions without being restricted by vanilla CLIP's text encoder's\ncontext window and ability limitations. Our experiments demonstrate that this\napproach brings substantial improvements in cross-modal tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP is one of the most important multimodal foundational models today. What\npowers CLIP's capabilities? The rich supervision signals provided by natural\nlanguage, the carrier of human knowledge, shape a powerful cross-modal\nrepresentation space. However, with the rapid advancements in large language\nmodels LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and\ngeneration are continually being pushed. This raises an intriguing question:\ncan the capabilities of LLMs be harnessed to further improve multimodal\nrepresentation learning? The potential benefits of incorporating LLMs into CLIP\nare clear. LLMs' strong textual understanding can fundamentally improve CLIP's\nability to handle image captions, drastically enhancing its ability to process\nlong and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs\nare trained on a vast corpus of text, possessing open-world knowledge. This\nallows them to expand on caption information during training, increasing the\nefficiency of the learning process. In this paper, we propose LLM2CLIP, a novel\napproach that embraces the power of LLMs to unlock CLIP's potential. By\nfine-tuning the LLM in the caption space with contrastive learning, we extract\nits textual capabilities into the output embeddings, significantly improving\nthe output layer's textual discriminability. We then design an efficient\ntraining process where the fine-tuned LLM acts as a powerful teacher for CLIP's\nvisual encoder. Thanks to the LLM's presence, we can now incorporate longer and\nmore complex captions without being restricted by vanilla CLIP's text encoder's\ncontext window and ability limitations. Our experiments demonstrate that this\napproach brings substantial improvements in cross-modal tasks."
                },
                "authors": [
                    {
                        "name": "Weiquan Huang"
                    },
                    {
                        "name": "Aoqi Wu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Liang Hu"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Xiyang Dai"
                    },
                    {
                        "name": "Dongdong Chen"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09116v1",
                "updated": "2024-11-14T01:29:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    29,
                    36,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T01:29:36Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    29,
                    36,
                    3,
                    319,
                    0
                ],
                "title": "P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent\n  Evaluation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent\n  Evaluation of LLMs"
                },
                "summary": "Recent advancements in large language models (LLMs) showcase varied\nmultilingual capabilities across tasks like translation, code generation, and\nreasoning. Previous assessments often limited their scope to fundamental\nnatural language processing (NLP) or isolated capability-specific tasks. To\nalleviate this drawback, we aim to present a comprehensive multilingual\nmultitask benchmark. First, we present a pipeline for selecting available and\nreasonable benchmarks from massive ones, addressing the oversight in previous\nwork regarding the utility of these benchmarks, i.e., their ability to\ndifferentiate between models being evaluated. Leveraging this pipeline, we\nintroduce P-MMEval, a large-scale benchmark covering effective fundamental and\ncapability-specialized datasets. Furthermore, P-MMEval delivers consistent\nlanguage coverage across various datasets and provides parallel samples.\nFinally, we conduct extensive experiments on representative multilingual model\nseries to compare performances across models, analyze dataset effectiveness,\nexamine prompt impacts on model performances, and explore the relationship\nbetween multilingual performances and factors such as tasks, model sizes, and\nlanguages. These insights offer valuable guidance for future research. The\ndataset is available at https://huggingface.co/datasets/Qwen/P-MMEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) showcase varied\nmultilingual capabilities across tasks like translation, code generation, and\nreasoning. Previous assessments often limited their scope to fundamental\nnatural language processing (NLP) or isolated capability-specific tasks. To\nalleviate this drawback, we aim to present a comprehensive multilingual\nmultitask benchmark. First, we present a pipeline for selecting available and\nreasonable benchmarks from massive ones, addressing the oversight in previous\nwork regarding the utility of these benchmarks, i.e., their ability to\ndifferentiate between models being evaluated. Leveraging this pipeline, we\nintroduce P-MMEval, a large-scale benchmark covering effective fundamental and\ncapability-specialized datasets. Furthermore, P-MMEval delivers consistent\nlanguage coverage across various datasets and provides parallel samples.\nFinally, we conduct extensive experiments on representative multilingual model\nseries to compare performances across models, analyze dataset effectiveness,\nexamine prompt impacts on model performances, and explore the relationship\nbetween multilingual performances and factors such as tasks, model sizes, and\nlanguages. These insights offer valuable guidance for future research. The\ndataset is available at https://huggingface.co/datasets/Qwen/P-MMEval."
                },
                "authors": [
                    {
                        "name": "Yidan Zhang"
                    },
                    {
                        "name": "Boyi Deng"
                    },
                    {
                        "name": "Yu Wan"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07870v2",
                "updated": "2024-11-13T23:02:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    23,
                    2,
                    41,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-12T15:26:17Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    15,
                    26,
                    17,
                    1,
                    317,
                    0
                ],
                "title": "Trustful LLMs: Customizing and Grounding Text Generation with Knowledge\n  Bases and Dual Decoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustful LLMs: Customizing and Grounding Text Generation with Knowledge\n  Bases and Dual Decoders"
                },
                "summary": "Although people are impressed by the content generation skills of large\nlanguage models, the use of LLMs, such as ChatGPT, is limited by the domain\ngrounding of the content. The correctness and groundedness of the generated\ncontent need to be based on a verified context, such as results from\nRetrieval-Augmented Generation (RAG). One important issue when adapting LLMs to\na customized domain is that the generated responses are often incomplete, or\nthe additions are not verified and may even be hallucinated. Prior studies on\nhallucination detection have focused on evaluation metrics, which are not\neasily adaptable to dynamic domains and can be vulnerable to attacks like\njail-breaking. In this work, we propose 1) a post-processing algorithm that\nleverages knowledge triplets in RAG context to correct hallucinations and 2) a\ndual-decoder model that fuses RAG context to guide the generation process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although people are impressed by the content generation skills of large\nlanguage models, the use of LLMs, such as ChatGPT, is limited by the domain\ngrounding of the content. The correctness and groundedness of the generated\ncontent need to be based on a verified context, such as results from\nRetrieval-Augmented Generation (RAG). One important issue when adapting LLMs to\na customized domain is that the generated responses are often incomplete, or\nthe additions are not verified and may even be hallucinated. Prior studies on\nhallucination detection have focused on evaluation metrics, which are not\neasily adaptable to dynamic domains and can be vulnerable to attacks like\njail-breaking. In this work, we propose 1) a post-processing algorithm that\nleverages knowledge triplets in RAG context to correct hallucinations and 2) a\ndual-decoder model that fuses RAG context to guide the generation process."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Zhu"
                    },
                    {
                        "name": "Jaya Krishna Mandivarapu"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Krishna Mandivarapu"
                },
                "author": "Jaya Krishna Mandivarapu",
                "arxiv_journal_ref": "EMNLP CustomNLP4U 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09073v1",
                "updated": "2024-11-13T22:56:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    22,
                    56,
                    0,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T22:56:00Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    22,
                    56,
                    0,
                    2,
                    318,
                    0
                ],
                "title": "Code-mixed LLM: Improve Large Language Models' Capability to Handle\n  Code-Mixing through Reinforcement Learning from AI Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-mixed LLM: Improve Large Language Models' Capability to Handle\n  Code-Mixing through Reinforcement Learning from AI Feedback"
                },
                "summary": "Code-mixing(CM) or code-switching(CSW) refers to the juxtaposition of\nlinguistic units from two or more languages during the conversation or\nsometimes even a single utterance. Code-mixing introduces unique challenges in\ndaily life, such as syntactic mismatches and semantic blending, that are rarely\nencountered in monolingual settings. Large language models (LLMs) have\nrevolutionized the field of natural language processing (NLP) by offering\nunprecedented capabilities in understanding human languages. However, the\neffectiveness of current state-of-the-art multilingual LLMs has not yet been\nfully explored in the CM scenario. To fill this gap, we first benchmark the\nperformance of multilingual LLMs on various code-mixing NLP tasks. Then we\npropose to improve the multilingual LLMs' ability to understand code-mixing\nthrough reinforcement learning from human feedback (RLHF) and code-mixed\nmachine translation tasks. Given the high-cost and time-consuming preference\nlabeling procedure, we improve this by utilizing LLMs as annotators to perform\nthe reinforcement learning from AI feedback (RLAIF). The experiments show the\neffectiveness of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-mixing(CM) or code-switching(CSW) refers to the juxtaposition of\nlinguistic units from two or more languages during the conversation or\nsometimes even a single utterance. Code-mixing introduces unique challenges in\ndaily life, such as syntactic mismatches and semantic blending, that are rarely\nencountered in monolingual settings. Large language models (LLMs) have\nrevolutionized the field of natural language processing (NLP) by offering\nunprecedented capabilities in understanding human languages. However, the\neffectiveness of current state-of-the-art multilingual LLMs has not yet been\nfully explored in the CM scenario. To fill this gap, we first benchmark the\nperformance of multilingual LLMs on various code-mixing NLP tasks. Then we\npropose to improve the multilingual LLMs' ability to understand code-mixing\nthrough reinforcement learning from human feedback (RLHF) and code-mixed\nmachine translation tasks. Given the high-cost and time-consuming preference\nlabeling procedure, we improve this by utilizing LLMs as annotators to perform\nthe reinforcement learning from AI feedback (RLAIF). The experiments show the\neffectiveness of the proposed method."
                },
                "authors": [
                    {
                        "name": "Wenbo Zhang"
                    },
                    {
                        "name": "Aditya Majumdar"
                    },
                    {
                        "name": "Amulya Yadav"
                    }
                ],
                "author_detail": {
                    "name": "Amulya Yadav"
                },
                "author": "Amulya Yadav",
                "arxiv_comment": "initial version: 5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09072v1",
                "updated": "2024-11-13T22:55:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    22,
                    55,
                    45,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T22:55:45Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    22,
                    55,
                    45,
                    2,
                    318,
                    0
                ],
                "title": "Continuous GNN-based Anomaly Detection on Edge using Efficient Adaptive\n  Knowledge Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous GNN-based Anomaly Detection on Edge using Efficient Adaptive\n  Knowledge Graph Learning"
                },
                "summary": "The increasing demand for robust security solutions across various industries\nhas made Video Anomaly Detection (VAD) a critical task in applications such as\nintelligent surveillance, evidence investigation, and violence detection.\nTraditional approaches to VAD often rely on finetuning large pre-trained\nmodels, which can be computationally expensive and impractical for real-time or\nresource-constrained environments. To address this, MissionGNN introduced a\nmore efficient method by training a graph neural network (GNN) using a fixed\nknowledge graph (KG) derived from large language models (LLMs) like GPT-4.\nWhile this approach demonstrated significant efficiency in computational power\nand memory, it faces limitations in dynamic environments where frequent updates\nto the KG are necessary due to evolving behavior trends and shifting data\npatterns. These updates typically require cloud-based computation, posing\nchallenges for edge computing applications. In this paper, we propose a novel\nframework that facilitates continuous KG adaptation directly on edge devices,\novercoming the limitations of cloud dependency. Our method dynamically modifies\nthe KG through a three-phase process: pruning, alternating, and creating nodes,\nenabling real-time adaptation to changing data trends. This continuous learning\napproach enhances the robustness of anomaly detection models, making them more\nsuitable for deployment in dynamic and resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for robust security solutions across various industries\nhas made Video Anomaly Detection (VAD) a critical task in applications such as\nintelligent surveillance, evidence investigation, and violence detection.\nTraditional approaches to VAD often rely on finetuning large pre-trained\nmodels, which can be computationally expensive and impractical for real-time or\nresource-constrained environments. To address this, MissionGNN introduced a\nmore efficient method by training a graph neural network (GNN) using a fixed\nknowledge graph (KG) derived from large language models (LLMs) like GPT-4.\nWhile this approach demonstrated significant efficiency in computational power\nand memory, it faces limitations in dynamic environments where frequent updates\nto the KG are necessary due to evolving behavior trends and shifting data\npatterns. These updates typically require cloud-based computation, posing\nchallenges for edge computing applications. In this paper, we propose a novel\nframework that facilitates continuous KG adaptation directly on edge devices,\novercoming the limitations of cloud dependency. Our method dynamically modifies\nthe KG through a three-phase process: pruning, alternating, and creating nodes,\nenabling real-time adaptation to changing data trends. This continuous learning\napproach enhances the robustness of anomaly detection models, making them more\nsuitable for deployment in dynamic and resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Sanggeon Yun"
                    },
                    {
                        "name": "Ryozo Masukawa"
                    },
                    {
                        "name": "William Youngwoo Chung"
                    },
                    {
                        "name": "Minhyoung Na"
                    },
                    {
                        "name": "Nathaniel Bastian"
                    },
                    {
                        "name": "Mohsen Imani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Imani"
                },
                "author": "Mohsen Imani",
                "arxiv_comment": "Accepted to DATE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09055v1",
                "updated": "2024-11-13T22:28:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    22,
                    28,
                    5,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T22:28:05Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    22,
                    28,
                    5,
                    2,
                    318,
                    0
                ],
                "title": "SAFELOC: Overcoming Data Poisoning Attacks in Heterogeneous Federated\n  Machine Learning for Indoor Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFELOC: Overcoming Data Poisoning Attacks in Heterogeneous Federated\n  Machine Learning for Indoor Localization"
                },
                "summary": "Machine learning (ML) based indoor localization solutions are critical for\nmany emerging applications, yet their efficacy is often compromised by\nhardware/software variations across mobile devices (i.e., device heterogeneity)\nand the threat of ML data poisoning attacks. Conventional methods aimed at\ncountering these challenges show limited resilience to the uncertainties\ncreated by these phenomena. In response, in this paper, we introduce SAFELOC, a\nnovel framework that not only minimizes localization errors under these\nchallenging conditions but also ensures model compactness for efficient mobile\ndevice deployment. Our framework targets a distributed and co-operative\nlearning environment that uses federated learning (FL) to preserve user data\nprivacy and assumes heterogeneous mobile devices carried by users (just like in\nmost real-world scenarios). Within this heterogeneous FL context, SAFELOC\nintroduces a novel fused neural network architecture that performs data\npoisoning detection and localization, with a low model footprint. Additionally,\na dynamic saliency map-based aggregation strategy is designed to adapt based on\nthe severity of the detected data poisoning scenario. Experimental evaluations\ndemonstrate that SAFELOC achieves improvements of up to 5.9x in mean\nlocalization error, 7.8x in worst-case localization error, and a 2.1x reduction\nin model inference latency compared to state-of-the-art indoor localization\nframeworks, across diverse building floorplans, mobile devices, and ML data\npoisoning attack scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) based indoor localization solutions are critical for\nmany emerging applications, yet their efficacy is often compromised by\nhardware/software variations across mobile devices (i.e., device heterogeneity)\nand the threat of ML data poisoning attacks. Conventional methods aimed at\ncountering these challenges show limited resilience to the uncertainties\ncreated by these phenomena. In response, in this paper, we introduce SAFELOC, a\nnovel framework that not only minimizes localization errors under these\nchallenging conditions but also ensures model compactness for efficient mobile\ndevice deployment. Our framework targets a distributed and co-operative\nlearning environment that uses federated learning (FL) to preserve user data\nprivacy and assumes heterogeneous mobile devices carried by users (just like in\nmost real-world scenarios). Within this heterogeneous FL context, SAFELOC\nintroduces a novel fused neural network architecture that performs data\npoisoning detection and localization, with a low model footprint. Additionally,\na dynamic saliency map-based aggregation strategy is designed to adapt based on\nthe severity of the detected data poisoning scenario. Experimental evaluations\ndemonstrate that SAFELOC achieves improvements of up to 5.9x in mean\nlocalization error, 7.8x in worst-case localization error, and a 2.1x reduction\nin model inference latency compared to state-of-the-art indoor localization\nframeworks, across diverse building floorplans, mobile devices, and ML data\npoisoning attack scenarios."
                },
                "authors": [
                    {
                        "name": "Akhil Singampalli"
                    },
                    {
                        "name": "Danish Gufran"
                    },
                    {
                        "name": "Sudeep Pasricha"
                    }
                ],
                "author_detail": {
                    "name": "Sudeep Pasricha"
                },
                "author": "Sudeep Pasricha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09050v1",
                "updated": "2024-11-13T22:10:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    22,
                    10,
                    7,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T22:10:07Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    22,
                    10,
                    7,
                    2,
                    318,
                    0
                ],
                "title": "The Systems Engineering Approach in Times of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Systems Engineering Approach in Times of Large Language Models"
                },
                "summary": "Using Large Language Models (LLMs) to address critical societal problems\nrequires adopting this novel technology into socio-technical systems. However,\nthe complexity of such systems and the nature of LLMs challenge such a vision.\nIt is unlikely that the solution to such challenges will come from the\nArtificial Intelligence (AI) community itself. Instead, the Systems Engineering\napproach is better equipped to facilitate the adoption of LLMs by prioritising\nthe problems and their context before any other aspects. This paper introduces\nthe challenges LLMs generate and surveys systems research efforts for\nengineering AI-based systems. We reveal how the systems engineering principles\nhave supported addressing similar issues to the ones LLMs pose and discuss our\nfindings to provide future directions for adopting LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models (LLMs) to address critical societal problems\nrequires adopting this novel technology into socio-technical systems. However,\nthe complexity of such systems and the nature of LLMs challenge such a vision.\nIt is unlikely that the solution to such challenges will come from the\nArtificial Intelligence (AI) community itself. Instead, the Systems Engineering\napproach is better equipped to facilitate the adoption of LLMs by prioritising\nthe problems and their context before any other aspects. This paper introduces\nthe challenges LLMs generate and surveys systems research efforts for\nengineering AI-based systems. We reveal how the systems engineering principles\nhave supported addressing similar issues to the ones LLMs pose and discuss our\nfindings to provide future directions for adopting LLMs."
                },
                "authors": [
                    {
                        "name": "Christian Cabrera"
                    },
                    {
                        "name": "Viviana Bastidas"
                    },
                    {
                        "name": "Jennifer Schooling"
                    },
                    {
                        "name": "Neil D. Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Neil D. Lawrence"
                },
                "author": "Neil D. Lawrence",
                "arxiv_comment": "This paper has been accepted for the upcoming 58th Hawaii\n  International Conference on System Sciences (HICSS-58)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09022v1",
                "updated": "2024-11-13T20:59:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    20,
                    59,
                    30,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T20:59:30Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    20,
                    59,
                    30,
                    2,
                    318,
                    0
                ],
                "title": "DART-LLM: Dependency-Aware Multi-Robot Task Decomposition and Execution\n  using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DART-LLM: Dependency-Aware Multi-Robot Task Decomposition and Execution\n  using Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant reasoning\ncapabilities in robotic systems. However, their deployment in multi-robot\nsystems remains fragmented and struggles to handle complex task dependencies\nand parallel execution. This study introduces the DART-LLM (Dependency-Aware\nMulti-Robot Task Decomposition and Execution using Large Language Models)\nsystem, designed to address these challenges. DART-LLM utilizes LLMs to parse\nnatural language instructions, decomposing them into multiple subtasks with\ndependencies to establish complex task sequences, thereby enhancing efficient\ncoordination and parallel execution in multi-robot systems. The system includes\nthe QA LLM module, Breakdown Function modules, Actuation module, and a\nVision-Language Model (VLM)-based object detection module, enabling task\ndecomposition and execution from natural language instructions to robotic\nactions. Experimental results demonstrate that DART-LLM excels in handling\nlong-horizon tasks and collaborative tasks with complex dependencies. Even when\nusing smaller models like Llama 3.1 8B, the system achieves good performance,\nhighlighting DART-LLM's robustness in terms of model size. Please refer to the\nproject website \\url{https://wyd0817.github.io/project-dart-llm/} for videos\nand code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant reasoning\ncapabilities in robotic systems. However, their deployment in multi-robot\nsystems remains fragmented and struggles to handle complex task dependencies\nand parallel execution. This study introduces the DART-LLM (Dependency-Aware\nMulti-Robot Task Decomposition and Execution using Large Language Models)\nsystem, designed to address these challenges. DART-LLM utilizes LLMs to parse\nnatural language instructions, decomposing them into multiple subtasks with\ndependencies to establish complex task sequences, thereby enhancing efficient\ncoordination and parallel execution in multi-robot systems. The system includes\nthe QA LLM module, Breakdown Function modules, Actuation module, and a\nVision-Language Model (VLM)-based object detection module, enabling task\ndecomposition and execution from natural language instructions to robotic\nactions. Experimental results demonstrate that DART-LLM excels in handling\nlong-horizon tasks and collaborative tasks with complex dependencies. Even when\nusing smaller models like Llama 3.1 8B, the system achieves good performance,\nhighlighting DART-LLM's robustness in terms of model size. Please refer to the\nproject website \\url{https://wyd0817.github.io/project-dart-llm/} for videos\nand code."
                },
                "authors": [
                    {
                        "name": "Yongdong Wang"
                    },
                    {
                        "name": "Runze Xiao"
                    },
                    {
                        "name": "Jun Younes Louhi Kasahara"
                    },
                    {
                        "name": "Ryosuke Yajima"
                    },
                    {
                        "name": "Keiji Nagatani"
                    },
                    {
                        "name": "Atsushi Yamashita"
                    },
                    {
                        "name": "Hajime Asama"
                    }
                ],
                "author_detail": {
                    "name": "Hajime Asama"
                },
                "author": "Hajime Asama",
                "arxiv_comment": "Submitted to the 2025 IEEE International Conference on Robotics &\n  Automation on September 15, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09009v1",
                "updated": "2024-11-13T20:30:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    20,
                    30,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T20:30:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    20,
                    30,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Cut Your Losses in Large-Vocabulary Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cut Your Losses in Large-Vocabulary Language Models"
                },
                "summary": "As language models grow ever larger, so do their vocabularies. This has\nshifted the memory footprint of LLMs during training disproportionately to one\nsingle layer: the cross-entropy in the loss computation. Cross-entropy builds\nup a logit matrix with entries for each pair of input tokens and vocabulary\nitems and, for small models, consumes an order of magnitude more memory than\nthe rest of the LLM combined. We propose Cut Cross-Entropy (CCE), a method that\ncomputes the cross-entropy loss without materializing the logits for all tokens\ninto global memory. Rather, CCE only computes the logit for the correct token\nand evaluates the log-sum-exp over all logits on the fly. We implement a custom\nkernel that performs the matrix multiplications and the log-sum-exp reduction\nover the vocabulary in flash memory, making global memory consumption for the\ncross-entropy computation negligible. This has a dramatic effect. Taking the\nGemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss\ncomputation from 24 GB to 1 MB, and the total training-time memory consumption\nof the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, we\nleverage the inherent sparsity of softmax and propose to skip elements of the\ngradient computation that have a negligible (i.e., below numerical precision)\ncontribution to the gradient. Experiments demonstrate that the dramatic\nreduction in memory consumption is accomplished without sacrificing training\nspeed or convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As language models grow ever larger, so do their vocabularies. This has\nshifted the memory footprint of LLMs during training disproportionately to one\nsingle layer: the cross-entropy in the loss computation. Cross-entropy builds\nup a logit matrix with entries for each pair of input tokens and vocabulary\nitems and, for small models, consumes an order of magnitude more memory than\nthe rest of the LLM combined. We propose Cut Cross-Entropy (CCE), a method that\ncomputes the cross-entropy loss without materializing the logits for all tokens\ninto global memory. Rather, CCE only computes the logit for the correct token\nand evaluates the log-sum-exp over all logits on the fly. We implement a custom\nkernel that performs the matrix multiplications and the log-sum-exp reduction\nover the vocabulary in flash memory, making global memory consumption for the\ncross-entropy computation negligible. This has a dramatic effect. Taking the\nGemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss\ncomputation from 24 GB to 1 MB, and the total training-time memory consumption\nof the classifier head from 28 GB to 1 GB. To improve the throughput of CCE, we\nleverage the inherent sparsity of softmax and propose to skip elements of the\ngradient computation that have a negligible (i.e., below numerical precision)\ncontribution to the gradient. Experiments demonstrate that the dramatic\nreduction in memory consumption is accomplished without sacrificing training\nspeed or convergence."
                },
                "authors": [
                    {
                        "name": "Erik Wijmans"
                    },
                    {
                        "name": "Brody Huval"
                    },
                    {
                        "name": "Alexander Hertzberg"
                    },
                    {
                        "name": "Vladlen Koltun"
                    },
                    {
                        "name": "Philipp Krähenbühl"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Krähenbühl"
                },
                "author": "Philipp Krähenbühl",
                "arxiv_comment": "Code is available at https://github.com/apple/ml-cross-entropy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03230v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03230v4",
                "updated": "2024-11-13T20:18:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    20,
                    18,
                    19,
                    2,
                    318,
                    0
                ],
                "published": "2024-06-05T13:06:33Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    13,
                    6,
                    33,
                    2,
                    157,
                    0
                ],
                "title": "Defending Large Language Models Against Attacks With Residual Stream\n  Activation Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending Large Language Models Against Attacks With Residual Stream\n  Activation Analysis"
                },
                "summary": "The widespread adoption of Large Language Models (LLMs), exemplified by\nOpenAI's ChatGPT, brings to the forefront the imperative to defend against\nadversarial threats on these models. These attacks, which manipulate an LLM's\noutput by introducing malicious inputs, undermine the model's integrity and the\ntrust users place in its outputs. In response to this challenge, our paper\npresents an innovative defensive strategy, given white box access to an LLM,\nthat harnesses residual activation analysis between transformer layers of the\nLLM. We apply a novel methodology for analyzing distinctive activation patterns\nin the residual streams for attack prompt classification. We curate multiple\ndatasets to demonstrate how this method of classification has high accuracy\nacross multiple types of attack scenarios, including our newly-created attack\ndataset. Furthermore, we enhance the model's resilience by integrating safety\nfine-tuning techniques for LLMs in order to measure its effect on our\ncapability to detect attacks. The results underscore the effectiveness of our\napproach in enhancing the detection and mitigation of adversarial inputs,\nadvancing the security framework within which LLMs operate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Large Language Models (LLMs), exemplified by\nOpenAI's ChatGPT, brings to the forefront the imperative to defend against\nadversarial threats on these models. These attacks, which manipulate an LLM's\noutput by introducing malicious inputs, undermine the model's integrity and the\ntrust users place in its outputs. In response to this challenge, our paper\npresents an innovative defensive strategy, given white box access to an LLM,\nthat harnesses residual activation analysis between transformer layers of the\nLLM. We apply a novel methodology for analyzing distinctive activation patterns\nin the residual streams for attack prompt classification. We curate multiple\ndatasets to demonstrate how this method of classification has high accuracy\nacross multiple types of attack scenarios, including our newly-created attack\ndataset. Furthermore, we enhance the model's resilience by integrating safety\nfine-tuning techniques for LLMs in order to measure its effect on our\ncapability to detect attacks. The results underscore the effectiveness of our\napproach in enhancing the detection and mitigation of adversarial inputs,\nadvancing the security framework within which LLMs operate."
                },
                "authors": [
                    {
                        "name": "Amelia Kawasaki"
                    },
                    {
                        "name": "Andrew Davis"
                    },
                    {
                        "name": "Houssam Abbas"
                    }
                ],
                "author_detail": {
                    "name": "Houssam Abbas"
                },
                "author": "Houssam Abbas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03230v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03230v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09003v1",
                "updated": "2024-11-13T20:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    20,
                    12,
                    55,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T20:12:55Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    20,
                    12,
                    55,
                    2,
                    318,
                    0
                ],
                "title": "Refusal in LLMs is an Affine Function",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal in LLMs is an Affine Function"
                },
                "summary": "We propose affine concept editing (ACE) as an approach for steering language\nmodels' behavior by intervening directly in activations. We begin with an\naffine decomposition of model activation vectors and show that prior methods\nfor steering model behavior correspond to subsets of terms of this\ndecomposition. We then provide a derivation of ACE and test it on refusal using\nLlama 3 8B and Hermes Eagle RWKV v5. ACE ultimately combines affine subspace\nprojection and activation addition to reliably control the model's refusal\nresponses across prompt types. We evaluate the results using LLM-based scoring\non a collection of harmful and harmless prompts. Our experiments demonstrate\nthat ACE consistently achieves more precise control over model behavior and\ngeneralizes to models where directional ablation via affine subspace projection\nalone produces incoherent outputs. Code for reproducing our results is\navailable at https://github.com/EleutherAI/steering-llama3 .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose affine concept editing (ACE) as an approach for steering language\nmodels' behavior by intervening directly in activations. We begin with an\naffine decomposition of model activation vectors and show that prior methods\nfor steering model behavior correspond to subsets of terms of this\ndecomposition. We then provide a derivation of ACE and test it on refusal using\nLlama 3 8B and Hermes Eagle RWKV v5. ACE ultimately combines affine subspace\nprojection and activation addition to reliably control the model's refusal\nresponses across prompt types. We evaluate the results using LLM-based scoring\non a collection of harmful and harmless prompts. Our experiments demonstrate\nthat ACE consistently achieves more precise control over model behavior and\ngeneralizes to models where directional ablation via affine subspace projection\nalone produces incoherent outputs. Code for reproducing our results is\navailable at https://github.com/EleutherAI/steering-llama3 ."
                },
                "authors": [
                    {
                        "name": "Thomas Marshall"
                    },
                    {
                        "name": "Adam Scherlis"
                    },
                    {
                        "name": "Nora Belrose"
                    }
                ],
                "author_detail": {
                    "name": "Nora Belrose"
                },
                "author": "Nora Belrose",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23463v2",
                "updated": "2024-11-13T19:34:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    19,
                    34,
                    22,
                    2,
                    318,
                    0
                ],
                "published": "2024-10-30T21:08:07Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    8,
                    7,
                    2,
                    304,
                    0
                ],
                "title": "MDCure: A Scalable Pipeline for Multi-Document Instruction-Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDCure: A Scalable Pipeline for Multi-Document Instruction-Following"
                },
                "summary": "Multi-document (MD) processing is crucial for LLMs to handle real-world tasks\nsuch as summarization and question-answering across large sets of documents.\nWhile LLMs have improved at processing long inputs, MD contexts still present\nchallenges, such as managing inter-document dependencies, redundancy, and\nincoherent structures. We introduce MDCure, a scalable and effective\nfine-tuning pipeline to enhance the MD capabilities of LLMs without the\ncomputational cost of pre-training or reliance on human annotated data. MDCure\nis based on generation of high-quality synthetic MD instruction data from sets\nof related articles via targeted prompts. We further introduce MDCureRM, a\nmulti-objective reward model which filters generated data based on their\ntraining utility for MD settings. With MDCure, we fine-tune a variety of LLMs,\nfrom the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in\nsize. Extensive evaluations on a wide range of MD and long-context benchmarks\nspanning various tasks show MDCure consistently improves performance over\npre-trained baselines and over corresponding base models by up to 75.5%. Our\ncode, datasets, and models are available at https://github.com/yale-nlp/MDCure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-document (MD) processing is crucial for LLMs to handle real-world tasks\nsuch as summarization and question-answering across large sets of documents.\nWhile LLMs have improved at processing long inputs, MD contexts still present\nchallenges, such as managing inter-document dependencies, redundancy, and\nincoherent structures. We introduce MDCure, a scalable and effective\nfine-tuning pipeline to enhance the MD capabilities of LLMs without the\ncomputational cost of pre-training or reliance on human annotated data. MDCure\nis based on generation of high-quality synthetic MD instruction data from sets\nof related articles via targeted prompts. We further introduce MDCureRM, a\nmulti-objective reward model which filters generated data based on their\ntraining utility for MD settings. With MDCure, we fine-tune a variety of LLMs,\nfrom the FlanT5, Qwen2, and LLAMA3.1 model families, up to 70B parameters in\nsize. Extensive evaluations on a wide range of MD and long-context benchmarks\nspanning various tasks show MDCure consistently improves performance over\npre-trained baselines and over corresponding base models by up to 75.5%. Our\ncode, datasets, and models are available at https://github.com/yale-nlp/MDCure."
                },
                "authors": [
                    {
                        "name": "Gabrielle Kaili-May Liu"
                    },
                    {
                        "name": "Bowen Shi"
                    },
                    {
                        "name": "Avi Caciularu"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08979v1",
                "updated": "2024-11-13T19:12:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    19,
                    12,
                    2,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T19:12:02Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    19,
                    12,
                    2,
                    2,
                    318,
                    0
                ],
                "title": "CoCoP: Enhancing Text Classification with LLM through Code Completion\n  Prompt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCoP: Enhancing Text Classification with LLM through Code Completion\n  Prompt"
                },
                "summary": "Text classification is a fundamental task in natural language processing\n(NLP), and large language models (LLMs) have demonstrated their capability to\nperform this task across various domains. However, the performance of LLMs\nheavily depends on the quality of their input prompts. Recent studies have also\nshown that LLMs exhibit remarkable results in code-related tasks. To leverage\nthe capabilities of LLMs in text classification, we propose the Code Completion\nPrompt (CoCoP) method, which transforms the text classification problem into a\ncode completion task. CoCoP significantly improves text classification\nperformance across diverse datasets by utilizing LLMs' code-completion\ncapability. For instance, CoCoP enhances the accuracy of the SST2 dataset by\nmore than 20%. Moreover, when CoCoP integrated with LLMs specifically designed\nfor code-related tasks (code models), such as CodeLLaMA, this method\ndemonstrates better or comparable performance to few-shot learning techniques\nwhile using only one-tenth of the model size. The source code of our proposed\nmethod will be available to the public upon the acceptance of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text classification is a fundamental task in natural language processing\n(NLP), and large language models (LLMs) have demonstrated their capability to\nperform this task across various domains. However, the performance of LLMs\nheavily depends on the quality of their input prompts. Recent studies have also\nshown that LLMs exhibit remarkable results in code-related tasks. To leverage\nthe capabilities of LLMs in text classification, we propose the Code Completion\nPrompt (CoCoP) method, which transforms the text classification problem into a\ncode completion task. CoCoP significantly improves text classification\nperformance across diverse datasets by utilizing LLMs' code-completion\ncapability. For instance, CoCoP enhances the accuracy of the SST2 dataset by\nmore than 20%. Moreover, when CoCoP integrated with LLMs specifically designed\nfor code-related tasks (code models), such as CodeLLaMA, this method\ndemonstrates better or comparable performance to few-shot learning techniques\nwhile using only one-tenth of the model size. The source code of our proposed\nmethod will be available to the public upon the acceptance of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad Mahdi Mohajeri"
                    },
                    {
                        "name": "Mohammad Javad Dousti"
                    },
                    {
                        "name": "Majid Nili Ahmadabadi"
                    }
                ],
                "author_detail": {
                    "name": "Majid Nili Ahmadabadi"
                },
                "author": "Majid Nili Ahmadabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08977v1",
                "updated": "2024-11-13T19:08:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    19,
                    8,
                    23,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T19:08:23Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    19,
                    8,
                    23,
                    2,
                    318,
                    0
                ],
                "title": "Robustness and Confounders in the Demographic Alignment of LLMs with\n  Human Perceptions of Offensiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness and Confounders in the Demographic Alignment of LLMs with\n  Human Perceptions of Offensiveness"
                },
                "summary": "Large language models (LLMs) are known to exhibit demographic biases, yet few\nstudies systematically evaluate these biases across multiple datasets or\naccount for confounding factors. In this work, we examine LLM alignment with\nhuman annotations in five offensive language datasets, comprising approximately\n220K annotations. Our findings reveal that while demographic traits,\nparticularly race, influence alignment, these effects are inconsistent across\ndatasets and often entangled with other factors. Confounders -- such as\ndocument difficulty, annotator sensitivity, and within-group agreement --\naccount for more variation in alignment patterns than demographic traits alone.\nSpecifically, alignment increases with higher annotator sensitivity and group\nagreement, while greater document difficulty corresponds to reduced alignment.\nOur results underscore the importance of multi-dataset analyses and\nconfounder-aware methodologies in developing robust measures of demographic\nbias in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to exhibit demographic biases, yet few\nstudies systematically evaluate these biases across multiple datasets or\naccount for confounding factors. In this work, we examine LLM alignment with\nhuman annotations in five offensive language datasets, comprising approximately\n220K annotations. Our findings reveal that while demographic traits,\nparticularly race, influence alignment, these effects are inconsistent across\ndatasets and often entangled with other factors. Confounders -- such as\ndocument difficulty, annotator sensitivity, and within-group agreement --\naccount for more variation in alignment patterns than demographic traits alone.\nSpecifically, alignment increases with higher annotator sensitivity and group\nagreement, while greater document difficulty corresponds to reduced alignment.\nOur results underscore the importance of multi-dataset analyses and\nconfounder-aware methodologies in developing robust measures of demographic\nbias in LLMs."
                },
                "authors": [
                    {
                        "name": "Shayan Alipour"
                    },
                    {
                        "name": "Indira Sen"
                    },
                    {
                        "name": "Mattia Samory"
                    },
                    {
                        "name": "Tanushree Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tanushree Mitra"
                },
                "author": "Tanushree Mitra",
                "arxiv_comment": "18 pages, 8 figures, ACL'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08968v1",
                "updated": "2024-11-13T19:02:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    19,
                    2,
                    36,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T19:02:36Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    19,
                    2,
                    36,
                    2,
                    318,
                    0
                ],
                "title": "Sparse Upcycling: Inference Inefficient Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Upcycling: Inference Inefficient Finetuning"
                },
                "summary": "Small, highly trained, open-source large language models are widely used due\nto their inference efficiency, but further improving their quality remains a\nchallenge. Sparse upcycling is a promising approach that transforms a\npretrained dense model into a Mixture-of-Experts (MoE) architecture, increasing\nthe model's parameter count and quality. In this work, we compare the\neffectiveness of sparse upcycling against continued pretraining (CPT) across\ndifferent model sizes, compute budgets, and pretraining durations. Our\nexperiments show that sparse upcycling can achieve better quality, with\nimprovements of over 20% relative to CPT in certain scenarios. However, this\ncomes with a significant inference cost, leading to 40% slowdowns in\nhigh-demand inference settings for larger models. Our findings highlight the\ntrade-off between model quality and inference efficiency, offering insights for\npractitioners seeking to balance model quality and deployment constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small, highly trained, open-source large language models are widely used due\nto their inference efficiency, but further improving their quality remains a\nchallenge. Sparse upcycling is a promising approach that transforms a\npretrained dense model into a Mixture-of-Experts (MoE) architecture, increasing\nthe model's parameter count and quality. In this work, we compare the\neffectiveness of sparse upcycling against continued pretraining (CPT) across\ndifferent model sizes, compute budgets, and pretraining durations. Our\nexperiments show that sparse upcycling can achieve better quality, with\nimprovements of over 20% relative to CPT in certain scenarios. However, this\ncomes with a significant inference cost, leading to 40% slowdowns in\nhigh-demand inference settings for larger models. Our findings highlight the\ntrade-off between model quality and inference efficiency, offering insights for\npractitioners seeking to balance model quality and deployment constraints."
                },
                "authors": [
                    {
                        "name": "Sasha Doubov"
                    },
                    {
                        "name": "Nikhil Sardana"
                    },
                    {
                        "name": "Vitaliy Chiley"
                    }
                ],
                "author_detail": {
                    "name": "Vitaliy Chiley"
                },
                "author": "Vitaliy Chiley",
                "arxiv_comment": "12 pages, 4 figures, To appear in the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08870v1",
                "updated": "2024-11-13T18:50:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    18,
                    50,
                    13,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T18:50:13Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    18,
                    50,
                    13,
                    2,
                    318,
                    0
                ],
                "title": "The Limited Impact of Medical Adaptation of Large Language and\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Limited Impact of Medical Adaptation of Large Language and\n  Vision-Language Models"
                },
                "summary": "Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare ten\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting and supervised fine-tuning regimes for medical question-answering\n(QA). For instance, across all tasks and model pairs we consider in the 3-shot\nsetting, medical LLMs only outperform their base models in 22.7% of cases,\nreach a (statistical) tie in 36.8% of cases, and are significantly worse than\ntheir base models in the remaining 40.5% of cases. Our conclusions are based on\n(i) comparing each medical model head-to-head, directly against the\ncorresponding base model; (ii) optimizing the prompts for each model separately\nin zero-/few-shot prompting; and (iii) accounting for statistical uncertainty\nin comparisons. While these basic practices are not consistently adopted in the\nliterature, our ablations show that they substantially impact conclusions.\nMeanwhile, we find that after fine-tuning on specific QA tasks, medical LLMs\ncan show performance improvements, but the benefits do not carry over to tasks\nbased on clinical notes. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare ten\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting and supervised fine-tuning regimes for medical question-answering\n(QA). For instance, across all tasks and model pairs we consider in the 3-shot\nsetting, medical LLMs only outperform their base models in 22.7% of cases,\nreach a (statistical) tie in 36.8% of cases, and are significantly worse than\ntheir base models in the remaining 40.5% of cases. Our conclusions are based on\n(i) comparing each medical model head-to-head, directly against the\ncorresponding base model; (ii) optimizing the prompts for each model separately\nin zero-/few-shot prompting; and (iii) accounting for statistical uncertainty\nin comparisons. While these basic practices are not consistently adopted in the\nliterature, our ablations show that they substantially impact conclusions.\nMeanwhile, we find that after fine-tuning on specific QA tasks, medical LLMs\ncan show performance improvements, but the benefits do not carry over to tasks\nbased on clinical notes. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies."
                },
                "authors": [
                    {
                        "name": "Daniel P. Jeong"
                    },
                    {
                        "name": "Pranav Mani"
                    },
                    {
                        "name": "Saurabh Garg"
                    },
                    {
                        "name": "Zachary C. Lipton"
                    },
                    {
                        "name": "Michael Oberst"
                    }
                ],
                "author_detail": {
                    "name": "Michael Oberst"
                },
                "author": "Michael Oberst",
                "arxiv_comment": "Extended version of EMNLP 2024 paper arXiv:2411.04118. Includes\n  additional results on clinical note QA tasks and supervised fine-tuning\n  evaluations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08862v1",
                "updated": "2024-11-13T18:44:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    18,
                    44,
                    30,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T18:44:30Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    18,
                    44,
                    30,
                    2,
                    318,
                    0
                ],
                "title": "LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs"
                },
                "summary": "We introduce LLMStinger, a novel approach that leverages Large Language\nModels (LLMs) to automatically generate adversarial suffixes for jailbreak\nattacks. Unlike traditional methods, which require complex prompt engineering\nor white-box access, LLMStinger uses a reinforcement learning (RL) loop to\nfine-tune an attacker LLM, generating new suffixes based on existing attacks\nfor harmful questions from the HarmBench benchmark. Our method significantly\noutperforms existing red-teaming approaches (we compared against 15 of the\nlatest methods), achieving a +57.2% improvement in Attack Success Rate (ASR) on\nLLaMA2-7B-chat and a +50.3% ASR increase on Claude 2, both models known for\ntheir extensive safety measures. Additionally, we achieved a 94.97% ASR on\nGPT-3.5 and 99.4% on Gemma-2B-it, demonstrating the robustness and adaptability\nof LLMStinger across open and closed-source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LLMStinger, a novel approach that leverages Large Language\nModels (LLMs) to automatically generate adversarial suffixes for jailbreak\nattacks. Unlike traditional methods, which require complex prompt engineering\nor white-box access, LLMStinger uses a reinforcement learning (RL) loop to\nfine-tune an attacker LLM, generating new suffixes based on existing attacks\nfor harmful questions from the HarmBench benchmark. Our method significantly\noutperforms existing red-teaming approaches (we compared against 15 of the\nlatest methods), achieving a +57.2% improvement in Attack Success Rate (ASR) on\nLLaMA2-7B-chat and a +50.3% ASR increase on Claude 2, both models known for\ntheir extensive safety measures. Additionally, we achieved a 94.97% ASR on\nGPT-3.5 and 99.4% on Gemma-2B-it, demonstrating the robustness and adaptability\nof LLMStinger across open and closed-source models."
                },
                "authors": [
                    {
                        "name": "Piyush Jha"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Vijay Ganesh"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Ganesh"
                },
                "author": "Vijay Ganesh",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06438v2",
                "updated": "2024-11-13T18:21:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    18,
                    21,
                    22,
                    2,
                    318,
                    0
                ],
                "published": "2024-07-08T22:40:15Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    22,
                    40,
                    15,
                    0,
                    190,
                    0
                ],
                "title": "A Single Transformer for Scalable Vision-Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Single Transformer for Scalable Vision-Language Modeling"
                },
                "summary": "We present SOLO, a single transformer for Scalable visiOn-Language mOdeling.\nCurrent large vision-language models (LVLMs) such as LLaVA mostly employ\nheterogeneous architectures that connect pre-trained visual encoders with large\nlanguage models (LLMs) to facilitate visual recognition and complex reasoning.\nAlthough achieving remarkable performance with relatively lightweight training,\nwe identify four primary scalability limitations: (1) The visual capacity is\nconstrained by pre-trained visual encoders, which are typically an order of\nmagnitude smaller than LLMs. (2) The heterogeneous architecture complicates the\nuse of established hardware and software infrastructure. (3) Study of scaling\nlaws on such architecture must consider three separate components - visual\nencoder, connector, and LLMs, which complicates the analysis. (4) The use of\nexisting visual encoders typically requires following a pre-defined\nspecification of image inputs pre-processing, for example, by reshaping inputs\nto fixed-resolution square images, which presents difficulties in processing\nand training on high-resolution images or those with unusual aspect ratio. A\nunified single Transformer architecture, like SOLO, effectively addresses these\nscalability concerns in LVLMs; however, its limited adoption in the modern\ncontext likely stems from the absence of reliable training recipes that balance\nboth modalities and ensure stable training for billion-scale models. In this\npaper, we introduce the first open-source training recipe for developing SOLO,\nan open-source 7B LVLM using moderate academic resources. The training recipe\ninvolves initializing from LLMs, sequential pre-training on ImageNet and\nweb-scale data, and instruction fine-tuning on our curated high-quality\ndatasets. On extensive evaluation, SOLO demonstrates performance comparable to\nLLaVA-v1.5-7B, particularly excelling in visual mathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SOLO, a single transformer for Scalable visiOn-Language mOdeling.\nCurrent large vision-language models (LVLMs) such as LLaVA mostly employ\nheterogeneous architectures that connect pre-trained visual encoders with large\nlanguage models (LLMs) to facilitate visual recognition and complex reasoning.\nAlthough achieving remarkable performance with relatively lightweight training,\nwe identify four primary scalability limitations: (1) The visual capacity is\nconstrained by pre-trained visual encoders, which are typically an order of\nmagnitude smaller than LLMs. (2) The heterogeneous architecture complicates the\nuse of established hardware and software infrastructure. (3) Study of scaling\nlaws on such architecture must consider three separate components - visual\nencoder, connector, and LLMs, which complicates the analysis. (4) The use of\nexisting visual encoders typically requires following a pre-defined\nspecification of image inputs pre-processing, for example, by reshaping inputs\nto fixed-resolution square images, which presents difficulties in processing\nand training on high-resolution images or those with unusual aspect ratio. A\nunified single Transformer architecture, like SOLO, effectively addresses these\nscalability concerns in LVLMs; however, its limited adoption in the modern\ncontext likely stems from the absence of reliable training recipes that balance\nboth modalities and ensure stable training for billion-scale models. In this\npaper, we introduce the first open-source training recipe for developing SOLO,\nan open-source 7B LVLM using moderate academic resources. The training recipe\ninvolves initializing from LLMs, sequential pre-training on ImageNet and\nweb-scale data, and instruction fine-tuning on our curated high-quality\ndatasets. On extensive evaluation, SOLO demonstrates performance comparable to\nLLaVA-v1.5-7B, particularly excelling in visual mathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Yangyi Chen"
                    },
                    {
                        "name": "Xingyao Wang"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "arxiv_comment": "Accepted to TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02538v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02538v2",
                "updated": "2024-11-13T18:04:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    18,
                    4,
                    44,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-04T19:17:17Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    19,
                    17,
                    17,
                    0,
                    309,
                    0
                ],
                "title": "MILU: A Multi-task Indic Language Understanding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILU: A Multi-task Indic Language Understanding Benchmark"
                },
                "summary": "Evaluating Large Language Models (LLMs) in low-resource and linguistically\ndiverse languages remains a significant challenge in NLP, particularly for\nlanguages using non-Latin scripts like those spoken in India. Existing\nbenchmarks predominantly focus on English, leaving substantial gaps in\nassessing LLM capabilities in these languages. We introduce MILU, a Multi task\nIndic Language Understanding Benchmark, a comprehensive evaluation benchmark\ndesigned to address this gap. MILU spans 8 domains and 42 subjects across 11\nIndic languages, reflecting both general and culturally specific knowledge.\nWith an India-centric design, incorporates material from regional and\nstate-level examinations, covering topics such as local history, arts,\nfestivals, and laws, alongside standard subjects like science and mathematics.\nWe evaluate over 45 LLMs, and find that current LLMs struggle with MILU, with\nGPT-4o achieving the highest average accuracy at 72 percent. Open multilingual\nmodels outperform language-specific fine-tuned models, which perform only\nslightly better than random baselines. Models also perform better in high\nresource languages as compared to low resource ones. Domain-wise analysis\nindicates that models perform poorly in culturally relevant areas like Arts and\nHumanities, Law and Governance compared to general fields like STEM. To the\nbest of our knowledge, MILU is the first of its kind benchmark focused on Indic\nlanguages, serving as a crucial step towards comprehensive cultural evaluation.\nAll code, benchmarks, and artifacts are publicly available to foster open\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) in low-resource and linguistically\ndiverse languages remains a significant challenge in NLP, particularly for\nlanguages using non-Latin scripts like those spoken in India. Existing\nbenchmarks predominantly focus on English, leaving substantial gaps in\nassessing LLM capabilities in these languages. We introduce MILU, a Multi task\nIndic Language Understanding Benchmark, a comprehensive evaluation benchmark\ndesigned to address this gap. MILU spans 8 domains and 42 subjects across 11\nIndic languages, reflecting both general and culturally specific knowledge.\nWith an India-centric design, incorporates material from regional and\nstate-level examinations, covering topics such as local history, arts,\nfestivals, and laws, alongside standard subjects like science and mathematics.\nWe evaluate over 45 LLMs, and find that current LLMs struggle with MILU, with\nGPT-4o achieving the highest average accuracy at 72 percent. Open multilingual\nmodels outperform language-specific fine-tuned models, which perform only\nslightly better than random baselines. Models also perform better in high\nresource languages as compared to low resource ones. Domain-wise analysis\nindicates that models perform poorly in culturally relevant areas like Arts and\nHumanities, Law and Governance compared to general fields like STEM. To the\nbest of our knowledge, MILU is the first of its kind benchmark focused on Indic\nlanguages, serving as a crucial step towards comprehensive cultural evaluation.\nAll code, benchmarks, and artifacts are publicly available to foster open\nresearch."
                },
                "authors": [
                    {
                        "name": "Sshubam Verma"
                    },
                    {
                        "name": "Mohammed Safi Ur Rahman Khan"
                    },
                    {
                        "name": "Vishwajeet Kumar"
                    },
                    {
                        "name": "Rudra Murthy"
                    },
                    {
                        "name": "Jaydeep Sen"
                    }
                ],
                "author_detail": {
                    "name": "Jaydeep Sen"
                },
                "author": "Jaydeep Sen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02538v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02538v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08813v1",
                "updated": "2024-11-13T17:51:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    17,
                    51,
                    57,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T17:51:57Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    17,
                    51,
                    57,
                    2,
                    318,
                    0
                ],
                "title": "Rethinking CyberSecEval: An LLM-Aided Approach to Evaluation Critique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking CyberSecEval: An LLM-Aided Approach to Evaluation Critique"
                },
                "summary": "A key development in the cybersecurity evaluations space is the work carried\nout by Meta, through their CyberSecEval approach. While this work is\nundoubtedly a useful contribution to a nascent field, there are notable\nfeatures that limit its utility. Key drawbacks focus on the insecure code\ndetection part of Meta's methodology. We explore these limitations, and use our\nexploration as a test case for LLM-assisted benchmark analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key development in the cybersecurity evaluations space is the work carried\nout by Meta, through their CyberSecEval approach. While this work is\nundoubtedly a useful contribution to a nascent field, there are notable\nfeatures that limit its utility. Key drawbacks focus on the insecure code\ndetection part of Meta's methodology. We explore these limitations, and use our\nexploration as a test case for LLM-assisted benchmark analysis."
                },
                "authors": [
                    {
                        "name": "Suhas Hariharan"
                    },
                    {
                        "name": "Zainab Ali Majid"
                    },
                    {
                        "name": "Jaime Raldua Veuthey"
                    },
                    {
                        "name": "Jacob Haimes"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Haimes"
                },
                "author": "Jacob Haimes",
                "arxiv_comment": "NeurIPS 2024, 2 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08804v1",
                "updated": "2024-11-13T17:38:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    17,
                    38,
                    7,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T17:38:07Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    17,
                    38,
                    7,
                    2,
                    318,
                    0
                ],
                "title": "FinRobot: AI Agent for Equity Research and Valuation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinRobot: AI Agent for Equity Research and Valuation with Large Language\n  Models"
                },
                "summary": "As financial markets grow increasingly complex, there is a rising need for\nautomated tools that can effectively assist human analysts in equity research,\nparticularly within sell-side research. While Generative AI (GenAI) has\nattracted significant attention in this field, existing AI solutions often fall\nshort due to their narrow focus on technical factors and limited capacity for\ndiscretionary judgment. These limitations hinder their ability to adapt to new\ndata in real-time and accurately assess risks, which diminishes their practical\nvalue for investors.\n  This paper presents FinRobot, the first AI agent framework specifically\ndesigned for equity research. FinRobot employs a multi-agent Chain of Thought\n(CoT) system, integrating both quantitative and qualitative analyses to emulate\nthe comprehensive reasoning of a human analyst. The system is structured around\nthree specialized agents: the Data-CoT Agent, which aggregates diverse data\nsources for robust financial integration; the Concept-CoT Agent, which mimics\nan analysts reasoning to generate actionable insights; and the Thesis-CoT\nAgent, which synthesizes these insights into a coherent investment thesis and\nreport. FinRobot provides thorough company analysis supported by precise\nnumerical data, industry-appropriate valuation metrics, and realistic risk\nassessments. Its dynamically updatable data pipeline ensures that research\nremains timely and relevant, adapting seamlessly to new financial information.\nUnlike existing automated research tools, such as CapitalCube and Wright\nReports, FinRobot delivers insights comparable to those produced by major\nbrokerage firms and fundamental research vendors. We open-source FinRobot at\n\\url{https://github. com/AI4Finance-Foundation/FinRobot}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As financial markets grow increasingly complex, there is a rising need for\nautomated tools that can effectively assist human analysts in equity research,\nparticularly within sell-side research. While Generative AI (GenAI) has\nattracted significant attention in this field, existing AI solutions often fall\nshort due to their narrow focus on technical factors and limited capacity for\ndiscretionary judgment. These limitations hinder their ability to adapt to new\ndata in real-time and accurately assess risks, which diminishes their practical\nvalue for investors.\n  This paper presents FinRobot, the first AI agent framework specifically\ndesigned for equity research. FinRobot employs a multi-agent Chain of Thought\n(CoT) system, integrating both quantitative and qualitative analyses to emulate\nthe comprehensive reasoning of a human analyst. The system is structured around\nthree specialized agents: the Data-CoT Agent, which aggregates diverse data\nsources for robust financial integration; the Concept-CoT Agent, which mimics\nan analysts reasoning to generate actionable insights; and the Thesis-CoT\nAgent, which synthesizes these insights into a coherent investment thesis and\nreport. FinRobot provides thorough company analysis supported by precise\nnumerical data, industry-appropriate valuation metrics, and realistic risk\nassessments. Its dynamically updatable data pipeline ensures that research\nremains timely and relevant, adapting seamlessly to new financial information.\nUnlike existing automated research tools, such as CapitalCube and Wright\nReports, FinRobot delivers insights comparable to those produced by major\nbrokerage firms and fundamental research vendors. We open-source FinRobot at\n\\url{https://github. com/AI4Finance-Foundation/FinRobot}."
                },
                "authors": [
                    {
                        "name": "Tianyu Zhou"
                    },
                    {
                        "name": "Pinqiao Wang"
                    },
                    {
                        "name": "Yilin Wu"
                    },
                    {
                        "name": "Hongyang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Yang"
                },
                "author": "Hongyang Yang",
                "arxiv_comment": "The 1st Workshop on LLMs and Generative AI for Finance, ICAIF 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03887v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03887v2",
                "updated": "2024-11-13T17:37:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    17,
                    37,
                    55,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-01T18:46:03Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    18,
                    46,
                    3,
                    4,
                    306,
                    0
                ],
                "title": "OML: Open, Monetizable, and Loyal AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OML: Open, Monetizable, and Loyal AI"
                },
                "summary": "Artificial Intelligence (AI) has steadily improved across a wide range of\ntasks. However, the development and deployment of AI are almost entirely\ncontrolled by a few powerful organizations that are racing to create Artificial\nGeneral Intelligence (AGI). The centralized entities make decisions with little\npublic oversight, shaping the future of humanity, often with unforeseen\nconsequences. In this paper, we propose OML, which stands for Open,\nMonetizable, and Loyal AI, an approach designed to democratize AI development.\nOML is realized through an interdisciplinary framework spanning AI, blockchain,\nand cryptography. We present several ideas for constructing OML using\ntechnologies such as Trusted Execution Environments (TEE), traditional\ncryptographic primitives like fully homomorphic encryption and functional\nencryption, obfuscation, and AI-native solutions rooted in the sample\ncomplexity and intrinsic hardness of AI tasks. A key innovation of our work is\nintroducing a new scientific field: AI-native cryptography. Unlike conventional\ncryptography, which focuses on discrete data and binary security guarantees,\nAI-native cryptography exploits the continuous nature of AI data\nrepresentations and their low-dimensional manifolds, focusing on improving\napproximate performance. One core idea is to transform AI attack methods, such\nas data poisoning, into security tools. This novel approach serves as a\nfoundation for OML 1.0 which uses model fingerprinting to protect the integrity\nand ownership of AI models. The spirit of OML is to establish a decentralized,\nopen, and transparent platform for AI development, enabling the community to\ncontribute, monetize, and take ownership of AI models. By decentralizing\ncontrol and ensuring transparency through blockchain technology, OML prevents\nthe concentration of power and provides accountability in AI development that\nhas not been possible before.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) has steadily improved across a wide range of\ntasks. However, the development and deployment of AI are almost entirely\ncontrolled by a few powerful organizations that are racing to create Artificial\nGeneral Intelligence (AGI). The centralized entities make decisions with little\npublic oversight, shaping the future of humanity, often with unforeseen\nconsequences. In this paper, we propose OML, which stands for Open,\nMonetizable, and Loyal AI, an approach designed to democratize AI development.\nOML is realized through an interdisciplinary framework spanning AI, blockchain,\nand cryptography. We present several ideas for constructing OML using\ntechnologies such as Trusted Execution Environments (TEE), traditional\ncryptographic primitives like fully homomorphic encryption and functional\nencryption, obfuscation, and AI-native solutions rooted in the sample\ncomplexity and intrinsic hardness of AI tasks. A key innovation of our work is\nintroducing a new scientific field: AI-native cryptography. Unlike conventional\ncryptography, which focuses on discrete data and binary security guarantees,\nAI-native cryptography exploits the continuous nature of AI data\nrepresentations and their low-dimensional manifolds, focusing on improving\napproximate performance. One core idea is to transform AI attack methods, such\nas data poisoning, into security tools. This novel approach serves as a\nfoundation for OML 1.0 which uses model fingerprinting to protect the integrity\nand ownership of AI models. The spirit of OML is to establish a decentralized,\nopen, and transparent platform for AI development, enabling the community to\ncontribute, monetize, and take ownership of AI models. By decentralizing\ncontrol and ensuring transparency through blockchain technology, OML prevents\nthe concentration of power and provides accountability in AI development that\nhas not been possible before."
                },
                "authors": [
                    {
                        "name": "Zerui Cheng"
                    },
                    {
                        "name": "Edoardo Contente"
                    },
                    {
                        "name": "Ben Finch"
                    },
                    {
                        "name": "Oleg Golev"
                    },
                    {
                        "name": "Jonathan Hayase"
                    },
                    {
                        "name": "Andrew Miller"
                    },
                    {
                        "name": "Niusha Moshrefi"
                    },
                    {
                        "name": "Anshul Nasery"
                    },
                    {
                        "name": "Sandeep Nailwal"
                    },
                    {
                        "name": "Sewoong Oh"
                    },
                    {
                        "name": "Himanshu Tyagi"
                    },
                    {
                        "name": "Pramod Viswanath"
                    }
                ],
                "author_detail": {
                    "name": "Pramod Viswanath"
                },
                "author": "Pramod Viswanath",
                "arxiv_comment": "60 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03887v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03887v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16527v2",
                "updated": "2024-11-13T17:30:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    17,
                    30,
                    33,
                    2,
                    318,
                    0
                ],
                "published": "2024-10-21T21:36:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    21,
                    36,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "Insights and Current Gaps in Open-Source LLM Vulnerability Scanners: A\n  Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights and Current Gaps in Open-Source LLM Vulnerability Scanners: A\n  Comparative Analysis"
                },
                "summary": "This report presents a comparative analysis of open-source vulnerability\nscanners for conversational large language models (LLMs). As LLMs become\nintegral to various applications, they also present potential attack surfaces,\nexposed to security risks such as information leakage and jailbreak attacks.\nOur study evaluates prominent scanners - Garak, Giskard, PyRIT, and\nCyberSecEval - that adapt red-teaming practices to expose these\nvulnerabilities. We detail the distinctive features and practical use of these\nscanners, outline unifying principles of their design and perform quantitative\nevaluations to compare them. These evaluations uncover significant reliability\nissues in detecting successful attacks, highlighting a fundamental gap for\nfuture development. Additionally, we contribute a preliminary labelled dataset,\nwhich serves as an initial step to bridge this gap. Based on the above, we\nprovide strategic recommendations to assist organizations choose the most\nsuitable scanner for their red-teaming needs, accounting for customizability,\ntest suite comprehensiveness, and industry-specific use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report presents a comparative analysis of open-source vulnerability\nscanners for conversational large language models (LLMs). As LLMs become\nintegral to various applications, they also present potential attack surfaces,\nexposed to security risks such as information leakage and jailbreak attacks.\nOur study evaluates prominent scanners - Garak, Giskard, PyRIT, and\nCyberSecEval - that adapt red-teaming practices to expose these\nvulnerabilities. We detail the distinctive features and practical use of these\nscanners, outline unifying principles of their design and perform quantitative\nevaluations to compare them. These evaluations uncover significant reliability\nissues in detecting successful attacks, highlighting a fundamental gap for\nfuture development. Additionally, we contribute a preliminary labelled dataset,\nwhich serves as an initial step to bridge this gap. Based on the above, we\nprovide strategic recommendations to assist organizations choose the most\nsuitable scanner for their red-teaming needs, accounting for customizability,\ntest suite comprehensiveness, and industry-specific use cases."
                },
                "authors": [
                    {
                        "name": "Jonathan Brokman"
                    },
                    {
                        "name": "Omer Hofman"
                    },
                    {
                        "name": "Oren Rachmil"
                    },
                    {
                        "name": "Inderjeet Singh"
                    },
                    {
                        "name": "Rathina Sabapathy Aishvariya Priya"
                    },
                    {
                        "name": "Vikas Pahuja"
                    },
                    {
                        "name": "Amit Giloni"
                    },
                    {
                        "name": "Roman Vainshtein"
                    },
                    {
                        "name": "Hisashi Kojima"
                    }
                ],
                "author_detail": {
                    "name": "Hisashi Kojima"
                },
                "author": "Hisashi Kojima",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08794v1",
                "updated": "2024-11-13T17:19:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    17,
                    19,
                    32,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T17:19:32Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    17,
                    19,
                    32,
                    2,
                    318,
                    0
                ],
                "title": "Evaluating World Models with LLM for Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating World Models with LLM for Decision Making"
                },
                "summary": "World model emerges as a key module in decision making, where MuZero and\nDreamer achieve remarkable successes in complex tasks. Recent work leverages\nLarge Language Models (LLMs) as general world simulators to simulate the\ndynamics of the world due to their generalizability. LLMs also serve as the\nworld model for deliberative reasoning in Reasoning via Planning (RAP) and Tree\nof Thought (ToT). However, the world models are either evaluated as a general\nworld simulator, or as a functional module of the agent, i.e., predicting the\ntransitions to assist the planning. In this work, we propose a comprehensive\nevaluation of the world models with LLMs from the decision making perspective.\nSpecifically, we leverage the 31 diverse environments from (Wang et al.,\n2023;2024) and curate the rule-based policy of each environment for the diverse\nevaluation. Then, we design three main tasks, i.e., policy verification, action\nproposal, and policy planning, where the world models can be used for decision\nmaking solely. Finally, we conduct the comprehensive evaluation of the advanced\nLLMs, i.e., GPT-4o and GPT-4o-mini, on the environments for the three main\ntasks under various settings. The key observations include: i) GPT-4o\nsignificantly outperforms GPT-4o-mini on the three main tasks, especially for\nthe tasks which require the domain knowledge, ii) the performance of the world\nmodel with LLM will be decreased for long-term decision-making tasks, and iii)\nthe combination of different functionalities of the world model will brings\nadditional unstabilities of the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "World model emerges as a key module in decision making, where MuZero and\nDreamer achieve remarkable successes in complex tasks. Recent work leverages\nLarge Language Models (LLMs) as general world simulators to simulate the\ndynamics of the world due to their generalizability. LLMs also serve as the\nworld model for deliberative reasoning in Reasoning via Planning (RAP) and Tree\nof Thought (ToT). However, the world models are either evaluated as a general\nworld simulator, or as a functional module of the agent, i.e., predicting the\ntransitions to assist the planning. In this work, we propose a comprehensive\nevaluation of the world models with LLMs from the decision making perspective.\nSpecifically, we leverage the 31 diverse environments from (Wang et al.,\n2023;2024) and curate the rule-based policy of each environment for the diverse\nevaluation. Then, we design three main tasks, i.e., policy verification, action\nproposal, and policy planning, where the world models can be used for decision\nmaking solely. Finally, we conduct the comprehensive evaluation of the advanced\nLLMs, i.e., GPT-4o and GPT-4o-mini, on the environments for the three main\ntasks under various settings. The key observations include: i) GPT-4o\nsignificantly outperforms GPT-4o-mini on the three main tasks, especially for\nthe tasks which require the domain knowledge, ii) the performance of the world\nmodel with LLM will be decreased for long-term decision-making tasks, and iii)\nthe combination of different functionalities of the world model will brings\nadditional unstabilities of the performance."
                },
                "authors": [
                    {
                        "name": "Chang Yang"
                    },
                    {
                        "name": "Xinrun Wang"
                    },
                    {
                        "name": "Junzhe Jiang"
                    },
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]