[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.08525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08525v1",
                "updated": "2025-10-09T17:50:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:50:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression"
                },
                "summary": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results."
                },
                "authors": [
                    {
                        "name": "Wenjie Du"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v2",
                "updated": "2025-10-09T17:45:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    45,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v2",
                "updated": "2025-10-09T17:38:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    38,
                    52,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "arxiv_comment": "Original version uploaded on Sep 22, 2025. (v2): Extended Table 2\n  with additional analysis and referenced it in Sec 5.2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08351v1",
                "updated": "2025-10-09T15:38:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:38:13Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "title": "FMCache: File-System Metadata Caching in Programmable Switches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMCache: File-System Metadata Caching in Programmable Switches"
                },
                "summary": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage."
                },
                "authors": [
                    {
                        "name": "Qingxiu Liu"
                    },
                    {
                        "name": "Jiazhen Cai"
                    },
                    {
                        "name": "Siyuan Sheng"
                    },
                    {
                        "name": "Yuhui Chen"
                    },
                    {
                        "name": "Lu Tang"
                    },
                    {
                        "name": "Zhirong Shen"
                    },
                    {
                        "name": "Patrick P. C. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Patrick P. C. Lee"
                },
                "arxiv_affiliation": "The Chinese University of Hong Kong",
                "author": "Patrick P. C. Lee",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08272v1",
                "updated": "2025-10-09T14:29:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    29,
                    54,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:29:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    29,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V\n  Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V\n  Processors"
                },
                "summary": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $37.5\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $37.5\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures."
                },
                "authors": [
                    {
                        "name": "Cédrick Austa"
                    },
                    {
                        "name": "Jan Tobias Mühlberg"
                    },
                    {
                        "name": "Jean-Michel Dricot"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Dricot"
                },
                "author": "Jean-Michel Dricot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08180v1",
                "updated": "2025-10-09T13:06:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    6,
                    16,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:06:16Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    6,
                    16,
                    3,
                    282,
                    0
                ],
                "title": "Towards Energy-Efficient Serverless Computing with Hardware Isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Energy-Efficient Serverless Computing with Hardware Isolation"
                },
                "summary": "Serverless computing provides just-in-time infrastructure provisioning with\nrapid elasticity and a finely-grained pricing model. As full control of\nresource allocation is in the hands of the cloud provider and applications only\nconsume resources when they actually perform work, we believe that serverless\ncomputing is uniquely positioned to maximize energy efficiency.\n  However, the focus of current serverless platforms is to run hundreds or\nthousands of serverless functions from different tenants on traditional server\nhardware, requiring expensive software isolation mechanisms and a high degree\nof overprovisioning, i.e., idle servers, to anticipate load spikes. With shared\ncaches, high clock frequencies, and many-core architectures, servers today are\noptimized for large, singular workloads but not to run thousands of isolated\nfunctions.\n  We propose rethinking the serverless hardware architecture to align it with\nthe requirements of serverless software. Specifically, we propose using\nhardware isolation with individual processors per function instead of software\nisolation resulting in a serverless hardware stack that consumes energy only\nwhen an application actually performs work. In preliminary evaluation with real\nhardware and a typical serverless workload we find that this could reduce\nenergy consumption overheads by 90.63% or an average 70.8MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides just-in-time infrastructure provisioning with\nrapid elasticity and a finely-grained pricing model. As full control of\nresource allocation is in the hands of the cloud provider and applications only\nconsume resources when they actually perform work, we believe that serverless\ncomputing is uniquely positioned to maximize energy efficiency.\n  However, the focus of current serverless platforms is to run hundreds or\nthousands of serverless functions from different tenants on traditional server\nhardware, requiring expensive software isolation mechanisms and a high degree\nof overprovisioning, i.e., idle servers, to anticipate load spikes. With shared\ncaches, high clock frequencies, and many-core architectures, servers today are\noptimized for large, singular workloads but not to run thousands of isolated\nfunctions.\n  We propose rethinking the serverless hardware architecture to align it with\nthe requirements of serverless software. Specifically, we propose using\nhardware isolation with individual processors per function instead of software\nisolation resulting in a serverless hardware stack that consumes energy only\nwhen an application actually performs work. In preliminary evaluation with real\nhardware and a typical serverless workload we find that this could reduce\nenergy consumption overheads by 90.63% or an average 70.8MW."
                },
                "authors": [
                    {
                        "name": "Natalie Carl"
                    },
                    {
                        "name": "Tobias Pfandzelter"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v2",
                "updated": "2025-10-09T13:03:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    3,
                    29,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v4",
                "updated": "2025-10-09T12:05:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    5,
                    4,
                    3,
                    282,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v2",
                "updated": "2025-10-09T12:01:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    1,
                    20,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Panagiotis Karras"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Jignesh M. Patel"
                },
                "author": "Jignesh M. Patel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v4",
                "updated": "2025-10-09T09:33:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    9,
                    33,
                    47,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23488v2",
                "updated": "2025-10-09T09:14:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    9,
                    14,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2025-06-30T03:22:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces"
                },
                "summary": "Wireless communication systems face challenges in meeting the demand for\nhigher data rates and reliable connectivity in complex environments. Stacked\nintelligent metasurfaces (SIMs) have emerged as a promising technology for\nadvanced wave-domain signal processing, where mobile SIMs can outperform fixed\ncounterparts. In this paper, we propose a novel unmanned aerial vehicle\n(UAV)-mounted SIM (UAV-SIM) assisted communication system within low-altitude\neconomy (LAE) networks, where UAVs act as both cache-enabled base stations and\nmobile SIM carriers to enhance uplink transmissions. To maximize network\ncapacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that\nintegrates user association, UAV-SIM three-dimensional positioning, and\nmulti-layer SIM phase shift design. Due to the non-convexity and NP-hardness of\nUSBJOP, we decompose it into three subproblems, which are the association\nbetween UAV-SIMs and users optimization problem (AUUOP), the UAV location\noptimization problem (ULOP), and the UAV-SIM phase shifts optimization problem\n(USPSOP). Then, we solve them through an alternating optimization strategy.\nSpecifically, AUUOP and ULOP are transformed into convex forms solvable via the\nCVX tool, while USPSOP is addressed by a generative artificial intelligence\n(GAI)-based hybrid optimization algorithm. Simulation results show that the\nproposed approach achieves approximately 1.5 times higher network capacity\ncompared with suboptimal schemes, effectively mitigates multi-user interference\nwith increasing SIM layers and meta-atoms, and reduces runtime by 10\\% while\nmaintaining solution quality, thereby demonstrating its practicality for\nreal-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communication systems face challenges in meeting the demand for\nhigher data rates and reliable connectivity in complex environments. Stacked\nintelligent metasurfaces (SIMs) have emerged as a promising technology for\nadvanced wave-domain signal processing, where mobile SIMs can outperform fixed\ncounterparts. In this paper, we propose a novel unmanned aerial vehicle\n(UAV)-mounted SIM (UAV-SIM) assisted communication system within low-altitude\neconomy (LAE) networks, where UAVs act as both cache-enabled base stations and\nmobile SIM carriers to enhance uplink transmissions. To maximize network\ncapacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that\nintegrates user association, UAV-SIM three-dimensional positioning, and\nmulti-layer SIM phase shift design. Due to the non-convexity and NP-hardness of\nUSBJOP, we decompose it into three subproblems, which are the association\nbetween UAV-SIMs and users optimization problem (AUUOP), the UAV location\noptimization problem (ULOP), and the UAV-SIM phase shifts optimization problem\n(USPSOP). Then, we solve them through an alternating optimization strategy.\nSpecifically, AUUOP and ULOP are transformed into convex forms solvable via the\nCVX tool, while USPSOP is addressed by a generative artificial intelligence\n(GAI)-based hybrid optimization algorithm. Simulation results show that the\nproposed approach achieves approximately 1.5 times higher network capacity\ncompared with suboptimal schemes, effectively mitigates multi-user interference\nwith increasing SIM layers and meta-atoms, and reduces runtime by 10\\% while\nmaintaining solution quality, thereby demonstrating its practicality for\nreal-world deployments."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Chuang Zhang"
                    },
                    {
                        "name": "Linyao Li"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This paper has been already submitted to TCCN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v4",
                "updated": "2025-10-09T02:37:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    2,
                    37,
                    26,
                    3,
                    282,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "arxiv_doi": "10.1145/3771283",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3771283",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.07467v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM) 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21467v2",
                "updated": "2025-10-09T01:43:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    43,
                    2,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-27T17:39:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "FlashDLM: Accelerating Diffusion Language Model Inference via Efficient\n  KV Caching and Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDLM: Accelerating Diffusion Language Model Inference via Efficient\n  KV Caching and Guided Diffusion"
                },
                "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized autoregressive (AR) models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver an average\nof 12.14x end-to-end speedup across various tasks with negligible accuracy\ndegradation. For the first time, diffusion language models achieve a comparable\nand even faster latency as the widely adopted autoregressive models. Our work\nsuccessfully paved the way for scaling up the diffusion language model to a\nbroader scope of applications across different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized autoregressive (AR) models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver an average\nof 12.14x end-to-end speedup across various tasks with negligible accuracy\ndegradation. For the first time, diffusion language models achieve a comparable\nand even faster latency as the widely adopted autoregressive models. Our work\nsuccessfully paved the way for scaling up the diffusion language model to a\nbroader scope of applications across different domains."
                },
                "authors": [
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Jian Meng"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Jae-sun Seo"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07667v1",
                "updated": "2025-10-09T01:40:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    40,
                    39,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T01:40:39Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    40,
                    39,
                    3,
                    282,
                    0
                ],
                "title": "An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit\n  Data Reuse Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit\n  Data Reuse Strategies"
                },
                "summary": "Neural radiance fields (NeRF) have transformed 3D reconstruction and\nrendering, facilitating photorealistic image synthesis from sparse viewpoints.\nThis work introduces an explicit data reuse neural rendering (EDR-NR)\narchitecture, which reduces frequent external memory accesses (EMAs) and cache\nmisses by exploiting the spatial locality from three phases, including rays,\nray packets (RPs), and samples. The EDR-NR architecture features a four-stage\nscheduler that clusters rays on the basis of Z-order, prioritize lagging rays\nwhen ray divergence happens, reorders RPs based on spatial proximity, and\nissues samples out-of-orderly (OoO) according to the availability of on-chip\nfeature data. In addition, a four-tier hierarchical RP marching (HRM) technique\nis integrated with an axis-aligned bounding box (AABB) to facilitate spatial\nskipping (SS), reducing redundant computations and improving throughput.\nMoreover, a balanced allocation strategy for feature storage is proposed to\nmitigate SRAM bank conflicts. Fabricated using a 40 nm process with a die area\nof 10.5 mmX, the EDR-NR chip demonstrates a 2.41X enhancement in normalized\nenergy efficiency, a 1.21X improvement in normalized area efficiency, a 1.20X\nincrease in normalized throughput, and a 53.42% reduction in on-chip SRAM\nconsumption compared to state-of-the-art accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural radiance fields (NeRF) have transformed 3D reconstruction and\nrendering, facilitating photorealistic image synthesis from sparse viewpoints.\nThis work introduces an explicit data reuse neural rendering (EDR-NR)\narchitecture, which reduces frequent external memory accesses (EMAs) and cache\nmisses by exploiting the spatial locality from three phases, including rays,\nray packets (RPs), and samples. The EDR-NR architecture features a four-stage\nscheduler that clusters rays on the basis of Z-order, prioritize lagging rays\nwhen ray divergence happens, reorders RPs based on spatial proximity, and\nissues samples out-of-orderly (OoO) according to the availability of on-chip\nfeature data. In addition, a four-tier hierarchical RP marching (HRM) technique\nis integrated with an axis-aligned bounding box (AABB) to facilitate spatial\nskipping (SS), reducing redundant computations and improving throughput.\nMoreover, a balanced allocation strategy for feature storage is proposed to\nmitigate SRAM bank conflicts. Fabricated using a 40 nm process with a die area\nof 10.5 mmX, the EDR-NR chip demonstrates a 2.41X enhancement in normalized\nenergy efficiency, a 1.21X improvement in normalized area efficiency, a 1.20X\nincrease in normalized throughput, and a 53.42% reduction in on-chip SRAM\nconsumption compared to state-of-the-art accelerators."
                },
                "authors": [
                    {
                        "name": "Binzhe Yuan"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yuefeng Zhang"
                    },
                    {
                        "name": "Haochuan Wan"
                    },
                    {
                        "name": "Zhechen Yuan"
                    },
                    {
                        "name": "Junsheng Chen"
                    },
                    {
                        "name": "Yunxiang He"
                    },
                    {
                        "name": "Junran Ding"
                    },
                    {
                        "name": "Xiaoming Zhang"
                    },
                    {
                        "name": "Chaolin Rao"
                    },
                    {
                        "name": "Wenyan Su"
                    },
                    {
                        "name": "Pingqiang Zhou"
                    },
                    {
                        "name": "Jingyi Yu"
                    },
                    {
                        "name": "Xin Lou"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lou"
                },
                "author": "Xin Lou",
                "arxiv_comment": "11 pages, 17 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07651v1",
                "updated": "2025-10-09T00:58:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    0,
                    58,
                    28,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T00:58:28Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    0,
                    58,
                    28,
                    3,
                    282,
                    0
                ],
                "title": "OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM\n  Inference"
                },
                "summary": "Large language models (LLMs) with extended context windows enable powerful\ndownstream applications but impose significant memory overhead, as caching all\nkey-value (KV) states scales linearly with sequence length and batch size.\nExisting cache eviction methods address this by exploiting attention sparsity,\nyet they typically rank tokens heuristically using accumulated attention\nweights without considering their true impact on attention outputs. We propose\nOptimal Brain Cache (OBCache), a principled framework that formulates cache\neviction as a layer-wise structured pruning problem. Building upon the Optimal\nBrain Damage (OBD) theory, OBCache quantifies token saliency by measuring the\nperturbation in attention outputs induced by pruning tokens, with closed-form\nscores derived for isolated keys, isolated values, and joint key-value pairs.\nOur scores account not only for attention weights but also for information from\nvalue states and attention outputs, thereby enhancing existing eviction\nstrategies with output-aware signals. Experiments on LLaMA and Qwen models\ndemonstrate that replacing the heuristic scores in existing works, which\nestimate token saliency across different query positions, with OBCache's\noutput-aware scores consistently improves long-context accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows enable powerful\ndownstream applications but impose significant memory overhead, as caching all\nkey-value (KV) states scales linearly with sequence length and batch size.\nExisting cache eviction methods address this by exploiting attention sparsity,\nyet they typically rank tokens heuristically using accumulated attention\nweights without considering their true impact on attention outputs. We propose\nOptimal Brain Cache (OBCache), a principled framework that formulates cache\neviction as a layer-wise structured pruning problem. Building upon the Optimal\nBrain Damage (OBD) theory, OBCache quantifies token saliency by measuring the\nperturbation in attention outputs induced by pruning tokens, with closed-form\nscores derived for isolated keys, isolated values, and joint key-value pairs.\nOur scores account not only for attention weights but also for information from\nvalue states and attention outputs, thereby enhancing existing eviction\nstrategies with output-aware signals. Experiments on LLaMA and Qwen models\ndemonstrate that replacing the heuristic scores in existing works, which\nestimate token saliency across different query positions, with OBCache's\noutput-aware scores consistently improves long-context accuracy."
                },
                "authors": [
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Xiyu Liang"
                    },
                    {
                        "name": "Jiaojiao Zhao"
                    },
                    {
                        "name": "Enmao Diao"
                    }
                ],
                "author_detail": {
                    "name": "Enmao Diao"
                },
                "author": "Enmao Diao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07499v1",
                "updated": "2025-10-08T19:52:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    52,
                    35,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T19:52:35Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    52,
                    35,
                    2,
                    281,
                    0
                ],
                "title": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs"
                },
                "summary": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL)."
                },
                "authors": [
                    {
                        "name": "Soyeong Jeong"
                    },
                    {
                        "name": "Taehee Jung"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    },
                    {
                        "name": "Joo-Kyung Kim"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07486v1",
                "updated": "2025-10-08T19:36:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    36,
                    11,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T19:36:11Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    36,
                    11,
                    2,
                    281,
                    0
                ],
                "title": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse\n  Decoding"
                },
                "summary": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500)."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Yilin Guan"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "14 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18344v2",
                "updated": "2025-10-08T18:16:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    18,
                    16,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-09-22T19:08:57Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding"
                },
                "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit)."
                },
                "authors": [
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Chun-Che Yang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07318v1",
                "updated": "2025-10-08T17:59:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:59:55Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Hippocampus Networks for Efficient Long-Context Modeling"
                },
                "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN."
                },
                "authors": [
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Weihao Yu"
                    },
                    {
                        "name": "Shu Zhong"
                    },
                    {
                        "name": "Qinghao Ye"
                    },
                    {
                        "name": "Xuehan Xiong"
                    },
                    {
                        "name": "Lai Wei"
                    }
                ],
                "author_detail": {
                    "name": "Lai Wei"
                },
                "author": "Lai Wei",
                "arxiv_comment": "Code: https://github.com/ByteDance-Seed/AHN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07297v1",
                "updated": "2025-10-08T17:51:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    51,
                    34,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:51:34Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    51,
                    34,
                    2,
                    281,
                    0
                ],
                "title": "Agentic generative AI for media content discovery at the national\n  football league",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic generative AI for media content discovery at the national\n  football league"
                },
                "summary": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines."
                },
                "authors": [
                    {
                        "name": "Henry Wang"
                    },
                    {
                        "name": "Md Sirajus Salekin"
                    },
                    {
                        "name": "Jake Lee"
                    },
                    {
                        "name": "Ross Claytor"
                    },
                    {
                        "name": "Shinan Zhang"
                    },
                    {
                        "name": "Michael Chi"
                    }
                ],
                "author_detail": {
                    "name": "Michael Chi"
                },
                "author": "Michael Chi",
                "arxiv_comment": "13 pages, 7 figures, International Sports Analytics Conference and\n  Exhibition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07293v1",
                "updated": "2025-10-08T17:50:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:50:16Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs"
                },
                "summary": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks."
                },
                "authors": [
                    {
                        "name": "Peize He"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Jiajie Huang"
                    },
                    {
                        "name": "Zehui Lei"
                    },
                    {
                        "name": "Zhuangcheng Gu"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Jiabing Yang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Zhifei Liu"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "26 pages, 23 figures, the code is available at\n  \\url{https://github.com/DabDans/AudioMarathon}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15347v2",
                "updated": "2025-10-08T00:06:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    6,
                    52,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-21T10:20:46Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025 Workshop on Multi-Turn Interactions in Large Language\n  Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04975v3",
                "updated": "2025-10-07T22:07:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    22,
                    7,
                    44,
                    1,
                    280,
                    0
                ],
                "published": "2024-11-07T18:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications"
                },
                "summary": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference"
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Aurick Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Aurick Qiao"
                },
                "author": "Aurick Qiao",
                "arxiv_comment": "NeurIPS 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06415v1",
                "updated": "2025-10-07T19:50:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T19:50:52Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "title": "Enhanced Breakdown Voltage in $β$-Ga$_2$O$_3$ Schottky Barrier\n  Diodes via Fast Neutron Irradiation and Electrothermal Annealing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Breakdown Voltage in $β$-Ga$_2$O$_3$ Schottky Barrier\n  Diodes via Fast Neutron Irradiation and Electrothermal Annealing"
                },
                "summary": "This study investigates the impact of fast neutron irradiation and\npost-radiation electro-thermal annealing on the electrical performance of\n$\\beta$-Ga$_2$O$_3$ Schottky barrier diodes. Devices irradiated with 1 MeV\nneutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation,\nincluding a drastic reduction in on-current and an increase in on-resistance.\nElectrothermal testing, conducted through simultaneous current-voltage (J-V)\nmeasurements and thermal annealing, resulted in significant recovery. After\nfour cycles of electro-thermal testing, the devices demonstrated significant\nimprovements in performance, with a substantial recovery of on-current and a\nreduction in on-resistance compared to the post-radiation condition,\napproaching pre-radiation levels. Most recovery occurred during the first two\ncycles, with diminishing improvements in later cycles, indicating that most\nthermally recoverable traps were mitigated early. Capacitance-voltage (C-V)\nmeasurements revealed a substantial reduction in carrier concentration,\ndecreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first\nelectro-thermal testing cycle, indicating an over 82% reduction. Following the\nthird cycle, the carrier concentration partially recovered to 9.9E15 cm^-3,\nreflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited\na remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a\n~325% improvement) after the first electro-thermal testing, attributed to the\nreduction in carrier concentration by compensating radiation-induced traps.\nSubsequent testing reduced breakdown voltage slightly to 940 V due to partial\nrecovery of carrier concentration, but it remained significantly higher than\npre-radiation levels, highlighting the promise of $\\beta$-Ga$_2$O$_3$ power\ndevices for high-power applications in radiation-intense environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the impact of fast neutron irradiation and\npost-radiation electro-thermal annealing on the electrical performance of\n$\\beta$-Ga$_2$O$_3$ Schottky barrier diodes. Devices irradiated with 1 MeV\nneutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation,\nincluding a drastic reduction in on-current and an increase in on-resistance.\nElectrothermal testing, conducted through simultaneous current-voltage (J-V)\nmeasurements and thermal annealing, resulted in significant recovery. After\nfour cycles of electro-thermal testing, the devices demonstrated significant\nimprovements in performance, with a substantial recovery of on-current and a\nreduction in on-resistance compared to the post-radiation condition,\napproaching pre-radiation levels. Most recovery occurred during the first two\ncycles, with diminishing improvements in later cycles, indicating that most\nthermally recoverable traps were mitigated early. Capacitance-voltage (C-V)\nmeasurements revealed a substantial reduction in carrier concentration,\ndecreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first\nelectro-thermal testing cycle, indicating an over 82% reduction. Following the\nthird cycle, the carrier concentration partially recovered to 9.9E15 cm^-3,\nreflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited\na remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a\n~325% improvement) after the first electro-thermal testing, attributed to the\nreduction in carrier concentration by compensating radiation-induced traps.\nSubsequent testing reduced breakdown voltage slightly to 940 V due to partial\nrecovery of carrier concentration, but it remained significantly higher than\npre-radiation levels, highlighting the promise of $\\beta$-Ga$_2$O$_3$ power\ndevices for high-power applications in radiation-intense environments."
                },
                "authors": [
                    {
                        "name": "Saleh Ahmed Khan"
                    },
                    {
                        "name": "Sudipto Saha"
                    },
                    {
                        "name": "Ahmed Ibreljic"
                    },
                    {
                        "name": "Stephen Margiotta"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Walid Amir"
                    },
                    {
                        "name": "Surajit Chakraborty"
                    },
                    {
                        "name": "Uttam Singisetti"
                    },
                    {
                        "name": "A F M Anhar Uddin Bhuiyan"
                    }
                ],
                "author_detail": {
                    "name": "A F M Anhar Uddin Bhuiyan"
                },
                "author": "A F M Anhar Uddin Bhuiyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06175v1",
                "updated": "2025-10-07T17:35:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:35:28Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "title": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization"
                },
                "summary": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Zhengyang Tong"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05686v1",
                "updated": "2025-10-07T08:43:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    8,
                    43,
                    7,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T08:43:07Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    8,
                    43,
                    7,
                    1,
                    280,
                    0
                ],
                "title": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and\n  Transport Assistant Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and\n  Transport Assistant Deployment"
                },
                "summary": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%)."
                },
                "authors": [
                    {
                        "name": "José Gómez-delaHiz"
                    },
                    {
                        "name": "Mohamed Faten Zhani"
                    },
                    {
                        "name": "Jaime Galán-Jiménez"
                    },
                    {
                        "name": "John Kaippallimalil"
                    }
                ],
                "author_detail": {
                    "name": "John Kaippallimalil"
                },
                "author": "John Kaippallimalil",
                "arxiv_comment": "10 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05529v1",
                "updated": "2025-10-07T02:39:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    39,
                    35,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T02:39:35Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    39,
                    35,
                    1,
                    280,
                    0
                ],
                "title": "H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model\n  Inference"
                },
                "summary": "Autoregressive decoding in large language models (LLMs) requires caching a\ngrowing list of past key-value (KV) pairs, making long-context inference a\nmemory-bound problem. While recent methods have explored quantizing the cache,\nevicting tokens, or using binary sketches for keys (e.g., Loki), these\napproaches often provide an incomplete solution by leaving one component (like\nvalues) uncompressed or by discarding context information. This paper\nintroduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression\nscheme that radically reduces memory usage without sacrificing context. H1B-KV\nrepresents each key vector using a 1-bit binary sketch, enabling\nhardware-friendly bitwise attention, and further compresses value vectors using\n4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter\nLLM to handle an 8k-token context with under 60 MB of cache memory - a 70x\nreduction. We demonstrate that after a lightweight finetuning, H1B-KV matches\nfull-precision performance not only on perplexity benchmarks but also on\ncomplex downstream tasks like mathematical reasoning (GSM8K), multi-task\nunderstanding (MMLU), and code generation (HumanEval). Our results show H1B-KV\nsignificantly outperforms leading quantization (KIVI), token eviction\n(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,\nestablishing it as a robust solution for deploying LLMs in memory-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding in large language models (LLMs) requires caching a\ngrowing list of past key-value (KV) pairs, making long-context inference a\nmemory-bound problem. While recent methods have explored quantizing the cache,\nevicting tokens, or using binary sketches for keys (e.g., Loki), these\napproaches often provide an incomplete solution by leaving one component (like\nvalues) uncompressed or by discarding context information. This paper\nintroduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression\nscheme that radically reduces memory usage without sacrificing context. H1B-KV\nrepresents each key vector using a 1-bit binary sketch, enabling\nhardware-friendly bitwise attention, and further compresses value vectors using\n4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter\nLLM to handle an 8k-token context with under 60 MB of cache memory - a 70x\nreduction. We demonstrate that after a lightweight finetuning, H1B-KV matches\nfull-precision performance not only on perplexity benchmarks but also on\ncomplex downstream tasks like mathematical reasoning (GSM8K), multi-task\nunderstanding (MMLU), and code generation (HumanEval). Our results show H1B-KV\nsignificantly outperforms leading quantization (KIVI), token eviction\n(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,\nestablishing it as a robust solution for deploying LLMs in memory-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Harshil Vejendla"
                    }
                ],
                "author_detail": {
                    "name": "Harshil Vejendla"
                },
                "author": "Harshil Vejendla",
                "arxiv_comment": "MIT URTC 2025 Technical Paper (Oral), 5 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05476v1",
                "updated": "2025-10-07T00:32:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    0,
                    32,
                    45,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T00:32:45Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    0,
                    32,
                    45,
                    1,
                    280,
                    0
                ],
                "title": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided\n  Inter-Node Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided\n  Inter-Node Communications"
                },
                "summary": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages."
                },
                "authors": [
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Jongryool Kim"
                    },
                    {
                        "name": "Byungil Koh"
                    },
                    {
                        "name": "Hoshik Kim"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_doi": "10.1145/3712285.3759816",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759816",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.05476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05373v1",
                "updated": "2025-10-06T21:08:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    21,
                    8,
                    11,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T21:08:11Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    21,
                    8,
                    11,
                    0,
                    279,
                    0
                ],
                "title": "KVLinC : KV Cache Quantization with Hadamard Rotation and Linear\n  Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLinC : KV Cache Quantization with Hadamard Rotation and Linear\n  Correction"
                },
                "summary": "Quantizing the key-value (KV) cache is a promising strategy for improving the\ninference efficiency of large language models (LLMs). However, aggressive\nquantization to very low precision (e.g., 2 bits) introduces significant errors\nin the stored key and value tensors, which propagate through the dot-product\nattention mechanism and ultimately degrade generation quality. To address this,\nwe propose KVLinC, a framework to mitigate attention errors introduced by KV\ncache quantization in the extreme low-precision regime. KVLinC combines a\nHadamard rotation, which reduces quantization error in values, with lightweight\nlinear correction adapters that explicitly compensate for errors introduced by\nquantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3\nmodel families, KVLinC consistently matches or surpasses strong baselines while\nachieving higher KV-cache compression. Furthermore, we implement a custom\nattention kernel that results in upto 2.55x faster inference compared to Flash\nAttention baseline, enabling efficient long-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing the key-value (KV) cache is a promising strategy for improving the\ninference efficiency of large language models (LLMs). However, aggressive\nquantization to very low precision (e.g., 2 bits) introduces significant errors\nin the stored key and value tensors, which propagate through the dot-product\nattention mechanism and ultimately degrade generation quality. To address this,\nwe propose KVLinC, a framework to mitigate attention errors introduced by KV\ncache quantization in the extreme low-precision regime. KVLinC combines a\nHadamard rotation, which reduces quantization error in values, with lightweight\nlinear correction adapters that explicitly compensate for errors introduced by\nquantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3\nmodel families, KVLinC consistently matches or surpasses strong baselines while\nachieving higher KV-cache compression. Furthermore, we implement a custom\nattention kernel that results in upto 2.55x faster inference compared to Flash\nAttention baseline, enabling efficient long-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "14 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05367v1",
                "updated": "2025-10-06T20:54:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    20,
                    54,
                    44,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T20:54:44Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    20,
                    54,
                    44,
                    0,
                    279,
                    0
                ],
                "title": "LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation"
                },
                "summary": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache ."
                },
                "authors": [
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Kaiyuan Deng"
                    },
                    {
                        "name": "Yushu Wu"
                    },
                    {
                        "name": "Zheng Zhan"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Bo Hui"
                    }
                ],
                "author_detail": {
                    "name": "Bo Hui"
                },
                "author": "Bo Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v2",
                "updated": "2025-10-06T17:09:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    9,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation. See code and resources at\nhttps://hdlm-colm.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation. See code and resources at\nhttps://hdlm-colm.github.io/"
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-André Noël"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-André Noël"
                },
                "author": "Pierre-André Noël",
                "arxiv_comment": "Published as a conference paper at COLM 2025 Website:\n  https://hdlm-colm.github.io/",
                "arxiv_journal_ref": "Second Conference on Language Modeling,\n  https://openreview.net/forum?id=rgq9BFXSFl (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19341v2",
                "updated": "2025-10-06T13:23:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    23,
                    4,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-16T09:14:15Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    14,
                    15,
                    1,
                    259,
                    0
                ],
                "title": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks"
                },
                "summary": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework."
                },
                "authors": [
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Peng Qin"
                    },
                    {
                        "name": "Yueyue Zhang"
                    },
                    {
                        "name": "Pao Cheng"
                    },
                    {
                        "name": "Jun Lu"
                    },
                    {
                        "name": "Yifei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Wang"
                },
                "author": "Yifei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04646v1",
                "updated": "2025-10-06T09:49:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    49,
                    14,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T09:49:14Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    49,
                    14,
                    0,
                    279,
                    0
                ],
                "title": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation"
                },
                "summary": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup."
                },
                "authors": [
                    {
                        "name": "Johanna Sommer"
                    },
                    {
                        "name": "John Rachwan"
                    },
                    {
                        "name": "Nils Fleischmann"
                    },
                    {
                        "name": "Stephan Günnemann"
                    },
                    {
                        "name": "Bertrand Charpentier"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Charpentier"
                },
                "author": "Bertrand Charpentier",
                "arxiv_comment": "Accepted at the AI for Science Workshop @ NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04525v1",
                "updated": "2025-10-06T06:30:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T06:30:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion"
                },
                "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers."
                },
                "authors": [
                    {
                        "name": "Satoshi Hayakawa"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Masaaki Imaizumi"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04492v1",
                "updated": "2025-10-06T05:04:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    5,
                    4,
                    57,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T05:04:57Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    5,
                    4,
                    57,
                    0,
                    279,
                    0
                ],
                "title": "Joint Probing and Scheduling for Cache-Aided Hybrid\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Probing and Scheduling for Cache-Aided Hybrid\n  Satellite-Terrestrial Networks"
                },
                "summary": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Sumei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Sumei Sun"
                },
                "author": "Sumei Sun",
                "arxiv_comment": "6 pages, IEEE Global Communications Conference (GLOBECOM), December\n  2025, Taipei, Taiwan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v3",
                "updated": "2025-10-06T04:28:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    28,
                    5,
                    0,
                    279,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration. Code is available at\nhttps://github.com/aSleepyTree/EB-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration. Code is available at\nhttps://github.com/aSleepyTree/EB-Cache."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04476v1",
                "updated": "2025-10-06T04:24:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    24,
                    23,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T04:24:23Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    4,
                    24,
                    23,
                    0,
                    279,
                    0
                ],
                "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed\n  Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressed Convolutional Attention: Efficient Attention in a Compressed\n  Latent Space"
                },
                "summary": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x."
                },
                "authors": [
                    {
                        "name": "Tomas Figliolia"
                    },
                    {
                        "name": "Nicholas Alonso"
                    },
                    {
                        "name": "Rishi Iyer"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Beren Millidge"
                    }
                ],
                "author_detail": {
                    "name": "Beren Millidge"
                },
                "author": "Beren Millidge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.18149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.18149v2",
                "updated": "2025-10-06T02:46:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    2,
                    46,
                    1,
                    0,
                    279,
                    0
                ],
                "published": "2024-03-26T23:17:05Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    23,
                    17,
                    5,
                    1,
                    86,
                    0
                ],
                "title": "Code Generation and Conic Constraints for Model-Predictive Control on\n  Microcontrollers with Conic-TinyMPC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Generation and Conic Constraints for Model-Predictive Control on\n  Microcontrollers with Conic-TinyMPC"
                },
                "summary": "Model-predictive control (MPC) is a powerful framework for controlling\ndynamic systems under constraints, but it remains challenging to deploy on\nresource-constrained platforms, especially for problems involving conic\nconstraints. To address this, we extend recent work developing fast,\nstructure-exploiting, cached ADMM solvers for embedded applications, to provide\nsupport for second-order cones, as well as C++ code generation from Python,\nMATLAB, and Julia for easy deployment. Microcontroller benchmarks show that our\nsolver provides up to a two-order-of-magnitude speedup, ranging from 10.6x to\n142.7x, over state-of-the-art embedded solvers on QP and SOCP problems, and\nenables us to fit order-of-magnitude larger problems in memory. We validate our\nsolver's deployed performance through simulation and hardware experiments,\nincluding conically-constrained trajectory tracking on a 27g Crazyflie\nquadrotor. To get started with Conic-TinyMPC, visit our documentation,\nexamples, and the open-source codebase at https://tinympc.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-predictive control (MPC) is a powerful framework for controlling\ndynamic systems under constraints, but it remains challenging to deploy on\nresource-constrained platforms, especially for problems involving conic\nconstraints. To address this, we extend recent work developing fast,\nstructure-exploiting, cached ADMM solvers for embedded applications, to provide\nsupport for second-order cones, as well as C++ code generation from Python,\nMATLAB, and Julia for easy deployment. Microcontroller benchmarks show that our\nsolver provides up to a two-order-of-magnitude speedup, ranging from 10.6x to\n142.7x, over state-of-the-art embedded solvers on QP and SOCP problems, and\nenables us to fit order-of-magnitude larger problems in memory. We validate our\nsolver's deployed performance through simulation and hardware experiments,\nincluding conically-constrained trajectory tracking on a 27g Crazyflie\nquadrotor. To get started with Conic-TinyMPC, visit our documentation,\nexamples, and the open-source codebase at https://tinympc.org."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Khai Nguyen"
                    },
                    {
                        "name": "Sam Schoedel"
                    },
                    {
                        "name": "Elakhya Nedumaran"
                    },
                    {
                        "name": "Moises Mata"
                    },
                    {
                        "name": "Brian Plancher"
                    },
                    {
                        "name": "Zachary Manchester"
                    }
                ],
                "author_detail": {
                    "name": "Zachary Manchester"
                },
                "author": "Zachary Manchester",
                "arxiv_comment": "First three authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.18149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.18149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v6",
                "updated": "2025-10-05T22:17:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    22,
                    17,
                    34,
                    6,
                    278,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "15 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00384v2",
                "updated": "2025-10-05T21:29:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    21,
                    29,
                    28,
                    6,
                    278,
                    0
                ],
                "published": "2025-05-31T04:27:22Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    4,
                    27,
                    22,
                    5,
                    151,
                    0
                ],
                "title": "Learning Semantics, Not Addresses: Runtime Neural Prefetching for Far\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Semantics, Not Addresses: Runtime Neural Prefetching for Far\n  Memory"
                },
                "summary": "Memory prefetching has long boosted CPU caches and is increasingly vital for\nfar-memory systems, where large portions of memory are offloaded to cheaper,\nremote tiers. While effective prefetching requires accurate prediction of\nfuture accesses, prior ML approaches have been limited to simulation or\nsmall-scale hardware. We introduce FarSight, the first Linux-based far-memory\nsystem to leverage deep learning by decoupling application semantics from\nruntime memory layout. This separation enables offline-trained models to\npredict access patterns over a compact ordinal vocabulary, which are resolved\nat runtime through lightweight mappings. Across four data-intensive workloads,\nFarSight delivers up to 3.6x higher performance than the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory prefetching has long boosted CPU caches and is increasingly vital for\nfar-memory systems, where large portions of memory are offloaded to cheaper,\nremote tiers. While effective prefetching requires accurate prediction of\nfuture accesses, prior ML approaches have been limited to simulation or\nsmall-scale hardware. We introduce FarSight, the first Linux-based far-memory\nsystem to leverage deep learning by decoupling application semantics from\nruntime memory layout. This separation enables offline-trained models to\npredict access patterns over a compact ordinal vocabulary, which are resolved\nat runtime through lightweight mappings. Across four data-intensive workloads,\nFarSight delivers up to 3.6x higher performance than the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Yutong Huang"
                    },
                    {
                        "name": "Zhiyuan Guo"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v2",
                "updated": "2025-10-05T18:13:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    18,
                    13,
                    39,
                    6,
                    278,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model has emerged as one of the most widely\nadopted generative models. However, serving diffusion models at the granularity\nof entire images introduces significant challenges, particularly under\nmulti-resolution workloads. First, image-level serving obstructs batching\nacross requests. Second, heterogeneous resolutions exhibit distinct locality\ncharacteristics, making it difficult to apply a uniform cache policy\neffectively.\n  To address these challenges, we present PatchedServe, a Patch Management\nFramework for SLO-Optimized Hybrid-Resolution Diffusion Serving. PatchedServe\nis the first SLO-optimized T2I diffusion serving framework designed to handle\nheterogeneous resolutions. Specifically, it incorporates a novel patch-based\nprocessing workflow that substantially improves throughput for\nhybrid-resolution inputs. Moreover, PatchedServe devises a patch-level cache\nreuse policy to fully exploit diffusion redundancies and integrates an\nSLO-aware scheduling algorithm with lightweight online latency prediction to\nimprove responsiveness. Our evaluation demonstrates that PatchedServe achieves\n30.1 % higher SLO satisfaction than the state-of-the-art diffusion serving\nsystem, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model has emerged as one of the most widely\nadopted generative models. However, serving diffusion models at the granularity\nof entire images introduces significant challenges, particularly under\nmulti-resolution workloads. First, image-level serving obstructs batching\nacross requests. Second, heterogeneous resolutions exhibit distinct locality\ncharacteristics, making it difficult to apply a uniform cache policy\neffectively.\n  To address these challenges, we present PatchedServe, a Patch Management\nFramework for SLO-Optimized Hybrid-Resolution Diffusion Serving. PatchedServe\nis the first SLO-optimized T2I diffusion serving framework designed to handle\nheterogeneous resolutions. Specifically, it incorporates a novel patch-based\nprocessing workflow that substantially improves throughput for\nhybrid-resolution inputs. Moreover, PatchedServe devises a patch-level cache\nreuse policy to fully exploit diffusion redundancies and integrates an\nSLO-aware scheduling algorithm with lightweight online latency prediction to\nimprove responsiveness. Our evaluation demonstrates that PatchedServe achieves\n30.1 % higher SLO satisfaction than the state-of-the-art diffusion serving\nsystem, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04188v1",
                "updated": "2025-10-05T13:01:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    13,
                    1,
                    8,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T13:01:08Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    13,
                    1,
                    8,
                    6,
                    278,
                    0
                ],
                "title": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for\n  Diffusion Transformers"
                },
                "summary": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining."
                },
                "authors": [
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Guantao Chen"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05176v1",
                "updated": "2025-10-05T12:09:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    12,
                    9,
                    14,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T12:09:14Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    12,
                    9,
                    14,
                    6,
                    278,
                    0
                ],
                "title": "PatternKV: Flattening KV Representation Expands Quantization Headroom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatternKV: Flattening KV Representation Expands Quantization Headroom"
                },
                "summary": "KV cache in autoregressive LLMs eliminates redundant recomputation but has\nemerged as the dominant memory and bandwidth bottleneck during inference,\nnotably with long contexts and test-time scaling. KV quantization is a key\nlever for reducing cache cost, but accuracy drops sharply as the native KV\ndistribution lacks flatness and thus maintains a wide quantization range. Prior\nwork focuses on isolating outliers, which caps their error but fails to flatten\nthe overall distribution, leaving performance fragile under low-bit settings.\nIn this work, we show that the K cache maintains a stable structure that\nevolves gradually with context, while the V cache carries latent semantic\nregularities. Building on these insights, we propose PatternKV, a\npattern-aligned residual quantization scheme. It mines representative pattern\nvectors online, aligns each KV vector to its nearest pattern, and quantizes\nonly the residual. This reshaping of the KV distribution flattens the\nquantization target and narrows its range, thereby improving the fidelity of\nlow-bit KV quantization. Across long-context and test-time scaling settings on\nmultiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08%\naverage 4-bit drop relative to FP16, improves test-time scaling accuracy by 10%\non average, and raises throughput by 1.4x while supporting 1.25x larger\nbatches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache in autoregressive LLMs eliminates redundant recomputation but has\nemerged as the dominant memory and bandwidth bottleneck during inference,\nnotably with long contexts and test-time scaling. KV quantization is a key\nlever for reducing cache cost, but accuracy drops sharply as the native KV\ndistribution lacks flatness and thus maintains a wide quantization range. Prior\nwork focuses on isolating outliers, which caps their error but fails to flatten\nthe overall distribution, leaving performance fragile under low-bit settings.\nIn this work, we show that the K cache maintains a stable structure that\nevolves gradually with context, while the V cache carries latent semantic\nregularities. Building on these insights, we propose PatternKV, a\npattern-aligned residual quantization scheme. It mines representative pattern\nvectors online, aligns each KV vector to its nearest pattern, and quantizes\nonly the residual. This reshaping of the KV distribution flattens the\nquantization target and narrows its range, thereby improving the fidelity of\nlow-bit KV quantization. Across long-context and test-time scaling settings on\nmultiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08%\naverage 4-bit drop relative to FP16, improves test-time scaling accuracy by 10%\non average, and raises throughput by 1.4x while supporting 1.25x larger\nbatches."
                },
                "authors": [
                    {
                        "name": "Ji Zhang"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Jiayi Shi"
                    },
                    {
                        "name": "Yueqi Zhang"
                    },
                    {
                        "name": "Chuyi Tan"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04153v1",
                "updated": "2025-10-05T11:09:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    11,
                    9,
                    10,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T11:09:10Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    11,
                    9,
                    10,
                    6,
                    278,
                    0
                ],
                "title": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy\n  Preservation"
                },
                "summary": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have gained significant popularity due to their remarkable\ncapabilities in image generation, albeit at the cost of intensive computation\nrequirement. Meanwhile, despite their widespread deployment in inference\nservices such as Midjourney, concerns about the potential leakage of sensitive\ninformation in uploaded user prompts have arisen. Existing solutions either\nlack rigorous privacy guarantees or fail to strike an effective balance between\nutility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play\nsafeguard that enables oblivious cloud-device hybrid generation. By oblivious,\neach input prompt is transformed into a set of semantically similar candidate\nprompts that differ only in sensitive attributes (e.g., gender, ethnicity). The\ncloud server processes all candidate prompts without knowing which one is the\nreal one, thus preventing any prompt leakage. To mitigate server cost, only a\nsmall portion of denoising steps is performed upon the large cloud model. The\nintermediate latents are then sent back to the client, which selects the\ntargeted latent and completes the remaining denoising using a small device\nmodel. Additionally, we analyze and incorporate several cache-based\naccelerations that leverage temporal and batch redundancy, effectively reducing\ncomputation cost with minimal utility degradation. Extensive experiments across\nmultiple datasets demonstrate that ObCLIP provides rigorous privacy and\ncomparable utility to cloud models with slightly increased server cost."
                },
                "authors": [
                    {
                        "name": "Haoqi Wu"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Ming Xu"
                    },
                    {
                        "name": "Li Wang"
                    },
                    {
                        "name": "Qiang Yan"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yan"
                },
                "author": "Qiang Yan",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v3",
                "updated": "2025-10-05T08:34:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    8,
                    34,
                    30,
                    6,
                    278,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xudong Wang"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04033v1",
                "updated": "2025-10-05T04:52:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    5,
                    4,
                    52,
                    26,
                    6,
                    278,
                    0
                ],
                "published": "2025-10-05T04:52:26Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    4,
                    52,
                    26,
                    6,
                    278,
                    0
                ],
                "title": "A global log for medical AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A global log for medical AI"
                },
                "summary": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern computer systems often rely on syslog, a simple, universal protocol\nthat records every critical event across heterogeneous infrastructure. However,\nhealthcare's rapidly growing clinical AI stack has no equivalent. As hospitals\nrush to pilot large language models and other AI-based clinical decision\nsupport tools, we still lack a standard way to record how, when, by whom, and\nfor whom these AI models are used. Without that transparency and visibility, it\nis challenging to measure real-world performance and outcomes, detect adverse\nevents, or correct bias or dataset drift. In the spirit of syslog, we introduce\nMedLog, a protocol for event-level logging of clinical AI. Any time an AI model\nis invoked to interact with a human, interface with another algorithm, or act\nindependently, a MedLog record is created. This record consists of nine core\nfields: header, model, user, target, inputs, artifacts, outputs, outcomes, and\nfeedback, providing a structured and consistent record of model activity. To\nencourage early adoption, especially in low-resource settings, and minimize the\ndata footprint, MedLog supports risk-based sampling, lifecycle-aware retention\npolicies, and write-behind caching; detailed traces for complex, agentic, or\nmulti-stage workflows can also be captured under MedLog. MedLog can catalyze\nthe development of new databases and software to store and analyze MedLog\nrecords. Realizing this vision would enable continuous surveillance, auditing,\nand iterative improvement of medical AI, laying the foundation for a new form\nof digital epidemiology."
                },
                "authors": [
                    {
                        "name": "Ayush Noori"
                    },
                    {
                        "name": "Adam Rodman"
                    },
                    {
                        "name": "Alan Karthikesalingam"
                    },
                    {
                        "name": "Bilal A. Mateen"
                    },
                    {
                        "name": "Christopher A. Longhurst"
                    },
                    {
                        "name": "Daniel Yang"
                    },
                    {
                        "name": "Dave deBronkart"
                    },
                    {
                        "name": "Gauden Galea"
                    },
                    {
                        "name": "Harold F. Wolf III"
                    },
                    {
                        "name": "Jacob Waxman"
                    },
                    {
                        "name": "Joshua C. Mandel"
                    },
                    {
                        "name": "Juliana Rotich"
                    },
                    {
                        "name": "Kenneth D. Mandl"
                    },
                    {
                        "name": "Maryam Mustafa"
                    },
                    {
                        "name": "Melissa Miles"
                    },
                    {
                        "name": "Nigam H. Shah"
                    },
                    {
                        "name": "Peter Lee"
                    },
                    {
                        "name": "Robert Korom"
                    },
                    {
                        "name": "Scott Mahoney"
                    },
                    {
                        "name": "Seth Hain"
                    },
                    {
                        "name": "Tien Yin Wong"
                    },
                    {
                        "name": "Trevor Mundel"
                    },
                    {
                        "name": "Vivek Natarajan"
                    },
                    {
                        "name": "Noa Dagan"
                    },
                    {
                        "name": "David A. Clifton"
                    },
                    {
                        "name": "Ran D. Balicer"
                    },
                    {
                        "name": "Isaac S. Kohane"
                    },
                    {
                        "name": "Marinka Zitnik"
                    }
                ],
                "author_detail": {
                    "name": "Marinka Zitnik"
                },
                "author": "Marinka Zitnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03851v1",
                "updated": "2025-10-04T15:52:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    52,
                    31,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T15:52:31Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    52,
                    31,
                    5,
                    277,
                    0
                ],
                "title": "Algorithm Generation via Creative Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm Generation via Creative Ideation"
                },
                "summary": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing system algorithms remains challenging, where the discontinuous\nnature of the solution space often forces system engineers to rely on generic\nheuristics at the expense of performance. We study whether LLMs can practically\ndrive algorithm generation, and find that they are biased towards well-known\ngeneric designs, rather than making the creative leaps needed to navigate the\ndiscontinuous solution space. To address this limitation, we introduce\nMetaMuse, a framework for creative ideation built on three self-reflection\nprinciples: (1) quantifying solution diversity and usefulness in measurable\nperformance space, rather than abstract idea space, (2) steering ideation\nthrough external stimuli, rather than internal randomness, and (3) constructing\nexecutable solutions using waypoint reasoning, rather than free-form\nchain-of-thought. Extensive evaluation shows that MetaMuse can generate\nhigh-performing solutions for two critical problems at a global cloud provider:\ncache replacement (reducing cache misses by up to 35.76%) and online bin\npacking (reducing bin usage by up to 30.93%)."
                },
                "authors": [
                    {
                        "name": "Ruiying Ma"
                    },
                    {
                        "name": "Chieh-Jan Mike Liang"
                    },
                    {
                        "name": "Yanjie Gao"
                    },
                    {
                        "name": "Francis Y. Yan"
                    }
                ],
                "author_detail": {
                    "name": "Francis Y. Yan"
                },
                "author": "Francis Y. Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03834v1",
                "updated": "2025-10-04T15:25:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    25,
                    4,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T15:25:04Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    15,
                    25,
                    4,
                    5,
                    277,
                    0
                ],
                "title": "Hybrid MBE Route to Adsorption-Controlled Growth of BaTiO3 Membranes\n  with Robust Polarization Switching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid MBE Route to Adsorption-Controlled Growth of BaTiO3 Membranes\n  with Robust Polarization Switching"
                },
                "summary": "Freestanding ferroelectric membranes are promising for flexible electronics,\nnonvolatile memory, photonics, and spintronics, but their synthesis is\nchallenged by the need for reproducibility with precise stoichiometric control.\nHere, we demonstrate the adsorption-controlled growth of single-crystalline,\nepitaxial BaTiO3 films by hybrid molecular beam epitaxy (MBE) on a binary oxide\nsacrificial layer. Using a simple water-droplet lift-off method, we obtained\nsubmillimeter- to millimeter-sized membranes that retained crystallinity, as\nconfirmed by high-resolution X-ray diffraction, and exhibited robust tetragonal\nsymmetry by Raman spectroscopy. Impedance spectroscopy confirmed a high\ndielectric constant of 1340, reflecting the robust dielectric response of the\nmembranes. Ferroelectric functionality was revealed by piezoresponse force\nmicroscopy (PFM) and further verified by polarization-electric field (P-E) loop\nmeasurements with Positive-Up-Negative-Down (PUND). The P-E loops exhibited a\nremnant polarization of 5 microC cm-2 and a coercive field of 63 kV cm-1. These\nresults were interpreted in relation to c- and a-domain configurations. These\nresults establish hybrid MBE as a generalizable route for producing\nstoichiometry-controlled ferroelectric membranes, enabling their integration\ninto next-generation flexible and multifunctional quantum oxide devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Freestanding ferroelectric membranes are promising for flexible electronics,\nnonvolatile memory, photonics, and spintronics, but their synthesis is\nchallenged by the need for reproducibility with precise stoichiometric control.\nHere, we demonstrate the adsorption-controlled growth of single-crystalline,\nepitaxial BaTiO3 films by hybrid molecular beam epitaxy (MBE) on a binary oxide\nsacrificial layer. Using a simple water-droplet lift-off method, we obtained\nsubmillimeter- to millimeter-sized membranes that retained crystallinity, as\nconfirmed by high-resolution X-ray diffraction, and exhibited robust tetragonal\nsymmetry by Raman spectroscopy. Impedance spectroscopy confirmed a high\ndielectric constant of 1340, reflecting the robust dielectric response of the\nmembranes. Ferroelectric functionality was revealed by piezoresponse force\nmicroscopy (PFM) and further verified by polarization-electric field (P-E) loop\nmeasurements with Positive-Up-Negative-Down (PUND). The P-E loops exhibited a\nremnant polarization of 5 microC cm-2 and a coercive field of 63 kV cm-1. These\nresults were interpreted in relation to c- and a-domain configurations. These\nresults establish hybrid MBE as a generalizable route for producing\nstoichiometry-controlled ferroelectric membranes, enabling their integration\ninto next-generation flexible and multifunctional quantum oxide devices."
                },
                "authors": [
                    {
                        "name": "S. Choo"
                    },
                    {
                        "name": "S. Varshney"
                    },
                    {
                        "name": "J. Shah"
                    },
                    {
                        "name": "A. K. Manjeshwar"
                    },
                    {
                        "name": "D. K. Lee"
                    },
                    {
                        "name": "K. A. Mkhoyan"
                    },
                    {
                        "name": "R. D. James"
                    },
                    {
                        "name": "B. Jalan"
                    }
                ],
                "author_detail": {
                    "name": "B. Jalan"
                },
                "author": "B. Jalan",
                "arxiv_comment": "22 pages 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03712v1",
                "updated": "2025-10-04T07:22:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "published": "2025-10-04T07:22:39Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems"
                },
                "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization."
                },
                "authors": [
                    {
                        "name": "Jahidul Arafat"
                    },
                    {
                        "name": "Kh. M. Moniruzzaman"
                    },
                    {
                        "name": "Shamim Hossain"
                    },
                    {
                        "name": "Fariha Tasmin"
                    },
                    {
                        "name": "Kamrujjaman"
                    },
                    {
                        "name": "Ahsan Habib Tareq"
                    }
                ],
                "author_detail": {
                    "name": "Ahsan Habib Tareq"
                },
                "author": "Ahsan Habib Tareq",
                "arxiv_comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M15, 90B25, 68T05, 90C29",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; C.2.4; D.2.5; D.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v3",
                "updated": "2025-10-04T05:59:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    5,
                    59,
                    1,
                    5,
                    277,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08134v3",
                "updated": "2025-10-04T05:28:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    5,
                    28,
                    39,
                    5,
                    277,
                    0
                ],
                "published": "2025-08-11T16:10:00Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    16,
                    10,
                    0,
                    0,
                    223,
                    0
                ],
                "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided\n  Region Control"
                },
                "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement."
                },
                "authors": [
                    {
                        "name": "Zeqian Long"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Kunyu Feng"
                    },
                    {
                        "name": "Xinhua Zhang"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Harry Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Project webpage is available at https://follow-your-shape.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05370v2",
                "updated": "2025-10-04T03:45:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    4,
                    3,
                    45,
                    40,
                    5,
                    277,
                    0
                ],
                "published": "2025-02-07T22:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "Taming Latency-Memory Trade-Off in MoE-Based LLM Serving via\n  Fine-Grained Expert Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Latency-Memory Trade-Off in MoE-Based LLM Serving via\n  Fine-Grained Expert Offloading"
                },
                "summary": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs.\n  To tame the latency-memory trade-off in MoE serving, we present FineMoE, a\nfine-grained expert offloading system for MoE serving that achieves low\ninference latency with memory efficiency. We design FineMoE to extract\nfine-grained expert selection patterns from MoE models and semantic hints from\ninput prompts to efficiently guide expert prefetching, caching, and offloading\ndecisions. FineMoE is prototyped on top of HuggingFace Transformers and\ndeployed on a six-GPU testbed. Experiments with open-source MoE models and\nreal-world workloads show that FineMoE reduces inference latency by 47% and\nimproves expert hit rate by 39% over state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs.\n  To tame the latency-memory trade-off in MoE serving, we present FineMoE, a\nfine-grained expert offloading system for MoE serving that achieves low\ninference latency with memory efficiency. We design FineMoE to extract\nfine-grained expert selection patterns from MoE models and semantic hints from\ninput prompts to efficiently guide expert prefetching, caching, and offloading\ndecisions. FineMoE is prototyped on top of HuggingFace Transformers and\ndeployed on a six-GPU testbed. Experiments with open-source MoE models and\nreal-world workloads show that FineMoE reduces inference latency by 47% and\nimproves expert hit rate by 39% over state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Xingqi Cui"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03215v1",
                "updated": "2025-10-03T17:52:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:52:32Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    52,
                    32,
                    4,
                    276,
                    0
                ],
                "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models"
                },
                "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C."
                },
                "authors": [
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Zihan Min"
                    },
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Jichao Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03198v1",
                "updated": "2025-10-03T17:35:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    16,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T17:35:16Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    17,
                    35,
                    16,
                    4,
                    276,
                    0
                ],
                "title": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation\n  on Minecraft",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation\n  on Minecraft"
                },
                "summary": "Autoregressive video diffusion models have proved effective for world\nmodeling and interactive scene generation, with Minecraft gameplay as a\nrepresentative application. To faithfully simulate play, a model must generate\nnatural content while exploring new scenes and preserve spatial consistency\nwhen revisiting explored areas. Under limited computation budgets, it must\ncompress and exploit historical cues within a finite context window, which\nexposes a trade-off: Temporal-only memory lacks long-term spatial consistency,\nwhereas adding spatial memory strengthens consistency but may degrade new scene\ngeneration quality when the model over-relies on insufficient spatial context.\nWe present Memory Forcing, a learning framework that pairs training protocols\nwith a geometry-indexed spatial memory. Hybrid Training exposes distinct\ngameplay regimes, guiding the model to rely on temporal memory during\nexploration and incorporate spatial memory for revisits. Chained Forward\nTraining extends autoregressive training with model rollouts, where chained\npredictions create larger pose variations and encourage reliance on spatial\nmemory for maintaining consistency. Point-to-Frame Retrieval efficiently\nretrieves history by mapping currently visible points to their source frames,\nwhile Incremental 3D Reconstruction maintains and updates an explicit 3D cache.\nExtensive experiments demonstrate that Memory Forcing achieves superior\nlong-term spatial consistency and generative quality across diverse\nenvironments, while maintaining computational efficiency for extended\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive video diffusion models have proved effective for world\nmodeling and interactive scene generation, with Minecraft gameplay as a\nrepresentative application. To faithfully simulate play, a model must generate\nnatural content while exploring new scenes and preserve spatial consistency\nwhen revisiting explored areas. Under limited computation budgets, it must\ncompress and exploit historical cues within a finite context window, which\nexposes a trade-off: Temporal-only memory lacks long-term spatial consistency,\nwhereas adding spatial memory strengthens consistency but may degrade new scene\ngeneration quality when the model over-relies on insufficient spatial context.\nWe present Memory Forcing, a learning framework that pairs training protocols\nwith a geometry-indexed spatial memory. Hybrid Training exposes distinct\ngameplay regimes, guiding the model to rely on temporal memory during\nexploration and incorporate spatial memory for revisits. Chained Forward\nTraining extends autoregressive training with model rollouts, where chained\npredictions create larger pose variations and encourage reliance on spatial\nmemory for maintaining consistency. Point-to-Frame Retrieval efficiently\nretrieves history by mapping currently visible points to their source frames,\nwhile Incremental 3D Reconstruction maintains and updates an explicit 3D cache.\nExtensive experiments demonstrate that Memory Forcing achieves superior\nlong-term spatial consistency and generative quality across diverse\nenvironments, while maintaining computational efficiency for extended\nsequences."
                },
                "authors": [
                    {
                        "name": "Junchao Huang"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Boyao Han"
                    },
                    {
                        "name": "Shaoshuai Shi"
                    },
                    {
                        "name": "Zhuotao Tian"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Li Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Li Jiang"
                },
                "author": "Li Jiang",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14837v2",
                "updated": "2025-10-03T15:37:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    15,
                    37,
                    19,
                    4,
                    276,
                    0
                ],
                "published": "2025-02-20T18:50:42Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    50,
                    42,
                    3,
                    51,
                    0
                ],
                "title": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent\n  Attention in Any Transformer-based LLMs"
                },
                "summary": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Latent Attention (MLA) is an innovative architecture proposed by\nDeepSeek, designed to ensure efficient and economical inference by\nsignificantly compressing the Key-Value (KV) cache into a latent vector.\nCompared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its\nvariants such as Grouped-Query Attention (GQA) exhibit significant cost\ndisadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA\nwithout pre-training from scratch is both meaningful and challenging. This\npaper proposes the first data-efficient fine-tuning method for transitioning\nfrom MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE,\nwe remove RoPE from dimensions of queries and keys that contribute less to the\nattention scores, for low-rank approximation, we introduce joint SVD\napproximations based on the pre-trained parameters of keys and values. These\ncarefully designed strategies enable MHA2MLA to recover performance using only\na small fraction (0.3% to 0.6%) of the data, significantly reducing inference\ncosts while seamlessly integrating with compression techniques such as KV cache\nquantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%,\nwith only a 0.5% drop in LongBench performance."
                },
                "authors": [
                    {
                        "name": "Tao Ji"
                    },
                    {
                        "name": "Bin Guo"
                    },
                    {
                        "name": "Yuanbin Wu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Lixing Shen"
                    },
                    {
                        "name": "Zhan Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    }
                ],
                "author_detail": {
                    "name": "Tao Gui"
                },
                "author": "Tao Gui",
                "arxiv_comment": "16 pages, 8 figures; Accepted to ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02866v1",
                "updated": "2025-10-03T10:06:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    6,
                    44,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T10:06:44Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    10,
                    6,
                    44,
                    4,
                    276,
                    0
                ],
                "title": "Life Estimation of HVDC Cable Insulation under Load Cycles: from\n  Macroscopic to Microscopic Charge Conduction Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Life Estimation of HVDC Cable Insulation under Load Cycles: from\n  Macroscopic to Microscopic Charge Conduction Modelling"
                },
                "summary": "This paper goes one step forward in the life estimation of HVDC cable\ninsulation under load cycles by introducing for the first time a microscopic\nmodel of charge conduction and transport i.e., Bipolar Charge Transport BCT\nmodel for electric field calculation inside the insulation thickness. The paper\nfirstly includes the development and the validation of BCT model with that\nfound in literature. Then, the parameters of the developed BCT model are\noptimized using Pulsed Electro-Acoustic PEA space charge measurements. Followed\nby the integration of the developed, validated and optimized model into the\nelectric field calculation for life estimation of a 500 kV DC-XLPE insulated\ncable subjected to Type Test load cycles according to Cigre Techical Brochure\n852. The developed microscopic model is compared to the macroscopic models\nalready found in the literature. The microscopic model shows a comparable\nelectric field inversion similarly to macroscopic models. However, the behavior\nof the microscopic model is noticed to be different under heating and cooling\nload cycles. In hot cable, the maximum electric field stabilizes at different\namplitude and position inside the insulation thickness in both models. This\ninvestigation has been carried out in the framework of the HEU-NEWGEN research\nproject.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper goes one step forward in the life estimation of HVDC cable\ninsulation under load cycles by introducing for the first time a microscopic\nmodel of charge conduction and transport i.e., Bipolar Charge Transport BCT\nmodel for electric field calculation inside the insulation thickness. The paper\nfirstly includes the development and the validation of BCT model with that\nfound in literature. Then, the parameters of the developed BCT model are\noptimized using Pulsed Electro-Acoustic PEA space charge measurements. Followed\nby the integration of the developed, validated and optimized model into the\nelectric field calculation for life estimation of a 500 kV DC-XLPE insulated\ncable subjected to Type Test load cycles according to Cigre Techical Brochure\n852. The developed microscopic model is compared to the macroscopic models\nalready found in the literature. The microscopic model shows a comparable\nelectric field inversion similarly to macroscopic models. However, the behavior\nof the microscopic model is noticed to be different under heating and cooling\nload cycles. In hot cable, the maximum electric field stabilizes at different\namplitude and position inside the insulation thickness in both models. This\ninvestigation has been carried out in the framework of the HEU-NEWGEN research\nproject."
                },
                "authors": [
                    {
                        "name": "Bassel Diban"
                    },
                    {
                        "name": "Giovanni Mazzanti"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Mazzanti"
                },
                "author": "Giovanni Mazzanti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02758v1",
                "updated": "2025-10-03T06:43:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    24,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:43:24Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    43,
                    24,
                    4,
                    276,
                    0
                ],
                "title": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via\n  Preemptive Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via\n  Preemptive Scheduling"
                },
                "summary": "Real-time LLM interactions demand streamed token generations, where text\ntokens are progressively generated and delivered to users while balancing two\nobjectives: responsiveness (i.e., low time-to-first-token) and steady\ngeneration (i.e.,required time-between-tokens). Standard LLM serving systems\nsuffer from the inflexibility caused by non-preemptive request scheduling and\nreactive memory management, leading to poor resource utilization and low\nrequest processing parallelism under request bursts. Therefore, we present\nTokenFlow, a novel LLM serving system with enhanced text streaming performance\nvia preemptive request scheduling and proactive key-value (KV) cache\nmanagement. TokenFlow dynamically prioritizes requests based on real-time token\nbuffer occupancy and token consumption rate, while actively transferring KV\ncache between GPU and CPU memory in the background and overlapping I/O with\ncomputation to minimize request preemption overhead. Extensive experiments on\nLlama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)\ndemonstrate that TokenFlow achieves up to 82.5% higher effective throughput\n(accounting for actual user consumption) while reducing P99 TTFT by up to\n80.2%, without degrading overall token throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time LLM interactions demand streamed token generations, where text\ntokens are progressively generated and delivered to users while balancing two\nobjectives: responsiveness (i.e., low time-to-first-token) and steady\ngeneration (i.e.,required time-between-tokens). Standard LLM serving systems\nsuffer from the inflexibility caused by non-preemptive request scheduling and\nreactive memory management, leading to poor resource utilization and low\nrequest processing parallelism under request bursts. Therefore, we present\nTokenFlow, a novel LLM serving system with enhanced text streaming performance\nvia preemptive request scheduling and proactive key-value (KV) cache\nmanagement. TokenFlow dynamically prioritizes requests based on real-time token\nbuffer occupancy and token consumption rate, while actively transferring KV\ncache between GPU and CPU memory in the background and overlapping I/O with\ncomputation to minimize request preemption overhead. Extensive experiments on\nLlama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)\ndemonstrate that TokenFlow achieves up to 82.5% higher effective throughput\n(accounting for actual user consumption) while reducing P99 TTFT by up to\n80.2%, without degrading overall token throughput."
                },
                "authors": [
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Chuheng Du"
                    },
                    {
                        "name": "Renyuan Liu"
                    },
                    {
                        "name": "Shuochao Yao"
                    },
                    {
                        "name": "Dingtian Yan"
                    },
                    {
                        "name": "Jiang Liao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Accepted by EuroSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02750v1",
                "updated": "2025-10-03T06:27:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    27,
                    33,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-03T06:27:33Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    6,
                    27,
                    33,
                    4,
                    276,
                    0
                ],
                "title": "Bayesian Test-time Adaptation for Object Recognition and Detection with\n  Vision-language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Test-time Adaptation for Object Recognition and Detection with\n  Vision-language Models"
                },
                "summary": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved\nremarkable success in object recognition and detection. However, their\nperformance often degrades under real-world distribution shifts. Test-time\nadaptation (TTA) aims to mitigate this issue by adapting models during\ninference. Existing methods either rely on computationally expensive\nbackpropagation, which hinders real-time deployment, or focus solely on\nlikelihood adaptation, which overlooks the critical role of the prior. Our\nprior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for\nobject recognition by introducing a training-free framework that incorporates\nadaptive priors. Building upon this foundation, we now present Bayesian Class\nAdaptation plus (BCA+), a unified, training-free framework for TTA for both\nobject recognition and detection. BCA+ introduces a dynamic cache that\nadaptively stores and updates class embeddings, spatial scales (for detection),\nand, crucially, adaptive class priors derived from historical predictions. We\nformulate adaptation as a Bayesian inference problem, where final predictions\nare generated by fusing the initial VLM output with a cache-based prediction.\nThis cache-based prediction combines a dynamically updated likelihood\n(measuring feature and scale similarity) and a prior (reflecting the evolving\nclass distribution). This dual-adaptation mechanism, coupled with\nuncertainty-guided fusion, enables BCA+ to correct both the model's semantic\nunderstanding and its contextual confidence. As a training-free method\nrequiring no backpropagation, BCA+ is highly efficient. Extensive experiments\ndemonstrate that BCA+ achieves state-of-the-art performance on both recognition\nand detection benchmarks."
                },
                "authors": [
                    {
                        "name": "Lihua Zhou"
                    },
                    {
                        "name": "Mao Ye"
                    },
                    {
                        "name": "Shuaifeng Li"
                    },
                    {
                        "name": "Nianxin Li"
                    },
                    {
                        "name": "Jinlin Wu"
                    },
                    {
                        "name": "Xiatian Zhu"
                    },
                    {
                        "name": "Lei Deng"
                    },
                    {
                        "name": "Hongbin Liu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02084v2",
                "updated": "2025-10-03T05:10:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    5,
                    10,
                    2,
                    4,
                    276,
                    0
                ],
                "published": "2025-10-02T14:50:50Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    50,
                    50,
                    3,
                    275,
                    0
                ],
                "title": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAIROS: Unified Training for Universal Non-Autoregressive Time Series\n  Forecasting"
                },
                "summary": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the World Wide Web, reliable time series forecasts provide the\nforward-looking signals that drive resource planning, cache placement, and\nanomaly response, enabling platforms to operate efficiently as user behavior\nand content distributions evolve. Compared with other domains, time series\nforecasting for Web applications requires much faster responsiveness to support\nreal-time decision making. We present KAIROS, a non-autoregressive time series\nforecasting framework that directly models segment-level multi-peak\ndistributions. Unlike autoregressive approaches, KAIROS avoids error\naccumulation and achieves just-in-time inference, while improving over existing\nnon-autoregressive models that collapse to over-smoothed predictions. Trained\non the large-scale corpus, KAIROS demonstrates strong zero-shot generalization\non six widely used benchmarks, delivering forecasting performance comparable to\nstate-of-the-art foundation models with similar scale, at a fraction of their\ninference cost. Beyond empirical results, KAIROS highlights the importance of\nnon-autoregressive design as a scalable paradigm for foundation models in time\nseries."
                },
                "authors": [
                    {
                        "name": "Kuiye Ding"
                    },
                    {
                        "name": "Fanda Fan"
                    },
                    {
                        "name": "Zheya Wang"
                    },
                    {
                        "name": "Hongxiao Li"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Chunjie Luo"
                    },
                    {
                        "name": "Jianfeng Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Zhan"
                },
                "author": "Jianfeng Zhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25188v2",
                "updated": "2025-10-03T00:40:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    3,
                    0,
                    40,
                    49,
                    4,
                    276,
                    0
                ],
                "published": "2025-09-29T17:59:54Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    59,
                    54,
                    0,
                    272,
                    0
                ],
                "title": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Learnable Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Parallel: Accelerating Diffusion Large Language Models via\n  Learnable Parallel Decoding"
                },
                "summary": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding in large language models (LLMs) requires\n$\\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting\ninference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token\ngeneration through iterative denoising. However, current parallel decoding\nstrategies rely on fixed, input-agnostic heuristics (e.g., confidence\nthresholds), which fail to adapt to input-specific characteristics, resulting\nin suboptimal speed-quality trade-offs across diverse NLP tasks. In this work,\nwe explore a more flexible and dynamic approach to parallel decoding. We\npropose Learning to Parallel Decode (Learn2PD), a framework that trains a\nlightweight and adaptive filter model to predict, for each token position,\nwhether the current prediction matches the final output. This learned filter\napproximates an oracle parallel decoding strategy that unmasks tokens only when\ncorrectly predicted. Importantly, the filter model is learned in a\npost-training manner, requiring only a small amount of computation to optimize\nit (minute-level GPU time). Additionally, we introduce End-of-Text Prediction\n(EoTP) to detect decoding completion at the end of sequence, avoiding redundant\ndecoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that\nour method achieves up to 22.58$\\times$ speedup without any performance drop,\nand up to 57.51$\\times$ when combined with KV-Cache."
                },
                "authors": [
                    {
                        "name": "Wenrui Bao"
                    },
                    {
                        "name": "Zhiben Chen"
                    },
                    {
                        "name": "Dan Xu"
                    },
                    {
                        "name": "Yuzhang Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhang Shang"
                },
                "author": "Yuzhang Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02613v1",
                "updated": "2025-10-02T23:16:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    23,
                    16,
                    35,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T23:16:35Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    23,
                    16,
                    35,
                    3,
                    275,
                    0
                ],
                "title": "ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts\n  Models"
                },
                "summary": "Mixture-of-Experts (MoE) models promise efficient scaling of large language\nmodels (LLMs) by activating only a small subset of experts per token, but their\nparallelized inference pipelines make elastic serving challenging. Existing\nstrategies fall short: horizontal scaling provisions entire replicas of the\ncurrent configuration, often tens to hundreds of accelerators, leading to\ncoarse granularity, long provisioning delays, and costly overprovisioning.\nVertical scaling offers finer adjustments but typically requires instance\nrestarts, incurring downtime. These limitations make current approaches\nill-suited for the bursty, short-lived traffic patterns common in cloud\ndeployments.\n  We present ElasticMoE, an elastic scaling framework for MoE LLMs that\nachieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE\ndecouples inference execution from memory operations, enabling scaling steps to\nproceed concurrently with serving. An HBM Management Module (HMM) reuses\nweights and KV caches via zero-copy remapping, while high-bandwidth\npeer-to-peer transfers bring newly added accelerators online without\ninterrupting service. A virtual memory based expert redistribution mechanism\nmigrates MoE experts without costly buffer reallocations, reducing peak memory\nusage during expert parallelism reconfiguration.\n  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that\nElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput\nduring scaling, and significantly improves SLO attainment compared to\nbaselines. By enabling fine-grained, concurrent scaling with minimal\ndisruption, ElasticMoE advances the practicality of deploying massive MoE LLMs\nin dynamic cloud environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models promise efficient scaling of large language\nmodels (LLMs) by activating only a small subset of experts per token, but their\nparallelized inference pipelines make elastic serving challenging. Existing\nstrategies fall short: horizontal scaling provisions entire replicas of the\ncurrent configuration, often tens to hundreds of accelerators, leading to\ncoarse granularity, long provisioning delays, and costly overprovisioning.\nVertical scaling offers finer adjustments but typically requires instance\nrestarts, incurring downtime. These limitations make current approaches\nill-suited for the bursty, short-lived traffic patterns common in cloud\ndeployments.\n  We present ElasticMoE, an elastic scaling framework for MoE LLMs that\nachieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE\ndecouples inference execution from memory operations, enabling scaling steps to\nproceed concurrently with serving. An HBM Management Module (HMM) reuses\nweights and KV caches via zero-copy remapping, while high-bandwidth\npeer-to-peer transfers bring newly added accelerators online without\ninterrupting service. A virtual memory based expert redistribution mechanism\nmigrates MoE experts without costly buffer reallocations, reducing peak memory\nusage during expert parallelism reconfiguration.\n  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that\nElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput\nduring scaling, and significantly improves SLO attainment compared to\nbaselines. By enabling fine-grained, concurrent scaling with minimal\ndisruption, ElasticMoE advances the practicality of deploying massive MoE LLMs\nin dynamic cloud environments."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Haley Li"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qintao Zhang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "arxiv_affiliation": "Huawei Technologies Canada",
                "author": "Zhenan Fan",
                "arxiv_comment": "19 pages, 15 figures, Under Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v5",
                "updated": "2025-10-02T19:25:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    19,
                    25,
                    29,
                    3,
                    275,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence after the aLoRA is invoked. This change crucially allows\naLoRA to accept the base model's KV cache of the input string, meaning that\naLoRA can be instantly activated whenever needed in a chain without recomputing\nthe prior keys and values. This enables building what we call intrinsics, i.e.\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\ntrain a set of aLoRA-based intrinsics models, demonstrating competitive\naccuracy with standard LoRA while significantly improving inference efficiency.\nWe contributed our Activated LoRA implementation to the Huggingface PEFT\nlibrary https://github.com/huggingface/peft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence after the aLoRA is invoked. This change crucially allows\naLoRA to accept the base model's KV cache of the input string, meaning that\naLoRA can be instantly activated whenever needed in a chain without recomputing\nthe prior keys and values. This enables building what we call intrinsics, i.e.\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\ntrain a set of aLoRA-based intrinsics models, demonstrating competitive\naccuracy with standard LoRA while significantly improving inference efficiency.\nWe contributed our Activated LoRA implementation to the Huggingface PEFT\nlibrary https://github.com/huggingface/peft."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v4",
                "updated": "2025-10-02T19:09:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    19,
                    9,
                    19,
                    3,
                    275,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17650v2",
                "updated": "2025-10-02T18:38:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    18,
                    38,
                    0,
                    3,
                    275,
                    0
                ],
                "published": "2025-09-22T11:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers"
                },
                "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical."
                },
                "authors": [
                    {
                        "name": "Soroush Mahdi"
                    },
                    {
                        "name": "Fardin Ayar"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    },
                    {
                        "name": "Mahdi Javanmardi"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Javanmardi"
                },
                "author": "Mahdi Javanmardi",
                "arxiv_comment": "project page: https://soroush-mim.github.io/projects/evict3r/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v3",
                "updated": "2025-10-02T18:20:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    18,
                    20,
                    18,
                    3,
                    275,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02312v1",
                "updated": "2025-10-02T17:59:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T17:59:51Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    51,
                    3,
                    275,
                    0
                ],
                "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation"
                },
                "summary": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference."
                },
                "authors": [
                    {
                        "name": "Anna Kuzina"
                    },
                    {
                        "name": "Maciej Pioro"
                    },
                    {
                        "name": "Paul N. Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03346v1",
                "updated": "2025-10-02T16:01:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    1,
                    54,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T16:01:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    16,
                    1,
                    54,
                    3,
                    275,
                    0
                ],
                "title": "KVComm: Enabling Efficient LLM Communication through Selective KV\n  Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComm: Enabling Efficient LLM Communication through Selective KV\n  Sharing"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-agent\nsystems, where effective inter-model communication is crucial. Existing\ncommunication protocols either rely on natural language, incurring high\ninference costs and information loss, or on hidden states, which suffer from\ninformation concentration bias and inefficiency. To address these limitations,\nwe propose KVComm, a novel communication framework that enables efficient\ncommunication between LLMs through selective sharing of KV pairs. KVComm\nleverages the rich information encoded in the KV pairs while avoiding the\npitfalls of hidden states. We introduce a KV layer-wise selection strategy\nbased on attention importance scores with a Gaussian prior to identify the most\ninformative KV pairs for communication. Extensive experiments across diverse\ntasks and model pairs demonstrate that KVComm achieves comparable performance\nto the upper-bound method, which directly merges inputs to one model without\nany communication, while transmitting as few as 30\\% of layers' KV pairs. Our\nstudy highlights the potential of KV pairs as an effective medium for inter-LLM\ncommunication, paving the way for scalable and efficient multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-agent\nsystems, where effective inter-model communication is crucial. Existing\ncommunication protocols either rely on natural language, incurring high\ninference costs and information loss, or on hidden states, which suffer from\ninformation concentration bias and inefficiency. To address these limitations,\nwe propose KVComm, a novel communication framework that enables efficient\ncommunication between LLMs through selective sharing of KV pairs. KVComm\nleverages the rich information encoded in the KV pairs while avoiding the\npitfalls of hidden states. We introduce a KV layer-wise selection strategy\nbased on attention importance scores with a Gaussian prior to identify the most\ninformative KV pairs for communication. Extensive experiments across diverse\ntasks and model pairs demonstrate that KVComm achieves comparable performance\nto the upper-bound method, which directly merges inputs to one model without\nany communication, while transmitting as few as 30\\% of layers' KV pairs. Our\nstudy highlights the potential of KV pairs as an effective medium for inter-LLM\ncommunication, paving the way for scalable and efficient multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Xiangyu Shi"
                    },
                    {
                        "name": "Marco Chiesa"
                    },
                    {
                        "name": "Gerald Q. Maguire Jr."
                    },
                    {
                        "name": "Dejan Kostic"
                    }
                ],
                "author_detail": {
                    "name": "Dejan Kostic"
                },
                "author": "Dejan Kostic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17356v2",
                "updated": "2025-10-02T14:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    42,
                    41,
                    3,
                    275,
                    0
                ],
                "published": "2025-08-24T13:30:00Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCache: Let Diffusion Model Determine Its Own Cache"
                },
                "summary": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine caching timings and adopting handcrafted rules for\nmulti-step cache utilization. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail to cope\nwith diverse samples. In this paper, a strong sample-specific correlation is\nrevealed between the variation patterns of the shallow-layer feature\ndifferences in the diffusion model and those of deep-layer features. Moreover,\nwe have observed that the features from different model layers form similar\ntrajectories. Based on these observations, we present DiCache, a novel\ntraining-free adaptive caching strategy for accelerating diffusion models at\nruntime, answering both when and how to cache within a unified framework.\nSpecifically, DiCache is composed of two principal components: (1) Online Probe\nProfiling Scheme leverages a shallow-layer online probe to obtain an on-the-fly\nindicator for the caching error in real time, enabling the model to dynamically\ncustomize the caching schedule for each sample. (2) Dynamic Cache Trajectory\nAlignment adaptively approximates the deep-layer feature output from multi-step\nhistorical caches based on the shallow-layer feature trajectory, facilitating\nhigher visual quality. Extensive experiments validate DiCache's capability in\nachieving higher efficiency and improved fidelity over state-of-the-art\napproaches on various leading diffusion models including WAN 2.1, HunyuanVideo\nand Flux.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine caching timings and adopting handcrafted rules for\nmulti-step cache utilization. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail to cope\nwith diverse samples. In this paper, a strong sample-specific correlation is\nrevealed between the variation patterns of the shallow-layer feature\ndifferences in the diffusion model and those of deep-layer features. Moreover,\nwe have observed that the features from different model layers form similar\ntrajectories. Based on these observations, we present DiCache, a novel\ntraining-free adaptive caching strategy for accelerating diffusion models at\nruntime, answering both when and how to cache within a unified framework.\nSpecifically, DiCache is composed of two principal components: (1) Online Probe\nProfiling Scheme leverages a shallow-layer online probe to obtain an on-the-fly\nindicator for the caching error in real time, enabling the model to dynamically\ncustomize the caching schedule for each sample. (2) Dynamic Cache Trajectory\nAlignment adaptively approximates the deep-layer feature output from multi-step\nhistorical caches based on the shallow-layer feature trajectory, facilitating\nhigher visual quality. Extensive experiments validate DiCache's capability in\nachieving higher efficiency and improved fidelity over state-of-the-art\napproaches on various leading diffusion models including WAN 2.1, HunyuanVideo\nand Flux."
                },
                "authors": [
                    {
                        "name": "Jiazi Bu"
                    },
                    {
                        "name": "Pengyang Ling"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "arxiv_comment": "Project Page: https://bujiazi.github.io/dicache.github.io/ Code:\n  https://github.com/Bujiazi/DiCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v3",
                "updated": "2025-10-02T14:09:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    14,
                    9,
                    3,
                    3,
                    275,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is widely adopted to accelerate inference and reduce memory\nconsumption in large language models (LLMs). While activation-weight joint\nquantization enables efficient low-precision decoding, it suffers from\nsubstantial performance degradation on multi-step reasoning tasks. We propose\nQSpec, a novel quantization paradigm that decouples efficiency from quality by\nintegrating two complementary schemes via speculative decoding: low-precision\njoint quantization for fast drafting and high-precision weight-only\nquantization for accurate verification. QSpec reuses both weights and KV cache\nacross stages, enabling near-zero-cost switching without retraining or\nauxiliary models. Compared to high-precision baselines, QSpec achieves up to\n1.64x speedup without quality degradation, and outperforms state-of-the-art\nspeculative decoding methods by up to 1.55x in batched settings. Furthermore,\nQSpec supports plug-and-play deployment and generalizes well across model\nscales, quantization methods, and workloads. These properties make QSpec a\npractical and scalable solution for high-fidelity quantized LLM serving under\nmemory-constrained scenarios. Our code is available at\nhttps://github.com/hku-netexplo-lab/QSpec."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "arxiv_journal_ref": "Proceedings of the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01884v1",
                "updated": "2025-10-02T10:49:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    49,
                    54,
                    3,
                    275,
                    0
                ],
                "published": "2025-10-02T10:49:54Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    10,
                    49,
                    54,
                    3,
                    275,
                    0
                ],
                "title": "Study of the $^{20}$Ne($p,γ$)$^{21}$Na reaction at LUNA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the $^{20}$Ne($p,γ$)$^{21}$Na reaction at LUNA"
                },
                "summary": "The NeNa-MgAl cycles are involved in the synthesis of Ne, Na, Mg, and Al\nisotopes. The $^{20}$Ne($p,\\gamma$)$^{21}$Na (Q = 2431.68 keV) reaction is the\nfirst and slowest reaction of the NeNa cycle and it controls the speed at which\nthe entire cycle proceeds. At the state of the art, the uncertainty on the\n20Ne(p,{\\gamma})21Na reaction rate affects the production of the elements in\nthe NeNa cycle. In particular, in the temperature range from 0.1 GK to 1 GK,\nthe rate is dominated by the 366 keV resonance corresponding to the excited\nstate of EX = 2797.5 keV and by the direct capture component. The present study\nfocus on the study of the 366 keV resonance and the direct capture below 400\nkeV. At LUNA (Laboratory for Underground Nuclear Astrophysics) the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction has been measured using the intense\nproton beam delivered by the LUNA 400 kV accelerator and a windowless\ndifferential-pumping gas target. The products of the reaction are detected with\ntwo high-purity germanium detectors. The experimental details and preliminary\nresults on the 366 keV resonance and on the direct capture component at very\nlow energies will be shown, together with their possible impact on the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NeNa-MgAl cycles are involved in the synthesis of Ne, Na, Mg, and Al\nisotopes. The $^{20}$Ne($p,\\gamma$)$^{21}$Na (Q = 2431.68 keV) reaction is the\nfirst and slowest reaction of the NeNa cycle and it controls the speed at which\nthe entire cycle proceeds. At the state of the art, the uncertainty on the\n20Ne(p,{\\gamma})21Na reaction rate affects the production of the elements in\nthe NeNa cycle. In particular, in the temperature range from 0.1 GK to 1 GK,\nthe rate is dominated by the 366 keV resonance corresponding to the excited\nstate of EX = 2797.5 keV and by the direct capture component. The present study\nfocus on the study of the 366 keV resonance and the direct capture below 400\nkeV. At LUNA (Laboratory for Underground Nuclear Astrophysics) the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction has been measured using the intense\nproton beam delivered by the LUNA 400 kV accelerator and a windowless\ndifferential-pumping gas target. The products of the reaction are detected with\ntwo high-purity germanium detectors. The experimental details and preliminary\nresults on the 366 keV resonance and on the direct capture component at very\nlow energies will be shown, together with their possible impact on the\n$^{20}$Ne($p,\\gamma$)$^{21}$Na reaction rate."
                },
                "authors": [
                    {
                        "name": "A. Caciolli"
                    }
                ],
                "author_detail": {
                    "name": "A. Caciolli"
                },
                "arxiv_affiliation": "on behalf of the LUNA collaboration",
                "author": "A. Caciolli",
                "arxiv_doi": "10.1051/epjconf/202429207005",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/epjconf/202429207005",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.01884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "EPJ Web Conf., 292 (2024) 07005",
                "arxiv_primary_category": {
                    "term": "nucl-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20211v2",
                "updated": "2025-10-02T04:11:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    2,
                    4,
                    11,
                    7,
                    3,
                    275,
                    0
                ],
                "published": "2025-05-26T16:52:40Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    52,
                    40,
                    0,
                    146,
                    0
                ],
                "title": "PiCa: Parameter-Efficient Fine-Tuning with Column Space Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiCa: Parameter-Efficient Fine-Tuning with Column Space Projection"
                },
                "summary": "Fine-tuning large foundation models is essential for building expert models\ntailored to specialized tasks and domains, but fully updating billions of\nparameters is computationally prohibitive. Reducing the number of trainable\nparameters using parameter-efficient fine-tuning is therefore crucial not only\nto reduce training costs but also to mitigate storage, caching, and serving\noverheads during deployment. Prior works, such as Singular Vectors-guided\nFine-Tuning, have shown that exploiting the geometry of pre-trained weights can\nsignificantly improve parameter-efficiency, but they lack a solid theoretical\nfoundation. In this paper, we introduce Parameter-efficient Fine-tuning with\nColumn Space Projection (PiCa), a novel theoretically grounded PEFT method. We\nprove that projecting gradients onto the principal column space of pre-trained\nweights provides an effective inductive bias for adaptation and further enhance\nparameter efficiency through a novel weight-sharing strategy. Across diverse\nNLP and vision tasks, PiCa consistently outperforms state-of-the-art baselines\nunder comparable or smaller parameter budgets, demonstrating both theoretical\nrigor and practical effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large foundation models is essential for building expert models\ntailored to specialized tasks and domains, but fully updating billions of\nparameters is computationally prohibitive. Reducing the number of trainable\nparameters using parameter-efficient fine-tuning is therefore crucial not only\nto reduce training costs but also to mitigate storage, caching, and serving\noverheads during deployment. Prior works, such as Singular Vectors-guided\nFine-Tuning, have shown that exploiting the geometry of pre-trained weights can\nsignificantly improve parameter-efficiency, but they lack a solid theoretical\nfoundation. In this paper, we introduce Parameter-efficient Fine-tuning with\nColumn Space Projection (PiCa), a novel theoretically grounded PEFT method. We\nprove that projecting gradients onto the principal column space of pre-trained\nweights provides an effective inductive bias for adaptation and further enhance\nparameter efficiency through a novel weight-sharing strategy. Across diverse\nNLP and vision tasks, PiCa consistently outperforms state-of-the-art baselines\nunder comparable or smaller parameter budgets, demonstrating both theoretical\nrigor and practical effectiveness."
                },
                "authors": [
                    {
                        "name": "Junseo Hwang"
                    },
                    {
                        "name": "Wonguk Cho"
                    },
                    {
                        "name": "Taesup Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taesup Kim"
                },
                "author": "Taesup Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07447v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07447v4",
                "updated": "2025-10-01T20:30:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    20,
                    30,
                    18,
                    2,
                    274,
                    0
                ],
                "published": "2024-11-12T00:10:34Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    0,
                    10,
                    34,
                    1,
                    317,
                    0
                ],
                "title": "Faster LLM Inference using DBMS-Inspired Preemption and Cache\n  Replacement Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster LLM Inference using DBMS-Inspired Preemption and Cache\n  Replacement Policies"
                },
                "summary": "LLMs are increasingly used world-wide from daily tasks to agentic systems and\ndata analytics, requiring significant GPU resources. LLM inference systems,\nhowever, are slow compared to database systems, and inference performance and\nmechanism have been often regarded as a black box, limiting the expansion of\nthe use of LLMs inside databases and other performance-critical applications.\nThis paper first analyzes the LLM inference performance and focuses on a data\nmanagement issue inside LLM inference. We find that inference systems lack an\nadequate resource cost model and optimization strategy to schedule requests\nwith their intermediate results in a cache reside in GPU memory when executing\nmultiple concurrent inference requests. We adapt classic database techniques by\nbuilding cost models for concurrent inference requests and a new cache\nreplacement policy tailored for LLM inference, which can substantially save GPU\ncosts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly used world-wide from daily tasks to agentic systems and\ndata analytics, requiring significant GPU resources. LLM inference systems,\nhowever, are slow compared to database systems, and inference performance and\nmechanism have been often regarded as a black box, limiting the expansion of\nthe use of LLMs inside databases and other performance-critical applications.\nThis paper first analyzes the LLM inference performance and focuses on a data\nmanagement issue inside LLM inference. We find that inference systems lack an\nadequate resource cost model and optimization strategy to schedule requests\nwith their intermediate results in a cache reside in GPU memory when executing\nmultiple concurrent inference requests. We adapt classic database techniques by\nbuilding cost models for concurrent inference requests and a new cache\nreplacement policy tailored for LLM inference, which can substantially save GPU\ncosts."
                },
                "authors": [
                    {
                        "name": "Kyoungmin Kim"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Kijae Hong"
                    },
                    {
                        "name": "Anastasia Ailamaki"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Ailamaki"
                },
                "author": "Anastasia Ailamaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07447v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07447v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v2",
                "updated": "2025-10-01T19:06:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    19,
                    6,
                    10,
                    2,
                    274,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09350v2",
                "updated": "2025-10-01T18:55:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    55,
                    20,
                    2,
                    274,
                    0
                ],
                "published": "2025-06-11T03:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation"
                },
                "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
                },
                "authors": [
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01336v1",
                "updated": "2025-10-01T18:04:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    4,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T18:04:14Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    18,
                    4,
                    14,
                    2,
                    274,
                    0
                ],
                "title": "HiSpec: Hierarchical Speculative Decoding for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiSpec: Hierarchical Speculative Decoding for LLMs"
                },
                "summary": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding accelerates LLM inference by using a smaller draft model\nto speculate tokens that a larger target model verifies. Verification is often\nthe bottleneck (e.g. verification is $4\\times$ slower than token generation\nwhen a 3B model speculates for a 70B target model), but most prior works focus\nonly on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces\nverification time by discarding inaccurate draft tokens early, but existing\nmethods incur substantial training overheads in incorporating the intermediate\nverifier, increase the memory footprint to orchestrate the intermediate\nverification step, and compromise accuracy by relying on approximate\nheuristics.\n  We propose $\\underline{\\textit{Hi}}\\textit{erarchical\n}\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for\nhigh-throughput speculative decoding that exploits $\\textit{early-exit (EE)\nmodels}$ for low-overhead intermediate verification. EE models allow tokens to\nexit early by skipping layer traversal and are explicitly trained so that\nhidden states at selected layers can be interpreted, making them uniquely\nsuited for intermediate verification without drastically increasing compute and\nmemory overheads. To improve resource-efficiency even further, we design a\nmethodology that enables HiSpec to re-use key-value caches and hidden states\nbetween the draft, intermediate verifier, and target models. To maintain\naccuracy, HiSpec periodically validates the draft tokens accepted by the\nintermediate verifier against the target model. Our evaluations using various\nrepresentative benchmarks and models show that HiSpec improves throughput by\n1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline\nsingle-layer speculation without compromising accuracy."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Sujay Sanghavi"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00948v1",
                "updated": "2025-10-01T14:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    21,
                    45,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T14:21:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    14,
                    21,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution"
                },
                "summary": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world videos often extend over thousands of frames. Existing video\nsuper-resolution (VSR) approaches, however, face two persistent challenges when\nprocessing long sequences: (1) inefficiency due to the heavy cost of multi-step\ndenoising for full-length sequences; and (2) poor scalability hindered by\ntemporal decomposition that causes artifacts and discontinuities. To break\nthese limits, we propose InfVSR, which novelly reformulates VSR as an\nautoregressive-one-step-diffusion paradigm. This enables streaming inference\nwhile fully leveraging pre-trained video diffusion priors. First, we adapt the\npre-trained DiT into a causal structure, maintaining both local and global\ncoherence via rolling KV-cache and joint visual guidance. Second, we distill\nthe diffusion process into a single step efficiently, with patch-wise pixel\nsupervision and cross-chunk distribution matching. Together, these designs\nenable efficient and scalable VSR for unbounded-length videos. To fill the gap\nin long-form video evaluation, we build a new benchmark tailored for extended\nsequences and further introduce semantic-level metrics to comprehensively\nassess temporal consistency. Our method pushes the frontier of long-form VSR,\nachieves state-of-the-art quality with enhanced semantic consistency, and\ndelivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will\nbe available at https://github.com/Kai-Liu001/InfVSR."
                },
                "authors": [
                    {
                        "name": "Ziqing Zhang"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Xi Li"
                    },
                    {
                        "name": "Yucong Chen"
                    },
                    {
                        "name": "Bingnan Duan"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Zhang"
                },
                "author": "Yulun Zhang",
                "arxiv_comment": "Code will be available at https://github.com/Kai-Liu001/InfVSR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26432v2",
                "updated": "2025-10-01T11:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    11,
                    26,
                    36,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-30T15:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    53,
                    56,
                    1,
                    273,
                    0
                ],
                "title": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block\n  Size"
                },
                "summary": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) are gaining attention for their\ninherent capacity for parallel decoding, offering a compelling alternative to\nautoregressive LLMs. Among various decoding strategies, blockwise\nsemi-autoregressive (semi-AR) approaches are widely adopted due to their\nnatural support for KV caching and their favorable accuracy-speed trade-off.\nHowever, this paper identifies two fundamental limitations in the conventional\nsemi-AR decoding approach that applies a fixed block size: i) late decoding\noverhead, where the unmasking of high-confidence tokens outside the current\nblock is unnecessarily delayed, and ii) premature decoding error, where\nlow-confidence tokens inside the current block are committed too early, leading\nto incorrect tokens. This paper presents the first systematic investigation\nchallenging the fixed block size assumption in semi-AR decoding. Through a\nstatistical analysis of confidence dynamics during the denoising process, we\nidentify a volatility band (VB) region during dLLM decoding, which encodes\nlocal semantic structure and can be used to guide adaptive block sizing.\nLeveraging these insights, we introduce AdaBlock-dLLM, a training-free,\nplug-and-play scheduler that adaptively aligns block boundaries with semantic\nsteps by adjusting block size during runtime. Extensive experiments across\ndiverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy\nimprovement under the same throughput budget. Beyond inference-time\noptimization, we hope our semantics-aware adaptive scheduling approach and\nconfidence-based analysis will inspire future training strategies for dLLMs."
                },
                "authors": [
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Yuto Karashima"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00636v1",
                "updated": "2025-10-01T08:12:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    12,
                    14,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T08:12:14Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    8,
                    12,
                    14,
                    2,
                    274,
                    0
                ],
                "title": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expected Attention: KV Cache Compression by Estimating Attention from\n  Future Queries Distribution"
                },
                "summary": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Maximilian Jeblick"
                    },
                    {
                        "name": "Simon Jégou"
                    }
                ],
                "author_detail": {
                    "name": "Simon Jégou"
                },
                "author": "Simon Jégou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00536v1",
                "updated": "2025-10-01T05:37:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    37,
                    54,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T05:37:54Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    37,
                    54,
                    2,
                    274,
                    0
                ],
                "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness"
                },
                "summary": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance."
                },
                "authors": [
                    {
                        "name": "Kung-Hsiang Huang"
                    },
                    {
                        "name": "Haoyi Qiu"
                    },
                    {
                        "name": "Yutong Dai"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25454v2",
                "updated": "2025-10-01T05:09:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    5,
                    9,
                    42,
                    2,
                    274,
                    0
                ],
                "published": "2025-09-29T20:00:29Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    20,
                    0,
                    29,
                    0,
                    272,
                    0
                ],
                "title": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSearch: Overcome the Bottleneck of Reinforcement Learning with\n  Verifiable Rewards via Monte Carlo Tree Search"
                },
                "summary": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although RLVR has become an essential component for developing advanced\nreasoning skills in LLMs, contemporary studies have documented training\nplateaus that emerge following thousands of optimization steps, demonstrating\nnotable decreases in performance gains despite increased computational\ninvestment. This limitation stems from the sparse exploration patterns inherent\nin current RLVR practices, where models rely on limited rollouts that often\nmiss critical reasoning paths and fail to provide systematic coverage of the\nsolution space. We present DeepSearch, a framework that integrates Monte Carlo\nTree Search directly into RLVR training. In contrast to existing methods that\nrely on tree search only at inference, DeepSearch embeds structured search into\nthe training loop, enabling systematic exploration and fine-grained credit\nassignment across reasoning steps. Through training-time exploration,\nDeepSearch addresses the fundamental bottleneck of insufficient exploration,\nwhich leads to diminishing performance improvements over prolonged training\nsteps. Our contributions include: (1) a global frontier selection strategy that\nprioritizes promising nodes across the search tree, (2) selection with\nentropy-based guidance that identifies confident paths for supervision, and (3)\nadaptive replay buffer training with solution caching for efficiency.\nExperiments on mathematical reasoning benchmarks show that DeepSearch achieves\n62.95% average accuracy and establishes a new state-of-the-art for 1.5B\nreasoning models - using 5.7x fewer GPU hours than extended training\napproaches. These results highlight the importance of strategic exploration\nover brute-force scaling and demonstrate the promise of algorithmic innovation\nfor advancing RLVR methodologies. DeepSearch establishes a new direction for\nscaling reasoning capabilities through systematic search rather than prolonged\ncomputation."
                },
                "authors": [
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Weihao Xuan"
                    },
                    {
                        "name": "Heli Qi"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Aaron Tu"
                    },
                    {
                        "name": "Li Erran Li"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01290v1",
                "updated": "2025-10-01T04:09:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    9,
                    2,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T04:09:02Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    4,
                    9,
                    2,
                    2,
                    274,
                    0
                ],
                "title": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning\n  Models"
                },
                "summary": "The long-output context generation of large reasoning models enables extended\nchain of thought (CoT) but also drives rapid growth of the key-value (KV)\ncache, quickly overwhelming GPU memory. To address this challenge, we propose\nThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on\nthe observation that attention sparsity reveals distinct thought types with\nvarying importance within the CoT. It applies a hybrid quantization-eviction\nstrategy, assigning token precision by thought importance and progressively\nevicting tokens from less critical thoughts as reasoning trajectories evolve.\nFurthermore, to implement ThinKV, we design a kernel that extends\nPagedAttention to enable efficient reuse of evicted tokens' memory slots,\neliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,\nGPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show\nthat ThinKV achieves near-lossless accuracy with less than 5% of the original\nKV cache, while improving performance with up to 5.8x higher inference\nthroughput over state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The long-output context generation of large reasoning models enables extended\nchain of thought (CoT) but also drives rapid growth of the key-value (KV)\ncache, quickly overwhelming GPU memory. To address this challenge, we propose\nThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on\nthe observation that attention sparsity reveals distinct thought types with\nvarying importance within the CoT. It applies a hybrid quantization-eviction\nstrategy, assigning token precision by thought importance and progressively\nevicting tokens from less critical thoughts as reasoning trajectories evolve.\nFurthermore, to implement ThinKV, we design a kernel that extends\nPagedAttention to enable efficient reuse of evicted tokens' memory slots,\neliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,\nGPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show\nthat ThinKV achieves near-lossless accuracy with less than 5% of the original\nKV cache, while improving performance with up to 5.8x higher inference\nthroughput over state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Akshat Ramachandran"
                    },
                    {
                        "name": "Marina Neseem"
                    },
                    {
                        "name": "Charbel Sakr"
                    },
                    {
                        "name": "Rangharajan Venkatesan"
                    },
                    {
                        "name": "Brucek Khailany"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01289v1",
                "updated": "2025-10-01T02:56:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    56,
                    59,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T02:56:59Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    2,
                    56,
                    59,
                    2,
                    274,
                    0
                ],
                "title": "Detailed Derivation of the Scalar Explicit Expressions Governing the\n  Electric Field, Current Density, and Volumetric Power Density in the Four\n  Types of Linear Divergent MHD Channels Under a Unidirectional Applied\n  Magnetic Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detailed Derivation of the Scalar Explicit Expressions Governing the\n  Electric Field, Current Density, and Volumetric Power Density in the Four\n  Types of Linear Divergent MHD Channels Under a Unidirectional Applied\n  Magnetic Field"
                },
                "summary": "The current study belongs to the field of applied mathematics in plasma\nphysics and electric power, where mathematical analysis of the algebraic\nequations governing the electric field vector, and the electric-current density\nfield vector within a Magnetohydrodynamic (MHD) linear two-dimensional\ndivergent supersonic channel is utilized to derive analytical expressions for\nthese important fields, as well as closed-form equations for the volumetric\npower density (output electric power per unit volume of the plasma channel).\nThe expressions presented here describe analytically the operation of the MHD\nchannel as an electric power source within an Open-Cycle Magnetohydrodynamic\n(OCMHD) generator. The four common types of the MHD linear channels are covered\nhere: namely, (1) continuous-electrode Faraday channel, (2) linear Hall\nchannel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode\nchannel. The mathematical results, their detailed derivation, and the companion\ngraphical illustrations aid in making a proper decision regarding which channel\ntype is the most suitable for a given application.Under typical operational\nconditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000\nm/s plasma speed, as well as an optimized load factor of 0.5, we estimate the\nfollowing numerical values (unsigned magnitudes) for the continuous-electrode\nFaraday channel (with a Hall parameter of 1): useful electric field (across the\nexternal electric load): 5 kV/m, useful electric current-density (between the\nterminal electrodes within the channel): 12.5 kA/m2 , volumetric power density\n(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric\nefficiency (for the electric field or voltage): 50%. For the Halllinear channel\n(with a Hall parameter of 5), these quantitative performance values become25\nkV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current study belongs to the field of applied mathematics in plasma\nphysics and electric power, where mathematical analysis of the algebraic\nequations governing the electric field vector, and the electric-current density\nfield vector within a Magnetohydrodynamic (MHD) linear two-dimensional\ndivergent supersonic channel is utilized to derive analytical expressions for\nthese important fields, as well as closed-form equations for the volumetric\npower density (output electric power per unit volume of the plasma channel).\nThe expressions presented here describe analytically the operation of the MHD\nchannel as an electric power source within an Open-Cycle Magnetohydrodynamic\n(OCMHD) generator. The four common types of the MHD linear channels are covered\nhere: namely, (1) continuous-electrode Faraday channel, (2) linear Hall\nchannel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode\nchannel. The mathematical results, their detailed derivation, and the companion\ngraphical illustrations aid in making a proper decision regarding which channel\ntype is the most suitable for a given application.Under typical operational\nconditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000\nm/s plasma speed, as well as an optimized load factor of 0.5, we estimate the\nfollowing numerical values (unsigned magnitudes) for the continuous-electrode\nFaraday channel (with a Hall parameter of 1): useful electric field (across the\nexternal electric load): 5 kV/m, useful electric current-density (between the\nterminal electrodes within the channel): 12.5 kA/m2 , volumetric power density\n(dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric\nefficiency (for the electric field or voltage): 50%. For the Halllinear channel\n(with a Hall parameter of 5), these quantitative performance values become25\nkV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%."
                },
                "authors": [
                    {
                        "name": "Osama A. Marzouk"
                    }
                ],
                "author_detail": {
                    "name": "Osama A. Marzouk"
                },
                "author": "Osama A. Marzouk",
                "arxiv_doi": "10.37256/cm.6420256918",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.37256/cm.6420256918",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.01289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "41 pages, 8 figures, 4 tables, published journal article,\n  peer-reviewed, open access",
                "arxiv_journal_ref": "Contemporary Mathematics. volume 6, issue 4, pages 4060-4100,\n  https://ojs.wiserpub.com/index.php/CM/article/view/6918 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00A79, 03H10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05131v1",
                "updated": "2025-10-01T01:28:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    28,
                    59,
                    2,
                    274,
                    0
                ],
                "published": "2025-10-01T01:28:59Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    1,
                    28,
                    59,
                    2,
                    274,
                    0
                ],
                "title": "Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task\n  Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task\n  Discovery"
                },
                "summary": "Head Start programs utilizing GoEngage face significant challenges when new\nor rotating staff attempt to locate appropriate Tasks (modules) on the platform\nhomepage. These difficulties arise from domain-specific jargon (e.g., IFPA,\nDRDP), system-specific nomenclature (e.g., Application Pool), and the inherent\nlimitations of lexical search in handling typos and varied word ordering. We\npropose a pragmatic hybrid semantic search system that synergistically combines\nlightweight typo-tolerant lexical retrieval, embedding-based vector similarity,\nand constrained large language model (LLM) re-ranking. Our approach leverages\nthe organization's existing Task Repository and Knowledge Base infrastructure\nwhile ensuring trustworthiness through low false-positive rates, evolvability\nto accommodate terminological changes, and economic efficiency via intelligent\ncaching, shortlist generation, and graceful degradation mechanisms. We provide\na comprehensive framework detailing required resources, a phased implementation\nstrategy with concrete milestones, an offline evaluation protocol utilizing\ncurated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online\nmeasurement methodology incorporating query success metrics, zero-result rates,\nand dwell-time proxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head Start programs utilizing GoEngage face significant challenges when new\nor rotating staff attempt to locate appropriate Tasks (modules) on the platform\nhomepage. These difficulties arise from domain-specific jargon (e.g., IFPA,\nDRDP), system-specific nomenclature (e.g., Application Pool), and the inherent\nlimitations of lexical search in handling typos and varied word ordering. We\npropose a pragmatic hybrid semantic search system that synergistically combines\nlightweight typo-tolerant lexical retrieval, embedding-based vector similarity,\nand constrained large language model (LLM) re-ranking. Our approach leverages\nthe organization's existing Task Repository and Knowledge Base infrastructure\nwhile ensuring trustworthiness through low false-positive rates, evolvability\nto accommodate terminological changes, and economic efficiency via intelligent\ncaching, shortlist generation, and graceful degradation mechanisms. We provide\na comprehensive framework detailing required resources, a phased implementation\nstrategy with concrete milestones, an offline evaluation protocol utilizing\ncurated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online\nmeasurement methodology incorporating query success metrics, zero-result rates,\nand dwell-time proxies."
                },
                "authors": [
                    {
                        "name": "Bowen Wei"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Wei"
                },
                "author": "Bowen Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02388v1",
                "updated": "2025-09-30T22:19:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    22,
                    19,
                    44,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T22:19:44Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    22,
                    19,
                    44,
                    1,
                    273,
                    0
                ],
                "title": "Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance on general\nQuestion Answering (QA), yet they often struggle in domain-specific scenarios\nwhere accurate and up-to-date information is required. Retrieval-Augmented\nGeneration (RAG) addresses this limitation by enriching LLMs with external\nknowledge, but existing systems primarily rely on unstructured documents, while\nlargely overlooking relational databases, which provide precise, timely, and\nefficiently queryable factual information, serving as indispensable\ninfrastructure in domains such as finance, healthcare, and scientific research.\nMotivated by this gap, we conduct a systematic analysis that reveals three\ncentral observations: (i) databases and documents offer complementary strengths\nacross queries, (ii) naively combining both sources introduces noise and cost\nwithout consistent accuracy gains, and (iii) selecting the most suitable source\nfor each query is crucial to balance effectiveness and efficiency. We further\nobserve that query types show consistent regularities in their alignment with\nretrieval paths, suggesting that routing decisions can be effectively guided by\nsystematic rules that capture these patterns. Building on these insights, we\npropose a rule-driven routing framework. A routing agent scores candidate\naugmentation paths based on explicit rules and selects the most suitable one; a\nrule-making expert agent refines the rules over time using QA feedback to\nmaintain adaptability; and a path-level meta-cache reuses past routing\ndecisions for semantically similar queries to reduce latency and cost.\nExperiments on three QA benchmarks demonstrate that our framework consistently\noutperforms static strategies and learned routing baselines, achieving higher\naccuracy while maintaining moderate computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance on general\nQuestion Answering (QA), yet they often struggle in domain-specific scenarios\nwhere accurate and up-to-date information is required. Retrieval-Augmented\nGeneration (RAG) addresses this limitation by enriching LLMs with external\nknowledge, but existing systems primarily rely on unstructured documents, while\nlargely overlooking relational databases, which provide precise, timely, and\nefficiently queryable factual information, serving as indispensable\ninfrastructure in domains such as finance, healthcare, and scientific research.\nMotivated by this gap, we conduct a systematic analysis that reveals three\ncentral observations: (i) databases and documents offer complementary strengths\nacross queries, (ii) naively combining both sources introduces noise and cost\nwithout consistent accuracy gains, and (iii) selecting the most suitable source\nfor each query is crucial to balance effectiveness and efficiency. We further\nobserve that query types show consistent regularities in their alignment with\nretrieval paths, suggesting that routing decisions can be effectively guided by\nsystematic rules that capture these patterns. Building on these insights, we\npropose a rule-driven routing framework. A routing agent scores candidate\naugmentation paths based on explicit rules and selects the most suitable one; a\nrule-making expert agent refines the rules over time using QA feedback to\nmaintain adaptability; and a path-level meta-cache reuses past routing\ndecisions for semantically similar queries to reduce latency and cost.\nExperiments on three QA benchmarks demonstrate that our framework consistently\noutperforms static strategies and learned routing baselines, achieving higher\naccuracy while maintaining moderate computational cost."
                },
                "authors": [
                    {
                        "name": "Haoyue Bai"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Shengyu Chen"
                    },
                    {
                        "name": "Zhengzhang Chen"
                    },
                    {
                        "name": "Lu-An Tang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00294v1",
                "updated": "2025-09-30T21:28:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    21,
                    28,
                    4,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T21:28:04Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    21,
                    28,
                    4,
                    1,
                    273,
                    0
                ],
                "title": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for\n  Diffusion Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for\n  Diffusion Large Language Models"
                },
                "summary": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of\nlanguage modeling beyond autoregressive next-token prediction. Thanks to their\nbidirectional attention mechanism, DLLMs are more capable of capturing the\nconnection of context, and thus show unique advantages in challenges like the\nfamous \"reversal curse\" or learning under data-constrained scenarios. However,\nthis bidirectional nature also brings an obstacle that DLLMs are not inherently\ncompatible with KV Cache, and consequently, the inference efficiency is not\ncompetitive compared with autoregressive models. Taking advantage of their\ninherent capability of multi-token prediction, existing parallel decoding\nalgorithms can speed up the DLLM inference, but at the cost of non-negligible\nperformance degradation. To overcome this challenge, we introduce Free\nDraft-and-Verification (Freedave), a novel fast sampling algorithm tailored for\nDLLMs that achieves lossless parallel decoding. Specifically, we propose a\npipeline of parallel-decoded candidate generation and verification, which is\nguaranteed to reproduce the same sequence generated by static sampling, without\nintroducing extra model forward calls. By applying Freedave, the throughput of\nDLLMs can be boosted up to $2.8\\times$ without performance degradation on math\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of\nlanguage modeling beyond autoregressive next-token prediction. Thanks to their\nbidirectional attention mechanism, DLLMs are more capable of capturing the\nconnection of context, and thus show unique advantages in challenges like the\nfamous \"reversal curse\" or learning under data-constrained scenarios. However,\nthis bidirectional nature also brings an obstacle that DLLMs are not inherently\ncompatible with KV Cache, and consequently, the inference efficiency is not\ncompetitive compared with autoregressive models. Taking advantage of their\ninherent capability of multi-token prediction, existing parallel decoding\nalgorithms can speed up the DLLM inference, but at the cost of non-negligible\nperformance degradation. To overcome this challenge, we introduce Free\nDraft-and-Verification (Freedave), a novel fast sampling algorithm tailored for\nDLLMs that achieves lossless parallel decoding. Specifically, we propose a\npipeline of parallel-decoded candidate generation and verification, which is\nguaranteed to reproduce the same sequence generated by static sampling, without\nintroducing extra model forward calls. By applying Freedave, the throughput of\nDLLMs can be boosted up to $2.8\\times$ without performance degradation on math\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Shutong Wu"
                    },
                    {
                        "name": "Jiawei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Zhang"
                },
                "author": "Jiawei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00231v1",
                "updated": "2025-09-30T19:55:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    55,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T19:55:26Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    55,
                    26,
                    1,
                    273,
                    0
                ],
                "title": "The Pitfalls of KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Pitfalls of KV Cache Compression"
                },
                "summary": "KV cache compression promises increased throughput and efficiency with\nnegligible loss in performance. While the gains in throughput are indisputable\nand recent literature has indeed shown minimal degradation on particular\nbenchmarks, in general the consequences of compression in realistic scenarios\nsuch as multi-instruction prompting have been insufficiently studied. In this\npaper, we identify several pitfalls practitioners should be aware of when\ndeploying KV cache compressed LLMs. Importantly, we show that certain\ninstructions degrade much more rapidly with compression, effectively causing\nthem to be completely ignored by the LLM. As a practical example of that, we\nhighlight system prompt leakage as a case study, empirically showing the impact\nof compression on leakage and general instruction following. We show several\nfactors that play a role in prompt leakage: compression method, instruction\norder, and KV eviction bias. We then propose simple changes to KV cache\neviction policies that can reduce the impact of these factors and improve the\noverall performance in multi-instruction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression promises increased throughput and efficiency with\nnegligible loss in performance. While the gains in throughput are indisputable\nand recent literature has indeed shown minimal degradation on particular\nbenchmarks, in general the consequences of compression in realistic scenarios\nsuch as multi-instruction prompting have been insufficiently studied. In this\npaper, we identify several pitfalls practitioners should be aware of when\ndeploying KV cache compressed LLMs. Importantly, we show that certain\ninstructions degrade much more rapidly with compression, effectively causing\nthem to be completely ignored by the LLM. As a practical example of that, we\nhighlight system prompt leakage as a case study, empirically showing the impact\nof compression on leakage and general instruction following. We show several\nfactors that play a role in prompt leakage: compression method, instruction\norder, and KV eviction bias. We then propose simple changes to KV cache\neviction policies that can reduce the impact of these factors and improve the\noverall performance in multi-instruction tasks."
                },
                "authors": [
                    {
                        "name": "Alex Chen"
                    },
                    {
                        "name": "Renato Geh"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Daniel Israel"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Israel"
                },
                "author": "Daniel Israel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00184v1",
                "updated": "2025-09-30T19:03:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    3,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T19:03:26Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    19,
                    3,
                    26,
                    1,
                    273,
                    0
                ],
                "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals\n  Long-Range Dependency Pitfalls"
                },
                "summary": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\n\\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are increasingly capable, yet still fail at a seemingly\nsimple task of multi-digit multiplication. In this work, we study why, by\nreverse-engineering a model that successfully learns multiplication via\n\\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of\nlong-range structure: Logit attributions and linear probes indicate that the\nmodel encodes the necessary long-range dependencies for multi-digit\nmultiplication. (2) Mechanism: the model encodes long-range dependencies using\nattention to construct a directed acyclic graph to ``cache'' and ``retrieve''\npairwise partial products. (3) Geometry: the model implements partial products\nin attention heads by forming Minkowski sums between pairs of digits, and\ndigits are represented using a Fourier basis, both of which are intuitive and\nefficient representations that the standard fine-tuning model lacks. With these\ninsights, we revisit the learning dynamics of standard fine-tuning and find\nthat the model converges to a local optimum that lacks the required long-range\ndependencies. We further validate this understanding by introducing an\nauxiliary loss that predicts the ``running sum'' via a linear regression probe,\nwhich provides an inductive bias that enables the model to successfully learn\nmulti-digit multiplication. In summary, by reverse-engineering the mechanisms\nof an implicit chain-of-thought model we uncover a pitfall for learning\nlong-range dependencies in Transformers and provide an example of how the\ncorrect inductive bias can address this issue."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Bai"
                    },
                    {
                        "name": "Itamar Pres"
                    },
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Chenhao Tan"
                    },
                    {
                        "name": "Stuart Shieber"
                    },
                    {
                        "name": "Fernanda Viégas"
                    },
                    {
                        "name": "Martin Wattenberg"
                    },
                    {
                        "name": "Andrew Lee"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lee"
                },
                "author": "Andrew Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23666v2",
                "updated": "2025-09-30T16:42:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    16,
                    42,
                    50,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T17:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    12,
                    42,
                    3,
                    149,
                    0
                ],
                "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoLA: Low-Rank Linear Attention With Sparse Caching"
                },
                "summary": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The per-token cost of transformer inference scales with context length,\npreventing its application to lifelong in-context learning. Linear attention is\nan efficient alternative that maintains a constant memory footprint, even on\ninfinite context lengths. While this is a potential candidate for lifelong\nlearning, it falls short in memory capacity. In this paper, we propose LoLA, a\ntraining-free augmentation to linear attention that boosts associative recall.\nLoLA distributes past key-value pairs from context into three memory systems:\n(i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. We show through ablations that our\nself-recall error metric is crucial to efficiently manage long-term associative\nmemories. On pass-key retrieval tasks, LoLA improves the base model's\nperformance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller\ncache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B\nand 8B parameter subquadratic models on zero-shot commonsense reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Luke McDermott"
                    },
                    {
                        "name": "Robert W. Heath Jr."
                    },
                    {
                        "name": "Rahul Parhi"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Parhi"
                },
                "author": "Rahul Parhi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v3",
                "updated": "2025-09-30T15:44:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    15,
                    44,
                    29,
                    1,
                    273,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_doi": "10.1109/TED.2025.3617043",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TED.2025.3617043",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to IEEE Trans. Elec. Dev. Work enabled in part by NanoIC\n  pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26328v1",
                "updated": "2025-09-30T14:40:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T14:40:18Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    40,
                    18,
                    1,
                    273,
                    0
                ],
                "title": "Fast-dLLM v2: Efficient Block-Diffusion LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM v2: Efficient Block-Diffusion LLM"
                },
                "summary": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) large language models (LLMs) have achieved remarkable\nperformance across a wide range of natural language tasks, yet their inherent\nsequential decoding limits inference efficiency. In this work, we propose\nFast-dLLM v2, a carefully designed block diffusion language model (dLLM) that\nefficiently adapts pretrained AR models into dLLMs for parallel text\ngeneration, requiring only approximately 1B tokens of fine-tuning. This\nrepresents a 500x reduction in training data compared to full-attention\ndiffusion LLMs such as Dream (580B tokens), while preserving the original\nmodel's performance. Our approach introduces a novel training recipe that\ncombines a block diffusion mechanism with a complementary attention mask,\nenabling blockwise bidirectional context modeling without sacrificing AR\ntraining objectives. To further accelerate decoding, we design a hierarchical\ncaching mechanism: a block-level cache that stores historical context\nrepresentations across blocks, and a sub-block cache that enables efficient\nparallel generation within partially decoded blocks. Coupled with our parallel\ndecoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR\ndecoding without compromising generation quality. Extensive experiments across\ndiverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR\nbaselines in accuracy, while delivering state-of-the-art efficiency among dLLMs\n- marking a significant step toward the practical deployment of fast and\naccurate LLMs. Code and model will be publicly released."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v4",
                "updated": "2025-09-30T14:13:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    14,
                    13,
                    20,
                    1,
                    273,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Accepted by NeurIPS 2025. Code at https://github.com/NVlabs/Long-RL\n  and model at https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17139v2",
                "updated": "2025-09-30T09:10:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    9,
                    10,
                    26,
                    1,
                    273,
                    0
                ],
                "published": "2025-02-24T13:30:30Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    13,
                    30,
                    30,
                    0,
                    55,
                    0
                ],
                "title": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCoder: Accelerating Repository-level Code Generation via Efficient\n  Retrieval and Verification"
                },
                "summary": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation is a latency-sensitive task that demands high timeliness.\nHowever, with the growing interest and inherent difficulty in repository-level\ncode generation, most existing code generation studies focus on improving the\ncorrectness of generated code while overlooking the inference efficiency, which\nis substantially affected by the overhead during LLM generation. Although there\nhas been work on accelerating LLM inference, these approaches are not tailored\nto the specific characteristics of code generation; instead, they treat code\nthe same as natural language sequences and ignore its unique syntax and\nsemantic characteristics, which are also crucial for improving efficiency.\nConsequently, these approaches exhibit limited effectiveness in code generation\ntasks, particularly for repository-level scenarios with considerable complexity\nand difficulty. To alleviate this issue, following draft-verification paradigm,\nwe propose FastCoder, a simple yet highly efficient inference acceleration\napproach specifically designed for code generation, without compromising the\nquality of the output. FastCoder constructs a multi-source datastore, providing\naccess to both general and project-specific knowledge, facilitating the\nretrieval of high-quality draft sequences. Moreover, FastCoder reduces the\nretrieval cost by controlling retrieval timing, and enhances efficiency through\nparallel retrieval and a context- and LLM preference-aware cache. Experimental\nresults show that FastCoder can reach up to 2.53x and 2.54x speedup compared to\nautoregressive decoding in repository-level and standalone code generation\ntasks, respectively, outperforming state-of-the-art inference acceleration\napproaches by up to 88%. FastCoder can also be integrated with existing\ncorrectness-focused code generation approaches to accelerate the LLM generation\nprocess, and reach a speedup exceeding 2.6x."
                },
                "authors": [
                    {
                        "name": "Qianhui Zhao"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Xiaoli Lian"
                    },
                    {
                        "name": "Qiaoyuanhe Meng"
                    },
                    {
                        "name": "Ziqian Jiao"
                    },
                    {
                        "name": "Zetong Zhou"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Lin Shi"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shi"
                },
                "author": "Lin Shi",
                "arxiv_comment": "Accepted by ASE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23416v2",
                "updated": "2025-09-30T02:51:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    51,
                    5,
                    1,
                    273,
                    0
                ],
                "published": "2025-05-29T13:05:47Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    13,
                    5,
                    47,
                    3,
                    149,
                    0
                ],
                "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction"
                },
                "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by $3$-$4\\times$ and FlashAttention decoding latency by\napproximately $2\\times$, with negligible performance loss in\nquestion-answering, retrieval, reasoning, and code comprehension tasks.\nEvaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with\ncontext lengths reaching up to 170K tokens. KVzip significantly outperforms\nexisting query-aware KV eviction methods, which suffer from performance\ndegradation even at a 90% cache budget ratio under multi-query scenarios."
                },
                "authors": [
                    {
                        "name": "Jang-Hyun Kim"
                    },
                    {
                        "name": "Jinuk Kim"
                    },
                    {
                        "name": "Sangwoo Kwon"
                    },
                    {
                        "name": "Jae W. Lee"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Hyun Oh Song"
                    }
                ],
                "author_detail": {
                    "name": "Hyun Oh Song"
                },
                "author": "Hyun Oh Song",
                "arxiv_comment": "NeurIPS 2025 Oral. Code: https://github.com/snu-mllab/KVzip",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25681v1",
                "updated": "2025-09-30T02:36:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "published": "2025-09-30T02:36:11Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    2,
                    36,
                    11,
                    1,
                    273,
                    0
                ],
                "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dVLA: Diffusion Vision-Language-Action Model with Multimodal\n  Chain-of-Thought"
                },
                "summary": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics."
                },
                "authors": [
                    {
                        "name": "Junjie Wen"
                    },
                    {
                        "name": "Minjie Zhu"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Yichen Zhu"
                    },
                    {
                        "name": "Yi Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Xu"
                },
                "author": "Yi Xu",
                "arxiv_comment": "technique report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25401v1",
                "updated": "2025-09-29T18:57:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T18:57:14Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    18,
                    57,
                    14,
                    0,
                    272,
                    0
                ],
                "title": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers"
                },
                "summary": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional\ncapabilities in visual synthesis, yet their deployment remains constrained by\nsubstantial computational demands. To alleviate this bottleneck, many\nsparsity-based acceleration methods have been proposed. However, their diverse\nsparsity patterns often require customized kernels for high-performance\ninference, limiting universality. We propose FlashOmni, a unified sparse\nattention engine compatible with arbitrary DiT architectures. FlashOmni\nintroduces flexible sparse symbols to standardize the representation of a wide\nrange of sparsity strategies, such as feature caching and block-sparse\nskipping. This unified abstraction enables the execution of diverse sparse\ncomputations within a single attention kernel. In addition, FlashOmni designs\noptimized sparse GEMMs for attention blocks, leveraging sparse symbols to\neliminate redundant computations and further improve efficiency. Experiments\ndemonstrate that FlashOmni delivers near-linear, closely matching the sparsity\nratio speedup (1:1) in attention and GEMM-$Q$, and achieves\n2.5$\\times$-3.8$\\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of\nthe theoretical limit). Applied with a multi-granularity sparsity strategy, it\nenables the Hunyuan model (33K) to achieve about 1.5$\\times$ end-to-end\nacceleration without degrading visual quality."
                },
                "authors": [
                    {
                        "name": "Liang Qiao"
                    },
                    {
                        "name": "Yue Dai"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Hongyu Kan"
                    },
                    {
                        "name": "Jun Shi"
                    },
                    {
                        "name": "Hong An"
                    }
                ],
                "author_detail": {
                    "name": "Hong An"
                },
                "author": "Hong An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25155v1",
                "updated": "2025-09-29T17:55:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T17:55:43Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "title": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Driven Performance Modeling for Causal Inference Operators on\n  Neural Processing Units"
                },
                "summary": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts."
                },
                "authors": [
                    {
                        "name": "Neelesh Gupta"
                    },
                    {
                        "name": "Rakshith Jayanth"
                    },
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IEEE HiPC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v2",
                "updated": "2025-09-29T15:20:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    20,
                    29,
                    0,
                    272,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "EMNLP 2025; 15 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v2",
                "updated": "2025-09-29T15:15:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    15,
                    15,
                    49,
                    0,
                    272,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24832v1",
                "updated": "2025-09-29T14:16:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T14:16:13Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts\n  via Token-Level LSH Matching"
                },
                "summary": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, the memory footprint of\nkey-value (KV) caches during inference has become a significant bottleneck.\nExisting approaches primarily focus on compressing KV caches within a single\nprompt or reusing shared prefixes or frequently ocurred text segments across\nprompts. However, such strategies are limited in scenarios where prompts are\nsemantically similar but lexically different, which frequently occurs in tasks\nsuch as multi-document summarization and conversational agents. We propose\n\\textit{SemShareKV}, a KV cache sharing and compression framework that\naccelerates LLM inference by reusing KVCache in semantically similar prompts.\nInstead of relying on exact token matches, SemShareKV applies fuzzy token\nmatching using locality-sensitive hashing (LSH) on token embeddings and\nincorporates Rotary Position Embedding (RoPE) to better preserve positional\ninformation. By selectively reusing relevant key-value pairs from a reference\nprompt's cache, SemShareKV reduces redundant computation while maintaining\noutput quality. Experiments on diverse summarization datasets show up to\n6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with\nnegligible quality degradation. These results highlight the potential of\nsemantic-aware cache sharing for efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinye Zhao"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis",
                "arxiv_comment": "11 figures, 14pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24791v1",
                "updated": "2025-09-29T13:45:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "published": "2025-09-29T13:45:35Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    13,
                    45,
                    35,
                    0,
                    272,
                    0
                ],
                "title": "Vision Function Layer in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Function Layer in Multimodal LLMs"
                },
                "summary": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study identifies that visual-related functional decoding is distributed\nacross different decoder layers in Multimodal Large Language Models (MLLMs).\nTypically, each function, such as counting, grounding, or OCR recognition,\nnarrows down to two or three layers, which we define as Vision Function Layers\n(VFL). Additionally, the depth and its order of different VFLs exhibits a\nconsistent pattern across different MLLMs, which is well-aligned with human\nbehaviors (e.g., recognition occurs first, followed by counting, and then\ngrounding). These findings are derived from Visual Token Swapping, our novel\nanalytical framework that modifies targeted KV cache entries to precisely\nelucidate layer-specific functions during decoding. Furthermore, these insights\noffer substantial utility in tailoring MLLMs for real-world downstream\napplications. For instance, when LoRA training is selectively applied to VFLs\nwhose functions align with the training data, VFL-LoRA not only outperform\nfull-LoRA but also prevent out-of-domain function forgetting. Moreover, by\nanalyzing the performance differential on training data when particular VFLs\nare ablated, VFL-select automatically classifies data by function, enabling\nhighly efficient data selection to directly bolster corresponding capabilities.\nConsequently, VFL-select surpasses human experts in data selection, and\nachieves 98% of full-data performance with only 20% of the original dataset.\nThis study delivers deeper comprehension of MLLM visual processing, fostering\nthe creation of more efficient, interpretable, and robust models."
                },
                "authors": [
                    {
                        "name": "Cheng Shi"
                    },
                    {
                        "name": "Yizhou Yu"
                    },
                    {
                        "name": "Sibei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Sibei Yang"
                },
                "author": "Sibei Yang",
                "arxiv_comment": "Accepted at NeurIPS 2025 (preview; camera-ready in preparation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.08575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08575v1",
                "updated": "2025-10-09T17:59:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    59,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:59:59Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    59,
                    3,
                    282,
                    0
                ],
                "title": "ReSplat: Learning Recurrent Gaussian Splats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSplat: Learning Recurrent Gaussian Splats"
                },
                "summary": "While feed-forward Gaussian splatting models provide computational efficiency\nand effectively handle sparse input settings, their performance is\nfundamentally limited by the reliance on a single forward pass during\ninference. We propose ReSplat, a feed-forward recurrent Gaussian splatting\nmodel that iteratively refines 3D Gaussians without explicitly computing\ngradients. Our key insight is that the Gaussian splatting rendering error\nserves as a rich feedback signal, guiding the recurrent network to learn\neffective Gaussian updates. This feedback signal naturally adapts to unseen\ndata distributions at test time, enabling robust generalization. To initialize\nthe recurrent process, we introduce a compact reconstruction model that\noperates in a $16 \\times$ subsampled space, producing $16 \\times$ fewer\nGaussians than previous per-pixel Gaussian models. This substantially reduces\ncomputational overhead and allows for efficient Gaussian updates. Extensive\nexperiments across varying of input views (2, 8, 16), resolutions ($256 \\times\n256$ to $540 \\times 960$), and datasets (DL3DV and RealEstate10K) demonstrate\nthat our method achieves state-of-the-art performance while significantly\nreducing the number of Gaussians and improving the rendering speed. Our project\npage is at https://haofeixu.github.io/resplat/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While feed-forward Gaussian splatting models provide computational efficiency\nand effectively handle sparse input settings, their performance is\nfundamentally limited by the reliance on a single forward pass during\ninference. We propose ReSplat, a feed-forward recurrent Gaussian splatting\nmodel that iteratively refines 3D Gaussians without explicitly computing\ngradients. Our key insight is that the Gaussian splatting rendering error\nserves as a rich feedback signal, guiding the recurrent network to learn\neffective Gaussian updates. This feedback signal naturally adapts to unseen\ndata distributions at test time, enabling robust generalization. To initialize\nthe recurrent process, we introduce a compact reconstruction model that\noperates in a $16 \\times$ subsampled space, producing $16 \\times$ fewer\nGaussians than previous per-pixel Gaussian models. This substantially reduces\ncomputational overhead and allows for efficient Gaussian updates. Extensive\nexperiments across varying of input views (2, 8, 16), resolutions ($256 \\times\n256$ to $540 \\times 960$), and datasets (DL3DV and RealEstate10K) demonstrate\nthat our method achieves state-of-the-art performance while significantly\nreducing the number of Gaussians and improving the rendering speed. Our project\npage is at https://haofeixu.github.io/resplat/."
                },
                "authors": [
                    {
                        "name": "Haofei Xu"
                    },
                    {
                        "name": "Daniel Barath"
                    },
                    {
                        "name": "Andreas Geiger"
                    },
                    {
                        "name": "Marc Pollefeys"
                    }
                ],
                "author_detail": {
                    "name": "Marc Pollefeys"
                },
                "author": "Marc Pollefeys",
                "arxiv_comment": "Project page: https://haofeixu.github.io/resplat/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08572v1",
                "updated": "2025-10-09T17:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    58,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    58,
                    3,
                    282,
                    0
                ],
                "title": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data\n  Generation"
                },
                "summary": "Scaling data and models has played a pivotal role in the remarkable progress\nof computer vision and language. Inspired by these domains, recent efforts in\nrobotics have similarly focused on scaling both data and model size to develop\nmore generalizable and robust policies. However, unlike vision and language,\nrobotics lacks access to internet-scale demonstrations across diverse robotic\ntasks and environments. As a result, the scale of existing datasets typically\nsuffers from the need for manual data collection and curation. To address this\nproblem, here we propose BLAZER, a framework that learns manipulation policies\nfrom automatically generated training data. We build on the zero-shot\ncapabilities of LLM planners and automatically generate demonstrations for\ndiverse manipulation tasks in simulation. Successful examples are then used to\nfinetune an LLM and to improve its planning capabilities without human\nsupervision. Notably, while BLAZER training requires access to the simulator's\nstate, we demonstrate direct transfer of acquired skills to sensor-based\nmanipulation. Through extensive experiments, we show BLAZER to significantly\nimprove zero-shot manipulation in both simulated and real environments.\nMoreover, BLAZER improves on tasks outside of its training pool and enables\ndownscaling of LLM models. Our code and data will be made publicly available on\nthe project page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling data and models has played a pivotal role in the remarkable progress\nof computer vision and language. Inspired by these domains, recent efforts in\nrobotics have similarly focused on scaling both data and model size to develop\nmore generalizable and robust policies. However, unlike vision and language,\nrobotics lacks access to internet-scale demonstrations across diverse robotic\ntasks and environments. As a result, the scale of existing datasets typically\nsuffers from the need for manual data collection and curation. To address this\nproblem, here we propose BLAZER, a framework that learns manipulation policies\nfrom automatically generated training data. We build on the zero-shot\ncapabilities of LLM planners and automatically generate demonstrations for\ndiverse manipulation tasks in simulation. Successful examples are then used to\nfinetune an LLM and to improve its planning capabilities without human\nsupervision. Notably, while BLAZER training requires access to the simulator's\nstate, we demonstrate direct transfer of acquired skills to sensor-based\nmanipulation. Through extensive experiments, we show BLAZER to significantly\nimprove zero-shot manipulation in both simulated and real environments.\nMoreover, BLAZER improves on tasks outside of its training pool and enables\ndownscaling of LLM models. Our code and data will be made publicly available on\nthe project page."
                },
                "authors": [
                    {
                        "name": "Rocktim Jyoti Das"
                    },
                    {
                        "name": "Harsh Singh"
                    },
                    {
                        "name": "Diana Turmakhan"
                    },
                    {
                        "name": "Muhammad Abdullah Sohail"
                    },
                    {
                        "name": "Mingfei Han"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Fabio Pizzati"
                    },
                    {
                        "name": "Ivan Laptev"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Laptev"
                },
                "author": "Ivan Laptev",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08569v1",
                "updated": "2025-10-09T17:59:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    55,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:59:55Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    55,
                    3,
                    282,
                    0
                ],
                "title": "ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive\n  Evaluation"
                },
                "summary": "Benchmarks are central to measuring the capabilities of large language models\nand guiding model development, yet widespread data leakage from pretraining\ncorpora undermines their validity. Models can match memorized content rather\nthan demonstrate true generalization, which inflates scores, distorts\ncross-model comparisons, and misrepresents progress. We introduce ArenaBencher,\na model-agnostic framework for automatic benchmark evolution that updates test\ncases while preserving comparability. Given an existing benchmark and a diverse\npool of models to be evaluated, ArenaBencher infers the core ability of each\ntest case, generates candidate question-answer pairs that preserve the original\nobjective, verifies correctness and intent with an LLM as a judge, and\naggregates feedback from multiple models to select candidates that expose\nshared weaknesses. The process runs iteratively with in-context demonstrations\nthat steer generation toward more challenging and diagnostic cases. We apply\nArenaBencher to math problem solving, commonsense reasoning, and safety domains\nand show that it produces verified, diverse, and fair updates that uncover new\nfailure modes, increase difficulty while preserving test objective alignment,\nand improve model separability. The framework provides a scalable path to\ncontinuously evolve benchmarks in step with the rapid progress of foundation\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks are central to measuring the capabilities of large language models\nand guiding model development, yet widespread data leakage from pretraining\ncorpora undermines their validity. Models can match memorized content rather\nthan demonstrate true generalization, which inflates scores, distorts\ncross-model comparisons, and misrepresents progress. We introduce ArenaBencher,\na model-agnostic framework for automatic benchmark evolution that updates test\ncases while preserving comparability. Given an existing benchmark and a diverse\npool of models to be evaluated, ArenaBencher infers the core ability of each\ntest case, generates candidate question-answer pairs that preserve the original\nobjective, verifies correctness and intent with an LLM as a judge, and\naggregates feedback from multiple models to select candidates that expose\nshared weaknesses. The process runs iteratively with in-context demonstrations\nthat steer generation toward more challenging and diagnostic cases. We apply\nArenaBencher to math problem solving, commonsense reasoning, and safety domains\nand show that it produces verified, diverse, and fair updates that uncover new\nfailure modes, increase difficulty while preserving test objective alignment,\nand improve model separability. The framework provides a scalable path to\ncontinuously evolve benchmarks in step with the rapid progress of foundation\nmodels."
                },
                "authors": [
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Jacob Dineen"
                    },
                    {
                        "name": "Yuxi Huang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Hoifung Poon"
                    },
                    {
                        "name": "Ben Zhou"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05034v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05034v3",
                "updated": "2025-10-09T17:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    45,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-06T17:10:44Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    10,
                    44,
                    0,
                    279,
                    0
                ],
                "title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large\n  Multimodal Models"
                },
                "summary": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training"
                },
                "authors": [
                    {
                        "name": "Yolo Yunlong Tang"
                    },
                    {
                        "name": "Jing Bi"
                    },
                    {
                        "name": "Pinxin Liu"
                    },
                    {
                        "name": "Zhenyu Pan"
                    },
                    {
                        "name": "Zhangyun Tan"
                    },
                    {
                        "name": "Qianxiang Shen"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Hang Hua"
                    },
                    {
                        "name": "Junjia Guo"
                    },
                    {
                        "name": "Yunzhong Xiao"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Xinyi Liu"
                    },
                    {
                        "name": "Yizhi Song"
                    },
                    {
                        "name": "Yuhe Nie"
                    },
                    {
                        "name": "Jia-Xing Zhong"
                    },
                    {
                        "name": "Bozheng Li"
                    },
                    {
                        "name": "Daiqing Qi"
                    },
                    {
                        "name": "Ziyun Zeng"
                    },
                    {
                        "name": "Ali Vosoughi"
                    },
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Daiki Shimada"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "The 1st version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05034v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05034v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08565v1",
                "updated": "2025-10-09T17:59:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    37,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:59:37Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    37,
                    3,
                    282,
                    0
                ],
                "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language\n  Models under Data Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language\n  Models under Data Constraints"
                },
                "summary": "Compositional training has been the de-facto paradigm in existing Multimodal\nLarge Language Models (MLLMs), where pre-trained vision encoders are connected\nwith pre-trained LLMs through continuous multimodal pre-training. However, the\nmultimodal scaling property of this paradigm remains difficult to explore due\nto the separated training. In this paper, we focus on the native training of\nMLLMs in an end-to-end manner and systematically study its design space and\nscaling property under a practical setting, i.e., data constraint. Through\ncareful study of various choices in MLLM, we obtain the optimal\nmeta-architecture that best balances performance and training cost. After that,\nwe further explore the scaling properties of the native MLLM and indicate the\npositively correlated scaling relationship between visual encoders and LLMs.\nBased on these findings, we propose a native MLLM called NaViL, combined with a\nsimple and cost-effective recipe. Experimental results on 14 multimodal\nbenchmarks confirm the competitive performance of NaViL against existing MLLMs.\nBesides that, our findings and results provide in-depth insights for the future\nstudy of native MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional training has been the de-facto paradigm in existing Multimodal\nLarge Language Models (MLLMs), where pre-trained vision encoders are connected\nwith pre-trained LLMs through continuous multimodal pre-training. However, the\nmultimodal scaling property of this paradigm remains difficult to explore due\nto the separated training. In this paper, we focus on the native training of\nMLLMs in an end-to-end manner and systematically study its design space and\nscaling property under a practical setting, i.e., data constraint. Through\ncareful study of various choices in MLLM, we obtain the optimal\nmeta-architecture that best balances performance and training cost. After that,\nwe further explore the scaling properties of the native MLLM and indicate the\npositively correlated scaling relationship between visual encoders and LLMs.\nBased on these findings, we propose a native MLLM called NaViL, combined with a\nsimple and cost-effective recipe. Experimental results on 14 multimodal\nbenchmarks confirm the competitive performance of NaViL against existing MLLMs.\nBesides that, our findings and results provide in-depth insights for the future\nstudy of native MLLMs."
                },
                "authors": [
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Weijie Su"
                    },
                    {
                        "name": "Hanming Deng"
                    },
                    {
                        "name": "Jinguo Zhu"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Ziran Zhu"
                    },
                    {
                        "name": "Yunpeng Liu"
                    },
                    {
                        "name": "Lewei Lu"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Jifeng Dai"
                    }
                ],
                "author_detail": {
                    "name": "Jifeng Dai"
                },
                "author": "Jifeng Dai",
                "arxiv_comment": "Accepted by NeurIPS 2025. 22 pages, link:\n  https://github.com/OpenGVLab/NaViL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08562v1",
                "updated": "2025-10-09T17:59:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    36,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:59:36Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    36,
                    3,
                    282,
                    0
                ],
                "title": "ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous\n  Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResAD: Normalized Residual Trajectory Modeling for End-to-End Autonomous\n  Driving"
                },
                "summary": "End-to-end autonomous driving (E2EAD) systems, which learn to predict future\ntrajectories directly from sensor data, are fundamentally challenged by the\ninherent spatio-temporal imbalance of trajectory data. This imbalance creates a\nsignificant optimization burden, causing models to learn spurious correlations\ninstead of causal inference, while also prioritizing uncertain, distant\npredictions, thereby compromising immediate safety. To address these issues, we\npropose ResAD, a novel Normalized Residual Trajectory Modeling framework.\nInstead of predicting the future trajectory directly, our approach reframes the\nlearning task to predict the residual deviation from a deterministic inertial\nreference. The inertial reference serves as a counterfactual, forcing the model\nto move beyond simple pattern recognition and instead identify the underlying\ncausal factors (e.g., traffic rules, obstacles) that necessitate deviations\nfrom a default, inertially-guided path. To deal with the optimization imbalance\ncaused by uncertain, long-term horizons, ResAD further incorporates Point-wise\nNormalization of the predicted residual. It re-weights the optimization\nobjective, preventing large-magnitude errors associated with distant, uncertain\nwaypoints from dominating the learning signal. Extensive experiments validate\nthe effectiveness of our framework. On the NAVSIM benchmark, ResAD achieves a\nstate-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two\ndenoising steps, demonstrating that our approach significantly simplifies the\nlearning task and improves model performance. The code will be released to\nfacilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end autonomous driving (E2EAD) systems, which learn to predict future\ntrajectories directly from sensor data, are fundamentally challenged by the\ninherent spatio-temporal imbalance of trajectory data. This imbalance creates a\nsignificant optimization burden, causing models to learn spurious correlations\ninstead of causal inference, while also prioritizing uncertain, distant\npredictions, thereby compromising immediate safety. To address these issues, we\npropose ResAD, a novel Normalized Residual Trajectory Modeling framework.\nInstead of predicting the future trajectory directly, our approach reframes the\nlearning task to predict the residual deviation from a deterministic inertial\nreference. The inertial reference serves as a counterfactual, forcing the model\nto move beyond simple pattern recognition and instead identify the underlying\ncausal factors (e.g., traffic rules, obstacles) that necessitate deviations\nfrom a default, inertially-guided path. To deal with the optimization imbalance\ncaused by uncertain, long-term horizons, ResAD further incorporates Point-wise\nNormalization of the predicted residual. It re-weights the optimization\nobjective, preventing large-magnitude errors associated with distant, uncertain\nwaypoints from dominating the learning signal. Extensive experiments validate\nthe effectiveness of our framework. On the NAVSIM benchmark, ResAD achieves a\nstate-of-the-art PDMS of 88.6 using a vanilla diffusion policy with only two\ndenoising steps, demonstrating that our approach significantly simplifies the\nlearning task and improves model performance. The code will be released to\nfacilitate further research."
                },
                "authors": [
                    {
                        "name": "Zhiyu Zheng"
                    },
                    {
                        "name": "Shaoyu Chen"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Xinbang Zhang"
                    },
                    {
                        "name": "Jialv Zou"
                    },
                    {
                        "name": "Xinggang Wang"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Lefei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lefei Zhang"
                },
                "author": "Lefei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08554v1",
                "updated": "2025-10-09T17:58:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    58,
                    7,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:58:07Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    58,
                    7,
                    3,
                    282,
                    0
                ],
                "title": "Improving Reasoning for Diffusion Language Models via Group Diffusion\n  Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Reasoning for Diffusion Language Models via Group Diffusion\n  Policy Optimization"
                },
                "summary": "Diffusion language models (DLMs) enable parallel, order-agnostic generation\nwith iterative refinement, offering a flexible alternative to autoregressive\nlarge language models (LLMs). However, adapting reinforcement learning (RL)\nfine-tuning to DLMs remains an open challenge because of the intractable\nlikelihood. Pioneering work such as diffu-GRPO estimated token-level\nlikelihoods via one-step unmasking. While computationally efficient, this\napproach is severely biased. A more principled foundation lies in\nsequence-level likelihoods, where the evidence lower bound (ELBO) serves as a\nsurrogate. Yet, despite this clean mathematical connection, ELBO-based methods\nhave seen limited adoption due to the prohibitive cost of likelihood\nevaluation. In this work, we revisit ELBO estimation and disentangle its\nsources of variance. This decomposition motivates reducing variance through\nfast, deterministic integral approximations along a few pivotal dimensions.\nBuilding on this insight, we introduce \\textbf{Group Diffusion Policy\nOptimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages\nsimple yet effective Semi-deterministic Monte Carlo schemes to mitigate the\nvariance explosion of ELBO estimators under vanilla double Monte Carlo\nsampling, yielding a provably lower-variance estimator under tight evaluation\nbudgets. Empirically, GDPO achieves consistent gains over pretrained\ncheckpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines,\non the majority of math, reasoning, and coding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) enable parallel, order-agnostic generation\nwith iterative refinement, offering a flexible alternative to autoregressive\nlarge language models (LLMs). However, adapting reinforcement learning (RL)\nfine-tuning to DLMs remains an open challenge because of the intractable\nlikelihood. Pioneering work such as diffu-GRPO estimated token-level\nlikelihoods via one-step unmasking. While computationally efficient, this\napproach is severely biased. A more principled foundation lies in\nsequence-level likelihoods, where the evidence lower bound (ELBO) serves as a\nsurrogate. Yet, despite this clean mathematical connection, ELBO-based methods\nhave seen limited adoption due to the prohibitive cost of likelihood\nevaluation. In this work, we revisit ELBO estimation and disentangle its\nsources of variance. This decomposition motivates reducing variance through\nfast, deterministic integral approximations along a few pivotal dimensions.\nBuilding on this insight, we introduce \\textbf{Group Diffusion Policy\nOptimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages\nsimple yet effective Semi-deterministic Monte Carlo schemes to mitigate the\nvariance explosion of ELBO estimators under vanilla double Monte Carlo\nsampling, yielding a provably lower-variance estimator under tight evaluation\nbudgets. Empirically, GDPO achieves consistent gains over pretrained\ncheckpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines,\non the majority of math, reasoning, and coding benchmarks."
                },
                "authors": [
                    {
                        "name": "Kevin Rojas"
                    },
                    {
                        "name": "Jiahe Lin"
                    },
                    {
                        "name": "Kashif Rasul"
                    },
                    {
                        "name": "Anderson Schneider"
                    },
                    {
                        "name": "Yuriy Nevmyvaka"
                    },
                    {
                        "name": "Molei Tao"
                    },
                    {
                        "name": "Wei Deng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Deng"
                },
                "author": "Wei Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08553v1",
                "updated": "2025-10-09T17:58:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    58,
                    1,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:58:01Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    58,
                    1,
                    3,
                    282,
                    0
                ],
                "title": "Dream to Recall: Imagination-Guided Experience Retrieval for\n  Memory-Persistent Vision-and-Language Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dream to Recall: Imagination-Guided Experience Retrieval for\n  Memory-Persistent Vision-and-Language Navigation"
                },
                "summary": "Vision-and-Language Navigation (VLN) requires agents to follow natural\nlanguage instructions through environments, with memory-persistent variants\ndemanding progressive improvement through accumulated experience. Existing\napproaches for memory-persistent VLN face critical limitations: they lack\neffective memory access mechanisms, instead relying on entire memory\nincorporation or fixed-horizon lookup, and predominantly store only\nenvironmental observations while neglecting navigation behavioral patterns that\nencode valuable decision-making strategies. We present Memoir, which employs\nimagination as a retrieval mechanism grounded by explicit memory: a world model\nimagines future navigation states as queries to selectively retrieve relevant\nenvironmental observations and behavioral histories. The approach comprises: 1)\na language-conditioned world model that imagines future states serving dual\npurposes: encoding experiences for storage and generating retrieval queries; 2)\nHybrid Viewpoint-Level Memory that anchors both observations and behavioral\npatterns to viewpoints, enabling hybrid retrieval; and 3) an\nexperience-augmented navigation model that integrates retrieved knowledge\nthrough specialized encoders. Extensive evaluation across diverse\nmemory-persistent VLN benchmarks with 10 distinctive testing scenarios\ndemonstrates Memoir's effectiveness: significant improvements across all\nscenarios, with 5.4% SPL gains on IR2R over the best memory-persistent\nbaseline, accompanied by 8.3x training speedup and 74% inference memory\nreduction. The results validate that predictive retrieval of both environmental\nand behavioral memories enables more effective navigation, with analysis\nindicating substantial headroom (73.3% vs 93.4% upper bound) for this\nimagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) requires agents to follow natural\nlanguage instructions through environments, with memory-persistent variants\ndemanding progressive improvement through accumulated experience. Existing\napproaches for memory-persistent VLN face critical limitations: they lack\neffective memory access mechanisms, instead relying on entire memory\nincorporation or fixed-horizon lookup, and predominantly store only\nenvironmental observations while neglecting navigation behavioral patterns that\nencode valuable decision-making strategies. We present Memoir, which employs\nimagination as a retrieval mechanism grounded by explicit memory: a world model\nimagines future navigation states as queries to selectively retrieve relevant\nenvironmental observations and behavioral histories. The approach comprises: 1)\na language-conditioned world model that imagines future states serving dual\npurposes: encoding experiences for storage and generating retrieval queries; 2)\nHybrid Viewpoint-Level Memory that anchors both observations and behavioral\npatterns to viewpoints, enabling hybrid retrieval; and 3) an\nexperience-augmented navigation model that integrates retrieved knowledge\nthrough specialized encoders. Extensive evaluation across diverse\nmemory-persistent VLN benchmarks with 10 distinctive testing scenarios\ndemonstrates Memoir's effectiveness: significant improvements across all\nscenarios, with 5.4% SPL gains on IR2R over the best memory-persistent\nbaseline, accompanied by 8.3x training speedup and 74% inference memory\nreduction. The results validate that predictive retrieval of both environmental\nand behavioral memories enables more effective navigation, with analysis\nindicating substantial headroom (73.3% vs 93.4% upper bound) for this\nimagination-guided paradigm. Code at https://github.com/xyz9911/Memoir."
                },
                "authors": [
                    {
                        "name": "Yunzhe Xu"
                    },
                    {
                        "name": "Yiyuan Pan"
                    },
                    {
                        "name": "Zhe Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Liu"
                },
                "author": "Zhe Liu",
                "arxiv_comment": "14 pages, 6 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08551v1",
                "updated": "2025-10-09T17:57:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    57,
                    38,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:57:38Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    57,
                    38,
                    3,
                    282,
                    0
                ],
                "title": "ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D\n  Reconstruction with Structured Scene Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARTDECO: Towards Efficient and High-Fidelity On-the-Fly 3D\n  Reconstruction with Structured Scene Representation"
                },
                "summary": "On-the-fly 3D reconstruction from monocular image sequences is a\nlong-standing challenge in computer vision, critical for applications such as\nreal-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff:\nper-scene optimization yields high fidelity but is computationally expensive,\nwhereas feed-forward foundation models enable real-time inference but struggle\nwith accuracy and robustness. In this work, we propose ARTDECO, a unified\nframework that combines the efficiency of feed-forward models with the\nreliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose\nestimation and point prediction, coupled with a Gaussian decoder that\ntransforms multi-scale features into structured 3D Gaussians. To sustain both\nfidelity and efficiency at scale, we design a hierarchical Gaussian\nrepresentation with a LoD-aware rendering strategy, which improves rendering\nfidelity while reducing redundancy. Experiments on eight diverse indoor and\noutdoor benchmarks show that ARTDECO delivers interactive performance\ncomparable to SLAM, robustness similar to feed-forward systems, and\nreconstruction quality close to per-scene optimization, providing a practical\npath toward on-the-fly digitization of real-world environments with both\naccurate geometry and high visual fidelity. Explore more demos on our project\npage: https://city-super.github.io/artdeco/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-the-fly 3D reconstruction from monocular image sequences is a\nlong-standing challenge in computer vision, critical for applications such as\nreal-to-sim, AR/VR, and robotics. Existing methods face a major tradeoff:\nper-scene optimization yields high fidelity but is computationally expensive,\nwhereas feed-forward foundation models enable real-time inference but struggle\nwith accuracy and robustness. In this work, we propose ARTDECO, a unified\nframework that combines the efficiency of feed-forward models with the\nreliability of SLAM-based pipelines. ARTDECO uses 3D foundation models for pose\nestimation and point prediction, coupled with a Gaussian decoder that\ntransforms multi-scale features into structured 3D Gaussians. To sustain both\nfidelity and efficiency at scale, we design a hierarchical Gaussian\nrepresentation with a LoD-aware rendering strategy, which improves rendering\nfidelity while reducing redundancy. Experiments on eight diverse indoor and\noutdoor benchmarks show that ARTDECO delivers interactive performance\ncomparable to SLAM, robustness similar to feed-forward systems, and\nreconstruction quality close to per-scene optimization, providing a practical\npath toward on-the-fly digitization of real-world environments with both\naccurate geometry and high visual fidelity. Explore more demos on our project\npage: https://city-super.github.io/artdeco/."
                },
                "authors": [
                    {
                        "name": "Guanghao Li"
                    },
                    {
                        "name": "Kerui Ren"
                    },
                    {
                        "name": "Linning Xu"
                    },
                    {
                        "name": "Zhewen Zheng"
                    },
                    {
                        "name": "Changjian Jiang"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Jian Pu"
                    },
                    {
                        "name": "Mulin Yu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03438v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03438v3",
                "updated": "2025-10-09T17:57:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    57,
                    35,
                    3,
                    282,
                    0
                ],
                "published": "2025-02-05T18:33:36Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    33,
                    36,
                    2,
                    36,
                    0
                ],
                "title": "BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic\n  Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic\n  Theorem Proving"
                },
                "summary": "Recent advancements in large language models (LLMs) have spurred growing\ninterest in automatic theorem proving using Lean4, where effective tree search\nmethods are crucial for navigating the underlying large proof search spaces.\nWhile the existing approaches primarily rely on value functions and/or Monte\nCarlo Tree Search (MCTS), the potential of simpler methods like Best-First Tree\nSearch (BFS) remains underexplored. In this paper, we investigate whether BFS\ncan achieve competitive performance in large-scale theorem proving tasks. We\npresent BFS-Prover, a scalable expert iteration framework, featuring three key\ninnovations. First, we implement strategic data filtering at each expert\niteration round, excluding problems solvable via beam search node expansion to\nfocus on harder cases. Second, we improve the sample efficiency of BFS through\nDirect Preference Optimization (DPO) applied to state-tactic pairs\nautomatically annotated with compiler error feedback, refining the LLM's policy\nto prioritize productive expansions. Third, we employ length normalization in\nBFS to encourage exploration of deeper proof paths. BFS-Prover achieves a\nstate-of-the-art score of $72.95\\%$ on the MiniF2F test set and therefore\nchallenges the perceived necessity of complex tree search methods,\ndemonstrating that BFS can achieve competitive performance when properly\nscaled. To facilitate further research and development in this area, we have\nopen-sourced our model at\nhttps://huggingface.co/ByteDance-Seed/BFS-Prover-V1-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have spurred growing\ninterest in automatic theorem proving using Lean4, where effective tree search\nmethods are crucial for navigating the underlying large proof search spaces.\nWhile the existing approaches primarily rely on value functions and/or Monte\nCarlo Tree Search (MCTS), the potential of simpler methods like Best-First Tree\nSearch (BFS) remains underexplored. In this paper, we investigate whether BFS\ncan achieve competitive performance in large-scale theorem proving tasks. We\npresent BFS-Prover, a scalable expert iteration framework, featuring three key\ninnovations. First, we implement strategic data filtering at each expert\niteration round, excluding problems solvable via beam search node expansion to\nfocus on harder cases. Second, we improve the sample efficiency of BFS through\nDirect Preference Optimization (DPO) applied to state-tactic pairs\nautomatically annotated with compiler error feedback, refining the LLM's policy\nto prioritize productive expansions. Third, we employ length normalization in\nBFS to encourage exploration of deeper proof paths. BFS-Prover achieves a\nstate-of-the-art score of $72.95\\%$ on the MiniF2F test set and therefore\nchallenges the perceived necessity of complex tree search methods,\ndemonstrating that BFS can achieve competitive performance when properly\nscaled. To facilitate further research and development in this area, we have\nopen-sourced our model at\nhttps://huggingface.co/ByteDance-Seed/BFS-Prover-V1-7B."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Chenguang Xi"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Xia Xiao"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Shen Zheng"
                    },
                    {
                        "name": "Kai Shen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shen"
                },
                "author": "Kai Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03438v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03438v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08549v1",
                "updated": "2025-10-09T17:56:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    56,
                    17,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:56:17Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    56,
                    17,
                    3,
                    282,
                    0
                ],
                "title": "Entropy Regularizing Activation: Boosting Continuous Control, Large\n  Language Models, and Image Classification with Activation as Entropy\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy Regularizing Activation: Boosting Continuous Control, Large\n  Language Models, and Image Classification with Activation as Entropy\n  Constraints"
                },
                "summary": "We propose ERA, a new paradigm that constrains the sampling entropy above\ngiven thresholds by applying specially designed activations to the outputs of\nmodels. Our approach demonstrates broad effectiveness across different domains:\n1) for large language models(LLMs), boosting the AIME 2025 score for\nQwen2.5-Math-7B by 37.4%; 2) for continuous control reinforcement learning\nagents, improving performance by more than 30% over strong baselines such as\nSAC on the challenging HumanoidBench; 3) for image classification, enhancing\nImageNet top-1 accuracy by 0.69% for ResNet-50. These gains are achieved with a\ncomputational overhead of less than 7%. Our work validates output activation as\na powerful tool for entropy control, opening a new direction for designing\nsimpler and more robust algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose ERA, a new paradigm that constrains the sampling entropy above\ngiven thresholds by applying specially designed activations to the outputs of\nmodels. Our approach demonstrates broad effectiveness across different domains:\n1) for large language models(LLMs), boosting the AIME 2025 score for\nQwen2.5-Math-7B by 37.4%; 2) for continuous control reinforcement learning\nagents, improving performance by more than 30% over strong baselines such as\nSAC on the challenging HumanoidBench; 3) for image classification, enhancing\nImageNet top-1 accuracy by 0.69% for ResNet-50. These gains are achieved with a\ncomputational overhead of less than 7%. Our work validates output activation as\na powerful tool for entropy control, opening a new direction for designing\nsimpler and more robust algorithms."
                },
                "authors": [
                    {
                        "name": "Zilin Kang"
                    },
                    {
                        "name": "Chonghua Liao"
                    },
                    {
                        "name": "Tingqiang Xu"
                    },
                    {
                        "name": "Huazhe Xu"
                    }
                ],
                "author_detail": {
                    "name": "Huazhe Xu"
                },
                "author": "Huazhe Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08544v1",
                "updated": "2025-10-09T17:55:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    55,
                    8,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:55:08Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    55,
                    8,
                    3,
                    282,
                    0
                ],
                "title": "SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM\n  Inference"
                },
                "summary": "Large Language Models (LLMs) have gained popularity in recent years, driving\nup the demand for inference. LLM inference is composed of two phases with\ndistinct characteristics: a compute-bound prefill phase followed by a\nmemory-bound decode phase. To efficiently serve LLMs, prior work proposes\nprefill-decode disaggregation to run each phase on separate hardware. However,\nexisting hardware poorly matches the different requirements of each phase.\nCurrent datacenter GPUs and TPUs follow a more-is-better design philosophy that\nmaximizes compute and memory resources, causing memory bandwidth\nunderutilization in the prefill phase and compute underutilization in the\ndecode phase. Such underutilization directly translates into increased serving\ncosts.\n  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting\na less-is-more methodology to design specialized chips tailored to the distinct\ncharacteristics of prefill and decode phases. The proposed Prefill Chips have\nlarger systolic arrays and use cost-effective GDDR memory, whereas the proposed\nDecode Chips retain high memory bandwidth but reduce compute capacity. Compared\nto modeled H100s, simulations show that the proposed Prefill Chips deliver 8%\nhigher prefill performance on average at 52% lower hardware cost, while the\nproposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.\n  End-to-end simulations on production traces show that SPAD reduces hardware\ncost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while\noffering the same performance. Even when models and workloads change, SPAD can\nreallocate either type of chip to run either phase and still achieve 11%-43%\nlower hardware costs, demonstrating the longevity of the SPAD design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained popularity in recent years, driving\nup the demand for inference. LLM inference is composed of two phases with\ndistinct characteristics: a compute-bound prefill phase followed by a\nmemory-bound decode phase. To efficiently serve LLMs, prior work proposes\nprefill-decode disaggregation to run each phase on separate hardware. However,\nexisting hardware poorly matches the different requirements of each phase.\nCurrent datacenter GPUs and TPUs follow a more-is-better design philosophy that\nmaximizes compute and memory resources, causing memory bandwidth\nunderutilization in the prefill phase and compute underutilization in the\ndecode phase. Such underutilization directly translates into increased serving\ncosts.\n  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting\na less-is-more methodology to design specialized chips tailored to the distinct\ncharacteristics of prefill and decode phases. The proposed Prefill Chips have\nlarger systolic arrays and use cost-effective GDDR memory, whereas the proposed\nDecode Chips retain high memory bandwidth but reduce compute capacity. Compared\nto modeled H100s, simulations show that the proposed Prefill Chips deliver 8%\nhigher prefill performance on average at 52% lower hardware cost, while the\nproposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.\n  End-to-end simulations on production traces show that SPAD reduces hardware\ncost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while\noffering the same performance. Even when models and workloads change, SPAD can\nreallocate either type of chip to run either phase and still achieve 11%-43%\nlower hardware costs, demonstrating the longevity of the SPAD design."
                },
                "authors": [
                    {
                        "name": "Hengrui Zhang"
                    },
                    {
                        "name": "Pratyush Patel"
                    },
                    {
                        "name": "August Ning"
                    },
                    {
                        "name": "David Wentzlaff"
                    }
                ],
                "author_detail": {
                    "name": "David Wentzlaff"
                },
                "author": "David Wentzlaff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08539v1",
                "updated": "2025-10-09T17:53:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    53,
                    41,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:53:41Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    53,
                    41,
                    3,
                    282,
                    0
                ],
                "title": "On the optimization dynamics of RLVR: Gradient gap and step size\n  thresholds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the optimization dynamics of RLVR: Gradient gap and step size\n  thresholds"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple\nbinary feedback to post-train large language models, has shown significant\nempirical success. However, a principled understanding of why it works has been\nlacking. This paper builds a theoretical foundation for RLVR by analyzing its\ntraining process at both the full-response (trajectory) and token levels.\nCentral to our analysis is a quantity called the Gradient Gap, which formalizes\nthe direction of improvement from low-reward to high-reward regions of the\nresponse space. We prove that convergence critically depends on aligning the\nupdate direction with this Gradient Gap. Moreover, we derive a sharp step-size\nthreshold based on the magnitude of the Gradient Gap: below it, learning\nconverges, whereas above it, performance collapses. Our theory further predicts\nhow the critical step size must scale with response length and the success\nrate, thereby explaining why practical heuristics such as length normalization\nimprove stability and showing that, with a fixed learning rate, the success\nrate can stagnate strictly below $100\\%$. We validate these predictions through\ncontrolled bandit simulations and LLM experiments, including training\nQwen2.5-7B with GRPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple\nbinary feedback to post-train large language models, has shown significant\nempirical success. However, a principled understanding of why it works has been\nlacking. This paper builds a theoretical foundation for RLVR by analyzing its\ntraining process at both the full-response (trajectory) and token levels.\nCentral to our analysis is a quantity called the Gradient Gap, which formalizes\nthe direction of improvement from low-reward to high-reward regions of the\nresponse space. We prove that convergence critically depends on aligning the\nupdate direction with this Gradient Gap. Moreover, we derive a sharp step-size\nthreshold based on the magnitude of the Gradient Gap: below it, learning\nconverges, whereas above it, performance collapses. Our theory further predicts\nhow the critical step size must scale with response length and the success\nrate, thereby explaining why practical heuristics such as length normalization\nimprove stability and showing that, with a fixed learning rate, the success\nrate can stagnate strictly below $100\\%$. We validate these predictions through\ncontrolled bandit simulations and LLM experiments, including training\nQwen2.5-7B with GRPO."
                },
                "authors": [
                    {
                        "name": "Joe Suk"
                    },
                    {
                        "name": "Yaqi Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yaqi Duan"
                },
                "author": "Yaqi Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08529v1",
                "updated": "2025-10-09T17:50:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    26,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:50:26Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    26,
                    3,
                    282,
                    0
                ],
                "title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards"
                },
                "summary": "Self-evolution is a central research topic in enabling large language model\n(LLM)-based agents to continually improve their capabilities after pretraining.\nRecent research has witnessed a transition from reinforcement learning\n(RL)-free to RL-based methods. Current RL-based methods either rely on dense\nexternal reward signals or extract intrinsic reward signals from LLMs\nthemselves. However, these approaches diverge from the self-evolution\nmechanisms observed in human intelligence, where individuals learn and improve\nthrough mutual discussion and collaboration. In this work, we introduce\nCo-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents\nto improve autonomously by learning from inter-agent interactions without\nexternal supervision. CoMAS generates intrinsic rewards from rich discussion\ndynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and\noptimizes each agent's policy through RL, thereby enabling decentralized and\nscalable co-evolution. Experimental results demonstrate that CoMAS consistently\noutperforms untrained agents and achieves state-of-the-art performance across\nmost evaluation settings. Ablation studies confirm the necessity of\ninteraction-based reward signals and reveal promising scalability as the number\nand diversity of agents increase. These findings establish CoMAS as a novel and\neffective paradigm for self-evolution in LLM-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-evolution is a central research topic in enabling large language model\n(LLM)-based agents to continually improve their capabilities after pretraining.\nRecent research has witnessed a transition from reinforcement learning\n(RL)-free to RL-based methods. Current RL-based methods either rely on dense\nexternal reward signals or extract intrinsic reward signals from LLMs\nthemselves. However, these approaches diverge from the self-evolution\nmechanisms observed in human intelligence, where individuals learn and improve\nthrough mutual discussion and collaboration. In this work, we introduce\nCo-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents\nto improve autonomously by learning from inter-agent interactions without\nexternal supervision. CoMAS generates intrinsic rewards from rich discussion\ndynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and\noptimizes each agent's policy through RL, thereby enabling decentralized and\nscalable co-evolution. Experimental results demonstrate that CoMAS consistently\noutperforms untrained agents and achieves state-of-the-art performance across\nmost evaluation settings. Ablation studies confirm the necessity of\ninteraction-based reward signals and reveal promising scalability as the number\nand diversity of agents increase. These findings establish CoMAS as a novel and\neffective paradigm for self-evolution in LLM-based agents."
                },
                "authors": [
                    {
                        "name": "Xiangyuan Xue"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Zaibin Zhang"
                    },
                    {
                        "name": "Yijiang Li"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Lei Bai"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bai"
                },
                "author": "Lei Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08527v1",
                "updated": "2025-10-09T17:50:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    22,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:50:22Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    22,
                    3,
                    282,
                    0
                ],
                "title": "FlexTraj: Image-to-Video Generation with Flexible Point Trajectory\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexTraj: Image-to-Video Generation with Flexible Point Trajectory\n  Control"
                },
                "summary": "We present FlexTraj, a framework for image-to-video generation with flexible\npoint trajectory control. FlexTraj introduces a unified point-based motion\nrepresentation that encodes each point with a segmentation ID, a temporally\nconsistent trajectory ID, and an optional color channel for appearance cues,\nenabling both dense and sparse trajectory control. Instead of injecting\ntrajectory conditions into the video generator through token concatenation or\nControlNet, FlexTraj employs an efficient sequence-concatenation scheme that\nachieves faster convergence, stronger controllability, and more efficient\ninference, while maintaining robustness under unaligned conditions. To train\nsuch a unified point trajectory-controlled video generator, FlexTraj adopts an\nannealing training strategy that gradually reduces reliance on complete\nsupervision and aligned condition. Experimental results demonstrate that\nFlexTraj enables multi-granularity, alignment-agnostic trajectory control for\nvideo generation, supporting various applications such as motion cloning,\ndrag-based image-to-video, motion interpolation, camera redirection, flexible\naction control and mesh animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present FlexTraj, a framework for image-to-video generation with flexible\npoint trajectory control. FlexTraj introduces a unified point-based motion\nrepresentation that encodes each point with a segmentation ID, a temporally\nconsistent trajectory ID, and an optional color channel for appearance cues,\nenabling both dense and sparse trajectory control. Instead of injecting\ntrajectory conditions into the video generator through token concatenation or\nControlNet, FlexTraj employs an efficient sequence-concatenation scheme that\nachieves faster convergence, stronger controllability, and more efficient\ninference, while maintaining robustness under unaligned conditions. To train\nsuch a unified point trajectory-controlled video generator, FlexTraj adopts an\nannealing training strategy that gradually reduces reliance on complete\nsupervision and aligned condition. Experimental results demonstrate that\nFlexTraj enables multi-granularity, alignment-agnostic trajectory control for\nvideo generation, supporting various applications such as motion cloning,\ndrag-based image-to-video, motion interpolation, camera redirection, flexible\naction control and mesh animations."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Zhang"
                    },
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dongdong Chen"
                    },
                    {
                        "name": "Jing Liao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liao"
                },
                "author": "Jing Liao",
                "arxiv_comment": "Project Page: https://bestzzhang.github.io/FlexTraj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08525v1",
                "updated": "2025-10-09T17:50:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:50:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression"
                },
                "summary": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results."
                },
                "authors": [
                    {
                        "name": "Wenjie Du"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08517v1",
                "updated": "2025-10-09T17:46:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    46,
                    39,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:46:39Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    46,
                    39,
                    3,
                    282,
                    0
                ],
                "title": "CaRT: Teaching LLM Agents to Know When They Know Enough",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaRT: Teaching LLM Agents to Know When They Know Enough"
                },
                "summary": "Many tasks require learned models to strategically gather relevant\ninformation over multiple rounds of interaction before actually acting on a\ntask. Strategic information gathering requires models to know not only how to\neffectively acquire information, but also when to stop gathering information\nand make a decision, in order to avoid overthinking or getting derailed when\nacting. In this paper, we formalize this problem and introduce Counterfactuals\nand Reasoning for Termination (CaRT), an approach for teaching LLMs when to\nstop seeking information. To appropriately learn when to terminate, CaRT\nfine-tunes LLMs using counterfactual pairs of trajectories, one where\ntermination is appropriate and a minimally modified version of the same\ntrajectory where it is not. It trains the LLM to explain the rationale for the\ntermination decision in either case via verbal reasoning, and imbues this\ncapability into the base LLM via fine-tuning. We instantiate CaRT in two\ndomains: interactive medical diagnosis and math problem solving. In both\ndomains, we find that CaRT improves the efficiency of information gathering and\ntask success rate compared to other fine-tuning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many tasks require learned models to strategically gather relevant\ninformation over multiple rounds of interaction before actually acting on a\ntask. Strategic information gathering requires models to know not only how to\neffectively acquire information, but also when to stop gathering information\nand make a decision, in order to avoid overthinking or getting derailed when\nacting. In this paper, we formalize this problem and introduce Counterfactuals\nand Reasoning for Termination (CaRT), an approach for teaching LLMs when to\nstop seeking information. To appropriately learn when to terminate, CaRT\nfine-tunes LLMs using counterfactual pairs of trajectories, one where\ntermination is appropriate and a minimally modified version of the same\ntrajectory where it is not. It trains the LLM to explain the rationale for the\ntermination decision in either case via verbal reasoning, and imbues this\ncapability into the base LLM via fine-tuning. We instantiate CaRT in two\ndomains: interactive medical diagnosis and math problem solving. In both\ndomains, we find that CaRT improves the efficiency of information gathering and\ntask success rate compared to other fine-tuning methods."
                },
                "authors": [
                    {
                        "name": "Grace Liu"
                    },
                    {
                        "name": "Yuxiao Qu"
                    },
                    {
                        "name": "Jeff Schneider"
                    },
                    {
                        "name": "Aarti Singh"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Aviral Kumar"
                },
                "author": "Aviral Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16616v2",
                "updated": "2025-10-09T17:46:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    46,
                    33,
                    3,
                    282,
                    0
                ],
                "published": "2025-06-19T21:27:03Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    21,
                    27,
                    3,
                    3,
                    170,
                    0
                ],
                "title": "LDI: Localized Data Imputation for Text-Rich Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LDI: Localized Data Imputation for Text-Rich Tables"
                },
                "summary": "Missing values are pervasive in real-world tabular data and can significantly\nimpair downstream analysis. Imputing them is especially challenging in\ntext-rich tables, where dependencies are implicit, complex, and dispersed\nacross long textual fields. Recent work has explored using Large Language\nModels (LLMs) for data imputation, yet existing approaches typically process\nentire tables or loosely related contexts, which can compromise accuracy,\nscalability, and explainability. We introduce LDI, a novel framework that\nleverages LLMs through localized reasoning, selecting a compact, contextually\nrelevant subset of attributes and tuples for each missing value. This targeted\nselection reduces noise, improves scalability, and provides transparent\nattribution by revealing which data influenced each prediction. Through\nextensive experiments on real and synthetic datasets, we demonstrate that LDI\nconsistently outperforms state-of-the-art imputation methods, achieving up to\n8% higher accuracy with hosted LLMs and even greater gains with local models.\nThe improved interpretability and robustness also make LDI well-suited for\nhigh-stakes data management applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing values are pervasive in real-world tabular data and can significantly\nimpair downstream analysis. Imputing them is especially challenging in\ntext-rich tables, where dependencies are implicit, complex, and dispersed\nacross long textual fields. Recent work has explored using Large Language\nModels (LLMs) for data imputation, yet existing approaches typically process\nentire tables or loosely related contexts, which can compromise accuracy,\nscalability, and explainability. We introduce LDI, a novel framework that\nleverages LLMs through localized reasoning, selecting a compact, contextually\nrelevant subset of attributes and tuples for each missing value. This targeted\nselection reduces noise, improves scalability, and provides transparent\nattribution by revealing which data influenced each prediction. Through\nextensive experiments on real and synthetic datasets, we demonstrate that LDI\nconsistently outperforms state-of-the-art imputation methods, achieving up to\n8% higher accuracy with hosted LLMs and even greater gains with local models.\nThe improved interpretability and robustness also make LDI well-suited for\nhigh-stakes data management applications."
                },
                "authors": [
                    {
                        "name": "Soroush Omidvartehrani"
                    },
                    {
                        "name": "Davood Rafiei"
                    }
                ],
                "author_detail": {
                    "name": "Davood Rafiei"
                },
                "author": "Davood Rafiei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16746v2",
                "updated": "2025-10-09T17:46:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    46,
                    9,
                    3,
                    282,
                    0
                ],
                "published": "2025-07-22T16:35:36Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    16,
                    35,
                    36,
                    1,
                    203,
                    0
                ],
                "title": "Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning"
                },
                "summary": "Humans often use visual aids, for example diagrams or sketches, when solving\ncomplex problems. Training multimodal models to do the same, known as Visual\nChain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf\nvisual CoT performance, which hinders reinforcement learning, and (2) the lack\nof high-quality visual CoT training data. We introduce $\\textbf{Zebra-CoT}$, a\ndiverse large-scale dataset with 182,384 samples, containing logically coherent\ninterleaved text-image reasoning traces. We focus on four categories of tasks\nwhere sketching or visual reasoning is especially natural, spanning scientific\nquestions such as geometry, physics, and algorithms; 2D visual reasoning tasks\nlike visual search and jigsaw puzzles; 3D reasoning tasks including 3D\nmulti-hop inference, embodied and robot planning; visual logic problems and\nstrategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT\ntraining corpus results in an improvement of +12% in our test-set accuracy and\nyields up to +13% performance gain on standard VLM benchmark evaluations.\nFine-tuning Bagel-7B yields a model that generates high-quality interleaved\nvisual reasoning chains, underscoring Zebra-CoT's effectiveness for developing\nmultimodal reasoning abilities. We open-source our dataset and models to\nsupport development and evaluation of visual CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans often use visual aids, for example diagrams or sketches, when solving\ncomplex problems. Training multimodal models to do the same, known as Visual\nChain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf\nvisual CoT performance, which hinders reinforcement learning, and (2) the lack\nof high-quality visual CoT training data. We introduce $\\textbf{Zebra-CoT}$, a\ndiverse large-scale dataset with 182,384 samples, containing logically coherent\ninterleaved text-image reasoning traces. We focus on four categories of tasks\nwhere sketching or visual reasoning is especially natural, spanning scientific\nquestions such as geometry, physics, and algorithms; 2D visual reasoning tasks\nlike visual search and jigsaw puzzles; 3D reasoning tasks including 3D\nmulti-hop inference, embodied and robot planning; visual logic problems and\nstrategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT\ntraining corpus results in an improvement of +12% in our test-set accuracy and\nyields up to +13% performance gain on standard VLM benchmark evaluations.\nFine-tuning Bagel-7B yields a model that generates high-quality interleaved\nvisual reasoning chains, underscoring Zebra-CoT's effectiveness for developing\nmultimodal reasoning abilities. We open-source our dataset and models to\nsupport development and evaluation of visual CoT."
                },
                "authors": [
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Charles Wang"
                    },
                    {
                        "name": "Deqing Fu"
                    },
                    {
                        "name": "Kaiyu Yue"
                    },
                    {
                        "name": "Zikui Cai"
                    },
                    {
                        "name": "Wang Bill Zhu"
                    },
                    {
                        "name": "Ollie Liu"
                    },
                    {
                        "name": "Peng Guo"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Tom Goldstein"
                    },
                    {
                        "name": "Micah Goldblum"
                    }
                ],
                "author_detail": {
                    "name": "Micah Goldblum"
                },
                "author": "Micah Goldblum",
                "arxiv_comment": "dataset link:\n  https://huggingface.co/datasets/multimodal-reasoning-lab/Zebra-CoT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v2",
                "updated": "2025-10-09T17:45:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    45,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16067v2",
                "updated": "2025-10-09T17:45:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    45,
                    15,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-22T03:38:06Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    3,
                    38,
                    6,
                    4,
                    234,
                    0
                ],
                "title": "Training a Foundation Model for Materials on a Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training a Foundation Model for Materials on a Budget"
                },
                "summary": "Foundation models for materials modeling are advancing quickly, but their\ntraining remains expensive, often placing state-of-the-art methods out of reach\nfor many research groups. We introduce Nequix, a compact E(3)-equivariant\npotential that pairs a simplified NequIP design with modern training practices,\nincluding equivariant root-mean-square layer normalization and the Muon\noptimizer, to retain accuracy while substantially reducing compute\nrequirements. Nequix has 700K parameters and was trained in 100 A100 GPU-hours.\nOn the Matbench-Discovery and MDR Phonon benchmarks, Nequix ranks third overall\nwhile requiring a 20 times lower training cost than most other methods, and it\ndelivers two orders of magnitude faster inference speed than the current\ntop-ranked model. We release model weights and fully reproducible codebase at\nhttps://github.com/atomicarchitects/nequix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models for materials modeling are advancing quickly, but their\ntraining remains expensive, often placing state-of-the-art methods out of reach\nfor many research groups. We introduce Nequix, a compact E(3)-equivariant\npotential that pairs a simplified NequIP design with modern training practices,\nincluding equivariant root-mean-square layer normalization and the Muon\noptimizer, to retain accuracy while substantially reducing compute\nrequirements. Nequix has 700K parameters and was trained in 100 A100 GPU-hours.\nOn the Matbench-Discovery and MDR Phonon benchmarks, Nequix ranks third overall\nwhile requiring a 20 times lower training cost than most other methods, and it\ndelivers two orders of magnitude faster inference speed than the current\ntop-ranked model. We release model weights and fully reproducible codebase at\nhttps://github.com/atomicarchitects/nequix."
                },
                "authors": [
                    {
                        "name": "Teddy Koker"
                    },
                    {
                        "name": "Mit Kotak"
                    },
                    {
                        "name": "Tess Smidt"
                    }
                ],
                "author_detail": {
                    "name": "Tess Smidt"
                },
                "author": "Tess Smidt",
                "arxiv_comment": "To appear at the NeurIPS 2025 Workshop on AI for Accelerated\n  Materials Discovery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08511v1",
                "updated": "2025-10-09T17:45:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    45,
                    5,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:45:05Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    45,
                    5,
                    3,
                    282,
                    0
                ],
                "title": "AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents"
                },
                "summary": "Large language models (LLMs) have shown impressive performance in general\nprogramming tasks. However, in Machine Learning Engineering (MLE) scenarios\nsuch as AutoML and Kaggle competitions, achieving high performance depends\nheavily on expert intervention and repeated adjustments rather than simply\ngenerating correct code. When applied directly to these tasks, LLMs often lack\nfine-grained domain priors, and existing MLE approaches that use linear or\ntree-structured searches limit knowledge transfer to adjacent hierarchical\nlinks. As a result, they cannot leverage past full trajectories or share\ninformation across branches, limiting self-evolving ability and search space\ndiversity. To address these limitations, we introduce AutoMLGen, an LLM-based\ncoding agent that integrates a domain knowledge base for high-quality prior\nguidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS\nretains the tree-guided exploration of MCTS while embedding a graph structure\ninto the expansion stage to enable dynamic path reorganization, historical\ntrajectory reuse, and multi-solution fusion to support both self-evolution and\ncollaborative learning. Combined with fine-grained operator sets, this design\nimproves stability and accelerates convergence. Evaluation on the MLE-Bench\nshows that AutoMLGen achieves state-of-the-art performance in numerous\ndimensions, such as the average medal rate and the valid submission rate, under\na 12-hour budget (half the standard runtime). The code is available at\nhttps://github.com/Alpha-Innovator/InternAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive performance in general\nprogramming tasks. However, in Machine Learning Engineering (MLE) scenarios\nsuch as AutoML and Kaggle competitions, achieving high performance depends\nheavily on expert intervention and repeated adjustments rather than simply\ngenerating correct code. When applied directly to these tasks, LLMs often lack\nfine-grained domain priors, and existing MLE approaches that use linear or\ntree-structured searches limit knowledge transfer to adjacent hierarchical\nlinks. As a result, they cannot leverage past full trajectories or share\ninformation across branches, limiting self-evolving ability and search space\ndiversity. To address these limitations, we introduce AutoMLGen, an LLM-based\ncoding agent that integrates a domain knowledge base for high-quality prior\nguidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS\nretains the tree-guided exploration of MCTS while embedding a graph structure\ninto the expansion stage to enable dynamic path reorganization, historical\ntrajectory reuse, and multi-solution fusion to support both self-evolution and\ncollaborative learning. Combined with fine-grained operator sets, this design\nimproves stability and accelerates convergence. Evaluation on the MLE-Bench\nshows that AutoMLGen achieves state-of-the-art performance in numerous\ndimensions, such as the average medal rate and the valid submission rate, under\na 12-hour budget (half the standard runtime). The code is available at\nhttps://github.com/Alpha-Innovator/InternAgent."
                },
                "authors": [
                    {
                        "name": "Shangheng Du"
                    },
                    {
                        "name": "Xiangchao Yan"
                    },
                    {
                        "name": "Dengyang Jiang"
                    },
                    {
                        "name": "Jiakang Yuan"
                    },
                    {
                        "name": "Yusong Hu"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Liang He"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Lei Bai"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bai"
                },
                "author": "Lei Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08510v1",
                "updated": "2025-10-09T17:44:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    44,
                    42,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:44:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    44,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "To Sink or Not to Sink: Visual Information Pathways in Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Sink or Not to Sink: Visual Information Pathways in Large\n  Vision-Language Models"
                },
                "summary": "Large Vision Language Models (LVLMs) have recently emerged as powerful\narchitectures capable of understanding and reasoning over both visual and\ntextual information. These models typically rely on two key components: a\nVision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual\ncontent into a sequence of image tokens and serves as the perceptual front-end\n-- the eyes of the model. In contrast, the LLM interprets these tokens to\nperform high-level reasoning, generates responses, and functions as the\ncognitive core -- the brain of the model. However, it remains unclear which\nvisual tokens contribute most significantly to understanding and reasoning, and\nhow effectively these signals are propagated from ViT to the LLM. While most\nexisting works have focused on identifying attention sinks, low-semantic tokens\nreceiving disproportionately high attention, within the LLM, we shift the focus\nto the vision encoder by identifying a class of high-norm visual tokens from\nViT, referred to as ViT attention sinks -- a problem that has been rarely\nstudied but is indeed very important for LVLMs. Our findings show that these\nViT sinks encapsulate high-level semantic concepts from images, allowing the\nLLM to perform more effective understanding and reasoning. Despite their\nimportance, these sink tokens are often overlooked in existing LVLM\narchitectures. To explore their contribution, we present both qualitative and\nquantitative analyses of the information embedded in these sink tokens. We also\npropose both training-free and training-based approaches to better leverage how\nthis information is interpreted by the LLM, and to what extent. By explicitly\nutilizing these tokens, we demonstrate substantial improvements across a range\nof LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT\nattention sinks in enhancing visual reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision Language Models (LVLMs) have recently emerged as powerful\narchitectures capable of understanding and reasoning over both visual and\ntextual information. These models typically rely on two key components: a\nVision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual\ncontent into a sequence of image tokens and serves as the perceptual front-end\n-- the eyes of the model. In contrast, the LLM interprets these tokens to\nperform high-level reasoning, generates responses, and functions as the\ncognitive core -- the brain of the model. However, it remains unclear which\nvisual tokens contribute most significantly to understanding and reasoning, and\nhow effectively these signals are propagated from ViT to the LLM. While most\nexisting works have focused on identifying attention sinks, low-semantic tokens\nreceiving disproportionately high attention, within the LLM, we shift the focus\nto the vision encoder by identifying a class of high-norm visual tokens from\nViT, referred to as ViT attention sinks -- a problem that has been rarely\nstudied but is indeed very important for LVLMs. Our findings show that these\nViT sinks encapsulate high-level semantic concepts from images, allowing the\nLLM to perform more effective understanding and reasoning. Despite their\nimportance, these sink tokens are often overlooked in existing LVLM\narchitectures. To explore their contribution, we present both qualitative and\nquantitative analyses of the information embedded in these sink tokens. We also\npropose both training-free and training-based approaches to better leverage how\nthis information is interpreted by the LLM, and to what extent. By explicitly\nutilizing these tokens, we demonstrate substantial improvements across a range\nof LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT\nattention sinks in enhancing visual reasoning."
                },
                "authors": [
                    {
                        "name": "Jiayun Luo"
                    },
                    {
                        "name": "Wan-Cyuan Fan"
                    },
                    {
                        "name": "Lyuyang Wang"
                    },
                    {
                        "name": "Xiangteng He"
                    },
                    {
                        "name": "Tanzila Rahman"
                    },
                    {
                        "name": "Purang Abolmaesumi"
                    },
                    {
                        "name": "Leonid Sigal"
                    }
                ],
                "author_detail": {
                    "name": "Leonid Sigal"
                },
                "author": "Leonid Sigal",
                "arxiv_comment": "Preprint. Project page: https://davidhalladay.github.io/diysink_demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08508v1",
                "updated": "2025-10-09T17:42:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    42,
                    51,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:42:51Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    42,
                    51,
                    3,
                    282,
                    0
                ],
                "title": "MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration"
                },
                "summary": "Real-world videos often suffer from complex degradations, such as noise,\ncompression artifacts, and low-light distortions, due to diverse acquisition\nand transmission conditions. Existing restoration methods typically require\nprofessional manual selection of specialized models or rely on monolithic\narchitectures that fail to generalize across varying degradations. Inspired by\nexpert experience, we propose MoA-VR, the first\n\\underline{M}ixture-\\underline{o}f-\\underline{A}gents \\underline{V}ideo\n\\underline{R}estoration system that mimics the reasoning and processing\nprocedures of human professionals through three coordinated agents: Degradation\nIdentification, Routing and Restoration, and Restoration Quality Assessment.\nSpecifically, we construct a large-scale and high-resolution video degradation\nrecognition benchmark and build a vision-language model (VLM) driven\ndegradation identifier. We further introduce a self-adaptive router powered by\nlarge language models (LLMs), which autonomously learns effective restoration\nstrategies by observing tool usage patterns. To assess intermediate and final\nprocessed video quality, we construct the \\underline{Res}tored\n\\underline{V}ideo \\underline{Q}uality (Res-VQ) dataset and design a dedicated\nVLM-based video quality assessment (VQA) model tailored for restoration tasks.\nExtensive experiments demonstrate that MoA-VR effectively handles diverse and\ncompound degradations, consistently outperforming existing baselines in terms\nof both objective metrics and perceptual quality. These results highlight the\npotential of integrating multimodal intelligence and modular reasoning in\ngeneral-purpose video restoration systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world videos often suffer from complex degradations, such as noise,\ncompression artifacts, and low-light distortions, due to diverse acquisition\nand transmission conditions. Existing restoration methods typically require\nprofessional manual selection of specialized models or rely on monolithic\narchitectures that fail to generalize across varying degradations. Inspired by\nexpert experience, we propose MoA-VR, the first\n\\underline{M}ixture-\\underline{o}f-\\underline{A}gents \\underline{V}ideo\n\\underline{R}estoration system that mimics the reasoning and processing\nprocedures of human professionals through three coordinated agents: Degradation\nIdentification, Routing and Restoration, and Restoration Quality Assessment.\nSpecifically, we construct a large-scale and high-resolution video degradation\nrecognition benchmark and build a vision-language model (VLM) driven\ndegradation identifier. We further introduce a self-adaptive router powered by\nlarge language models (LLMs), which autonomously learns effective restoration\nstrategies by observing tool usage patterns. To assess intermediate and final\nprocessed video quality, we construct the \\underline{Res}tored\n\\underline{V}ideo \\underline{Q}uality (Res-VQ) dataset and design a dedicated\nVLM-based video quality assessment (VQA) model tailored for restoration tasks.\nExtensive experiments demonstrate that MoA-VR effectively handles diverse and\ncompound degradations, consistently outperforming existing baselines in terms\nof both objective metrics and perceptual quality. These results highlight the\npotential of integrating multimodal intelligence and modular reasoning in\ngeneral-purpose video restoration systems."
                },
                "authors": [
                    {
                        "name": "Lu Liu"
                    },
                    {
                        "name": "Chunlei Cai"
                    },
                    {
                        "name": "Shaocheng Shen"
                    },
                    {
                        "name": "Jianfeng Liang"
                    },
                    {
                        "name": "Weimin Ouyang"
                    },
                    {
                        "name": "Tianxiao Ye"
                    },
                    {
                        "name": "Jian Mao"
                    },
                    {
                        "name": "Huiyu Duan"
                    },
                    {
                        "name": "Jiangchao Yao"
                    },
                    {
                        "name": "Xiaoyun Zhang"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Guangtao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhai"
                },
                "author": "Guangtao Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08506v1",
                "updated": "2025-10-09T17:41:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    41,
                    57,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:41:57Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    41,
                    57,
                    3,
                    282,
                    0
                ],
                "title": "Neologism Learning for Controllability and Self-Verbalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neologism Learning for Controllability and Self-Verbalization"
                },
                "summary": "Humans invent new words when there is a rising demand for a new useful\nconcept (e.g., doomscrolling). We explore and validate a similar idea in our\ncommunication with LLMs: introducing new words to better understand and control\nthe models, expanding on the recently introduced neologism learning. This\nmethod introduces a new word by adding a new word embedding and training with\nexamples that exhibit the concept with no other changes in model parameters. We\nshow that adding a new word allows for control of concepts such as flattery,\nincorrect answers, text length, as well as more complex concepts in AxBench. We\ndiscover that neologisms can also further our understanding of the model via\nself-verbalization: models can describe what each new word means to them in\nnatural language, like explaining that a word that represents a concept of\nincorrect answers means ``a lack of complete, coherent, or meaningful\nanswers...'' To validate self-verbalizations, we introduce plug-in evaluation:\nwe insert the verbalization into the context of a model and measure whether it\ncontrols the target concept. In some self-verbalizations, we find machine-only\nsynonyms: words that seem unrelated to humans but cause similar behavior in\nmachines. Finally, we show how neologism learning can jointly learn multiple\nconcepts in multiple words.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans invent new words when there is a rising demand for a new useful\nconcept (e.g., doomscrolling). We explore and validate a similar idea in our\ncommunication with LLMs: introducing new words to better understand and control\nthe models, expanding on the recently introduced neologism learning. This\nmethod introduces a new word by adding a new word embedding and training with\nexamples that exhibit the concept with no other changes in model parameters. We\nshow that adding a new word allows for control of concepts such as flattery,\nincorrect answers, text length, as well as more complex concepts in AxBench. We\ndiscover that neologisms can also further our understanding of the model via\nself-verbalization: models can describe what each new word means to them in\nnatural language, like explaining that a word that represents a concept of\nincorrect answers means ``a lack of complete, coherent, or meaningful\nanswers...'' To validate self-verbalizations, we introduce plug-in evaluation:\nwe insert the verbalization into the context of a model and measure whether it\ncontrols the target concept. In some self-verbalizations, we find machine-only\nsynonyms: words that seem unrelated to humans but cause similar behavior in\nmachines. Finally, we show how neologism learning can jointly learn multiple\nconcepts in multiple words."
                },
                "authors": [
                    {
                        "name": "John Hewitt"
                    },
                    {
                        "name": "Oyvind Tafjord"
                    },
                    {
                        "name": "Robert Geirhos"
                    },
                    {
                        "name": "Been Kim"
                    }
                ],
                "author_detail": {
                    "name": "Been Kim"
                },
                "author": "Been Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20600v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20600v4",
                "updated": "2025-10-09T17:41:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    41,
                    24,
                    3,
                    282,
                    0
                ],
                "published": "2024-10-27T21:20:18Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    21,
                    20,
                    18,
                    6,
                    301,
                    0
                ],
                "title": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way\n  Intelligibility Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way\n  Intelligibility Protocol"
                },
                "summary": "Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems. Our code is available\nat https://github.com/karannb/interact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems. Our code is available\nat https://github.com/karannb/interact."
                },
                "authors": [
                    {
                        "name": "Harshvardhan Mestha"
                    },
                    {
                        "name": "Karan Bania"
                    },
                    {
                        "name": "Shreyas V Sathyanarayana"
                    },
                    {
                        "name": "Sidong Liu"
                    },
                    {
                        "name": "Ashwin Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Ashwin Srinivasan"
                },
                "author": "Ashwin Srinivasan",
                "arxiv_comment": "Multi-Turn Interactions in Large Language Models (MTI-LLM) Workshop\n  at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20600v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20600v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18114v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18114v2",
                "updated": "2025-10-09T17:39:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    39,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-04-25T06:37:29Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    37,
                    29,
                    4,
                    115,
                    0
                ],
                "title": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection"
                },
                "summary": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them."
                },
                "authors": [
                    {
                        "name": "Atharva Kulkarni"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Joel Ruben Antony Moniz"
                    },
                    {
                        "name": "Xiou Ge"
                    },
                    {
                        "name": "Bo-Hsiang Tseng"
                    },
                    {
                        "name": "Dhivya Piraviperumal"
                    },
                    {
                        "name": "Swabha Swayamdipta"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "Accepted at EMNLP 2025 Findings (Short)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18114v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18114v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v2",
                "updated": "2025-10-09T17:38:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    38,
                    52,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "arxiv_comment": "Original version uploaded on Sep 22, 2025. (v2): Extended Table 2\n  with additional analysis and referenced it in Sec 5.2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11194v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11194v3",
                "updated": "2025-10-09T17:32:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    32,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2024-02-17T05:10:18Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    5,
                    10,
                    18,
                    5,
                    48,
                    0
                ],
                "title": "Evaluating LLMs' Mathematical Reasoning in Financial Document Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs' Mathematical Reasoning in Financial Document Question\n  Answering"
                },
                "summary": "Large Language Models (LLMs), excel in natural language understanding, but\ntheir capability for complex mathematical reasoning with an amalgamation of\nstructured tables and unstructured text is uncertain. This study explores LLMs'\nmathematical reasoning on four financial tabular question-answering datasets:\nTATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with\nvarious models and prompting techniques, we assess how LLMs adapt to complex\ntables and mathematical tasks. We focus on sensitivity to table complexity and\nperformance variations with an increasing number of arithmetic reasoning steps.\nThe results provide insights into LLMs' capabilities and limitations in\nhandling complex mathematical scenarios for semi-structured tables. Ultimately,\nwe introduce a novel prompting technique tailored to semi-structured documents,\nmatching or outperforming other baselines in performance while providing a\nnuanced understanding of LLMs abilities for such a task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), excel in natural language understanding, but\ntheir capability for complex mathematical reasoning with an amalgamation of\nstructured tables and unstructured text is uncertain. This study explores LLMs'\nmathematical reasoning on four financial tabular question-answering datasets:\nTATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with\nvarious models and prompting techniques, we assess how LLMs adapt to complex\ntables and mathematical tasks. We focus on sensitivity to table complexity and\nperformance variations with an increasing number of arithmetic reasoning steps.\nThe results provide insights into LLMs' capabilities and limitations in\nhandling complex mathematical scenarios for semi-structured tables. Ultimately,\nwe introduce a novel prompting technique tailored to semi-structured documents,\nmatching or outperforming other baselines in performance while providing a\nnuanced understanding of LLMs abilities for such a task."
                },
                "authors": [
                    {
                        "name": "Pragya Srivastava"
                    },
                    {
                        "name": "Manuj Malik"
                    },
                    {
                        "name": "Vivek Gupta"
                    },
                    {
                        "name": "Tanuja Ganu"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "arxiv_comment": "26 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11194v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11194v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08489v1",
                "updated": "2025-10-09T17:30:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    30,
                    1,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:30:01Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    30,
                    1,
                    3,
                    282,
                    0
                ],
                "title": "Implementing Semantic Join Operators Efficiently",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing Semantic Join Operators Efficiently"
                },
                "summary": "Semantic query processing engines often support semantic joins, enabling\nusers to match rows that satisfy conditions specified in natural language. Such\njoin conditions can be evaluated using large language models (LLMs) that solve\nnovel tasks without task-specific training.\n  Currently, many semantic query processing engines implement semantic joins\nvia nested loops, invoking the LLM to evaluate the join condition on row pairs.\nInstead, this paper proposes a novel algorithm, inspired by the block nested\nloops join operator implementation in traditional database systems. The\nproposed algorithm integrates batches of rows from both input tables into a\nsingle prompt. The goal of the LLM invocation is to identify all matching row\npairs in the current input. The paper introduces formulas that can be used to\noptimize the size of the row batches, taking into account constraints on the\nsize of the LLM context window (limiting both input and output size). An\nadaptive variant of the proposed algorithm refers to cases in which the size of\nthe output is difficult to estimate. A formal analysis of asymptotic processing\ncosts, as well as empirical results, demonstrates that the proposed approach\nreduces costs significantly and performs well compared to join implementations\nused by recent semantic query processing engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic query processing engines often support semantic joins, enabling\nusers to match rows that satisfy conditions specified in natural language. Such\njoin conditions can be evaluated using large language models (LLMs) that solve\nnovel tasks without task-specific training.\n  Currently, many semantic query processing engines implement semantic joins\nvia nested loops, invoking the LLM to evaluate the join condition on row pairs.\nInstead, this paper proposes a novel algorithm, inspired by the block nested\nloops join operator implementation in traditional database systems. The\nproposed algorithm integrates batches of rows from both input tables into a\nsingle prompt. The goal of the LLM invocation is to identify all matching row\npairs in the current input. The paper introduces formulas that can be used to\noptimize the size of the row batches, taking into account constraints on the\nsize of the LLM context window (limiting both input and output size). An\nadaptive variant of the proposed algorithm refers to cases in which the size of\nthe output is difficult to estimate. A formal analysis of asymptotic processing\ncosts, as well as empirical results, demonstrates that the proposed approach\nreduces costs significantly and performs well compared to join implementations\nused by recent semantic query processing engines."
                },
                "authors": [
                    {
                        "name": "Immanuel Trummer"
                    }
                ],
                "author_detail": {
                    "name": "Immanuel Trummer"
                },
                "author": "Immanuel Trummer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23752v2",
                "updated": "2025-10-09T17:29:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    29,
                    59,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-29T17:59:38Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    38,
                    3,
                    149,
                    0
                ],
                "title": "ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks"
                },
                "summary": "Recent progress in large language models (LLMs) has enabled tool-augmented\nagents capable of solving complex real-world tasks through step-by-step\nreasoning. However, existing evaluations often focus on general-purpose or\nmultimodal scenarios, leaving a gap in domain-specific benchmarks that assess\ntool-use capabilities in complex remote sensing use cases. We present ThinkGeo,\nan agentic benchmark designed to evaluate LLM-driven agents on remote sensing\ntasks via structured tool use and multi-step planning. Inspired by\ntool-interaction paradigms, ThinkGeo includes human-curated queries spanning a\nwide range of real-world applications such as urban planning, disaster\nassessment and change analysis, environmental monitoring, transportation\nanalysis, aviation monitoring, recreational infrastructure, and industrial site\nanalysis. Queries are grounded in satellite or aerial imagery, including both\noptical RGB and SAR data, and require agents to reason through a diverse\ntoolset. We implement a ReAct-style interaction loop and evaluate both open and\nclosed-source LLMs (e.g., GPT-4o, Qwen2.5) on 486 structured agentic tasks with\n1,773 expert-verified reasoning steps. The benchmark reports both step-wise\nexecution metrics and final answer correctness. Our analysis reveals notable\ndisparities in tool accuracy and planning consistency across models. ThinkGeo\nprovides the first extensive testbed for evaluating how tool-enabled LLMs\nhandle spatial reasoning in remote sensing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has enabled tool-augmented\nagents capable of solving complex real-world tasks through step-by-step\nreasoning. However, existing evaluations often focus on general-purpose or\nmultimodal scenarios, leaving a gap in domain-specific benchmarks that assess\ntool-use capabilities in complex remote sensing use cases. We present ThinkGeo,\nan agentic benchmark designed to evaluate LLM-driven agents on remote sensing\ntasks via structured tool use and multi-step planning. Inspired by\ntool-interaction paradigms, ThinkGeo includes human-curated queries spanning a\nwide range of real-world applications such as urban planning, disaster\nassessment and change analysis, environmental monitoring, transportation\nanalysis, aviation monitoring, recreational infrastructure, and industrial site\nanalysis. Queries are grounded in satellite or aerial imagery, including both\noptical RGB and SAR data, and require agents to reason through a diverse\ntoolset. We implement a ReAct-style interaction loop and evaluate both open and\nclosed-source LLMs (e.g., GPT-4o, Qwen2.5) on 486 structured agentic tasks with\n1,773 expert-verified reasoning steps. The benchmark reports both step-wise\nexecution metrics and final answer correctness. Our analysis reveals notable\ndisparities in tool accuracy and planning consistency across models. ThinkGeo\nprovides the first extensive testbed for evaluating how tool-enabled LLMs\nhandle spatial reasoning in remote sensing."
                },
                "authors": [
                    {
                        "name": "Akashah Shabbir"
                    },
                    {
                        "name": "Muhammad Akhtar Munir"
                    },
                    {
                        "name": "Akshay Dudhane"
                    },
                    {
                        "name": "Muhammad Umer Sheikh"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "name": "Paolo Fraccaro"
                    },
                    {
                        "name": "Juan Bernabe Moreno"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    },
                    {
                        "name": "Salman Khan"
                    }
                ],
                "author_detail": {
                    "name": "Salman Khan"
                },
                "author": "Salman Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08483v1",
                "updated": "2025-10-09T17:24:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    24,
                    54,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:24:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    24,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepPrune: Parallel Scaling without Inter-trace Redundancy"
                },
                "summary": "Parallel scaling has emerged as a powerful paradigm to enhance reasoning\ncapabilities in large language models (LLMs) by generating multiple\nChain-of-Thought (CoT) traces simultaneously. However, this approach introduces\nsignificant computational inefficiency due to inter-trace redundancy -- our\nanalysis reveals that over 80% of parallel reasoning traces yield identical\nfinal answers, representing substantial wasted computation. To address this\ncritical efficiency bottleneck, we propose DeepPrune, a novel framework that\nenables efficient parallel scaling through dynamic pruning. Our method features\na specialized judge model trained with focal loss and oversampling techniques\nto accurately predict answer equivalence from partial reasoning traces which\nrealizes 0.87 AUROC on equivalence prediction, combined with an online greedy\nclustering algorithm that dynamically prunes redundant paths while preserving\nanswer diversity. Comprehensive evaluations across three challenging benchmarks\n(AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that\nDeepPrune achieves remarkable token reduction by over 80% compared to\nconventional consensus sampling on most cases, while maintaining competitive\naccuracy within 3 percentage points. Our work establishes a new standard for\nefficient parallel reasoning, making high-performance reasoning more efficient.\nOur code and data are here: https://deepprune.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel scaling has emerged as a powerful paradigm to enhance reasoning\ncapabilities in large language models (LLMs) by generating multiple\nChain-of-Thought (CoT) traces simultaneously. However, this approach introduces\nsignificant computational inefficiency due to inter-trace redundancy -- our\nanalysis reveals that over 80% of parallel reasoning traces yield identical\nfinal answers, representing substantial wasted computation. To address this\ncritical efficiency bottleneck, we propose DeepPrune, a novel framework that\nenables efficient parallel scaling through dynamic pruning. Our method features\na specialized judge model trained with focal loss and oversampling techniques\nto accurately predict answer equivalence from partial reasoning traces which\nrealizes 0.87 AUROC on equivalence prediction, combined with an online greedy\nclustering algorithm that dynamically prunes redundant paths while preserving\nanswer diversity. Comprehensive evaluations across three challenging benchmarks\n(AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that\nDeepPrune achieves remarkable token reduction by over 80% compared to\nconventional consensus sampling on most cases, while maintaining competitive\naccuracy within 3 percentage points. Our work establishes a new standard for\nefficient parallel reasoning, making high-performance reasoning more efficient.\nOur code and data are here: https://deepprune.github.io/"
                },
                "authors": [
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Yaxuan Li"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "15 pages, 4 figures, please check out the project page:\n  https://deepprune.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08482v1",
                "updated": "2025-10-09T17:21:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    21,
                    59,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:21:59Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    21,
                    59,
                    3,
                    282,
                    0
                ],
                "title": "The Visual Iconicity Challenge: Evaluating Vision-Language Models on\n  Sign Language Form-Meaning Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Visual Iconicity Challenge: Evaluating Vision-Language Models on\n  Sign Language Form-Meaning Mapping"
                },
                "summary": "Iconicity, the resemblance between linguistic form and meaning, is pervasive\nin signed languages, offering a natural testbed for visual grounding. For\nvision-language models (VLMs), the challenge is to recover such essential\nmappings from dynamic human motion rather than static context. We introduce the\n\\textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts\npsycholinguistic measures to evaluate VLMs on three tasks: (i) phonological\nsign-form prediction (e.g., handshape, location), (ii) transparency (inferring\nmeaning from visual form), and (iii) graded iconicity ratings. We assess $13$\nstate-of-the-art VLMs in zero- and few-shot settings on Sign Language of the\nNetherlands and compare them to human baselines. On \\textit{phonological form\nprediction}, VLMs recover some handshape and location detail but remain below\nhuman performance; on \\textit{transparency}, they are far from human baselines;\nand only top models correlate moderately with human \\textit{iconicity ratings}.\nInterestingly, \\textit{models with stronger phonological form prediction\ncorrelate better with human iconicity judgment}, indicating shared sensitivity\nto visually grounded structure. Our findings validate these diagnostic tasks\nand motivate human-centric signals and embodied learning methods for modelling\niconicity and improving visual grounding in multimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iconicity, the resemblance between linguistic form and meaning, is pervasive\nin signed languages, offering a natural testbed for visual grounding. For\nvision-language models (VLMs), the challenge is to recover such essential\nmappings from dynamic human motion rather than static context. We introduce the\n\\textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts\npsycholinguistic measures to evaluate VLMs on three tasks: (i) phonological\nsign-form prediction (e.g., handshape, location), (ii) transparency (inferring\nmeaning from visual form), and (iii) graded iconicity ratings. We assess $13$\nstate-of-the-art VLMs in zero- and few-shot settings on Sign Language of the\nNetherlands and compare them to human baselines. On \\textit{phonological form\nprediction}, VLMs recover some handshape and location detail but remain below\nhuman performance; on \\textit{transparency}, they are far from human baselines;\nand only top models correlate moderately with human \\textit{iconicity ratings}.\nInterestingly, \\textit{models with stronger phonological form prediction\ncorrelate better with human iconicity judgment}, indicating shared sensitivity\nto visually grounded structure. Our findings validate these diagnostic tasks\nand motivate human-centric signals and embodied learning methods for modelling\niconicity and improving visual grounding in multimodal models."
                },
                "authors": [
                    {
                        "name": "Onur Keleş"
                    },
                    {
                        "name": "Aslı Özyürek"
                    },
                    {
                        "name": "Gerardo Ortega"
                    },
                    {
                        "name": "Kadir Gökgö"
                    },
                    {
                        "name": "Esam Ghaleb"
                    }
                ],
                "author_detail": {
                    "name": "Esam Ghaleb"
                },
                "author": "Esam Ghaleb",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08481v1",
                "updated": "2025-10-09T17:20:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    20,
                    54,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:20:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    20,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "Forecasting the Buzz: Enriching Hashtag Popularity Prediction with LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting the Buzz: Enriching Hashtag Popularity Prediction with LLM\n  Reasoning"
                },
                "summary": "Hashtag trends ignite campaigns, shift public opinion, and steer millions of\ndollars in advertising spend, yet forecasting which tag goes viral is elusive.\nClassical regressors digest surface features but ignore context, while large\nlanguage models (LLMs) excel at contextual reasoning but misestimate numbers.\nWe present BuzzProphet, a reasoning-augmented hashtag popularity prediction\nframework that (1) instructs an LLM to articulate a hashtag's topical virality,\naudience reach, and timing advantage; (2) utilizes these popularity-oriented\nrationales to enrich the input features; and (3) regresses on these inputs. To\nfacilitate evaluation, we release HashView, a 7,532-hashtag benchmark curated\nfrom social media. Across diverse regressor-LLM combinations, BuzzProphet\nreduces RMSE by up to 2.8% and boosts correlation by 30% over baselines, while\nproducing human-readable rationales. Results demonstrate that using LLMs as\ncontext reasoners rather than numeric predictors injects domain insight into\ntabular models, yielding an interpretable and deployable solution for social\nmedia trend forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hashtag trends ignite campaigns, shift public opinion, and steer millions of\ndollars in advertising spend, yet forecasting which tag goes viral is elusive.\nClassical regressors digest surface features but ignore context, while large\nlanguage models (LLMs) excel at contextual reasoning but misestimate numbers.\nWe present BuzzProphet, a reasoning-augmented hashtag popularity prediction\nframework that (1) instructs an LLM to articulate a hashtag's topical virality,\naudience reach, and timing advantage; (2) utilizes these popularity-oriented\nrationales to enrich the input features; and (3) regresses on these inputs. To\nfacilitate evaluation, we release HashView, a 7,532-hashtag benchmark curated\nfrom social media. Across diverse regressor-LLM combinations, BuzzProphet\nreduces RMSE by up to 2.8% and boosts correlation by 30% over baselines, while\nproducing human-readable rationales. Results demonstrate that using LLMs as\ncontext reasoners rather than numeric predictors injects domain insight into\ntabular models, yielding an interpretable and deployable solution for social\nmedia trend forecasting."
                },
                "authors": [
                    {
                        "name": "Yifei Xu"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Herun Wan"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Zhen Hou"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "arxiv_doi": "10.1145/3746252.3760970",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3760970",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.08481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08480v1",
                "updated": "2025-10-09T17:20:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    20,
                    44,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:20:44Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    20,
                    44,
                    3,
                    282,
                    0
                ],
                "title": "Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-STAR: Reinforcing Open-Vocabulary Action Recognition with Tools"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated remarkable\npotential in bridging visual and textual reasoning, yet their reliance on\ntext-centric priors often limits their ability to disentangle semantically\nsimilar actions in open-vocabulary scenarios. To address this, we propose\nVideo-STAR, a framework that harmonizes contextual sub-motion decomposition\nwith tool-augmented reinforcement learning for open-vocabulary action\nrecognition (OVAR). Unlike prior methods that treat actions as monolithic\nentities, our approach innovatively decomposes actions into discriminative\nsub-motions for fine-grained matching while dynamically invoking\ndomain-specific tools for cross-modal interleaving, thereby enabling\ncategory-specific reasoning capacity and reducing cross-modal hallucination.\nMoreover, by designing a hierarchical reward that balances tool-usage\nefficiency, sub-motion relevance, and structural coherence in reasoning, our\nmethod autonomously leverages external tools to prioritize sub-motion patterns\nwithout explicit supervision, transmitting from text-centric reasoning to\nvisually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2,\nKinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art\nperformance, outperforming existing methods in distinguishing fine-grained\nactions and handling cross-modal hallucination, validating our excellent\nrobustness and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated remarkable\npotential in bridging visual and textual reasoning, yet their reliance on\ntext-centric priors often limits their ability to disentangle semantically\nsimilar actions in open-vocabulary scenarios. To address this, we propose\nVideo-STAR, a framework that harmonizes contextual sub-motion decomposition\nwith tool-augmented reinforcement learning for open-vocabulary action\nrecognition (OVAR). Unlike prior methods that treat actions as monolithic\nentities, our approach innovatively decomposes actions into discriminative\nsub-motions for fine-grained matching while dynamically invoking\ndomain-specific tools for cross-modal interleaving, thereby enabling\ncategory-specific reasoning capacity and reducing cross-modal hallucination.\nMoreover, by designing a hierarchical reward that balances tool-usage\nefficiency, sub-motion relevance, and structural coherence in reasoning, our\nmethod autonomously leverages external tools to prioritize sub-motion patterns\nwithout explicit supervision, transmitting from text-centric reasoning to\nvisually grounded inference. Extensive evaluations on HMDB-51, UCF-101, SSv2,\nKinetics-400, and Kinetics-600 datasets demonstrate our state-of-the-art\nperformance, outperforming existing methods in distinguishing fine-grained\nactions and handling cross-modal hallucination, validating our excellent\nrobustness and generalization."
                },
                "authors": [
                    {
                        "name": "Zhenlong Yuan"
                    },
                    {
                        "name": "Xiangyan Qu"
                    },
                    {
                        "name": "Chengxuan Qian"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Jing Tang"
                    },
                    {
                        "name": "Lei Sun"
                    },
                    {
                        "name": "Xiangxiang Chu"
                    },
                    {
                        "name": "Dapeng Zhang"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Shuo Li"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Li"
                },
                "author": "Shuo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10190v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10190v3",
                "updated": "2025-10-09T17:20:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    20,
                    18,
                    3,
                    282,
                    0
                ],
                "published": "2024-10-14T06:22:11Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    6,
                    22,
                    11,
                    0,
                    288,
                    0
                ],
                "title": "Language Model Embeddings Can Be Sufficient for Bayesian Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model Embeddings Can Be Sufficient for Bayesian Optimization"
                },
                "summary": "Bayesian Optimization is ubiquitous in experimental design and black-box\noptimization for improving search efficiency. However, most existing approaches\nrely on regression models which are limited to fixed search spaces and\nstructured, tabular input features. This paper explores the use of LLM\nembeddings over string inputs for in-context regression in Bayesian\nOptimization. Our results show that representing inputs as strings enables\ngeneral-purpose regression across diverse domains, including synthetic,\ncombinatorial, and hyperparameter optimization. Furthermore, our approach\nachieves optimization performance comparable to state-of-the-art Gaussian\nProcess-based methods such as Google Vizier, and demonstrates potential for\nbroader and more flexible applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Optimization is ubiquitous in experimental design and black-box\noptimization for improving search efficiency. However, most existing approaches\nrely on regression models which are limited to fixed search spaces and\nstructured, tabular input features. This paper explores the use of LLM\nembeddings over string inputs for in-context regression in Bayesian\nOptimization. Our results show that representing inputs as strings enables\ngeneral-purpose regression across diverse domains, including synthetic,\ncombinatorial, and hyperparameter optimization. Furthermore, our approach\nachieves optimization performance comparable to state-of-the-art Gaussian\nProcess-based methods such as Google Vizier, and demonstrates potential for\nbroader and more flexible applications."
                },
                "authors": [
                    {
                        "name": "Tung Nguyen"
                    },
                    {
                        "name": "Qiuyi Zhang"
                    },
                    {
                        "name": "Bangding Yang"
                    },
                    {
                        "name": "Chansoo Lee"
                    },
                    {
                        "name": "Jorg Bornschein"
                    },
                    {
                        "name": "Yingjie Miao"
                    },
                    {
                        "name": "Sagi Perel"
                    },
                    {
                        "name": "Yutian Chen"
                    },
                    {
                        "name": "Xingyou Song"
                    }
                ],
                "author_detail": {
                    "name": "Xingyou Song"
                },
                "author": "Xingyou Song",
                "arxiv_comment": "Code can be found in\n  https://github.com/google-research/optformer/tree/main/optformer/embed_then_regress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10190v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10190v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02227v2",
                "updated": "2025-10-09T17:18:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    18,
                    49,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-02T17:14:00Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    14,
                    0,
                    3,
                    275,
                    0
                ],
                "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Yuan"
                    },
                    {
                        "name": "Yujuan Ding"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Jinyu Cai"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23045v2",
                "updated": "2025-10-09T17:13:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    13,
                    2,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-27T01:49:13Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    1,
                    49,
                    13,
                    5,
                    270,
                    0
                ],
                "title": "Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied to software engineering\n(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent\nframeworks with multi-turn interactions and workflow-based Agentless methods\nwith single-turn verifiable steps. We argue these paradigms are not mutually\nexclusive: reasoning-intensive Agentless training induces skill priors,\nincluding localization, code edit, and self-reflection that enable efficient\nand effective SWE-Agent adaptation. In this work, we first curate the Agentless\ntraining recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\\%\non SWE-bench Verified, the best among workflow approaches. With additional SFT\nadaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to\n48.6\\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These\nresults show that structured skill priors from Agentless training can bridge\nworkflow and agentic frameworks for transferable coding agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied to software engineering\n(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent\nframeworks with multi-turn interactions and workflow-based Agentless methods\nwith single-turn verifiable steps. We argue these paradigms are not mutually\nexclusive: reasoning-intensive Agentless training induces skill priors,\nincluding localization, code edit, and self-reflection that enable efficient\nand effective SWE-Agent adaptation. In this work, we first curate the Agentless\ntraining recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\\%\non SWE-bench Verified, the best among workflow approaches. With additional SFT\nadaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to\n48.6\\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These\nresults show that structured skill priors from Agentless training can bridge\nworkflow and agentic frameworks for transferable coding agents."
                },
                "authors": [
                    {
                        "name": "Zonghan Yang"
                    },
                    {
                        "name": "Shengjie Wang"
                    },
                    {
                        "name": "Kelin Fu"
                    },
                    {
                        "name": "Wenyang He"
                    },
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Yanhao Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Zhenxing Hu"
                    },
                    {
                        "name": "Kaitai Zhang"
                    },
                    {
                        "name": "Shuyi Wang"
                    },
                    {
                        "name": "Huarong Chen"
                    },
                    {
                        "name": "Flood Sung"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Tianyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Liu"
                },
                "author": "Tianyu Liu",
                "arxiv_comment": "58 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14756v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14756v2",
                "updated": "2025-10-09T17:09:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    9,
                    12,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-20T15:54:48Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    54,
                    48,
                    1,
                    140,
                    0
                ],
                "title": "LLINBO: Trustworthy LLM-in-the-Loop Bayesian Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLINBO: Trustworthy LLM-in-the-Loop Bayesian Optimization"
                },
                "summary": "Bayesian optimization (BO) is a sequential decision-making tool widely used\nfor optimizing expensive black-box functions. Recently, Large Language Models\n(LLMs) have shown remarkable adaptability in low-data regimes, making them\npromising tools for black-box optimization by leveraging contextual knowledge\nto propose high-quality query points. However, relying solely on LLMs as\noptimization agents introduces risks due to their lack of explicit surrogate\nmodeling and calibrated uncertainty, as well as their inherently opaque\ninternal mechanisms. This structural opacity makes it difficult to characterize\nor control the exploration-exploitation trade-off, ultimately undermining\ntheoretical tractability and reliability. To address this, we propose LLINBO:\nLLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with\nstatistical surrogate experts (e.g., Gaussian Processes (GP)). The core\nphilosophy is to leverage contextual reasoning strengths of LLMs for early\nexploration, while relying on principled statistical models to guide efficient\nexploitation. Specifically, we introduce three mechanisms that enable this\ncollaboration and establish their theoretical guarantees. We end the paper with\na real-life proof-of-concept in the context of 3D printing. The code to\nreproduce the results can be found at\nhttps://github.com/UMDataScienceLab/LLM-in-the-Loop-BO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian optimization (BO) is a sequential decision-making tool widely used\nfor optimizing expensive black-box functions. Recently, Large Language Models\n(LLMs) have shown remarkable adaptability in low-data regimes, making them\npromising tools for black-box optimization by leveraging contextual knowledge\nto propose high-quality query points. However, relying solely on LLMs as\noptimization agents introduces risks due to their lack of explicit surrogate\nmodeling and calibrated uncertainty, as well as their inherently opaque\ninternal mechanisms. This structural opacity makes it difficult to characterize\nor control the exploration-exploitation trade-off, ultimately undermining\ntheoretical tractability and reliability. To address this, we propose LLINBO:\nLLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with\nstatistical surrogate experts (e.g., Gaussian Processes (GP)). The core\nphilosophy is to leverage contextual reasoning strengths of LLMs for early\nexploration, while relying on principled statistical models to guide efficient\nexploitation. Specifically, we introduce three mechanisms that enable this\ncollaboration and establish their theoretical guarantees. We end the paper with\na real-life proof-of-concept in the context of 3D printing. The code to\nreproduce the results can be found at\nhttps://github.com/UMDataScienceLab/LLM-in-the-Loop-BO."
                },
                "authors": [
                    {
                        "name": "Chih-Yu Chang"
                    },
                    {
                        "name": "Milad Azvar"
                    },
                    {
                        "name": "Chinedum Okwudire"
                    },
                    {
                        "name": "Raed Al Kontar"
                    }
                ],
                "author_detail": {
                    "name": "Raed Al Kontar"
                },
                "author": "Raed Al Kontar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14756v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14756v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08468v1",
                "updated": "2025-10-09T17:09:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    9,
                    4,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:09:04Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    9,
                    4,
                    3,
                    282,
                    0
                ],
                "title": "Dynamic Automated Deduction by Contradiction Separation: The Standard\n  Extension Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Automated Deduction by Contradiction Separation: The Standard\n  Extension Algorithm"
                },
                "summary": "Automated deduction seeks to enable machines to reason with mathematical\nprecision and logical completeness. Classical resolution-based systems, such as\nProver9, E, and Vampire, rely on binary inference, which inherently limits\nmulti-clause synergy during proof search. The Contradiction Separation\nExtension (CSE) framework, introduced by Xu et al. (2018), overcame this\ntheoretical limitation by extending deduction beyond binary inference. However,\nthe original work did not specify how contradictions are algorithmically\nconstructed and extended in practice. This paper presents the Standard\nExtension algorithm, the first explicit procedural realization of contradiction\nseparation reasoning. The proposed method dynamically constructs contradictions\nthrough complementary literal extension, thereby operationalizing the CSE\ntheory within a unified algorithm for satisfiability and unsatisfiability\nchecking. The algorithm's soundness and completeness are formally proven, and\nits effectiveness is supported indirectly through the performance of CSE-based\nsystems, including CSE, CSE-E, CSI-E, and CSI-Enig in major automated reasoning\ncompetitions (CASC) in the last few years. These results confirm that the\nStandard Extension mechanism constitutes a robust and practically validated\nfoundation for dynamic, multi-clause automated deduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated deduction seeks to enable machines to reason with mathematical\nprecision and logical completeness. Classical resolution-based systems, such as\nProver9, E, and Vampire, rely on binary inference, which inherently limits\nmulti-clause synergy during proof search. The Contradiction Separation\nExtension (CSE) framework, introduced by Xu et al. (2018), overcame this\ntheoretical limitation by extending deduction beyond binary inference. However,\nthe original work did not specify how contradictions are algorithmically\nconstructed and extended in practice. This paper presents the Standard\nExtension algorithm, the first explicit procedural realization of contradiction\nseparation reasoning. The proposed method dynamically constructs contradictions\nthrough complementary literal extension, thereby operationalizing the CSE\ntheory within a unified algorithm for satisfiability and unsatisfiability\nchecking. The algorithm's soundness and completeness are formally proven, and\nits effectiveness is supported indirectly through the performance of CSE-based\nsystems, including CSE, CSE-E, CSI-E, and CSI-Enig in major automated reasoning\ncompetitions (CASC) in the last few years. These results confirm that the\nStandard Extension mechanism constitutes a robust and practically validated\nfoundation for dynamic, multi-clause automated deduction."
                },
                "authors": [
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Xingxing He"
                    },
                    {
                        "name": "Shuwei Chen"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Xiaomei Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Xiaomei Zhong"
                },
                "author": "Xiaomei Zhong",
                "arxiv_comment": "36 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08827v3",
                "updated": "2025-10-09T17:08:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    8,
                    52,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-10T17:59:43Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    43,
                    2,
                    253,
                    0
                ],
                "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Reinforcement Learning for Large Reasoning Models"
                },
                "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs"
                },
                "authors": [
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Kai Tian"
                    },
                    {
                        "name": "Guoli Jia"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Sihang Zeng"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Haozhan Li"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Yuru Wang"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Fangfu Liu"
                    },
                    {
                        "name": "Xiang Xu"
                    },
                    {
                        "name": "Jiaze Ma"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Zonglin Li"
                    },
                    {
                        "name": "Huayu Chen"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Zhenzhao Yuan"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "Fixed typos; added missing and recent citations (117 -> 120 pages)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08466v1",
                "updated": "2025-10-09T17:07:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    7,
                    55,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:07:55Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    7,
                    55,
                    3,
                    282,
                    0
                ],
                "title": "In-Context Clustering with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Clustering with Large Language Models"
                },
                "summary": "We propose In-Context Clustering (ICC), a flexible LLM-based procedure for\nclustering data from diverse distributions. Unlike traditional clustering\nalgorithms constrained by predefined similarity measures, ICC flexibly captures\ncomplex relationships among inputs through an attention mechanism. We show that\npretrained LLMs exhibit impressive zero-shot clustering capabilities on\ntext-encoded numeric data, with attention matrices showing salient cluster\npatterns. Spectral clustering using attention matrices offers surprisingly\ncompetitive performance. We further enhance the clustering capabilities of LLMs\non numeric and image data through fine-tuning using the Next Token Prediction\n(NTP) loss. Moreover, the flexibility of LLM prompting enables text-conditioned\nimage clustering, a capability that classical clustering methods lack. Our work\nextends in-context learning to an unsupervised setting, showcasing the\neffectiveness and flexibility of LLMs for clustering. Our code is available at\nhttps://agenticlearning.ai/icc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose In-Context Clustering (ICC), a flexible LLM-based procedure for\nclustering data from diverse distributions. Unlike traditional clustering\nalgorithms constrained by predefined similarity measures, ICC flexibly captures\ncomplex relationships among inputs through an attention mechanism. We show that\npretrained LLMs exhibit impressive zero-shot clustering capabilities on\ntext-encoded numeric data, with attention matrices showing salient cluster\npatterns. Spectral clustering using attention matrices offers surprisingly\ncompetitive performance. We further enhance the clustering capabilities of LLMs\non numeric and image data through fine-tuning using the Next Token Prediction\n(NTP) loss. Moreover, the flexibility of LLM prompting enables text-conditioned\nimage clustering, a capability that classical clustering methods lack. Our work\nextends in-context learning to an unsupervised setting, showcasing the\neffectiveness and flexibility of LLMs for clustering. Our code is available at\nhttps://agenticlearning.ai/icc."
                },
                "authors": [
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Mengye Ren"
                    },
                    {
                        "name": "Andrew Gordon Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Gordon Wilson"
                },
                "author": "Andrew Gordon Wilson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08464v1",
                "updated": "2025-10-09T17:07:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    7,
                    30,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:07:30Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    7,
                    30,
                    3,
                    282,
                    0
                ],
                "title": "Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be\n  Recovered",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be\n  Recovered"
                },
                "summary": "Vision-Language-Action (VLA) models have advanced robotic capabilities but\nremain challenging to deploy on resource-limited hardware. Pruning has enabled\nefficient compression of large language models (LLMs), yet it is largely\nunderstudied in robotics. Surprisingly, we observe that pruning VLA models\nleads to drastic degradation and increased safety violations. We introduce\nGLUESTICK, a post-pruning recovery method that restores much of the original\nmodel's functionality while retaining sparsity benefits. Our method performs a\none-time interpolation between the dense and pruned models in weight-space to\ncompute a corrective term. This correction is used during inference by each\npruned layer to recover lost capabilities with minimal overhead. GLUESTICK\nrequires no additional training, is agnostic to the pruning algorithm, and\nintroduces a single hyperparameter that controls the tradeoff between\nefficiency and accuracy. Across diverse VLA architectures and tasks in\nmanipulation and navigation, GLUESTICK achieves competitive memory efficiency\nwhile substantially recovering success rates and reducing safety violations.\nAdditional material can be found at: https://gluestick-vla.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have advanced robotic capabilities but\nremain challenging to deploy on resource-limited hardware. Pruning has enabled\nefficient compression of large language models (LLMs), yet it is largely\nunderstudied in robotics. Surprisingly, we observe that pruning VLA models\nleads to drastic degradation and increased safety violations. We introduce\nGLUESTICK, a post-pruning recovery method that restores much of the original\nmodel's functionality while retaining sparsity benefits. Our method performs a\none-time interpolation between the dense and pruned models in weight-space to\ncompute a corrective term. This correction is used during inference by each\npruned layer to recover lost capabilities with minimal overhead. GLUESTICK\nrequires no additional training, is agnostic to the pruning algorithm, and\nintroduces a single hyperparameter that controls the tradeoff between\nefficiency and accuracy. Across diverse VLA architectures and tasks in\nmanipulation and navigation, GLUESTICK achieves competitive memory efficiency\nwhile substantially recovering success rates and reducing safety violations.\nAdditional material can be found at: https://gluestick-vla.github.io/."
                },
                "authors": [
                    {
                        "name": "Jason Jabbour"
                    },
                    {
                        "name": "Dong-Ki Kim"
                    },
                    {
                        "name": "Max Smith"
                    },
                    {
                        "name": "Jay Patrikar"
                    },
                    {
                        "name": "Radhika Ghosal"
                    },
                    {
                        "name": "Youhui Wang"
                    },
                    {
                        "name": "Ali Agha"
                    },
                    {
                        "name": "Vijay Janapa Reddi"
                    },
                    {
                        "name": "Shayegan Omidshafiei"
                    }
                ],
                "author_detail": {
                    "name": "Shayegan Omidshafiei"
                },
                "author": "Shayegan Omidshafiei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08460v1",
                "updated": "2025-10-09T17:04:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    4,
                    28,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:04:28Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    4,
                    28,
                    3,
                    282,
                    0
                ],
                "title": "LeWiDi-2025 at NLPerspectives: The Third Edition of the Learning with\n  Disagreements Shared Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeWiDi-2025 at NLPerspectives: The Third Edition of the Learning with\n  Disagreements Shared Task"
                },
                "summary": "Many researchers have reached the conclusion that AI models should be trained\nto be aware of the possibility of variation and disagreement in human\njudgments, and evaluated as per their ability to recognize such variation. The\nLEWIDI series of shared tasks on Learning With Disagreements was established to\npromote this approach to training and evaluating AI models, by making suitable\ndatasets more accessible and by developing evaluation methods. The third\nedition of the task builds on this goal by extending the LEWIDI benchmark to\nfour datasets spanning paraphrase identification, irony detection, sarcasm\ndetection, and natural language inference, with labeling schemes that include\nnot only categorical judgments as in previous editions, but ordinal judgments\nas well. Another novelty is that we adopt two complementary paradigms to\nevaluate disagreement-aware systems: the soft-label approach, in which models\npredict population-level distributions of judgments, and the perspectivist\napproach, in which models predict the interpretations of individual annotators.\nCrucially, we moved beyond standard metrics such as cross-entropy, and tested\nnew evaluation metrics for the two paradigms. The task attracted diverse\nparticipation, and the results provide insights into the strengths and\nlimitations of methods to modeling variation. Together, these contributions\nstrengthen LEWIDI as a framework and provide new resources, benchmarks, and\nfindings to support the development of disagreement-aware technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many researchers have reached the conclusion that AI models should be trained\nto be aware of the possibility of variation and disagreement in human\njudgments, and evaluated as per their ability to recognize such variation. The\nLEWIDI series of shared tasks on Learning With Disagreements was established to\npromote this approach to training and evaluating AI models, by making suitable\ndatasets more accessible and by developing evaluation methods. The third\nedition of the task builds on this goal by extending the LEWIDI benchmark to\nfour datasets spanning paraphrase identification, irony detection, sarcasm\ndetection, and natural language inference, with labeling schemes that include\nnot only categorical judgments as in previous editions, but ordinal judgments\nas well. Another novelty is that we adopt two complementary paradigms to\nevaluate disagreement-aware systems: the soft-label approach, in which models\npredict population-level distributions of judgments, and the perspectivist\napproach, in which models predict the interpretations of individual annotators.\nCrucially, we moved beyond standard metrics such as cross-entropy, and tested\nnew evaluation metrics for the two paradigms. The task attracted diverse\nparticipation, and the results provide insights into the strengths and\nlimitations of methods to modeling variation. Together, these contributions\nstrengthen LEWIDI as a framework and provide new resources, benchmarks, and\nfindings to support the development of disagreement-aware technologies."
                },
                "authors": [
                    {
                        "name": "Elisa Leonardelli"
                    },
                    {
                        "name": "Silvia Casola"
                    },
                    {
                        "name": "Siyao Peng"
                    },
                    {
                        "name": "Giulia Rizzi"
                    },
                    {
                        "name": "Valerio Basile"
                    },
                    {
                        "name": "Elisabetta Fersini"
                    },
                    {
                        "name": "Diego Frassinelli"
                    },
                    {
                        "name": "Hyewon Jang"
                    },
                    {
                        "name": "Maja Pavlovic"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Massimo Poesio"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Poesio"
                },
                "author": "Massimo Poesio",
                "arxiv_comment": "14 pages; LeWiDi-2025 shared task description paper at NLPerspective\n  workshop at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08457v1",
                "updated": "2025-10-09T17:03:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    3,
                    28,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:03:28Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    3,
                    28,
                    3,
                    282,
                    0
                ],
                "title": "ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level\n  Entropy Shaping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level\n  Entropy Shaping"
                },
                "summary": "Recent advances in multimodal large reasoning models (MLRMs) have\nsubstantially improved their ability to solve complex textual and visual tasks.\nHowever, these models tend to overthink on simple problems, producing\nunnecessarily lengthy reasoning traces, while under-exploring on challenging\nones, leading to missed solutions. To address this imbalance, we propose ARES,\na unified open-source framework for adaptive reasoning that dynamically\nallocates exploration effort based on task difficulty. Our approach is\nmotivated by two key empirical findings: (i) while single-token entropy is\nnoisy, high window-entropy (HWE) tokens (token-level entropies averaged under a\nsliding window) can reliably capture reasoning-critical moments; and (ii)\nreducing HWE usage benefits easy problems, while increasing it is essential for\nsolving hard ones. Building on these insights, ARES introduces a two-stage\ntraining pipeline. In the Adaptive Cold-Start stage, we curate multimodal and\ntextual data paired with reasoning traces of length proportional to problem\ndifficulty, equipping the model with initial difficulty awareness. In the\nsecond stage, we develop Adaptive Entropy Policy Optimization (AEPO), which\nuses HWE tokens as exploration triggers to decide when to explore, and a\nhierarchical entropy reward with dynamic KL control to decide how much to\nexplore. Extensive experiments demonstrate that ARES achieves superior\nperformance and reasoning efficiency across diverse mathematical, logical, and\nmultimodal benchmarks, while closing the gap to leading commercial systems\nunder significantly lower inference costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal large reasoning models (MLRMs) have\nsubstantially improved their ability to solve complex textual and visual tasks.\nHowever, these models tend to overthink on simple problems, producing\nunnecessarily lengthy reasoning traces, while under-exploring on challenging\nones, leading to missed solutions. To address this imbalance, we propose ARES,\na unified open-source framework for adaptive reasoning that dynamically\nallocates exploration effort based on task difficulty. Our approach is\nmotivated by two key empirical findings: (i) while single-token entropy is\nnoisy, high window-entropy (HWE) tokens (token-level entropies averaged under a\nsliding window) can reliably capture reasoning-critical moments; and (ii)\nreducing HWE usage benefits easy problems, while increasing it is essential for\nsolving hard ones. Building on these insights, ARES introduces a two-stage\ntraining pipeline. In the Adaptive Cold-Start stage, we curate multimodal and\ntextual data paired with reasoning traces of length proportional to problem\ndifficulty, equipping the model with initial difficulty awareness. In the\nsecond stage, we develop Adaptive Entropy Policy Optimization (AEPO), which\nuses HWE tokens as exploration triggers to decide when to explore, and a\nhierarchical entropy reward with dynamic KL control to decide how much to\nexplore. Extensive experiments demonstrate that ARES achieves superior\nperformance and reasoning efficiency across diverse mathematical, logical, and\nmultimodal benchmarks, while closing the gap to leading commercial systems\nunder significantly lower inference costs."
                },
                "authors": [
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Yue Guo"
                    },
                    {
                        "name": "Yimeng Ye"
                    },
                    {
                        "name": "Shijue Huang"
                    },
                    {
                        "name": "Wenbo Hu"
                    },
                    {
                        "name": "Haoxi Li"
                    },
                    {
                        "name": "Manyuan Zhang"
                    },
                    {
                        "name": "Jiayu Chen"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07242v2",
                "updated": "2025-10-09T17:01:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    1,
                    54,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-08T17:09:41Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    9,
                    41,
                    2,
                    281,
                    0
                ],
                "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense"
                },
                "summary": "Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning."
                },
                "authors": [
                    {
                        "name": "Leitian Tao"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Swarnadeep Saha"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Jason E Weston"
                    },
                    {
                        "name": "Ping Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ping Yu"
                },
                "author": "Ping Yu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08455v1",
                "updated": "2025-10-09T17:01:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    1,
                    54,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:01:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    1,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "A first look at quasar-galaxy clustering at $z\\simeq7.3$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A first look at quasar-galaxy clustering at $z\\simeq7.3$"
                },
                "summary": "We present JWST observations of the environments surrounding two\nhigh-redshift quasars -- J0252$-$0503 at $z = 7.0$ and J1007$+$2115 at $z =\n7.5$ -- which enable the first constraints on quasar-galaxy clustering at $z\n\\sim 7.3$. Galaxies in the vicinity of the quasars are selected through\nground-based and JWST/NIRCam imaging and then spectroscopically confirmed with\nJWST/NIRSpec using the multi-shutter assembly (MSA). Over both fields, we\nidentify 51 $z>5$ galaxies, of which eight are found within a $\\Delta\nv_{\\textrm{LOS}}=\\pm1500 \\rm{km} \\rm{s}^{-1}$ line-of-sight velocity window\nfrom the quasars and another eight in the background. The galaxy J0252\\_8713,\nlocated just $7\\,\\rm{pkpc}$ and $\\Delta v_{\\textrm{LOS}} \\approx\n360\\,\\rm{km}\\,\\rm{s}^{-1}$ from quasar J0252$-$0503, emerges as a compelling\ncandidate for one of the most distant quasar-galaxy mergers. Combining the\ngalaxy discoveries over the two fields, we measure the quasar-galaxy\ncross-correlation and obtain a correlation length of\n$r_0^{\\rm{QG}}\\approx7.6_{-1.6}^{+1.7}\\,h^{-1}\\,\\rm{cMpc}$, based on a\npower-law model with a fixed slope of $\\gamma_{\\rm{QG}} = 2.0$. Under the\nassumption that quasars and galaxies trace the same underlying dark matter\ndensity fluctuations, we infer a minimum dark matter halo mass for $z\\simeq7.3$\nquasars of $\\log_{10}(M_{\\textrm{halo, min}}/\\textrm{M}_{\\odot})= 11.6\\pm0.6$\nin a halo model framework. Compared to measurements from EIGER at $\\langle z\n\\rangle = 6.25$ and ASPIRE at $\\langle z \\rangle = 6.7$ (where\n$\\log_{10}(M_{\\textrm{halo, min}}/\\textrm{M}_{\\odot}) \\gtrsim 12.3$), our\nclustering results provide tentative evidence for a non-monotonic redshift\nevolution of quasar clustering properties. We further estimate a quasar duty\ncycle of $f_{\\rm{duty}}\\approx0.1\\%$, consistent with constraints from quasar\nproximity zones and IGM damping wings. (abridged)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present JWST observations of the environments surrounding two\nhigh-redshift quasars -- J0252$-$0503 at $z = 7.0$ and J1007$+$2115 at $z =\n7.5$ -- which enable the first constraints on quasar-galaxy clustering at $z\n\\sim 7.3$. Galaxies in the vicinity of the quasars are selected through\nground-based and JWST/NIRCam imaging and then spectroscopically confirmed with\nJWST/NIRSpec using the multi-shutter assembly (MSA). Over both fields, we\nidentify 51 $z>5$ galaxies, of which eight are found within a $\\Delta\nv_{\\textrm{LOS}}=\\pm1500 \\rm{km} \\rm{s}^{-1}$ line-of-sight velocity window\nfrom the quasars and another eight in the background. The galaxy J0252\\_8713,\nlocated just $7\\,\\rm{pkpc}$ and $\\Delta v_{\\textrm{LOS}} \\approx\n360\\,\\rm{km}\\,\\rm{s}^{-1}$ from quasar J0252$-$0503, emerges as a compelling\ncandidate for one of the most distant quasar-galaxy mergers. Combining the\ngalaxy discoveries over the two fields, we measure the quasar-galaxy\ncross-correlation and obtain a correlation length of\n$r_0^{\\rm{QG}}\\approx7.6_{-1.6}^{+1.7}\\,h^{-1}\\,\\rm{cMpc}$, based on a\npower-law model with a fixed slope of $\\gamma_{\\rm{QG}} = 2.0$. Under the\nassumption that quasars and galaxies trace the same underlying dark matter\ndensity fluctuations, we infer a minimum dark matter halo mass for $z\\simeq7.3$\nquasars of $\\log_{10}(M_{\\textrm{halo, min}}/\\textrm{M}_{\\odot})= 11.6\\pm0.6$\nin a halo model framework. Compared to measurements from EIGER at $\\langle z\n\\rangle = 6.25$ and ASPIRE at $\\langle z \\rangle = 6.7$ (where\n$\\log_{10}(M_{\\textrm{halo, min}}/\\textrm{M}_{\\odot}) \\gtrsim 12.3$), our\nclustering results provide tentative evidence for a non-monotonic redshift\nevolution of quasar clustering properties. We further estimate a quasar duty\ncycle of $f_{\\rm{duty}}\\approx0.1\\%$, consistent with constraints from quasar\nproximity zones and IGM damping wings. (abridged)"
                },
                "authors": [
                    {
                        "name": "Jan-Torge Schindler"
                    },
                    {
                        "name": "Joseph F. Hennawi"
                    },
                    {
                        "name": "Frederick B. Davies"
                    },
                    {
                        "name": "Sarah E. I. Bosman"
                    },
                    {
                        "name": "Feige Wang"
                    },
                    {
                        "name": "Jinyi Yang"
                    },
                    {
                        "name": "Anna-Christina Eilers"
                    },
                    {
                        "name": "Xiaohui Fan"
                    },
                    {
                        "name": "Koki Kakiichi"
                    },
                    {
                        "name": "Elia Pizzati"
                    },
                    {
                        "name": "Riccardo Nanni"
                    }
                ],
                "author_detail": {
                    "name": "Riccardo Nanni"
                },
                "author": "Riccardo Nanni",
                "arxiv_comment": "17 pages, 7+2 figures; submitted, any comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08439v1",
                "updated": "2025-10-09T16:52:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    52,
                    1,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T16:52:01Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    52,
                    1,
                    3,
                    282,
                    0
                ],
                "title": "xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement\n  Learning"
                },
                "summary": "Modern LLM deployments confront a widening cost-performance spectrum: premium\nmodels deliver strong reasoning but are expensive, while lightweight models are\neconomical yet brittle on complex tasks. Static escalation rules and keyword\nheuristics under-utilize this spectrum and fail to adapt across task types. We\npresent xRouter, a tool-calling-based routing system in which a learned router\ncan either answer directly or invoke one or more external models. The router is\ntrained end-to-end with reinforcement learning using an explicit, cost-aware\nreward that encodes cost-performance trade-offs, eliminating the need for\nhand-engineered routing rules. Our implementation encompasses the full\nreinforcement learning framework, including reward and cost accounting, as well\nas the deployment and evaluation pipelines. Across diverse benchmarks, xRouter\nachieves strong cost-performance trade-offs (e.g., substantial cost reductions\nat comparable task completion rates), and provides empirical insights into what\nreliably helps learned routing and what does not, ranging from model\ntrainability to the difficulty of eliciting sophisticated orchestration\nbehaviors in small open models. We hope these findings and our open\nimplementation will serve as a practical substrate for advancing learned,\ncost-aware LLM orchestration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM deployments confront a widening cost-performance spectrum: premium\nmodels deliver strong reasoning but are expensive, while lightweight models are\neconomical yet brittle on complex tasks. Static escalation rules and keyword\nheuristics under-utilize this spectrum and fail to adapt across task types. We\npresent xRouter, a tool-calling-based routing system in which a learned router\ncan either answer directly or invoke one or more external models. The router is\ntrained end-to-end with reinforcement learning using an explicit, cost-aware\nreward that encodes cost-performance trade-offs, eliminating the need for\nhand-engineered routing rules. Our implementation encompasses the full\nreinforcement learning framework, including reward and cost accounting, as well\nas the deployment and evaluation pipelines. Across diverse benchmarks, xRouter\nachieves strong cost-performance trade-offs (e.g., substantial cost reductions\nat comparable task completion rates), and provides empirical insights into what\nreliably helps learned routing and what does not, ranging from model\ntrainability to the difficulty of eliciting sophisticated orchestration\nbehaviors in small open models. We hope these findings and our open\nimplementation will serve as a practical substrate for advancing learned,\ncost-aware LLM orchestration."
                },
                "authors": [
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Shirley Kokane"
                    },
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Jielin Qiu"
                    },
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "24 Pages, 4 Figures, 2 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07231v2",
                "updated": "2025-10-09T16:46:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    46,
                    30,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-08T17:00:49Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    0,
                    49,
                    2,
                    281,
                    0
                ],
                "title": "Benchmarking LLM Causal Reasoning with Scientifically Validated\n  Relationships",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM Causal Reasoning with Scientifically Validated\n  Relationships"
                },
                "summary": "Causal reasoning is fundamental for Large Language Models (LLMs) to\nunderstand genuine cause-and-effect relationships beyond pattern matching.\nExisting benchmarks suffer from critical limitations such as reliance on\nsynthetic data and narrow domain coverage. We introduce a novel benchmark\nconstructed from casually identified relationships extracted from top-tier\neconomics and finance journals, drawing on rigorous methodologies including\ninstrumental variables, difference-in-differences, and regression discontinuity\ndesigns. Our benchmark comprises 40,379 evaluation items covering five task\ntypes across domains such as health, environment, technology, law, and culture.\nExperimental results on eight state-of-the-art LLMs reveal substantial\nlimitations, with the best model achieving only 57.6\\% accuracy. Moreover,\nmodel scale does not consistently translate to superior performance, and even\nadvanced reasoning models struggle with fundamental causal relationship\nidentification. These findings underscore a critical gap between current LLM\ncapabilities and demands of reliable causal reasoning in high-stakes\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal reasoning is fundamental for Large Language Models (LLMs) to\nunderstand genuine cause-and-effect relationships beyond pattern matching.\nExisting benchmarks suffer from critical limitations such as reliance on\nsynthetic data and narrow domain coverage. We introduce a novel benchmark\nconstructed from casually identified relationships extracted from top-tier\neconomics and finance journals, drawing on rigorous methodologies including\ninstrumental variables, difference-in-differences, and regression discontinuity\ndesigns. Our benchmark comprises 40,379 evaluation items covering five task\ntypes across domains such as health, environment, technology, law, and culture.\nExperimental results on eight state-of-the-art LLMs reveal substantial\nlimitations, with the best model achieving only 57.6\\% accuracy. Moreover,\nmodel scale does not consistently translate to superior performance, and even\nadvanced reasoning models struggle with fundamental causal relationship\nidentification. These findings underscore a critical gap between current LLM\ncapabilities and demands of reliable causal reasoning in high-stakes\napplications."
                },
                "authors": [
                    {
                        "name": "Donggyu Lee"
                    },
                    {
                        "name": "Sungwon Park"
                    },
                    {
                        "name": "Yerin Hwang"
                    },
                    {
                        "name": "Hyoshin Kim"
                    },
                    {
                        "name": "Hyunwoo Oh"
                    },
                    {
                        "name": "Jungwon Kim"
                    },
                    {
                        "name": "Meeyoung Cha"
                    },
                    {
                        "name": "Sangyoon Park"
                    },
                    {
                        "name": "Jihee Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jihee Kim"
                },
                "author": "Jihee Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.14738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.14738v3",
                "updated": "2025-10-09T16:45:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    45,
                    28,
                    3,
                    282,
                    0
                ],
                "published": "2025-03-18T21:14:12Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    21,
                    14,
                    12,
                    1,
                    77,
                    0
                ],
                "title": "DESI DR2 Results II: Measurements of Baryon Acoustic Oscillations and\n  Cosmological Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DESI DR2 Results II: Measurements of Baryon Acoustic Oscillations and\n  Cosmological Constraints"
                },
                "summary": "We present baryon acoustic oscillation (BAO) measurements from more than 14\nmillion galaxies and quasars drawn from the Dark Energy Spectroscopic\nInstrument (DESI) Data Release 2 (DR2), based on three years of operation. For\ncosmology inference, these galaxy measurements are combined with DESI\nLyman-$\\alpha$ forest BAO results presented in a companion paper. The DR2 BAO\nresults are consistent with DESI DR1 and SDSS, and their distance-redshift\nrelationship matches those from recent compilations of supernovae (SNe) over\nthe same redshift range. The results are well described by a flat $\\Lambda$CDM\nmodel, but the parameters preferred by BAO are in mild, $2.3\\sigma$ tension\nwith those determined from the cosmic microwave background (CMB), although the\nDESI results are consistent with the acoustic angular scale $\\theta_*$ that is\nwell-measured by Planck. This tension is alleviated by dark energy with a\ntime-evolving equation of state parametrized by $w_0$ and $w_a$, which provides\na better fit to the data, with a favored solution in the quadrant with $w_0>-1$\nand $w_a<0$. This solution is preferred over $\\Lambda$CDM at $3.1\\sigma$ for\nthe combination of DESI BAO and CMB data. When also including SNe, the\npreference for a dynamical dark energy model over $\\Lambda$CDM ranges from\n$2.8-4.2\\sigma$ depending on which SNe sample is used. We present evidence from\nother data combinations which also favor the same behavior at high\nsignificance. From the combination of DESI and CMB we derive 95% upper limits\non the sum of neutrino masses, finding $\\sum m_\\nu<0.064$ eV assuming\n$\\Lambda$CDM and $\\sum m_\\nu<0.16$ eV in the $w_0w_a$ model. Unless there is an\nunknown systematic error associated with one or more datasets, it is clear that\n$\\Lambda$CDM is being challenged by the combination of DESI BAO with other\nmeasurements and that dynamical dark energy offers a possible solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present baryon acoustic oscillation (BAO) measurements from more than 14\nmillion galaxies and quasars drawn from the Dark Energy Spectroscopic\nInstrument (DESI) Data Release 2 (DR2), based on three years of operation. For\ncosmology inference, these galaxy measurements are combined with DESI\nLyman-$\\alpha$ forest BAO results presented in a companion paper. The DR2 BAO\nresults are consistent with DESI DR1 and SDSS, and their distance-redshift\nrelationship matches those from recent compilations of supernovae (SNe) over\nthe same redshift range. The results are well described by a flat $\\Lambda$CDM\nmodel, but the parameters preferred by BAO are in mild, $2.3\\sigma$ tension\nwith those determined from the cosmic microwave background (CMB), although the\nDESI results are consistent with the acoustic angular scale $\\theta_*$ that is\nwell-measured by Planck. This tension is alleviated by dark energy with a\ntime-evolving equation of state parametrized by $w_0$ and $w_a$, which provides\na better fit to the data, with a favored solution in the quadrant with $w_0>-1$\nand $w_a<0$. This solution is preferred over $\\Lambda$CDM at $3.1\\sigma$ for\nthe combination of DESI BAO and CMB data. When also including SNe, the\npreference for a dynamical dark energy model over $\\Lambda$CDM ranges from\n$2.8-4.2\\sigma$ depending on which SNe sample is used. We present evidence from\nother data combinations which also favor the same behavior at high\nsignificance. From the combination of DESI and CMB we derive 95% upper limits\non the sum of neutrino masses, finding $\\sum m_\\nu<0.064$ eV assuming\n$\\Lambda$CDM and $\\sum m_\\nu<0.16$ eV in the $w_0w_a$ model. Unless there is an\nunknown systematic error associated with one or more datasets, it is clear that\n$\\Lambda$CDM is being challenged by the combination of DESI BAO with other\nmeasurements and that dynamical dark energy offers a possible solution."
                },
                "authors": [
                    {
                        "name": "DESI Collaboration"
                    },
                    {
                        "name": "M. Abdul-Karim"
                    },
                    {
                        "name": "J. Aguilar"
                    },
                    {
                        "name": "S. Ahlen"
                    },
                    {
                        "name": "S. Alam"
                    },
                    {
                        "name": "L. Allen"
                    },
                    {
                        "name": "C. Allende Prieto"
                    },
                    {
                        "name": "O. Alves"
                    },
                    {
                        "name": "A. Anand"
                    },
                    {
                        "name": "U. Andrade"
                    },
                    {
                        "name": "E. Armengaud"
                    },
                    {
                        "name": "A. Aviles"
                    },
                    {
                        "name": "S. Bailey"
                    },
                    {
                        "name": "C. Baltay"
                    },
                    {
                        "name": "P. Bansal"
                    },
                    {
                        "name": "A. Bault"
                    },
                    {
                        "name": "J. Behera"
                    },
                    {
                        "name": "S. BenZvi"
                    },
                    {
                        "name": "D. Bianchi"
                    },
                    {
                        "name": "C. Blake"
                    },
                    {
                        "name": "S. Brieden"
                    },
                    {
                        "name": "A. Brodzeller"
                    },
                    {
                        "name": "D. Brooks"
                    },
                    {
                        "name": "E. Buckley-Geer"
                    },
                    {
                        "name": "E. Burtin"
                    },
                    {
                        "name": "R. Calderon"
                    },
                    {
                        "name": "R. Canning"
                    },
                    {
                        "name": "A. Carnero Rosell"
                    },
                    {
                        "name": "P. Carrilho"
                    },
                    {
                        "name": "L. Casas"
                    },
                    {
                        "name": "F. J. Castander"
                    },
                    {
                        "name": "R. Cereskaite"
                    },
                    {
                        "name": "M. Charles"
                    },
                    {
                        "name": "E. Chaussidon"
                    },
                    {
                        "name": "J. Chaves-Montero"
                    },
                    {
                        "name": "D. Chebat"
                    },
                    {
                        "name": "X. Chen"
                    },
                    {
                        "name": "T. Claybaugh"
                    },
                    {
                        "name": "S. Cole"
                    },
                    {
                        "name": "A. P. Cooper"
                    },
                    {
                        "name": "A. Cuceu"
                    },
                    {
                        "name": "K. S. Dawson"
                    },
                    {
                        "name": "A. de la Macorra"
                    },
                    {
                        "name": "A. de Mattia"
                    },
                    {
                        "name": "N. Deiosso"
                    },
                    {
                        "name": "J. Della Costa"
                    },
                    {
                        "name": "R. Demina"
                    },
                    {
                        "name": "A. Dey"
                    },
                    {
                        "name": "B. Dey"
                    },
                    {
                        "name": "Z. Ding"
                    },
                    {
                        "name": "P. Doel"
                    },
                    {
                        "name": "J. Edelstein"
                    },
                    {
                        "name": "D. J. Eisenstein"
                    },
                    {
                        "name": "W. Elbers"
                    },
                    {
                        "name": "P. Fagrelius"
                    },
                    {
                        "name": "K. Fanning"
                    },
                    {
                        "name": "E. Fernández-García"
                    },
                    {
                        "name": "S. Ferraro"
                    },
                    {
                        "name": "A. Font-Ribera"
                    },
                    {
                        "name": "J. E. Forero-Romero"
                    },
                    {
                        "name": "C. S. Frenk"
                    },
                    {
                        "name": "C. Garcia-Quintero"
                    },
                    {
                        "name": "L. H. Garrison"
                    },
                    {
                        "name": "E. Gaztañaga"
                    },
                    {
                        "name": "H. Gil-Marín"
                    },
                    {
                        "name": "S. Gontcho A Gontcho"
                    },
                    {
                        "name": "D. Gonzalez"
                    },
                    {
                        "name": "A. X. Gonzalez-Morales"
                    },
                    {
                        "name": "C. Gordon"
                    },
                    {
                        "name": "D. Green"
                    },
                    {
                        "name": "G. Gutierrez"
                    },
                    {
                        "name": "J. Guy"
                    },
                    {
                        "name": "B. Hadzhiyska"
                    },
                    {
                        "name": "C. Hahn"
                    },
                    {
                        "name": "S. He"
                    },
                    {
                        "name": "M. Herbold"
                    },
                    {
                        "name": "H. K. Herrera-Alcantar"
                    },
                    {
                        "name": "M. Ho"
                    },
                    {
                        "name": "K. Honscheid"
                    },
                    {
                        "name": "C. Howlett"
                    },
                    {
                        "name": "D. Huterer"
                    },
                    {
                        "name": "M. Ishak"
                    },
                    {
                        "name": "S. Juneau"
                    },
                    {
                        "name": "N. V. Kamble"
                    },
                    {
                        "name": "N. G. Karaçaylı"
                    },
                    {
                        "name": "R. Kehoe"
                    },
                    {
                        "name": "S. Kent"
                    },
                    {
                        "name": "A. G. Kim"
                    },
                    {
                        "name": "D. Kirkby"
                    },
                    {
                        "name": "T. Kisner"
                    },
                    {
                        "name": "S. E. Koposov"
                    },
                    {
                        "name": "A. Kremin"
                    },
                    {
                        "name": "A. Krolewski"
                    },
                    {
                        "name": "O. Lahav"
                    },
                    {
                        "name": "C. Lamman"
                    },
                    {
                        "name": "M. Landriau"
                    },
                    {
                        "name": "D. Lang"
                    },
                    {
                        "name": "J. Lasker"
                    },
                    {
                        "name": "J. M. Le Goff"
                    },
                    {
                        "name": "L. Le Guillou"
                    },
                    {
                        "name": "A. Leauthaud"
                    },
                    {
                        "name": "M. E. Levi"
                    },
                    {
                        "name": "Q. Li"
                    },
                    {
                        "name": "T. S. Li"
                    },
                    {
                        "name": "K. Lodha"
                    },
                    {
                        "name": "M. Lokken"
                    },
                    {
                        "name": "F. Lozano-Rodríguez"
                    },
                    {
                        "name": "C. Magneville"
                    },
                    {
                        "name": "M. Manera"
                    },
                    {
                        "name": "P. Martini"
                    },
                    {
                        "name": "W. L. Matthewson"
                    },
                    {
                        "name": "A. Meisner"
                    },
                    {
                        "name": "J. Mena-Fernández"
                    },
                    {
                        "name": "A. Menegas"
                    },
                    {
                        "name": "T. Mergulhão"
                    },
                    {
                        "name": "R. Miquel"
                    },
                    {
                        "name": "J. Moustakas"
                    },
                    {
                        "name": "A. Muñoz-Gutiérrez"
                    },
                    {
                        "name": "D. Muñoz-Santos"
                    },
                    {
                        "name": "A. D. Myers"
                    },
                    {
                        "name": "S. Nadathur"
                    },
                    {
                        "name": "K. Naidoo"
                    },
                    {
                        "name": "L. Napolitano"
                    },
                    {
                        "name": "J. A. Newman"
                    },
                    {
                        "name": "G. Niz"
                    },
                    {
                        "name": "H. E. Noriega"
                    },
                    {
                        "name": "E. Paillas"
                    },
                    {
                        "name": "N. Palanque-Delabrouille"
                    },
                    {
                        "name": "J. Pan"
                    },
                    {
                        "name": "J. Peacock"
                    },
                    {
                        "name": "Marcos Pellejero Ibanez"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "A. Pérez-Fernández"
                    },
                    {
                        "name": "I. Pérez-Ràfols"
                    },
                    {
                        "name": "M. M. Pieri"
                    },
                    {
                        "name": "C. Poppett"
                    },
                    {
                        "name": "F. Prada"
                    },
                    {
                        "name": "D. Rabinowitz"
                    },
                    {
                        "name": "A. Raichoor"
                    },
                    {
                        "name": "C. Ramírez-Pérez"
                    },
                    {
                        "name": "M. Rashkovetskyi"
                    },
                    {
                        "name": "C. Ravoux"
                    },
                    {
                        "name": "J. Rich"
                    },
                    {
                        "name": "A. Rocher"
                    },
                    {
                        "name": "C. Rockosi"
                    },
                    {
                        "name": "J. Rohlf"
                    },
                    {
                        "name": "J. O. Román-Herrera"
                    },
                    {
                        "name": "A. J. Ross"
                    },
                    {
                        "name": "G. Rossi"
                    },
                    {
                        "name": "R. Ruggeri"
                    },
                    {
                        "name": "V. Ruhlmann-Kleider"
                    },
                    {
                        "name": "L. Samushia"
                    },
                    {
                        "name": "E. Sanchez"
                    },
                    {
                        "name": "N. Sanders"
                    },
                    {
                        "name": "D. Schlegel"
                    },
                    {
                        "name": "M. Schubnell"
                    },
                    {
                        "name": "H. Seo"
                    },
                    {
                        "name": "A. Shafieloo"
                    },
                    {
                        "name": "R. Sharples"
                    },
                    {
                        "name": "J. Silber"
                    },
                    {
                        "name": "F. Sinigaglia"
                    },
                    {
                        "name": "D. Sprayberry"
                    },
                    {
                        "name": "T. Tan"
                    },
                    {
                        "name": "G. Tarlé"
                    },
                    {
                        "name": "P. Taylor"
                    },
                    {
                        "name": "W. Turner"
                    },
                    {
                        "name": "L. A. Ureña-López"
                    },
                    {
                        "name": "R. Vaisakh"
                    },
                    {
                        "name": "F. Valdes"
                    },
                    {
                        "name": "G. Valogiannis"
                    },
                    {
                        "name": "M. Vargas-Magaña"
                    },
                    {
                        "name": "L. Verde"
                    },
                    {
                        "name": "M. Walther"
                    },
                    {
                        "name": "B. A. Weaver"
                    },
                    {
                        "name": "D. H. Weinberg"
                    },
                    {
                        "name": "M. White"
                    },
                    {
                        "name": "M. Wolfson"
                    },
                    {
                        "name": "C. Yèche"
                    },
                    {
                        "name": "J. Yu"
                    },
                    {
                        "name": "E. A. Zaborowski"
                    },
                    {
                        "name": "P. Zarrouk"
                    },
                    {
                        "name": "Z. Zhai"
                    },
                    {
                        "name": "H. Zhang"
                    },
                    {
                        "name": "C. Zhao"
                    },
                    {
                        "name": "G. B. Zhao"
                    },
                    {
                        "name": "R. Zhou"
                    },
                    {
                        "name": "H. Zou"
                    }
                ],
                "author_detail": {
                    "name": "H. Zou"
                },
                "author": "H. Zou",
                "arxiv_doi": "10.1103/tr6y-kpc6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/tr6y-kpc6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.14738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.14738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "40 pages, 18 figures. This DESI Collaboration Publication is part of\n  the Data Release 2 publication series (see\n  https://data.desi.lbl.gov/doc/papers ). Updated to match version published in\n  Phys. Rev. D",
                "arxiv_journal_ref": "Phys. Rev. D 112, 083515, 2025",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22889v2",
                "updated": "2025-10-09T16:45:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    45,
                    21,
                    3,
                    282,
                    0
                ],
                "published": "2025-06-15T05:09:20Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    5,
                    9,
                    20,
                    6,
                    166,
                    0
                ],
                "title": "Confident-Knowledge Diversity Drives Human-Human and Human-AI Free\n  Discussion Synergy and Reveals Pure-AI Discussion Shortfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confident-Knowledge Diversity Drives Human-Human and Human-AI Free\n  Discussion Synergy and Reveals Pure-AI Discussion Shortfalls"
                },
                "summary": "Conversations transform individual knowledge into collective insight,\nenabling collaborators to solve problems more accurately than they could alone.\nWhether dialogues among large language models (LLMs) can replicate the\nsynergistic gains observed in human discussion remains unclear. We\nsystematically compared four interaction settings: LLM-LLM pairs, LLM trios,\nhuman trios, and human-LLM pairs, using validated medical multiple-choice\nquestions. Agents answered individually, engaged in open-ended discussion, then\nre-answered, allowing us to quantify conversational gains. Interactions that\nincluded humans consistently yielded synergy (post-discussion accuracy\nincreased for both stronger and weaker participants), whereas purely LLM groups\ndid not improve and often declined. To explain and prospectively predict when\nunstructured dialogue helps, we introduce an agent-agnostic confident-knowledge\nframework that models each participant by performance (accuracy) and\nconfidence. This framework quantifies confident-knowledge diversity, the degree\nto which one agent tends to be correct when another is uncertain, and yields a\nconservative upper bound on gains achievable via confidence-informed decisions,\nwhich we term Potential Conversation Synergy. Across humans, LLMs, and mixed\nteams, this metric prospectively predicts observed conversational improvements:\nwhen confident-knowledge diversity is low (as in LLM-only groups), discussion\ndoesn't improve performance; when it is present (as in human or human-LLM\ngroups), free-form dialogue reliably lifts accuracy. These findings propose a\nnew concept and method for AI collaboration: quantifying confident-knowledge\ndiversity to prospectively predict conversational gains and guide team\nselection and interaction design in both multi-agent and human-AI settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversations transform individual knowledge into collective insight,\nenabling collaborators to solve problems more accurately than they could alone.\nWhether dialogues among large language models (LLMs) can replicate the\nsynergistic gains observed in human discussion remains unclear. We\nsystematically compared four interaction settings: LLM-LLM pairs, LLM trios,\nhuman trios, and human-LLM pairs, using validated medical multiple-choice\nquestions. Agents answered individually, engaged in open-ended discussion, then\nre-answered, allowing us to quantify conversational gains. Interactions that\nincluded humans consistently yielded synergy (post-discussion accuracy\nincreased for both stronger and weaker participants), whereas purely LLM groups\ndid not improve and often declined. To explain and prospectively predict when\nunstructured dialogue helps, we introduce an agent-agnostic confident-knowledge\nframework that models each participant by performance (accuracy) and\nconfidence. This framework quantifies confident-knowledge diversity, the degree\nto which one agent tends to be correct when another is uncertain, and yields a\nconservative upper bound on gains achievable via confidence-informed decisions,\nwhich we term Potential Conversation Synergy. Across humans, LLMs, and mixed\nteams, this metric prospectively predicts observed conversational improvements:\nwhen confident-knowledge diversity is low (as in LLM-only groups), discussion\ndoesn't improve performance; when it is present (as in human or human-LLM\ngroups), free-form dialogue reliably lifts accuracy. These findings propose a\nnew concept and method for AI collaboration: quantifying confident-knowledge\ndiversity to prospectively predict conversational gains and guide team\nselection and interaction design in both multi-agent and human-AI settings."
                },
                "authors": [
                    {
                        "name": "Tom Sheffer"
                    },
                    {
                        "name": "Alon Miron"
                    },
                    {
                        "name": "Asael Sklar"
                    },
                    {
                        "name": "Yaniv Dover"
                    },
                    {
                        "name": "Ariel Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Goldstein"
                },
                "author": "Ariel Goldstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11943v2",
                "updated": "2025-10-09T16:39:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    39,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-15T14:03:06Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    3,
                    6,
                    0,
                    258,
                    0
                ],
                "title": "Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic Agents with Modal Logic for Autonomous Diagnostics"
                },
                "summary": "The development of intelligent agents, particularly those powered by language\nmodels (LMs), has shown the critical role in various environments that require\nintelligent and autonomous decision. Environments are not passive testing\ngrounds and they represent the data required for agents to learn and exhibit\nvery challenging conditions that require adaptive, complex and autonomous\ncapacity to make decisions. While the paradigm of scaling models and datasets\nhas led to remarkable emergent capabilities, we argue that scaling the\nstructure, fidelity, and logical consistency of agent reasoning within these\nenvironments is a crucial, yet underexplored, dimension of AI research. This\npaper introduces a neuro-symbolic multi-agent architecture where the belief\nstates of individual agents are formally represented as Kripke models. This\nfoundational choice enables them to reason about known concepts of\n\\emph{possibility} and \\emph{necessity} using the formal language of modal\nlogic. In this work, we use of immutable, domain-specific knowledge to make\ninfere information, which is encoded as logical constraints essential for\nproper diagnosis. In the proposed model, we show constraints that actively\nguide the hypothesis generation of LMs, effectively preventing them from\nreaching physically or logically untenable conclusions. In a high-fidelity\nsimulated particle accelerator environment, our system successfully diagnoses\ncomplex, cascading failures by combining the powerful semantic intuition of LMs\nwith the rigorous, verifiable validation of modal logic and a factual world\nmodel and showcasing a viable path toward more robust, reliable, and verifiable\nautonomous agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of intelligent agents, particularly those powered by language\nmodels (LMs), has shown the critical role in various environments that require\nintelligent and autonomous decision. Environments are not passive testing\ngrounds and they represent the data required for agents to learn and exhibit\nvery challenging conditions that require adaptive, complex and autonomous\ncapacity to make decisions. While the paradigm of scaling models and datasets\nhas led to remarkable emergent capabilities, we argue that scaling the\nstructure, fidelity, and logical consistency of agent reasoning within these\nenvironments is a crucial, yet underexplored, dimension of AI research. This\npaper introduces a neuro-symbolic multi-agent architecture where the belief\nstates of individual agents are formally represented as Kripke models. This\nfoundational choice enables them to reason about known concepts of\n\\emph{possibility} and \\emph{necessity} using the formal language of modal\nlogic. In this work, we use of immutable, domain-specific knowledge to make\ninfere information, which is encoded as logical constraints essential for\nproper diagnosis. In the proposed model, we show constraints that actively\nguide the hypothesis generation of LMs, effectively preventing them from\nreaching physically or logically untenable conclusions. In a high-fidelity\nsimulated particle accelerator environment, our system successfully diagnoses\ncomplex, cascading failures by combining the powerful semantic intuition of LMs\nwith the rigorous, verifiable validation of modal logic and a factual world\nmodel and showcasing a viable path toward more robust, reliable, and verifiable\nautonomous agents."
                },
                "authors": [
                    {
                        "name": "Antonin Sulc"
                    },
                    {
                        "name": "Thorsten Hellert"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Hellert"
                },
                "author": "Thorsten Hellert",
                "arxiv_comment": "10 pages, 1 figure, Scaling Environments for Agents (SEA) Workshop at\n  NeuralIPS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08421v1",
                "updated": "2025-10-09T16:37:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    37,
                    54,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T16:37:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    37,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "BASILISK III. Stress-testing the Conditional Luminosity Function model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BASILISK III. Stress-testing the Conditional Luminosity Function model"
                },
                "summary": "The Conditional Luminosity Function (CLF) is an effective and flexible way of\ncharacterizing the galaxy-halo connection. However, it is subject to a\nparticular choice for its parametrization, which acts as a prior assumption.\nMost studies have been restricted to what has become a standard CLF\nparametrization with little to no variation. The goal of this paper is to\ninvestigate whether this model is sufficient to fully characterize the\nsmall-scale data extracted from spectroscopic surveys and to gauge how adding\nor removing degrees of freedom impact the inference regarding the galaxy-halo\nconnection. After extensive validation with realistic mock data, we use\nBasilisk, a highly constraining Bayesian hierarchical tool to model the\nkinematics and abundance of satellite galaxies, to test the standard CLF model\nagainst a slew of more flexible variants. In particular, we test whether the\nSDSS data favour any of these variants in terms of a goodness-of-fit\nimprovement, and identify the models that are sufficiently flexible, beyond\nwhich additional model freedom is not demanded by the data. We show that some\nof these additional degrees of freedom, which have hitherto not been\nconsidered, result in a drastic improvement of the fit and cause significant\nchanges in the inferred galaxy-halo connection. This highlights that an\nempirical model comes with an implicit prior about the parametrization form,\nwhich needs to be addressed to ensure that it is sufficiently flexible to\ncapture the complexity of the data and to safeguard against a biased inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Conditional Luminosity Function (CLF) is an effective and flexible way of\ncharacterizing the galaxy-halo connection. However, it is subject to a\nparticular choice for its parametrization, which acts as a prior assumption.\nMost studies have been restricted to what has become a standard CLF\nparametrization with little to no variation. The goal of this paper is to\ninvestigate whether this model is sufficient to fully characterize the\nsmall-scale data extracted from spectroscopic surveys and to gauge how adding\nor removing degrees of freedom impact the inference regarding the galaxy-halo\nconnection. After extensive validation with realistic mock data, we use\nBasilisk, a highly constraining Bayesian hierarchical tool to model the\nkinematics and abundance of satellite galaxies, to test the standard CLF model\nagainst a slew of more flexible variants. In particular, we test whether the\nSDSS data favour any of these variants in terms of a goodness-of-fit\nimprovement, and identify the models that are sufficiently flexible, beyond\nwhich additional model freedom is not demanded by the data. We show that some\nof these additional degrees of freedom, which have hitherto not been\nconsidered, result in a drastic improvement of the fit and cause significant\nchanges in the inferred galaxy-halo connection. This highlights that an\nempirical model comes with an implicit prior about the parametrization form,\nwhich needs to be addressed to ensure that it is sufficiently flexible to\ncapture the complexity of the data and to safeguard against a biased inference."
                },
                "authors": [
                    {
                        "name": "Kaustav Mitra"
                    },
                    {
                        "name": "Frank C. van den Bosch"
                    }
                ],
                "author_detail": {
                    "name": "Frank C. van den Bosch"
                },
                "author": "Frank C. van den Bosch",
                "arxiv_comment": "18+2 pages, 8+1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08415v1",
                "updated": "2025-10-09T16:35:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    35,
                    12,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T16:35:12Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    35,
                    12,
                    3,
                    282,
                    0
                ],
                "title": "Stochastic Volatility-in-mean VARs with Time-Varying Skewness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Volatility-in-mean VARs with Time-Varying Skewness"
                },
                "summary": "This paper introduces a Bayesian vector autoregression (BVAR) with stochastic\nvolatility-in-mean and time-varying skewness. Unlike previous approaches, the\nproposed model allows both volatility and skewness to directly affect\nmacroeconomic variables. We provide a Gibbs sampling algorithm for posterior\ninference and apply the model to quarterly data for the US and the UK.\nEmpirical results show that skewness shocks have economically significant\neffects on output, inflation and spreads, often exceeding the impact of\nvolatility shocks. In a pseudo-real-time forecasting exercise, the proposed\nmodel outperforms existing alternatives in many cases. Moreover, the model\nproduces sharper measures of tail risk, revealing that standard stochastic\nvolatility models tend to overstate uncertainty. These findings highlight the\nimportance of incorporating time-varying skewness for capturing macro-financial\nrisks and improving forecast performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a Bayesian vector autoregression (BVAR) with stochastic\nvolatility-in-mean and time-varying skewness. Unlike previous approaches, the\nproposed model allows both volatility and skewness to directly affect\nmacroeconomic variables. We provide a Gibbs sampling algorithm for posterior\ninference and apply the model to quarterly data for the US and the UK.\nEmpirical results show that skewness shocks have economically significant\neffects on output, inflation and spreads, often exceeding the impact of\nvolatility shocks. In a pseudo-real-time forecasting exercise, the proposed\nmodel outperforms existing alternatives in many cases. Moreover, the model\nproduces sharper measures of tail risk, revealing that standard stochastic\nvolatility models tend to overstate uncertainty. These findings highlight the\nimportance of incorporating time-varying skewness for capturing macro-financial\nrisks and improving forecast performance."
                },
                "authors": [
                    {
                        "name": "Leonardo N. Ferreira"
                    },
                    {
                        "name": "Haroon Mumtaz"
                    },
                    {
                        "name": "Ana Skoblar"
                    }
                ],
                "author_detail": {
                    "name": "Ana Skoblar"
                },
                "author": "Ana Skoblar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17320v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17320v2",
                "updated": "2025-10-09T16:34:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    34,
                    9,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-24T12:00:41Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    12,
                    0,
                    41,
                    6,
                    236,
                    0
                ],
                "title": "AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for\n  Interpretable LLM Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for\n  Interpretable LLM Representations"
                },
                "summary": "Understanding the internal representations of large language models (LLMs)\nremains a central challenge for interpretability research. Sparse autoencoders\n(SAEs) offer a promising solution by decomposing activations into interpretable\nfeatures, but existing approaches rely on fixed sparsity constraints that fail\nto account for input complexity. We propose AdaptiveK SAE (Adaptive Top K\nSparse Autoencoders), a novel framework that dynamically adjusts sparsity\nlevels based on the semantic complexity of each input. Leveraging linear\nprobes, we demonstrate that context complexity is linearly encoded in LLM\nrepresentations, and we use this signal to guide feature allocation during\ntraining. Experiments across ten language models (from 70M to 14B parameters)\ndemonstrate that this complexity-driven adaptation significantly outperforms\nfixed-sparsity approaches on reconstruction fidelity, explained variance,\ncosine similarity and interpretability metrics while eliminating the\ncomputational burden of extensive hyperparameter tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the internal representations of large language models (LLMs)\nremains a central challenge for interpretability research. Sparse autoencoders\n(SAEs) offer a promising solution by decomposing activations into interpretable\nfeatures, but existing approaches rely on fixed sparsity constraints that fail\nto account for input complexity. We propose AdaptiveK SAE (Adaptive Top K\nSparse Autoencoders), a novel framework that dynamically adjusts sparsity\nlevels based on the semantic complexity of each input. Leveraging linear\nprobes, we demonstrate that context complexity is linearly encoded in LLM\nrepresentations, and we use this signal to guide feature allocation during\ntraining. Experiments across ten language models (from 70M to 14B parameters)\ndemonstrate that this complexity-driven adaptation significantly outperforms\nfixed-sparsity approaches on reconstruction fidelity, explained variance,\ncosine similarity and interpretability metrics while eliminating the\ncomputational burden of extensive hyperparameter tuning."
                },
                "authors": [
                    {
                        "name": "Yifei Yao"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17320v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00300v2",
                "updated": "2025-10-09T16:26:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    26,
                    32,
                    3,
                    282,
                    0
                ],
                "published": "2024-11-30T00:58:48Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    0,
                    58,
                    48,
                    5,
                    335,
                    0
                ],
                "title": "Aligning LLM+PDDL Symbolic Plans with Human Objective Specifications\n  through Evolutionary Algorithm Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning LLM+PDDL Symbolic Plans with Human Objective Specifications\n  through Evolutionary Algorithm Guidance"
                },
                "summary": "Automated planning using a symbolic planning language, such as PDDL, is a\ngeneral approach to producing optimal plans to achieve a stated goal. However,\ncreating suitable machine understandable descriptions of the planning domain,\nproblem, and goal requires expertise in the planning language, limiting the\nutility of these tools for non-expert humans. Recent efforts have explored\nutilizing a symbolic planner in conjunction with a large language model to\ngenerate plans from natural language descriptions given by a non-expert human\n(LLM+PDDL). Our approach performs initial translation of goal specifications to\na set of PDDL goal constraints using an LLM; such translations often result in\nimprecise symbolic specifications, which are difficult to validate directly. We\naccount for this using an evolutionary approach to generate a population of\nsymbolic goal specifications with slight differences from the initial\ntranslation, and utilize a trained LSTM-based validation model to assess\nwhether each induced plan in the population adheres to the natural language\nspecifications. We evaluate our approach on a collection of prototypical\nspecifications in a notional naval disaster recovery task, and demonstrate that\nour evolutionary approach improve adherence of generated plans to natural\nlanguage specifications when compared to plans generated using only LLM\ntranslations. The code for our method can be found at\nhttps://github.com/owenonline/PlanCritic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated planning using a symbolic planning language, such as PDDL, is a\ngeneral approach to producing optimal plans to achieve a stated goal. However,\ncreating suitable machine understandable descriptions of the planning domain,\nproblem, and goal requires expertise in the planning language, limiting the\nutility of these tools for non-expert humans. Recent efforts have explored\nutilizing a symbolic planner in conjunction with a large language model to\ngenerate plans from natural language descriptions given by a non-expert human\n(LLM+PDDL). Our approach performs initial translation of goal specifications to\na set of PDDL goal constraints using an LLM; such translations often result in\nimprecise symbolic specifications, which are difficult to validate directly. We\naccount for this using an evolutionary approach to generate a population of\nsymbolic goal specifications with slight differences from the initial\ntranslation, and utilize a trained LSTM-based validation model to assess\nwhether each induced plan in the population adheres to the natural language\nspecifications. We evaluate our approach on a collection of prototypical\nspecifications in a notional naval disaster recovery task, and demonstrate that\nour evolutionary approach improve adherence of generated plans to natural\nlanguage specifications when compared to plans generated using only LLM\ntranslations. The code for our method can be found at\nhttps://github.com/owenonline/PlanCritic."
                },
                "authors": [
                    {
                        "name": "Owen Burns"
                    },
                    {
                        "name": "Dana Hughes"
                    },
                    {
                        "name": "Katia Sycara"
                    }
                ],
                "author_detail": {
                    "name": "Katia Sycara"
                },
                "author": "Katia Sycara",
                "arxiv_doi": "10.1109/CASE58245.2025.11163939",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CASE58245.2025.11163939",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.00300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 5 figures",
                "arxiv_journal_ref": "2025 IEEE 21st International Conference on Automation Science and\n  Engineering (CASE), Los Angeles, CA, USA, 2025, pp. 1584-1590",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08394v1",
                "updated": "2025-10-09T16:15:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    15,
                    46,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T16:15:46Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    15,
                    46,
                    3,
                    282,
                    0
                ],
                "title": "Spectral Prefiltering of Neural Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral Prefiltering of Neural Fields"
                },
                "summary": "Neural fields excel at representing continuous visual signals but typically\noperate at a single, fixed resolution. We present a simple yet powerful method\nto optimize neural fields that can be prefiltered in a single forward pass. Key\ninnovations and features include: (1) We perform convolutional filtering in the\ninput domain by analytically scaling Fourier feature embeddings with the\nfilter's frequency response. (2) This closed-form modulation generalizes beyond\nGaussian filtering and supports other parametric filters (Box and Lanczos) that\nare unseen at training time. (3) We train the neural field using single-sample\nMonte Carlo estimates of the filtered signal. Our method is fast during both\ntraining and inference, and imposes no additional constraints on the network\narchitecture. We show quantitative and qualitative improvements over existing\nmethods for neural-field filtering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural fields excel at representing continuous visual signals but typically\noperate at a single, fixed resolution. We present a simple yet powerful method\nto optimize neural fields that can be prefiltered in a single forward pass. Key\ninnovations and features include: (1) We perform convolutional filtering in the\ninput domain by analytically scaling Fourier feature embeddings with the\nfilter's frequency response. (2) This closed-form modulation generalizes beyond\nGaussian filtering and supports other parametric filters (Box and Lanczos) that\nare unseen at training time. (3) We train the neural field using single-sample\nMonte Carlo estimates of the filtered signal. Our method is fast during both\ntraining and inference, and imposes no additional constraints on the network\narchitecture. We show quantitative and qualitative improvements over existing\nmethods for neural-field filtering."
                },
                "authors": [
                    {
                        "name": "Mustafa B. Yaldiz"
                    },
                    {
                        "name": "Ishit Mehta"
                    },
                    {
                        "name": "Nithin Raghavan"
                    },
                    {
                        "name": "Andreas Meuleman"
                    },
                    {
                        "name": "Tzu-Mao Li"
                    },
                    {
                        "name": "Ravi Ramamoorthi"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Ramamoorthi"
                },
                "author": "Ravi Ramamoorthi",
                "arxiv_comment": "16 pages, 10 figures, to be published in Siggraph Asia 2025, Website:\n  https://myaldiz.info/assets/spnf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08392v1",
                "updated": "2025-10-09T16:14:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    14,
                    46,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T16:14:46Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    14,
                    46,
                    3,
                    282,
                    0
                ],
                "title": "MeanVC: Lightweight and Streaming Zero-Shot Voice Conversion via Mean\n  Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanVC: Lightweight and Streaming Zero-Shot Voice Conversion via Mean\n  Flows"
                },
                "summary": "Zero-shot voice conversion (VC) aims to transfer timbre from a source speaker\nto any unseen target speaker while preserving linguistic content. Growing\napplication scenarios demand models with streaming inference capabilities. This\nhas created a pressing need for models that are simultaneously fast,\nlightweight, and high-fidelity. However, existing streaming methods typically\nrely on either autoregressive (AR) or non-autoregressive (NAR) frameworks,\nwhich either require large parameter sizes to achieve strong performance or\nstruggle to generalize to unseen speakers. In this study, we propose MeanVC, a\nlightweight and streaming zero-shot VC approach. MeanVC introduces a diffusion\ntransformer with a chunk-wise autoregressive denoising strategy, combining the\nstrengths of both AR and NAR paradigms for efficient streaming processing. By\nintroducing mean flows, MeanVC regresses the average velocity field during\ntraining, enabling zero-shot VC with superior speech quality and speaker\nsimilarity in a single sampling step by directly mapping from the start to the\nendpoint of the flow trajectory. Additionally, we incorporate diffusion\nadversarial post-training to mitigate over-smoothing and further enhance speech\nquality. Experimental results demonstrate that MeanVC significantly outperforms\nexisting zero-shot streaming VC systems, achieving superior conversion quality\nwith higher efficiency and significantly fewer parameters. Audio demos and code\nare publicly available at https://aslp-lab.github.io/MeanVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot voice conversion (VC) aims to transfer timbre from a source speaker\nto any unseen target speaker while preserving linguistic content. Growing\napplication scenarios demand models with streaming inference capabilities. This\nhas created a pressing need for models that are simultaneously fast,\nlightweight, and high-fidelity. However, existing streaming methods typically\nrely on either autoregressive (AR) or non-autoregressive (NAR) frameworks,\nwhich either require large parameter sizes to achieve strong performance or\nstruggle to generalize to unseen speakers. In this study, we propose MeanVC, a\nlightweight and streaming zero-shot VC approach. MeanVC introduces a diffusion\ntransformer with a chunk-wise autoregressive denoising strategy, combining the\nstrengths of both AR and NAR paradigms for efficient streaming processing. By\nintroducing mean flows, MeanVC regresses the average velocity field during\ntraining, enabling zero-shot VC with superior speech quality and speaker\nsimilarity in a single sampling step by directly mapping from the start to the\nendpoint of the flow trajectory. Additionally, we incorporate diffusion\nadversarial post-training to mitigate over-smoothing and further enhance speech\nquality. Experimental results demonstrate that MeanVC significantly outperforms\nexisting zero-shot streaming VC systems, achieving superior conversion quality\nwith higher efficiency and significantly fewer parameters. Audio demos and code\nare publicly available at https://aslp-lab.github.io/MeanVC."
                },
                "authors": [
                    {
                        "name": "Guobin Ma"
                    },
                    {
                        "name": "Jixun Yao"
                    },
                    {
                        "name": "Ziqian Ning"
                    },
                    {
                        "name": "Yuepeng Jiang"
                    },
                    {
                        "name": "Lingxin Xiong"
                    },
                    {
                        "name": "Lei Xie"
                    },
                    {
                        "name": "Pengcheng Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Pengcheng Zhu"
                },
                "author": "Pengcheng Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22536v2",
                "updated": "2025-10-09T16:13:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    13,
                    53,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-26T16:16:49Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    16,
                    49,
                    4,
                    269,
                    0
                ],
                "title": "InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced\n  Language Models"
                },
                "summary": "The immense computational cost of training Large Language Models (LLMs)\npresents a major barrier to innovation. While FP8 training offers a promising\nsolution with significant theoretical efficiency gains, its widespread adoption\nhas been hindered by the lack of a comprehensive, open-source training recipe.\nTo bridge this gap, we introduce an end-to-end FP8 training recipe that\nseamlessly integrates continual pre-training and supervised fine-tuning. Our\nmethodology employs a fine-grained, hybrid-granularity quantization strategy to\nmaintain numerical fidelity while maximizing computational efficiency. Through\nextensive experiments, including the continue pre-training of models on a\n160B-token corpus, we demonstrate that our recipe is not only remarkably stable\nbut also essentially lossless, achieving performance on par with the BF16\nbaseline across a suite of reasoning benchmarks. Crucially, this is achieved\nwith substantial efficiency improvements, including up to a 22% reduction in\ntraining time, a 14% decrease in peak memory usage, and a 19% increase in\nthroughput. Our results establish FP8 as a practical and robust alternative to\nBF16, and we will release the accompanying code to further democratize\nlarge-scale model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense computational cost of training Large Language Models (LLMs)\npresents a major barrier to innovation. While FP8 training offers a promising\nsolution with significant theoretical efficiency gains, its widespread adoption\nhas been hindered by the lack of a comprehensive, open-source training recipe.\nTo bridge this gap, we introduce an end-to-end FP8 training recipe that\nseamlessly integrates continual pre-training and supervised fine-tuning. Our\nmethodology employs a fine-grained, hybrid-granularity quantization strategy to\nmaintain numerical fidelity while maximizing computational efficiency. Through\nextensive experiments, including the continue pre-training of models on a\n160B-token corpus, we demonstrate that our recipe is not only remarkably stable\nbut also essentially lossless, achieving performance on par with the BF16\nbaseline across a suite of reasoning benchmarks. Crucially, this is achieved\nwith substantial efficiency improvements, including up to a 22% reduction in\ntraining time, a 14% decrease in peak memory usage, and a 19% increase in\nthroughput. Our results establish FP8 as a practical and robust alternative to\nBF16, and we will release the accompanying code to further democratize\nlarge-scale model training."
                },
                "authors": [
                    {
                        "name": "Wenjun Wang"
                    },
                    {
                        "name": "Shuo Cai"
                    },
                    {
                        "name": "Congkai Xie"
                    },
                    {
                        "name": "Mingfa Feng"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Kejing Yang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Yuan Xie"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "arxiv_comment": "This paper has been withdrawn by the authors due to a significant bug\n  discovered in our data processing pipeline. This bug affects the validity of\n  the experimental results, and we can no longer stand by the conclusions\n  presented",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08389v1",
                "updated": "2025-10-09T16:12:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    12,
                    12,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T16:12:12Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    12,
                    12,
                    3,
                    282,
                    0
                ],
                "title": "Revisiting Hallucination Detection with Effective Rank-based Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Hallucination Detection with Effective Rank-based Uncertainty"
                },
                "summary": "Detecting hallucinations in large language models (LLMs) remains a\nfundamental challenge for their trustworthy deployment. Going beyond basic\nuncertainty-driven hallucination detection frameworks, we propose a simple yet\npowerful method that quantifies uncertainty by measuring the effective rank of\nhidden states derived from multiple model outputs and different layers.\nGrounded in the spectral analysis of representations, our approach provides\ninterpretable insights into the model's internal reasoning process through\nsemantic variations, while requiring no extra knowledge or additional modules,\nthus offering a combination of theoretical elegance and practical efficiency.\nMeanwhile, we theoretically demonstrate the necessity of quantifying\nuncertainty both internally (representations of a single response) and\nexternally (different responses), providing a justification for using\nrepresentations among different layers and responses from LLMs to detect\nhallucinations. Extensive experiments demonstrate that our method effectively\ndetects hallucinations and generalizes robustly across various scenarios,\ncontributing to a new paradigm of hallucination detection for LLM truthfulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting hallucinations in large language models (LLMs) remains a\nfundamental challenge for their trustworthy deployment. Going beyond basic\nuncertainty-driven hallucination detection frameworks, we propose a simple yet\npowerful method that quantifies uncertainty by measuring the effective rank of\nhidden states derived from multiple model outputs and different layers.\nGrounded in the spectral analysis of representations, our approach provides\ninterpretable insights into the model's internal reasoning process through\nsemantic variations, while requiring no extra knowledge or additional modules,\nthus offering a combination of theoretical elegance and practical efficiency.\nMeanwhile, we theoretically demonstrate the necessity of quantifying\nuncertainty both internally (representations of a single response) and\nexternally (different responses), providing a justification for using\nrepresentations among different layers and responses from LLMs to detect\nhallucinations. Extensive experiments demonstrate that our method effectively\ndetects hallucinations and generalizes robustly across various scenarios,\ncontributing to a new paradigm of hallucination detection for LLM truthfulness."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Guanzhang Yue"
                    },
                    {
                        "name": "Meng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Meng Sun"
                },
                "author": "Meng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08388v1",
                "updated": "2025-10-09T16:12:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    12,
                    10,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T16:12:10Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    12,
                    10,
                    3,
                    282,
                    0
                ],
                "title": "If Probable, Then Acceptable? Understanding Conditional Acceptability\n  Judgments in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If Probable, Then Acceptable? Understanding Conditional Acceptability\n  Judgments in Large Language Models"
                },
                "summary": "Conditional acceptability refers to how plausible a conditional statement is\nperceived to be. It plays an important role in communication and reasoning, as\nit influences how individuals interpret implications, assess arguments, and\nmake decisions based on hypothetical scenarios. When humans evaluate how\nacceptable a conditional \"If A, then B\" is, their judgments are influenced by\ntwo main factors: the $\\textit{conditional probability}$ of $B$ given $A$, and\nthe $\\textit{semantic relevance}$ of the antecedent $A$ given the consequent\n$B$ (i.e., whether $A$ meaningfully supports $B$). While prior work has\nexamined how large language models (LLMs) draw inferences about conditional\nstatements, it remains unclear how these models judge the\n$\\textit{acceptability}$ of such statements. To address this gap, we present a\ncomprehensive study of LLMs' conditional acceptability judgments across\ndifferent model families, sizes, and prompting strategies. Using linear\nmixed-effects models and ANOVA tests, we find that models are sensitive to both\nconditional probability and semantic relevance-though to varying degrees\ndepending on architecture and prompting style. A comparison with human data\nreveals that while LLMs incorporate probabilistic and semantic cues, they do so\nless consistently than humans. Notably, larger models do not necessarily align\nmore closely with human judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional acceptability refers to how plausible a conditional statement is\nperceived to be. It plays an important role in communication and reasoning, as\nit influences how individuals interpret implications, assess arguments, and\nmake decisions based on hypothetical scenarios. When humans evaluate how\nacceptable a conditional \"If A, then B\" is, their judgments are influenced by\ntwo main factors: the $\\textit{conditional probability}$ of $B$ given $A$, and\nthe $\\textit{semantic relevance}$ of the antecedent $A$ given the consequent\n$B$ (i.e., whether $A$ meaningfully supports $B$). While prior work has\nexamined how large language models (LLMs) draw inferences about conditional\nstatements, it remains unclear how these models judge the\n$\\textit{acceptability}$ of such statements. To address this gap, we present a\ncomprehensive study of LLMs' conditional acceptability judgments across\ndifferent model families, sizes, and prompting strategies. Using linear\nmixed-effects models and ANOVA tests, we find that models are sensitive to both\nconditional probability and semantic relevance-though to varying degrees\ndepending on architecture and prompting style. A comparison with human data\nreveals that while LLMs incorporate probabilistic and semantic cues, they do so\nless consistently than humans. Notably, larger models do not necessarily align\nmore closely with human judgments."
                },
                "authors": [
                    {
                        "name": "Jasmin Orth"
                    },
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "22 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08383v1",
                "updated": "2025-10-09T16:08:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    8,
                    5,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T16:08:05Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    8,
                    5,
                    3,
                    282,
                    0
                ],
                "title": "QAgent: A modular Search Agent with Interactive Query Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QAgent: A modular Search Agent with Interactive Query Understanding"
                },
                "summary": "Large language models (LLMs) excel at natural language tasks but are limited\nby their static parametric knowledge, especially in knowledge-intensive task.\nRetrieval-augmented generation (RAG) mitigates this by integrating external\ninformation. However, (1) traditional RAG struggles with complex query\nunderstanding, and (2) even search agents trained with reinforcement learning\n(RL), despite their promise, still face generalization and deployment\nchallenges. To address these limitations, we propose QAgent, a unified agentic\nRAG framework that employs a search agent for adaptive retrieval. This agent\noptimizes its understanding of the query through interactive reasoning and\nretrieval. To facilitate real-world application, we focus on modular search\nagent for query understanding that are plug-and-play in complex systems.\nSecifically, the agent follows a multi-step decision process trained with RL to\nmaximize retrieval quality and support accurate downstream answers. We further\nanalyze the strengths and weaknesses of end-to-end RL and propose a strategy\nthat focuses on effective retrieval, thereby enhancing generalization in LLM\napplications. Experiments show QAgent excels at QA and serves as a\nplug-and-play module for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at natural language tasks but are limited\nby their static parametric knowledge, especially in knowledge-intensive task.\nRetrieval-augmented generation (RAG) mitigates this by integrating external\ninformation. However, (1) traditional RAG struggles with complex query\nunderstanding, and (2) even search agents trained with reinforcement learning\n(RL), despite their promise, still face generalization and deployment\nchallenges. To address these limitations, we propose QAgent, a unified agentic\nRAG framework that employs a search agent for adaptive retrieval. This agent\noptimizes its understanding of the query through interactive reasoning and\nretrieval. To facilitate real-world application, we focus on modular search\nagent for query understanding that are plug-and-play in complex systems.\nSecifically, the agent follows a multi-step decision process trained with RL to\nmaximize retrieval quality and support accurate downstream answers. We further\nanalyze the strengths and weaknesses of end-to-end RL and propose a strategy\nthat focuses on effective retrieval, thereby enhancing generalization in LLM\napplications. Experiments show QAgent excels at QA and serves as a\nplug-and-play module for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Lei Shen"
                    },
                    {
                        "name": "Lujie Niu"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "Code is available at https://github.com/OpenStellarTeam/QAgent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03336v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03336v3",
                "updated": "2025-10-09T16:01:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    1,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-07-04T06:49:02Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    49,
                    2,
                    4,
                    185,
                    0
                ],
                "title": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky"
                },
                "summary": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents."
                },
                "authors": [
                    {
                        "name": "Ashutosh Hathidara"
                    },
                    {
                        "name": "Julien Yu"
                    },
                    {
                        "name": "Sebastian Schreiber"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Schreiber"
                },
                "author": "Sebastian Schreiber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03336v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03336v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20435v2",
                "updated": "2025-10-09T16:00:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    0,
                    15,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-26T18:31:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    31,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "The Shape of Adversarial Influence: Characterizing LLM Latent Spaces\n  with Persistent Homology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Shape of Adversarial Influence: Characterizing LLM Latent Spaces\n  with Persistent Homology"
                },
                "summary": "Existing interpretability methods for Large Language Models (LLMs) often fall\nshort by focusing on linear directions or isolated features, overlooking the\nhigh-dimensional, nonlinear, and relational geometry within model\nrepresentations. This study focuses on how adversarial inputs systematically\naffect the internal representation spaces of LLMs, a topic which remains poorly\nunderstood. We propose persistent homology (PH), a tool from topological data\nanalysis, as a principled framework to characterize the multi-scale dynamics\nwithin LLM activations. Using PH, we systematically analyze six\nstate-of-the-art models under two distinct adversarial conditions, indirect\nprompt injection and backdoor fine-tuning, and identify a consistent\ntopological signature of adversarial influence. Across architectures and model\nsizes, adversarial inputs induce ``topological compression'', where the latent\nspace becomes structurally simpler, collapsing from varied, compact,\nsmall-scale features into fewer, dominant, and more dispersed large-scale ones.\nThis topological signature is statistically robust across layers, highly\ndiscriminative, and provides interpretable insights into how adversarial\neffects emerge and propagate. By quantifying the shape of activations and\nneuronal information flow, our architecture-agnostic framework reveals\nfundamental invariants of representational change, offering a complementary\nperspective to existing interpretability methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing interpretability methods for Large Language Models (LLMs) often fall\nshort by focusing on linear directions or isolated features, overlooking the\nhigh-dimensional, nonlinear, and relational geometry within model\nrepresentations. This study focuses on how adversarial inputs systematically\naffect the internal representation spaces of LLMs, a topic which remains poorly\nunderstood. We propose persistent homology (PH), a tool from topological data\nanalysis, as a principled framework to characterize the multi-scale dynamics\nwithin LLM activations. Using PH, we systematically analyze six\nstate-of-the-art models under two distinct adversarial conditions, indirect\nprompt injection and backdoor fine-tuning, and identify a consistent\ntopological signature of adversarial influence. Across architectures and model\nsizes, adversarial inputs induce ``topological compression'', where the latent\nspace becomes structurally simpler, collapsing from varied, compact,\nsmall-scale features into fewer, dominant, and more dispersed large-scale ones.\nThis topological signature is statistically robust across layers, highly\ndiscriminative, and provides interpretable insights into how adversarial\neffects emerge and propagate. By quantifying the shape of activations and\nneuronal information flow, our architecture-agnostic framework reveals\nfundamental invariants of representational change, offering a complementary\nperspective to existing interpretability methods."
                },
                "authors": [
                    {
                        "name": "Aideen Fay"
                    },
                    {
                        "name": "Inés García-Redondo"
                    },
                    {
                        "name": "Qiquan Wang"
                    },
                    {
                        "name": "Haim Dubossarsky"
                    },
                    {
                        "name": "Anthea Monod"
                    }
                ],
                "author_detail": {
                    "name": "Anthea Monod"
                },
                "author": "Anthea Monod",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01616v3",
                "updated": "2025-10-09T15:58:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    58,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-02T22:36:24Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    22,
                    36,
                    24,
                    4,
                    122,
                    0
                ],
                "title": "Phantora: Maximizing Code Reuse in Simulation-based Machine Learning\n  System Performance Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phantora: Maximizing Code Reuse in Simulation-based Machine Learning\n  System Performance Estimation"
                },
                "summary": "Modern machine learning (ML) training workloads place substantial demands on\nboth computational and communication resources. Consequently, accurate\nperformance estimation has become increasingly critical for guiding system\ndesign decisions, such as the selection of parallelization strategies, cluster\nconfigurations, and hardware provisioning. Existing simulation-based\nperformance estimation requires reimplementing the ML framework in a simulator,\nwhich demands significant manual effort and is hard to maintain as ML\nframeworks evolve rapidly.\n  This paper introduces Phantora, a hybrid GPU cluster simulator designed for\nperformance estimation of ML training workloads. Phantora executes unmodified\nML frameworks as is within a distributed, containerized environment. Each\ncontainer emulates the behavior of a GPU server in a large-scale cluster, while\nPhantora intercepts and simulates GPU- and communication-related operations to\nprovide high-fidelity performance estimation. We call this approach hybrid\nsimulation of ML systems, in contrast to traditional methods that simulate\nstatic workloads. The primary advantage of hybrid simulation is that it allows\ndirect reuse of ML framework source code in simulation, avoiding the need for\nreimplementation. Our evaluation shows that Phantora provides accuracy\ncomparable to static workload simulation while supporting three\nstate-of-the-art LLM training frameworks out-of-the-box. In addition, Phantora\noperates on a single GPU, eliminating the need for the resource-intensive trace\ncollection and workload extraction steps required by traditional trace-based\nsimulators. Phantora is open-sourced at https://github.com/QDelta/Phantora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern machine learning (ML) training workloads place substantial demands on\nboth computational and communication resources. Consequently, accurate\nperformance estimation has become increasingly critical for guiding system\ndesign decisions, such as the selection of parallelization strategies, cluster\nconfigurations, and hardware provisioning. Existing simulation-based\nperformance estimation requires reimplementing the ML framework in a simulator,\nwhich demands significant manual effort and is hard to maintain as ML\nframeworks evolve rapidly.\n  This paper introduces Phantora, a hybrid GPU cluster simulator designed for\nperformance estimation of ML training workloads. Phantora executes unmodified\nML frameworks as is within a distributed, containerized environment. Each\ncontainer emulates the behavior of a GPU server in a large-scale cluster, while\nPhantora intercepts and simulates GPU- and communication-related operations to\nprovide high-fidelity performance estimation. We call this approach hybrid\nsimulation of ML systems, in contrast to traditional methods that simulate\nstatic workloads. The primary advantage of hybrid simulation is that it allows\ndirect reuse of ML framework source code in simulation, avoiding the need for\nreimplementation. Our evaluation shows that Phantora provides accuracy\ncomparable to static workload simulation while supporting three\nstate-of-the-art LLM training frameworks out-of-the-box. In addition, Phantora\noperates on a single GPU, eliminating the need for the resource-intensive trace\ncollection and workload extraction steps required by traditional trace-based\nsimulators. Phantora is open-sourced at https://github.com/QDelta/Phantora."
                },
                "authors": [
                    {
                        "name": "Jianxing Qin"
                    },
                    {
                        "name": "Jingrong Chen"
                    },
                    {
                        "name": "Xinhao Kong"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Tianjun Yuan"
                    },
                    {
                        "name": "Liang Luo"
                    },
                    {
                        "name": "Zhaodong Wang"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Tingjun Chen"
                    },
                    {
                        "name": "Alvin R. Lebeck"
                    },
                    {
                        "name": "Danyang Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Danyang Zhuo"
                },
                "author": "Danyang Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08373v1",
                "updated": "2025-10-09T15:56:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    56,
                    18,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:56:18Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    56,
                    18,
                    3,
                    282,
                    0
                ],
                "title": "DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching"
                },
                "summary": "Recent advances in text-to-speech (TTS) synthesis, particularly those\nleveraging large language models (LLMs), have significantly improved\nexpressiveness and naturalness. However, generating human-like, interactive\ndialogue speech remains challenging. Current systems face limitations due to\nthe scarcity of dual-track data and difficulties in achieving naturalness,\ncontextual coherence, and interactional dynamics, such as turn-taking,\noverlapping speech, and speaker consistency, in multi-turn conversations. To\naddress these challenges, we propose DialoSpeech, a dual-track architecture\ncombining a large language model with Chunked Flow Matching for expressive,\nhuman-like dialogue speech synthesis. DialoSpeech generates natural multi-turn\nconversations with coherent speaker turns and natural overlaps, supporting both\nChinese and English and cross-lingual speech synthesis. We introduce a data\nprocessing pipeline to construct dual-track dialogue datasets, facilitating\nscalable training and experimental validation. Experiments show that our model\noutperforms baselines, offering a solution for generating human-like spoken\ndialogues. Audio samples are available at\nhttps://tiamojames.github.io/DialoSpeech",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-speech (TTS) synthesis, particularly those\nleveraging large language models (LLMs), have significantly improved\nexpressiveness and naturalness. However, generating human-like, interactive\ndialogue speech remains challenging. Current systems face limitations due to\nthe scarcity of dual-track data and difficulties in achieving naturalness,\ncontextual coherence, and interactional dynamics, such as turn-taking,\noverlapping speech, and speaker consistency, in multi-turn conversations. To\naddress these challenges, we propose DialoSpeech, a dual-track architecture\ncombining a large language model with Chunked Flow Matching for expressive,\nhuman-like dialogue speech synthesis. DialoSpeech generates natural multi-turn\nconversations with coherent speaker turns and natural overlaps, supporting both\nChinese and English and cross-lingual speech synthesis. We introduce a data\nprocessing pipeline to construct dual-track dialogue datasets, facilitating\nscalable training and experimental validation. Experiments show that our model\noutperforms baselines, offering a solution for generating human-like spoken\ndialogues. Audio samples are available at\nhttps://tiamojames.github.io/DialoSpeech"
                },
                "authors": [
                    {
                        "name": "Hanke Xie"
                    },
                    {
                        "name": "Dake Guo"
                    },
                    {
                        "name": "Chengyou Wang"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Wenjie Tian"
                    },
                    {
                        "name": "Xinfa Zhu"
                    },
                    {
                        "name": "Xinsheng Wang"
                    },
                    {
                        "name": "Xiulin Li"
                    },
                    {
                        "name": "Guanqiong Miao"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08372v1",
                "updated": "2025-10-09T15:55:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    55,
                    28,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:55:28Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    55,
                    28,
                    3,
                    282,
                    0
                ],
                "title": "On the Relationship Between the Choice of Representation and In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Relationship Between the Choice of Representation and In-Context\n  Learning"
                },
                "summary": "In-context learning (ICL) is the ability of a large language model (LLM) to\nlearn a new task from a few demonstrations presented as part of the context.\nPast studies have attributed a large portion of the success of ICL to the way\nthese in-context demonstrations are represented, particularly to how labels are\nrepresented in classification tasks. On the other hand, observations of the\nlearning capacity of ICL (i.e., the extent to which more in-context\ndemonstrations can lead to higher performance) have been mixed, and ICL is\noften thought to occur only under specific conditions. The interaction between\nthese two aspects in ICL, representation and learning, has not been studied in\ndepth until now. We hypothesize that they are largely independent of one\nanother, such that the representation of demonstrations determines the baseline\naccuracy of ICL, while learning from additional demonstrations improves only on\ntop of this baseline. We validate this hypothesis by developing an optimization\nalgorithm that can enumerate a spectrum of possible label sets\n(representations) varying in semantic relevance. We then perform ICL with\nvarying numbers of in-context demonstrations for each of these label sets. We\nobserved that learning happens regardless of the quality of the label set\nitself, although its efficiency, measured by the slope of improvement over\nin-context demonstrations, is conditioned on both the label set quality and the\nparameter count of the underlying language model. Despite the emergence of\nlearning, the relative quality (accuracy) of the choice of a label set\n(representation) is largely maintained throughout learning, confirming our\nhypothesis and implying their orthogonality. Our work reveals a previously\nunderexplored aspect of ICL: the independent effects of learning from\ndemonstrations and their representations on ICL performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is the ability of a large language model (LLM) to\nlearn a new task from a few demonstrations presented as part of the context.\nPast studies have attributed a large portion of the success of ICL to the way\nthese in-context demonstrations are represented, particularly to how labels are\nrepresented in classification tasks. On the other hand, observations of the\nlearning capacity of ICL (i.e., the extent to which more in-context\ndemonstrations can lead to higher performance) have been mixed, and ICL is\noften thought to occur only under specific conditions. The interaction between\nthese two aspects in ICL, representation and learning, has not been studied in\ndepth until now. We hypothesize that they are largely independent of one\nanother, such that the representation of demonstrations determines the baseline\naccuracy of ICL, while learning from additional demonstrations improves only on\ntop of this baseline. We validate this hypothesis by developing an optimization\nalgorithm that can enumerate a spectrum of possible label sets\n(representations) varying in semantic relevance. We then perform ICL with\nvarying numbers of in-context demonstrations for each of these label sets. We\nobserved that learning happens regardless of the quality of the label set\nitself, although its efficiency, measured by the slope of improvement over\nin-context demonstrations, is conditioned on both the label set quality and the\nparameter count of the underlying language model. Despite the emergence of\nlearning, the relative quality (accuracy) of the choice of a label set\n(representation) is largely maintained throughout learning, confirming our\nhypothesis and implying their orthogonality. Our work reveals a previously\nunderexplored aspect of ICL: the independent effects of learning from\ndemonstrations and their representations on ICL performance."
                },
                "authors": [
                    {
                        "name": "Ioana Marinescu"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    },
                    {
                        "name": "Eric Karl Oermann"
                    }
                ],
                "author_detail": {
                    "name": "Eric Karl Oermann"
                },
                "author": "Eric Karl Oermann",
                "arxiv_comment": "25 pages, 6 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13468v2",
                "updated": "2025-10-09T15:54:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    54,
                    27,
                    3,
                    282,
                    0
                ],
                "published": "2025-07-17T18:21:45Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    18,
                    21,
                    45,
                    3,
                    198,
                    0
                ],
                "title": "ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in\n  Human-Robot Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in\n  Human-Robot Conversations"
                },
                "summary": "The integration of large language models (LLMs) into conversational robots\nhas made human-robot conversations more dynamic. Yet, LLM-powered\nconversational robots remain prone to errors, e.g., misunderstanding user\nintent, prematurely interrupting users, or failing to respond altogether.\nDetecting and addressing these failures is critical for preventing\nconversational breakdowns, avoiding task disruptions, and sustaining user\ntrust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal\ndataset of LLM-powered conversational robot failures during human-robot\nconversations and encourages researchers to benchmark machine learning models\ndesigned to detect robot failures. The dataset includes 16 hours of dyadic\nhuman-robot interactions, incorporating facial, speech, and head movement\nfeatures. Each interaction is annotated with the presence or absence of robot\nerrors from the system perspective, and perceived user intention to correct for\na mismatch between robot behavior and user expectation. Participants are\ninvited to form teams and develop machine learning models that detect these\nfailures using multimodal data. Submissions will be evaluated using various\nperformance metrics, including detection accuracy and false positive rate. This\nchallenge represents another key step toward improving failure detection in\nhuman-robot interaction through social signal analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) into conversational robots\nhas made human-robot conversations more dynamic. Yet, LLM-powered\nconversational robots remain prone to errors, e.g., misunderstanding user\nintent, prematurely interrupting users, or failing to respond altogether.\nDetecting and addressing these failures is critical for preventing\nconversational breakdowns, avoiding task disruptions, and sustaining user\ntrust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal\ndataset of LLM-powered conversational robot failures during human-robot\nconversations and encourages researchers to benchmark machine learning models\ndesigned to detect robot failures. The dataset includes 16 hours of dyadic\nhuman-robot interactions, incorporating facial, speech, and head movement\nfeatures. Each interaction is annotated with the presence or absence of robot\nerrors from the system perspective, and perceived user intention to correct for\na mismatch between robot behavior and user expectation. Participants are\ninvited to form teams and develop machine learning models that detect these\nfailures using multimodal data. Submissions will be evaluated using various\nperformance metrics, including detection accuracy and false positive rate. This\nchallenge represents another key step toward improving failure detection in\nhuman-robot interaction through social signal analysis."
                },
                "authors": [
                    {
                        "name": "Shiye Cao"
                    },
                    {
                        "name": "Maia Stiber"
                    },
                    {
                        "name": "Amama Mahmood"
                    },
                    {
                        "name": "Maria Teresa Parreira"
                    },
                    {
                        "name": "Wendy Ju"
                    },
                    {
                        "name": "Micol Spitale"
                    },
                    {
                        "name": "Hatice Gunes"
                    },
                    {
                        "name": "Chien-Ming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Ming Huang"
                },
                "author": "Chien-Ming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01696v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01696v3",
                "updated": "2025-10-09T15:53:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    53,
                    40,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-03T10:00:38Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    10,
                    0,
                    38,
                    6,
                    215,
                    0
                ],
                "title": "CoCoA: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge\n  Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCoA: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge\n  Synergy"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs),\nespecially for knowledge-intensive tasks. Despite its advantages, current RAG\nmethods often struggle to fully exploit knowledge during generation. In\nparticular, the synergy between the model's internal parametric knowledge and\nexternal retrieved knowledge remains limited. Retrieved contents may sometimes\nmislead generation, while certain generated content can guide the model toward\nmore accurate outputs. In this work, we propose Collaborative Chain-of-Agents,\na framework designed to enhance explicitly synergy over both parametric and\nretrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent\nRAG framework that first performs conditional knowledge induction and then\nreasons answers. Building on this, we develop CoCoA, a long-chain training\nstrategy that synthesizes extended multi-agent reasoning trajectories from\nCoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability\nto explicitly integrate and jointly leverage parametric and retrieved\nknowledge. Experimental results demonstrate the superiority of CoCoA in\nopen-domain QA and multi-hop QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs),\nespecially for knowledge-intensive tasks. Despite its advantages, current RAG\nmethods often struggle to fully exploit knowledge during generation. In\nparticular, the synergy between the model's internal parametric knowledge and\nexternal retrieved knowledge remains limited. Retrieved contents may sometimes\nmislead generation, while certain generated content can guide the model toward\nmore accurate outputs. In this work, we propose Collaborative Chain-of-Agents,\na framework designed to enhance explicitly synergy over both parametric and\nretrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent\nRAG framework that first performs conditional knowledge induction and then\nreasons answers. Building on this, we develop CoCoA, a long-chain training\nstrategy that synthesizes extended multi-agent reasoning trajectories from\nCoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability\nto explicitly integrate and jointly leverage parametric and retrieved\nknowledge. Experimental results demonstrate the superiority of CoCoA in\nopen-domain QA and multi-hop QA."
                },
                "authors": [
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Haochun Wang"
                    },
                    {
                        "name": "Lizhe Zhang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "code available at https://github.com/liunian-Jay/CoCoA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01696v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01696v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08365v1",
                "updated": "2025-10-09T15:51:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    51,
                    5,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:51:05Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    51,
                    5,
                    3,
                    282,
                    0
                ],
                "title": "Two-Stage Voting for Robust and Efficient Suicide Risk Detection on\n  Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Stage Voting for Robust and Efficient Suicide Risk Detection on\n  Social Media"
                },
                "summary": "Suicide rates have risen worldwide in recent years, underscoring the urgent\nneed for proactive prevention strategies. Social media provides valuable\nsignals, as many at-risk individuals - who often avoid formal help due to\nstigma - choose instead to share their distress online. Yet detecting implicit\nsuicidal ideation, conveyed indirectly through metaphor, sarcasm, or subtle\nemotional cues, remains highly challenging. Lightweight models like BERT handle\nexplicit signals but fail on subtle implicit ones, while large language models\n(LLMs) capture nuance at prohibitive computational cost. To address this gap,\nwe propose a two-stage voting architecture that balances efficiency and\nrobustness. In Stage 1, a lightweight BERT classifier rapidly resolves\nhigh-confidence explicit cases. In Stage 2, ambiguous inputs are escalated to\neither (i) a multi-perspective LLM voting framework to maximize recall on\nimplicit ideation, or (ii) a feature-based ML ensemble guided by\npsychologically grounded indicators extracted via prompt-engineered LLMs for\nefficiency and interpretability. To the best of our knowledge, this is among\nthe first works to operationalize LLM-extracted psychological features as\nstructured vectors for suicide risk detection. On two complementary datasets -\nexplicit-dominant Reddit and implicit-only DeepSuiMind - our framework\noutperforms single-model baselines, achieving 98.0% F1 on explicit cases, 99.7%\non implicit ones, and reducing the cross-domain gap below 2%, while\nsignificantly lowering LLM cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suicide rates have risen worldwide in recent years, underscoring the urgent\nneed for proactive prevention strategies. Social media provides valuable\nsignals, as many at-risk individuals - who often avoid formal help due to\nstigma - choose instead to share their distress online. Yet detecting implicit\nsuicidal ideation, conveyed indirectly through metaphor, sarcasm, or subtle\nemotional cues, remains highly challenging. Lightweight models like BERT handle\nexplicit signals but fail on subtle implicit ones, while large language models\n(LLMs) capture nuance at prohibitive computational cost. To address this gap,\nwe propose a two-stage voting architecture that balances efficiency and\nrobustness. In Stage 1, a lightweight BERT classifier rapidly resolves\nhigh-confidence explicit cases. In Stage 2, ambiguous inputs are escalated to\neither (i) a multi-perspective LLM voting framework to maximize recall on\nimplicit ideation, or (ii) a feature-based ML ensemble guided by\npsychologically grounded indicators extracted via prompt-engineered LLMs for\nefficiency and interpretability. To the best of our knowledge, this is among\nthe first works to operationalize LLM-extracted psychological features as\nstructured vectors for suicide risk detection. On two complementary datasets -\nexplicit-dominant Reddit and implicit-only DeepSuiMind - our framework\noutperforms single-model baselines, achieving 98.0% F1 on explicit cases, 99.7%\non implicit ones, and reducing the cross-domain gap below 2%, while\nsignificantly lowering LLM cost."
                },
                "authors": [
                    {
                        "name": "Yukai Song"
                    },
                    {
                        "name": "Pengfei Zhou"
                    },
                    {
                        "name": "César Escobar-Viera"
                    },
                    {
                        "name": "Candice Biernesser"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Jingtong Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jingtong Hu"
                },
                "author": "Jingtong Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08359v1",
                "updated": "2025-10-09T15:44:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    44,
                    16,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:44:16Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    44,
                    16,
                    3,
                    282,
                    0
                ],
                "title": "Doubly Robust Estimation with Stabilized Weights for Binary Proximal\n  Outcomes in Micro-Randomized Trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly Robust Estimation with Stabilized Weights for Binary Proximal\n  Outcomes in Micro-Randomized Trials"
                },
                "summary": "Micro-randomized trials (MRTs) are increasingly used to evaluate mobile\nhealth interventions with binary proximal outcomes. Standard inverse\nprobability weighting (IPW) estimators are unbiased but unstable in small\nsamples or under extreme randomization. Estimated mean excursion effect (EMEE)\nimproves efficiency but lacks double robustness. We propose a doubly robust\nEMEE (DR-EMEE) with stabilized and truncated weights, combining per-decision\nIPW and outcome regression. We prove double robustness, asymptotic efficiency,\nand provide finite-sample variance corrections, with extensions to machine\nlearning nuisance estimators. In simulations, DR-EMEE reduces root mean squared\nerror, improves coverage, and achieves up to twofold efficiency gains over IPW\nand five to ten percent over EMEE. Applications to HeartSteps, PAMAP2, and\nmHealth datasets confirm stable and efficient inference across both randomized\nand observational settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Micro-randomized trials (MRTs) are increasingly used to evaluate mobile\nhealth interventions with binary proximal outcomes. Standard inverse\nprobability weighting (IPW) estimators are unbiased but unstable in small\nsamples or under extreme randomization. Estimated mean excursion effect (EMEE)\nimproves efficiency but lacks double robustness. We propose a doubly robust\nEMEE (DR-EMEE) with stabilized and truncated weights, combining per-decision\nIPW and outcome regression. We prove double robustness, asymptotic efficiency,\nand provide finite-sample variance corrections, with extensions to machine\nlearning nuisance estimators. In simulations, DR-EMEE reduces root mean squared\nerror, improves coverage, and achieves up to twofold efficiency gains over IPW\nand five to ten percent over EMEE. Applications to HeartSteps, PAMAP2, and\nmHealth datasets confirm stable and efficient inference across both randomized\nand observational settings."
                },
                "authors": [
                    {
                        "name": "Jinho Cha"
                    },
                    {
                        "name": "Eunchan Cha"
                    }
                ],
                "author_detail": {
                    "name": "Eunchan Cha"
                },
                "author": "Eunchan Cha",
                "arxiv_comment": "32 pages, 7 figures, planned to submit to Biostatistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08357v1",
                "updated": "2025-10-09T15:42:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    42,
                    47,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:42:47Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    42,
                    47,
                    3,
                    282,
                    0
                ],
                "title": "Learning to Mitigate Post-Outage Load Surges: A Data-Driven Framework\n  for Electrifying and Decarbonizing Grids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Mitigate Post-Outage Load Surges: A Data-Driven Framework\n  for Electrifying and Decarbonizing Grids"
                },
                "summary": "Electrification and decarbonization are transforming power system demand and\nrecovery dynamics, yet their implications for post-outage load surges remain\npoorly understood. Here we analyze a metropolitan-scale heterogeneous dataset\nfor Indianapolis comprising 30,046 feeder-level outages between 2020 and 2024,\nlinked to smart meters and submetering, to quantify the causal impact of\nelectric vehicles (EVs), heat pumps (HPs) and distributed energy resources\n(DERs) on restoration surges. Statistical analysis and causal forest inference\ndemonstrate that rising penetrations of all three assets significantly increase\nsurge ratios, with effects strongly modulated by restoration timing, outage\nduration and weather conditions. We develop a component-aware multi-task\nTransformer estimator that disaggregates EV, HP and DER contributions, and\napply it to project historical outages under counterfactual 2035 adoption\npathways. In a policy-aligned pathway, evening restorations emerge as the\nbinding reliability constraint, with exceedance probabilities of 0.057 when\n30\\% of system load is restored within the first 15 minutes. Mitigation\nmeasures, probabilistic EV restarts, short thermostat offsets and accelerated\nDER reconnection, reduce exceedance to 0.019 and eliminate it entirely when\n20\\% or less of system load is restored. These results demonstrate that\ntransition-era surges are asset-driven and causally linked to electrification\nand decarbonization, but can be effectively managed through integrated\noperational strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrification and decarbonization are transforming power system demand and\nrecovery dynamics, yet their implications for post-outage load surges remain\npoorly understood. Here we analyze a metropolitan-scale heterogeneous dataset\nfor Indianapolis comprising 30,046 feeder-level outages between 2020 and 2024,\nlinked to smart meters and submetering, to quantify the causal impact of\nelectric vehicles (EVs), heat pumps (HPs) and distributed energy resources\n(DERs) on restoration surges. Statistical analysis and causal forest inference\ndemonstrate that rising penetrations of all three assets significantly increase\nsurge ratios, with effects strongly modulated by restoration timing, outage\nduration and weather conditions. We develop a component-aware multi-task\nTransformer estimator that disaggregates EV, HP and DER contributions, and\napply it to project historical outages under counterfactual 2035 adoption\npathways. In a policy-aligned pathway, evening restorations emerge as the\nbinding reliability constraint, with exceedance probabilities of 0.057 when\n30\\% of system load is restored within the first 15 minutes. Mitigation\nmeasures, probabilistic EV restarts, short thermostat offsets and accelerated\nDER reconnection, reduce exceedance to 0.019 and eliminate it entirely when\n20\\% or less of system load is restored. These results demonstrate that\ntransition-era surges are asset-driven and causally linked to electrification\nand decarbonization, but can be effectively managed through integrated\noperational strategies."
                },
                "authors": [
                    {
                        "name": "Wenlong Shi"
                    },
                    {
                        "name": "Dingwei Wang"
                    },
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Zhaoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoyu Wang"
                },
                "author": "Zhaoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08354v1",
                "updated": "2025-10-09T15:41:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    41,
                    3,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:41:03Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    41,
                    3,
                    3,
                    282,
                    0
                ],
                "title": "Mephisto: Self-Improving Large Language Model-Based Agents for Automated\n  Interpretation of Multi-band Galaxy Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mephisto: Self-Improving Large Language Model-Based Agents for Automated\n  Interpretation of Multi-band Galaxy Observations"
                },
                "summary": "Astronomical research has long relied on human expertise to interpret complex\ndata and formulate scientific hypotheses. In this study, we introduce Mephisto\n-- a multi-agent collaboration framework powered by large language models\n(LLMs) that emulates human-like reasoning for analyzing multi-band galaxy\nobservations. Mephisto interfaces with the CIGALE codebase (a library of\nspectral energy distribution, SED, models) to iteratively refine physical\nmodels against observational data. It conducts deliberate reasoning via tree\nsearch, accumulates knowledge through self-play, and dynamically updates its\nknowledge base. Validated across diverse galaxy populations -- including the\nJames Webb Space Telescope's recently discovered \"Little Red Dot\" galaxies --\nwe show that Mephisto demonstrates proficiency in inferring the physical\nproperties of galaxies from multi-band photometry, positioning it as a\npromising research copilot for astronomers. Unlike prior black-box machine\nlearning approaches in astronomy, Mephisto offers a transparent, human-aligned\nreasoning process that integrates seamlessly with existing research practices.\nThis work underscores the possibility of LLM-driven agent-based research for\nastronomy, establishes a foundation for fully automated, end-to-end artificial\nintelligence (AI)-powered scientific workflows, and unlocks new avenues for\nAI-augmented discoveries in astronomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Astronomical research has long relied on human expertise to interpret complex\ndata and formulate scientific hypotheses. In this study, we introduce Mephisto\n-- a multi-agent collaboration framework powered by large language models\n(LLMs) that emulates human-like reasoning for analyzing multi-band galaxy\nobservations. Mephisto interfaces with the CIGALE codebase (a library of\nspectral energy distribution, SED, models) to iteratively refine physical\nmodels against observational data. It conducts deliberate reasoning via tree\nsearch, accumulates knowledge through self-play, and dynamically updates its\nknowledge base. Validated across diverse galaxy populations -- including the\nJames Webb Space Telescope's recently discovered \"Little Red Dot\" galaxies --\nwe show that Mephisto demonstrates proficiency in inferring the physical\nproperties of galaxies from multi-band photometry, positioning it as a\npromising research copilot for astronomers. Unlike prior black-box machine\nlearning approaches in astronomy, Mephisto offers a transparent, human-aligned\nreasoning process that integrates seamlessly with existing research practices.\nThis work underscores the possibility of LLM-driven agent-based research for\nastronomy, establishes a foundation for fully automated, end-to-end artificial\nintelligence (AI)-powered scientific workflows, and unlocks new avenues for\nAI-augmented discoveries in astronomy."
                },
                "authors": [
                    {
                        "name": "Zechang Sun"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "Yaobo Liang"
                    },
                    {
                        "name": "Nan Duan"
                    },
                    {
                        "name": "Song Huang"
                    },
                    {
                        "name": "Zheng Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cai"
                },
                "author": "Zheng Cai",
                "arxiv_comment": "17 pages main text + 13 pages appendix. A conference abstract is\n  available at arXiv:2409.14807. Submitted to AAS journal. Comments and\n  feedback are welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05117v2",
                "updated": "2025-10-09T15:27:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    27,
                    25,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-05T13:59:25Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    59,
                    25,
                    4,
                    248,
                    0
                ],
                "title": "HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of\n  Manufactured Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of\n  Manufactured Solutions"
                },
                "summary": "We present HyPINO, a multi-physics neural operator designed for zero-shot\ngeneralization across a broad class of parametric PDEs without requiring\ntask-specific fine-tuning. Our approach combines a Swin Transformer-based\nhypernetwork with mixed supervision: (i) labeled data from analytical solutions\ngenerated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled\nsamples optimized using physics-informed objectives. The model maps PDE\nparametrizations to target Physics-Informed Neural Networks (PINNs) and can\nhandle linear elliptic, hyperbolic, and parabolic equations in two dimensions\nwith varying source terms, geometries, and mixed Dirichlet/Neumann boundary\nconditions, including interior boundaries. HyPINO achieves strong zero-shot\naccuracy on seven benchmark problems from PINN literature, outperforming\nU-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we\nintroduce an iterative refinement procedure that compares the physics of the\ngenerated PINN to the requested PDE and uses the discrepancy to generate a\n\"delta\" PINN. Summing their contributions and repeating this process forms an\nensemble whose combined solution progressively reduces the error on six\nbenchmarks and achieves over 100x gain in average $L_2$ loss in the best case,\nwhile retaining forward-only inference. Additionally, we evaluate the\nfine-tuning behavior of PINNs initialized by HyPINO and show that they converge\nfaster and to lower final error than both randomly initialized and\nReptile-meta-learned PINNs on five benchmarks, performing on par on the\nremaining two. Our results highlight the potential of this scalable approach as\na foundation for extending neural operators toward solving increasingly\ncomplex, nonlinear, and high-dimensional PDE problems. The code and model\nweights are publicly available at https://github.com/rbischof/hypino.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present HyPINO, a multi-physics neural operator designed for zero-shot\ngeneralization across a broad class of parametric PDEs without requiring\ntask-specific fine-tuning. Our approach combines a Swin Transformer-based\nhypernetwork with mixed supervision: (i) labeled data from analytical solutions\ngenerated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled\nsamples optimized using physics-informed objectives. The model maps PDE\nparametrizations to target Physics-Informed Neural Networks (PINNs) and can\nhandle linear elliptic, hyperbolic, and parabolic equations in two dimensions\nwith varying source terms, geometries, and mixed Dirichlet/Neumann boundary\nconditions, including interior boundaries. HyPINO achieves strong zero-shot\naccuracy on seven benchmark problems from PINN literature, outperforming\nU-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we\nintroduce an iterative refinement procedure that compares the physics of the\ngenerated PINN to the requested PDE and uses the discrepancy to generate a\n\"delta\" PINN. Summing their contributions and repeating this process forms an\nensemble whose combined solution progressively reduces the error on six\nbenchmarks and achieves over 100x gain in average $L_2$ loss in the best case,\nwhile retaining forward-only inference. Additionally, we evaluate the\nfine-tuning behavior of PINNs initialized by HyPINO and show that they converge\nfaster and to lower final error than both randomly initialized and\nReptile-meta-learned PINNs on five benchmarks, performing on par on the\nremaining two. Our results highlight the potential of this scalable approach as\na foundation for extending neural operators toward solving increasingly\ncomplex, nonlinear, and high-dimensional PDE problems. The code and model\nweights are publicly available at https://github.com/rbischof/hypino."
                },
                "authors": [
                    {
                        "name": "Rafael Bischof"
                    },
                    {
                        "name": "Michal Piovarči"
                    },
                    {
                        "name": "Michael A. Kraus"
                    },
                    {
                        "name": "Siddhartha Mishra"
                    },
                    {
                        "name": "Bernd Bickel"
                    }
                ],
                "author_detail": {
                    "name": "Bernd Bickel"
                },
                "author": "Bernd Bickel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02300v2",
                "updated": "2025-10-09T15:25:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    25,
                    23,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-02T17:59:06Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    59,
                    6,
                    3,
                    275,
                    0
                ],
                "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models"
                },
                "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference."
                },
                "authors": [
                    {
                        "name": "Runqian Wang"
                    },
                    {
                        "name": "Yilun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Du"
                },
                "author": "Yilun Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08338v1",
                "updated": "2025-10-09T15:24:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    24,
                    48,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:24:48Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    24,
                    48,
                    3,
                    282,
                    0
                ],
                "title": "LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation\n  of Likert Ratings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation\n  of Likert Ratings"
                },
                "summary": "Consumer research costs companies billions annually yet suffers from panel\nbiases and limited scale. Large language models (LLMs) offer an alternative by\nsimulating synthetic consumers, but produce unrealistic response distributions\nwhen asked directly for numerical ratings. We present semantic similarity\nrating (SSR), a method that elicits textual responses from LLMs and maps these\nto Likert distributions using embedding similarity to reference statements.\nTesting on an extensive dataset comprising 57 personal care product surveys\nconducted by a leading corporation in that market (9,300 human responses), SSR\nachieves 90% of human test-retest reliability while maintaining realistic\nresponse distributions (KS similarity > 0.85). Additionally, these synthetic\nrespondents provide rich qualitative feedback explaining their ratings. This\nframework enables scalable consumer research simulations while preserving\ntraditional survey metrics and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consumer research costs companies billions annually yet suffers from panel\nbiases and limited scale. Large language models (LLMs) offer an alternative by\nsimulating synthetic consumers, but produce unrealistic response distributions\nwhen asked directly for numerical ratings. We present semantic similarity\nrating (SSR), a method that elicits textual responses from LLMs and maps these\nto Likert distributions using embedding similarity to reference statements.\nTesting on an extensive dataset comprising 57 personal care product surveys\nconducted by a leading corporation in that market (9,300 human responses), SSR\nachieves 90% of human test-retest reliability while maintaining realistic\nresponse distributions (KS similarity > 0.85). Additionally, these synthetic\nrespondents provide rich qualitative feedback explaining their ratings. This\nframework enables scalable consumer research simulations while preserving\ntraditional survey metrics and interpretability."
                },
                "authors": [
                    {
                        "name": "Benjamin F. Maier"
                    },
                    {
                        "name": "Ulf Aslak"
                    },
                    {
                        "name": "Luca Fiaschi"
                    },
                    {
                        "name": "Nina Rismal"
                    },
                    {
                        "name": "Kemble Fletcher"
                    },
                    {
                        "name": "Christian C. Luhmann"
                    },
                    {
                        "name": "Robbie Dow"
                    },
                    {
                        "name": "Kli Pappas"
                    },
                    {
                        "name": "Thomas V. Wiecki"
                    }
                ],
                "author_detail": {
                    "name": "Thomas V. Wiecki"
                },
                "author": "Thomas V. Wiecki",
                "arxiv_comment": "28 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00566v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00566v4",
                "updated": "2025-10-09T15:23:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    23,
                    15,
                    3,
                    282,
                    0
                ],
                "published": "2025-03-01T17:29:26Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    17,
                    29,
                    26,
                    5,
                    60,
                    0
                ],
                "title": "Instructor-Worker Large Language Model System for Policy Recommendation:\n  a Case Study on Air Quality Analysis of the January 2025 Los Angeles\n  Wildfires",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instructor-Worker Large Language Model System for Policy Recommendation:\n  a Case Study on Air Quality Analysis of the January 2025 Los Angeles\n  Wildfires"
                },
                "summary": "The Los Angeles wildfires of January 2025 caused more than 250 billion\ndollars in damage and lasted for nearly an entire month before containment.\nFollowing our previous work, the Digital Twin Building, we modify and leverage\nthe multi-agent large language model framework as well as the cloud-mapping\nintegration to study the air quality during the Los Angeles wildfires. Recent\nadvances in large language models have allowed for out-of-the-box automated\nlarge-scale data analysis. We use a multi-agent large language system comprised\nof an Instructor agent and Worker agents. Upon receiving the users'\ninstructions, the Instructor agent retrieves the data from the cloud platform\nand produces instruction prompts to the Worker agents. The Worker agents then\nanalyze the data and provide summaries. The summaries are finally input back\ninto the Instructor agent, which then provides the final data analysis. We test\nthis system's capability for data-based policy recommendation by assessing our\nInstructor-Worker LLM system's health recommendations based on air quality\nduring the Los Angeles wildfires.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Los Angeles wildfires of January 2025 caused more than 250 billion\ndollars in damage and lasted for nearly an entire month before containment.\nFollowing our previous work, the Digital Twin Building, we modify and leverage\nthe multi-agent large language model framework as well as the cloud-mapping\nintegration to study the air quality during the Los Angeles wildfires. Recent\nadvances in large language models have allowed for out-of-the-box automated\nlarge-scale data analysis. We use a multi-agent large language system comprised\nof an Instructor agent and Worker agents. Upon receiving the users'\ninstructions, the Instructor agent retrieves the data from the cloud platform\nand produces instruction prompts to the Worker agents. The Worker agents then\nanalyze the data and provide summaries. The summaries are finally input back\ninto the Instructor agent, which then provides the final data analysis. We test\nthis system's capability for data-based policy recommendation by assessing our\nInstructor-Worker LLM system's health recommendations based on air quality\nduring the Los Angeles wildfires."
                },
                "authors": [
                    {
                        "name": "Kyle Gao"
                    },
                    {
                        "name": "Dening Lu"
                    },
                    {
                        "name": "Liangzhi Li"
                    },
                    {
                        "name": "Nan Chen"
                    },
                    {
                        "name": "Hongjie He"
                    },
                    {
                        "name": "Linlin Xu"
                    },
                    {
                        "name": "Jonathan Li"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Li"
                },
                "author": "Jonathan Li",
                "arxiv_doi": "10.1016/j.jag.2025.104774",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jag.2025.104774",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.00566v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00566v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08333v1",
                "updated": "2025-10-09T15:22:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    22,
                    20,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:22:20Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    22,
                    20,
                    3,
                    282,
                    0
                ],
                "title": "New Machine Learning Approaches for Intrusion Detection in ADS-B",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New Machine Learning Approaches for Intrusion Detection in ADS-B"
                },
                "summary": "With the growing reliance on the vulnerable Automatic Dependent\nSurveillance-Broadcast (ADS-B) protocol in air traffic management (ATM),\nensuring security is critical. This study investigates emerging machine\nlearning models and training strategies to improve AI-based intrusion detection\nsystems (IDS) for ADS-B. Focusing on ground-based ATM systems, we evaluate two\ndeep learning IDS implementations: one using a transformer encoder and the\nother an extended Long Short-Term Memory (xLSTM) network, marking the first\nxLSTM-based IDS for ADS-B. A transfer learning strategy was employed, involving\npre-training on benign ADS-B messages and fine-tuning with labeled data\ncontaining instances of tampered messages. Results show this approach\noutperforms existing methods, particularly in identifying subtle attacks that\nprogressively undermine situational awareness. The xLSTM-based IDS achieves an\nF1-score of 98.9%, surpassing the transformer-based model at 94.3%. Tests on\nunseen attacks validated the generalization ability of the xLSTM model.\nInference latency analysis shows that the 7.26-second delay introduced by the\nxLSTM-based IDS fits within the Secondary Surveillance Radar (SSR) refresh\ninterval (5-12 s), although it may be restrictive for time-critical operations.\nWhile the transformer-based IDS achieves a 2.1-second latency, it does so at\nthe cost of lower detection performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing reliance on the vulnerable Automatic Dependent\nSurveillance-Broadcast (ADS-B) protocol in air traffic management (ATM),\nensuring security is critical. This study investigates emerging machine\nlearning models and training strategies to improve AI-based intrusion detection\nsystems (IDS) for ADS-B. Focusing on ground-based ATM systems, we evaluate two\ndeep learning IDS implementations: one using a transformer encoder and the\nother an extended Long Short-Term Memory (xLSTM) network, marking the first\nxLSTM-based IDS for ADS-B. A transfer learning strategy was employed, involving\npre-training on benign ADS-B messages and fine-tuning with labeled data\ncontaining instances of tampered messages. Results show this approach\noutperforms existing methods, particularly in identifying subtle attacks that\nprogressively undermine situational awareness. The xLSTM-based IDS achieves an\nF1-score of 98.9%, surpassing the transformer-based model at 94.3%. Tests on\nunseen attacks validated the generalization ability of the xLSTM model.\nInference latency analysis shows that the 7.26-second delay introduced by the\nxLSTM-based IDS fits within the Secondary Surveillance Radar (SSR) refresh\ninterval (5-12 s), although it may be restrictive for time-critical operations.\nWhile the transformer-based IDS achieves a 2.1-second latency, it does so at\nthe cost of lower detection performance."
                },
                "authors": [
                    {
                        "name": "Mikaëla Ngamboé"
                    },
                    {
                        "name": "Jean-Simon Marrocco"
                    },
                    {
                        "name": "Jean-Yves Ouattara"
                    },
                    {
                        "name": "José M. Fernandez"
                    },
                    {
                        "name": "Gabriela Nicolescu"
                    }
                ],
                "author_detail": {
                    "name": "Gabriela Nicolescu"
                },
                "author": "Gabriela Nicolescu",
                "arxiv_comment": "This is the author's version of the work accepted for publication\n  Digital Avionics Systems Conference (DASC) 2025. The final version will be\n  available via IEEE Xplore",
                "arxiv_journal_ref": "44th Digital Avionics Systems Conference (DASC), Sep 2025,\n  Montreal, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08329v1",
                "updated": "2025-10-09T15:17:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    17,
                    28,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:17:28Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    17,
                    28,
                    3,
                    282,
                    0
                ],
                "title": "AutoRed: A Free-form Adversarial Prompt Generation Framework for\n  Automated Red Teaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRed: A Free-form Adversarial Prompt Generation Framework for\n  Automated Red Teaming"
                },
                "summary": "The safety of Large Language Models (LLMs) is crucial for the development of\ntrustworthy AI applications. Existing red teaming methods often rely on seed\ninstructions, which limits the semantic diversity of the synthesized\nadversarial prompts. We propose AutoRed, a free-form adversarial prompt\ngeneration framework that removes the need for seed instructions. AutoRed\noperates in two stages: (1) persona-guided adversarial instruction generation,\nand (2) a reflection loop to iteratively refine low-quality prompts. To improve\nefficiency, we introduce a verifier to assess prompt harmfulness without\nquerying the target models. Using AutoRed, we build two red teaming datasets --\nAutoRed-Medium and AutoRed-Hard -- and evaluate eight state-of-the-art LLMs.\nAutoRed achieves higher attack success rates and better generalization than\nexisting baselines. Our results highlight the limitations of seed-based\napproaches and demonstrate the potential of free-form red teaming for LLM\nsafety evaluation. We will open source our datasets in the near future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safety of Large Language Models (LLMs) is crucial for the development of\ntrustworthy AI applications. Existing red teaming methods often rely on seed\ninstructions, which limits the semantic diversity of the synthesized\nadversarial prompts. We propose AutoRed, a free-form adversarial prompt\ngeneration framework that removes the need for seed instructions. AutoRed\noperates in two stages: (1) persona-guided adversarial instruction generation,\nand (2) a reflection loop to iteratively refine low-quality prompts. To improve\nefficiency, we introduce a verifier to assess prompt harmfulness without\nquerying the target models. Using AutoRed, we build two red teaming datasets --\nAutoRed-Medium and AutoRed-Hard -- and evaluate eight state-of-the-art LLMs.\nAutoRed achieves higher attack success rates and better generalization than\nexisting baselines. Our results highlight the limitations of seed-based\napproaches and demonstrate the potential of free-form red teaming for LLM\nsafety evaluation. We will open source our datasets in the near future."
                },
                "authors": [
                    {
                        "name": "Muxi Diao"
                    },
                    {
                        "name": "Yutao Mou"
                    },
                    {
                        "name": "Keqing He"
                    },
                    {
                        "name": "Hanbo Song"
                    },
                    {
                        "name": "Lulu Zhao"
                    },
                    {
                        "name": "Shikun Zhang"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Kongming Liang"
                    },
                    {
                        "name": "Zhanyu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhanyu Ma"
                },
                "author": "Zhanyu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07626v2",
                "updated": "2025-10-09T15:04:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    4,
                    26,
                    3,
                    282,
                    0
                ],
                "published": "2025-07-10T10:50:51Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    10,
                    50,
                    51,
                    3,
                    191,
                    0
                ],
                "title": "Upper Expected Meeting Times for Interdependent Stochastic Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upper Expected Meeting Times for Interdependent Stochastic Agents"
                },
                "summary": "We analyse the problem of meeting times for interdependent stochastic agents:\nrandom walkers whose behaviour is stochastic but controlled by their selections\nfrom some set of allowed actions, and the inference problem of when these\nagents will be in the same state for the first time. We consider the case where\nwe are epistemically uncertain about the selected actions of these agents, and\nshow how their behaviour can be modelled using imprecise Markov chains. This\nallows us to use results and algorithms from the literature, to exactly compute\nbounds on their meeting time, which are tight with respect to our epistemic\nuncertainty models. We focus on the two-agent case, but discuss how it can be\nnaturally extended to an arbitrary number of agents, and how the corresponding\ncombinatorial explosion can be partly mitigated by exploiting symmetries\ninherent in the problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyse the problem of meeting times for interdependent stochastic agents:\nrandom walkers whose behaviour is stochastic but controlled by their selections\nfrom some set of allowed actions, and the inference problem of when these\nagents will be in the same state for the first time. We consider the case where\nwe are epistemically uncertain about the selected actions of these agents, and\nshow how their behaviour can be modelled using imprecise Markov chains. This\nallows us to use results and algorithms from the literature, to exactly compute\nbounds on their meeting time, which are tight with respect to our epistemic\nuncertainty models. We focus on the two-agent case, but discuss how it can be\nnaturally extended to an arbitrary number of agents, and how the corresponding\ncombinatorial explosion can be partly mitigated by exploiting symmetries\ninherent in the problem."
                },
                "authors": [
                    {
                        "name": "Marco Sangalli"
                    },
                    {
                        "name": "Erik Quaeghebeur"
                    },
                    {
                        "name": "Thomas Krak"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Krak"
                },
                "author": "Thomas Krak",
                "arxiv_doi": "10.1007/978-3-032-05134-9_17",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-05134-9_17",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.07626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Symbolic and Quantitative Approaches to Reasoning with\n  Uncertainty, Vol. 16099, Lecture Notes in Artificial Intelligence (LNAI)\n  Springer (2025) 238-252",
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08317v1",
                "updated": "2025-10-09T15:02:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    2,
                    56,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:02:56Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    2,
                    56,
                    3,
                    282,
                    0
                ],
                "title": "Iterated Agent for Symbolic Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterated Agent for Symbolic Regression"
                },
                "summary": "Symbolic regression (SR), the automated discovery of mathematical expressions\nfrom data, is a cornerstone of scientific inquiry. However, it is often\nhindered by the combinatorial explosion of the search space and a tendency to\noverfit. Popular methods, rooted in genetic programming, explore this space\nsyntactically, often yielding overly complex, uninterpretable models. This\npaper introduces IdeaSearchFitter, a framework that employs Large Language\nModels (LLMs) as semantic operators within an evolutionary search. By\ngenerating candidate expressions guided by natural-language rationales, our\nmethod biases discovery towards models that are not only accurate but also\nconceptually coherent and interpretable. We demonstrate IdeaSearchFitter's\nefficacy across diverse challenges: it achieves competitive, noise-robust\nperformance on the Feynman Symbolic Regression Database (FSReD), outperforming\nseveral strong baselines; discovers mechanistically aligned models with good\naccuracy-complexity trade-offs on real-world data; and derives compact,\nphysically-motivated parametrizations for Parton Distribution Functions in a\nfrontier high-energy physics application. IdeaSearchFitter is a specialized\nmodule within our broader iterated agent framework, IdeaSearch, which is\npublicly available at https://www.ideasearch.cn/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic regression (SR), the automated discovery of mathematical expressions\nfrom data, is a cornerstone of scientific inquiry. However, it is often\nhindered by the combinatorial explosion of the search space and a tendency to\noverfit. Popular methods, rooted in genetic programming, explore this space\nsyntactically, often yielding overly complex, uninterpretable models. This\npaper introduces IdeaSearchFitter, a framework that employs Large Language\nModels (LLMs) as semantic operators within an evolutionary search. By\ngenerating candidate expressions guided by natural-language rationales, our\nmethod biases discovery towards models that are not only accurate but also\nconceptually coherent and interpretable. We demonstrate IdeaSearchFitter's\nefficacy across diverse challenges: it achieves competitive, noise-robust\nperformance on the Feynman Symbolic Regression Database (FSReD), outperforming\nseveral strong baselines; discovers mechanistically aligned models with good\naccuracy-complexity trade-offs on real-world data; and derives compact,\nphysically-motivated parametrizations for Parton Distribution Functions in a\nfrontier high-energy physics application. IdeaSearchFitter is a specialized\nmodule within our broader iterated agent framework, IdeaSearch, which is\npublicly available at https://www.ideasearch.cn/."
                },
                "authors": [
                    {
                        "name": "Zhuo-Yang Song"
                    },
                    {
                        "name": "Zeyu Cai"
                    },
                    {
                        "name": "Shutao Zhang"
                    },
                    {
                        "name": "Jiashen Wei"
                    },
                    {
                        "name": "Jichen Pan"
                    },
                    {
                        "name": "Shi Qiu"
                    },
                    {
                        "name": "Qing-Hong Cao"
                    },
                    {
                        "name": "Tie-Jiun Hou"
                    },
                    {
                        "name": "Xiaohui Liu"
                    },
                    {
                        "name": "Ming-xing Luo"
                    },
                    {
                        "name": "Hua Xing Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xing Zhu"
                },
                "author": "Hua Xing Zhu",
                "arxiv_comment": "45 pages, 22 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20749v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20749v2",
                "updated": "2025-10-09T15:01:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    1,
                    48,
                    3,
                    282,
                    0
                ],
                "published": "2024-10-28T05:28:51Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    28,
                    51,
                    0,
                    302,
                    0
                ],
                "title": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs"
                },
                "summary": "Despite the impressive generative abilities of black-box large language\nmodels (LLMs), their inherent opacity hinders further advancements in\ncapabilities such as reasoning, planning, and personalization. Existing works\naim to enhance LLM capabilities via domain-specific adaptation, which require\nadditional training on accessible model parameters, an infeasible option for\nblack-box LLMs. To address this challenge, we introduce Matryoshka Pilot\n(M-Pilot), a lightweight white-box LLM controller that guides a large-scale\nblack-box LLM generator by decomposing complex tasks into a series of\nintermediate outputs. Specifically, we consider the black-box LLM as an\nenvironment, with M-Pilot serving as a policy to provide intermediate guidance\nthrough prompts for driving the black-box LLM. M-Pilot is trained to pivot the\noutputs of the black-box LLM aligning with preferences during iterative\ninteraction, which enables controllable multi-turn generation and\nself-improvement in optimizing intermediate guidance. Empirical evaluations on\ndiverse tasks demonstrate that our method effectively enhances the capabilities\nof black-box LLMs in complex, long-horizon tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive generative abilities of black-box large language\nmodels (LLMs), their inherent opacity hinders further advancements in\ncapabilities such as reasoning, planning, and personalization. Existing works\naim to enhance LLM capabilities via domain-specific adaptation, which require\nadditional training on accessible model parameters, an infeasible option for\nblack-box LLMs. To address this challenge, we introduce Matryoshka Pilot\n(M-Pilot), a lightweight white-box LLM controller that guides a large-scale\nblack-box LLM generator by decomposing complex tasks into a series of\nintermediate outputs. Specifically, we consider the black-box LLM as an\nenvironment, with M-Pilot serving as a policy to provide intermediate guidance\nthrough prompts for driving the black-box LLM. M-Pilot is trained to pivot the\noutputs of the black-box LLM aligning with preferences during iterative\ninteraction, which enables controllable multi-turn generation and\nself-improvement in optimizing intermediate guidance. Empirical evaluations on\ndiverse tasks demonstrate that our method effectively enhances the capabilities\nof black-box LLMs in complex, long-horizon tasks."
                },
                "authors": [
                    {
                        "name": "Changhao Li"
                    },
                    {
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "name": "Rushi Qiang"
                    },
                    {
                        "name": "Haotian Sun"
                    },
                    {
                        "name": "Hanjun Dai"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Bo Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dai"
                },
                "author": "Bo Dai",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20749v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13220v2",
                "updated": "2025-10-09T14:57:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    57,
                    42,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-17T11:49:16Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    11,
                    49,
                    16,
                    6,
                    229,
                    0
                ],
                "title": "MCPSecBench: A Systematic Security Benchmark and Playground for Testing\n  Model Context Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCPSecBench: A Systematic Security Benchmark and Playground for Testing\n  Model Context Protocols"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications via the Model Context Protocol (MCP), a universal, open standard\nfor connecting AI agents with data sources and external tools. While MCP\nenhances the capabilities of LLM-based agents, it also introduces new security\nrisks and expands their attack surfaces. In this paper, we present the first\nsystematic taxonomy of MCP security, identifying 17 attack types across 4\nprimary attack surfaces. We introduce MCPSecBench, a comprehensive security\nbenchmark and playground that integrates prompt datasets, MCP servers, MCP\nclients, attack scripts, and protection mechanisms to evaluate these attacks\nacross three major MCP providers. Our benchmark is modular and extensible,\nallowing researchers to incorporate custom implementations of clients, servers,\nand transport protocols for systematic security assessment. Experimental\nresults show that over 85% of the identified attacks successfully compromise at\nleast one platform, with core vulnerabilities universally affecting Claude,\nOpenAI, and Cursor, while prompt-based and tool-centric attacks exhibit\nconsiderable variability across different hosts and models. In addition,\ncurrent protection mechanisms have little effect against these attacks.\nOverall, MCPSecBench standardizes the evaluation of MCP security and enables\nrigorous testing across all MCP layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications via the Model Context Protocol (MCP), a universal, open standard\nfor connecting AI agents with data sources and external tools. While MCP\nenhances the capabilities of LLM-based agents, it also introduces new security\nrisks and expands their attack surfaces. In this paper, we present the first\nsystematic taxonomy of MCP security, identifying 17 attack types across 4\nprimary attack surfaces. We introduce MCPSecBench, a comprehensive security\nbenchmark and playground that integrates prompt datasets, MCP servers, MCP\nclients, attack scripts, and protection mechanisms to evaluate these attacks\nacross three major MCP providers. Our benchmark is modular and extensible,\nallowing researchers to incorporate custom implementations of clients, servers,\nand transport protocols for systematic security assessment. Experimental\nresults show that over 85% of the identified attacks successfully compromise at\nleast one platform, with core vulnerabilities universally affecting Claude,\nOpenAI, and Cursor, while prompt-based and tool-centric attacks exhibit\nconsiderable variability across different hosts and models. In addition,\ncurrent protection mechanisms have little effect against these attacks.\nOverall, MCPSecBench standardizes the evaluation of MCP security and enables\nrigorous testing across all MCP layers."
                },
                "authors": [
                    {
                        "name": "Yixuan Yang"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Yufan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yufan Chen"
                },
                "author": "Yufan Chen",
                "arxiv_comment": "This is a technical report from Lingnan University, Hong Kong. Code\n  is available at https://github.com/AIS2Lab/MCPSecBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08309v1",
                "updated": "2025-10-09T14:57:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    57,
                    12,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:57:12Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    57,
                    12,
                    3,
                    282,
                    0
                ],
                "title": "Two-Stage Trigonometric Regression for Modeling Circadian Rhythms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Stage Trigonometric Regression for Modeling Circadian Rhythms"
                },
                "summary": "Gene expression levels, hormone secretion, and internal body temperature each\noscillate over an approximately 24-hour cycle, or display circadian rhythms.\nMany circadian biology studies have investigated how these rhythms vary across\ncohorts, uncovering associations between atypical rhythms and diseases such as\ncancer, metabolic syndrome, and sleep disorders. A challenge in analyzing\ncircadian biology data is that the oscillation peak and trough times for a\nphenomenon differ across individuals. If these individual-level differences are\nnot accounted for in trigonometric regression, which is prevalent in circadian\nbiology studies, then estimates of the population-level amplitude parameters\ncan suffer from attenuation bias. This attenuation bias could lead to\ninaccurate study conclusions. To address attenuation bias, we propose a refined\ntwo-stage (RTS) method for trigonometric regression given longitudinal data\nobtained from each individual participating in a study. In the first stage, the\nparameters of individual-level models are estimated. In the second stage,\ntransformations of these individual-level estimates are aggregated to produce\npopulation-level parameter estimates for inference. Simulation studies show\nthat our RTS method mitigates bias in parameter estimation, obtains greater\nstatistical power, and maintains appropriate type I error control when compared\nto the standard two-stage (STS) method, which ignores individual-level\ndifferences in peak and trough times. The only exception for parameter\nestimation and statistical power occurs when the oscillation amplitudes are\nweak relative to random variability in the data and the sample size is small.\nIllustrations with cortisol level data and heart rate data show that our RTS\nmethod obtains larger population-level amplitude parameter estimates and\nsmaller $p$-values for multiple hypothesis tests when compared to the STS\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gene expression levels, hormone secretion, and internal body temperature each\noscillate over an approximately 24-hour cycle, or display circadian rhythms.\nMany circadian biology studies have investigated how these rhythms vary across\ncohorts, uncovering associations between atypical rhythms and diseases such as\ncancer, metabolic syndrome, and sleep disorders. A challenge in analyzing\ncircadian biology data is that the oscillation peak and trough times for a\nphenomenon differ across individuals. If these individual-level differences are\nnot accounted for in trigonometric regression, which is prevalent in circadian\nbiology studies, then estimates of the population-level amplitude parameters\ncan suffer from attenuation bias. This attenuation bias could lead to\ninaccurate study conclusions. To address attenuation bias, we propose a refined\ntwo-stage (RTS) method for trigonometric regression given longitudinal data\nobtained from each individual participating in a study. In the first stage, the\nparameters of individual-level models are estimated. In the second stage,\ntransformations of these individual-level estimates are aggregated to produce\npopulation-level parameter estimates for inference. Simulation studies show\nthat our RTS method mitigates bias in parameter estimation, obtains greater\nstatistical power, and maintains appropriate type I error control when compared\nto the standard two-stage (STS) method, which ignores individual-level\ndifferences in peak and trough times. The only exception for parameter\nestimation and statistical power occurs when the oscillation amplitudes are\nweak relative to random variability in the data and the sample size is small.\nIllustrations with cortisol level data and heart rate data show that our RTS\nmethod obtains larger population-level amplitude parameter estimates and\nsmaller $p$-values for multiple hypothesis tests when compared to the STS\nmethod."
                },
                "authors": [
                    {
                        "name": "Michael T. Gorczyca"
                    },
                    {
                        "name": "Jenna D. Li"
                    },
                    {
                        "name": "Charissa M. Newkirk"
                    },
                    {
                        "name": "Arjun S. Srivatsa"
                    },
                    {
                        "name": "Hugo F. M. Milan"
                    }
                ],
                "author_detail": {
                    "name": "Hugo F. M. Milan"
                },
                "author": "Hugo F. M. Milan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08308v1",
                "updated": "2025-10-09T14:57:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    57,
                    10,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:57:10Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    57,
                    10,
                    3,
                    282,
                    0
                ],
                "title": "First Try Matters: Revisiting the Role of Reflection in Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Try Matters: Revisiting the Role of Reflection in Reasoning Models"
                },
                "summary": "Large language models have recently demonstrated significant gains in\nreasoning ability, often attributed to their capacity to generate longer chains\nof thought and engage in reflective reasoning. However, the contribution of\nreflections to performance improvement remains unclear. In this paper, we\nsystematically analyze the rollouts of eight reasoning models on five\nmathematical datasets. We focus on reflective behaviours where the model has\nalready produced an answer but continues reflecting before finalizing its\noutput. Our analysis reveals that reflections are predominantly confirmatory\nand rarely alter the model's initial answer, a pattern consistent across models\nand datasets. To understand the role of reflections in training, we construct\nsupervised fine-tuning (SFT) datasets with varying amounts of reflection steps.\nWe observe that training models on rollouts with more reflection steps\nprimarily enhances first-answer correctness rather than the ability to correct\ninitially wrong answers through reflections. This motivates us to propose a\nquestion-aware early-stopping method that enhances inference-time token\nefficiency by stopping the reasoning process once a few plausible candidate\nanswers are generated, thereby reducing unnecessary reflection steps. Motivated\nby this, we further propose to dynamically truncate the reflections after a\ncandidate answer has appeared during generation, which reduces reasoning tokens\nby 24.5% across five mathematical datasets, within a 2.9% drop in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have recently demonstrated significant gains in\nreasoning ability, often attributed to their capacity to generate longer chains\nof thought and engage in reflective reasoning. However, the contribution of\nreflections to performance improvement remains unclear. In this paper, we\nsystematically analyze the rollouts of eight reasoning models on five\nmathematical datasets. We focus on reflective behaviours where the model has\nalready produced an answer but continues reflecting before finalizing its\noutput. Our analysis reveals that reflections are predominantly confirmatory\nand rarely alter the model's initial answer, a pattern consistent across models\nand datasets. To understand the role of reflections in training, we construct\nsupervised fine-tuning (SFT) datasets with varying amounts of reflection steps.\nWe observe that training models on rollouts with more reflection steps\nprimarily enhances first-answer correctness rather than the ability to correct\ninitially wrong answers through reflections. This motivates us to propose a\nquestion-aware early-stopping method that enhances inference-time token\nefficiency by stopping the reasoning process once a few plausible candidate\nanswers are generated, thereby reducing unnecessary reflection steps. Motivated\nby this, we further propose to dynamically truncate the reflections after a\ncandidate answer has appeared during generation, which reduces reasoning tokens\nby 24.5% across five mathematical datasets, within a 2.9% drop in accuracy."
                },
                "authors": [
                    {
                        "name": "Liwei Kang"
                    },
                    {
                        "name": "Yue Deng"
                    },
                    {
                        "name": "Yao Xiao"
                    },
                    {
                        "name": "Zhanfeng Mo"
                    },
                    {
                        "name": "Wee Sun Lee"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08303v1",
                "updated": "2025-10-09T14:55:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    55,
                    4,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:55:04Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    55,
                    4,
                    3,
                    282,
                    0
                ],
                "title": "Dynamic Features Adaptation in Networking: Toward Flexible training and\n  Explainable inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Features Adaptation in Networking: Toward Flexible training and\n  Explainable inference"
                },
                "summary": "As AI becomes a native component of 6G network control, AI models must adapt\nto continuously changing conditions, including the introduction of new features\nand measurements driven by multi-vendor deployments, hardware upgrades, and\nevolving service requirements. To address this growing need for flexible\nlearning in non-stationary environments, this vision paper highlights Adaptive\nRandom Forests (ARFs) as a reliable solution for dynamic feature adaptation in\ncommunication network scenarios. We show that iterative training of ARFs can\neffectively lead to stable predictions, with accuracy improving over time as\nmore features are added. In addition, we highlight the importance of\nexplainability in AI-driven networks, proposing Drift-Aware Feature Importance\n(DAFI) as an efficient XAI feature importance (FI) method. DAFI uses a\ndistributional drift detector to signal when to apply computationally intensive\nFI methods instead of lighter alternatives. Our tests on 3 different datasets\nindicate that our approach reduces runtime by up to 2 times, while producing\nmore consistent feature importance values. Together, ARFs and DAFI provide a\npromising framework to build flexible AI methods adapted to 6G network\nuse-cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI becomes a native component of 6G network control, AI models must adapt\nto continuously changing conditions, including the introduction of new features\nand measurements driven by multi-vendor deployments, hardware upgrades, and\nevolving service requirements. To address this growing need for flexible\nlearning in non-stationary environments, this vision paper highlights Adaptive\nRandom Forests (ARFs) as a reliable solution for dynamic feature adaptation in\ncommunication network scenarios. We show that iterative training of ARFs can\neffectively lead to stable predictions, with accuracy improving over time as\nmore features are added. In addition, we highlight the importance of\nexplainability in AI-driven networks, proposing Drift-Aware Feature Importance\n(DAFI) as an efficient XAI feature importance (FI) method. DAFI uses a\ndistributional drift detector to signal when to apply computationally intensive\nFI methods instead of lighter alternatives. Our tests on 3 different datasets\nindicate that our approach reduces runtime by up to 2 times, while producing\nmore consistent feature importance values. Together, ARFs and DAFI provide a\npromising framework to build flexible AI methods adapted to 6G network\nuse-cases."
                },
                "authors": [
                    {
                        "name": "Yannis Belkhiter"
                    },
                    {
                        "name": "Seshu Tirupathi"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Merim Dzaferagic"
                    },
                    {
                        "name": "John D. Kelleher"
                    }
                ],
                "author_detail": {
                    "name": "John D. Kelleher"
                },
                "author": "John D. Kelleher",
                "arxiv_comment": "Accepted at AI4NextG Workshop, NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13925v2",
                "updated": "2025-10-09T14:52:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    52,
                    21,
                    3,
                    282,
                    0
                ],
                "published": "2025-02-19T18:04:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    4,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual\n  Narratives in Image Sequences?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual\n  Narratives in Image Sequences?"
                },
                "summary": "Large Multimodal Models (LMMs) have achieved remarkable success across\nvarious visual-language tasks. However, existing benchmarks predominantly focus\non single-image understanding, leaving the analysis of image sequences largely\nunexplored. To address this limitation, we introduce StripCipher, a\ncomprehensive benchmark designed to evaluate capabilities of LMMs to comprehend\nand reason over sequential images. StripCipher comprises a human-annotated\ndataset and three challenging subtasks: visual narrative comprehension,\ncontextual frame prediction, and temporal narrative reordering. Our evaluation\nof 16 state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a\nsignificant performance gap compared to human capabilities, particularly in\ntasks that require reordering shuffled sequential images. For instance, GPT-4o\nachieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower\nthan human performance. Further quantitative analysis discuss several factors,\nsuch as input format of images, affecting the performance of LLMs in sequential\nunderstanding, underscoring the fundamental challenges that remain in the\ndevelopment of LMMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) have achieved remarkable success across\nvarious visual-language tasks. However, existing benchmarks predominantly focus\non single-image understanding, leaving the analysis of image sequences largely\nunexplored. To address this limitation, we introduce StripCipher, a\ncomprehensive benchmark designed to evaluate capabilities of LMMs to comprehend\nand reason over sequential images. StripCipher comprises a human-annotated\ndataset and three challenging subtasks: visual narrative comprehension,\ncontextual frame prediction, and temporal narrative reordering. Our evaluation\nof 16 state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a\nsignificant performance gap compared to human capabilities, particularly in\ntasks that require reordering shuffled sequential images. For instance, GPT-4o\nachieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower\nthan human performance. Further quantitative analysis discuss several factors,\nsuch as input format of images, affecting the performance of LLMs in sequential\nunderstanding, underscoring the fundamental challenges that remain in the\ndevelopment of LMMs."
                },
                "authors": [
                    {
                        "name": "Xiaochen Wang"
                    },
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Jialin Song"
                    },
                    {
                        "name": "Longyu Guan"
                    },
                    {
                        "name": "Yixin Yang"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Weiyao Luo"
                    },
                    {
                        "name": "Yifan Pu"
                    },
                    {
                        "name": "Yiru Wang"
                    },
                    {
                        "name": "Xiangdi Meng"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12450v2",
                "updated": "2025-10-09T14:51:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    51,
                    59,
                    3,
                    282,
                    0
                ],
                "published": "2025-06-14T11:09:50Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    11,
                    9,
                    50,
                    5,
                    165,
                    0
                ],
                "title": "Language Surgery in Multilingual Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Surgery in Multilingual Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across tasks and languages, revolutionizing natural language\nprocessing. This paper investigates the naturally emerging representation\nalignment in LLMs, particularly in the middle layers, and its implications for\ndisentangling language-specific and language-agnostic information. We\nempirically confirm the existence of this alignment, analyze its behavior in\ncomparison to explicitly designed alignment models, and demonstrate its\npotential for language-specific manipulation without semantic degradation.\nBuilding on these findings, we propose Inference-Time Language Control (ITLC),\na novel method that leverages latent injection to enable precise cross-lingual\nlanguage control and mitigate language confusion in LLMs. Our experiments\nhighlight ITLC's strong cross-lingual control capabilities while preserving\nsemantic integrity in target languages. Furthermore, we demonstrate its\neffectiveness in alleviating the cross-lingual language confusion problem,\nwhich persists even in current large-scale LLMs, leading to inconsistent\nlanguage generation. This work advances our understanding of representation\nalignment in LLMs and introduces a practical solution for enhancing their\nmonolingual and cross-lingual performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across tasks and languages, revolutionizing natural language\nprocessing. This paper investigates the naturally emerging representation\nalignment in LLMs, particularly in the middle layers, and its implications for\ndisentangling language-specific and language-agnostic information. We\nempirically confirm the existence of this alignment, analyze its behavior in\ncomparison to explicitly designed alignment models, and demonstrate its\npotential for language-specific manipulation without semantic degradation.\nBuilding on these findings, we propose Inference-Time Language Control (ITLC),\na novel method that leverages latent injection to enable precise cross-lingual\nlanguage control and mitigate language confusion in LLMs. Our experiments\nhighlight ITLC's strong cross-lingual control capabilities while preserving\nsemantic integrity in target languages. Furthermore, we demonstrate its\neffectiveness in alleviating the cross-lingual language confusion problem,\nwhich persists even in current large-scale LLMs, leading to inconsistent\nlanguage generation. This work advances our understanding of representation\nalignment in LLMs and introduces a practical solution for enhancing their\nmonolingual and cross-lingual performance."
                },
                "authors": [
                    {
                        "name": "Joanito Agili Lopo"
                    },
                    {
                        "name": "Muhammad Ravi Shulthan Habibi"
                    },
                    {
                        "name": "Tack Hwa Wong"
                    },
                    {
                        "name": "Muhammad Ilham Ghozali"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Genta Indra Winata"
                    },
                    {
                        "name": "Peerat Limkonchotiwat"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Samuel Cahyawijaya"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Cahyawijaya"
                },
                "author": "Samuel Cahyawijaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08294v1",
                "updated": "2025-10-09T14:45:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    45,
                    13,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:45:13Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    45,
                    13,
                    3,
                    282,
                    0
                ],
                "title": "Counterfactual Identifiability via Dynamic Optimal Transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Identifiability via Dynamic Optimal Transport"
                },
                "summary": "We address the open question of counterfactual identification for\nhigh-dimensional multivariate outcomes from observational data. Pearl (2000)\nargues that counterfactuals must be identifiable (i.e., recoverable from the\nobserved data distribution) to justify causal claims. A recent line of work on\ncounterfactual inference shows promising results but lacks identification,\nundermining the causal validity of its estimates. To address this, we establish\na foundation for multivariate counterfactual identification using\ncontinuous-time flows, including non-Markovian settings under standard\ncriteria. We characterise the conditions under which flow matching yields a\nunique, monotone and rank-preserving counterfactual transport map with tools\nfrom dynamic optimal transport, ensuring consistent inference. Building on\nthis, we validate the theory in controlled scenarios with counterfactual\nground-truth and demonstrate improvements in axiomatic counterfactual soundness\non real images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the open question of counterfactual identification for\nhigh-dimensional multivariate outcomes from observational data. Pearl (2000)\nargues that counterfactuals must be identifiable (i.e., recoverable from the\nobserved data distribution) to justify causal claims. A recent line of work on\ncounterfactual inference shows promising results but lacks identification,\nundermining the causal validity of its estimates. To address this, we establish\na foundation for multivariate counterfactual identification using\ncontinuous-time flows, including non-Markovian settings under standard\ncriteria. We characterise the conditions under which flow matching yields a\nunique, monotone and rank-preserving counterfactual transport map with tools\nfrom dynamic optimal transport, ensuring consistent inference. Building on\nthis, we validate the theory in controlled scenarios with counterfactual\nground-truth and demonstrate improvements in axiomatic counterfactual soundness\non real images."
                },
                "authors": [
                    {
                        "name": "Fabio De Sousa Ribeiro"
                    },
                    {
                        "name": "Ainkaran Santhirasekaram"
                    },
                    {
                        "name": "Ben Glocker"
                    }
                ],
                "author_detail": {
                    "name": "Ben Glocker"
                },
                "author": "Ben Glocker",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16723v2",
                "updated": "2025-10-09T14:40:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    40,
                    25,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-22T14:32:23Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    32,
                    23,
                    3,
                    142,
                    0
                ],
                "title": "LLM Fingerprinting via Semantically Conditioned Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Fingerprinting via Semantically Conditioned Watermarks"
                },
                "summary": "Most LLM fingerprinting methods teach the model to respond to a few fixed\nqueries with predefined atypical responses (keys). This memorization often does\nnot survive common deployment steps such as finetuning or quantization, and\nsuch keys can be easily detected and filtered from LLM responses, ultimately\nbreaking the fingerprint. To overcome these limitations we introduce LLM\nfingerprinting via semantically conditioned watermarks, replacing fixed query\nsets with a broad semantic domain, and replacing brittle atypical keys with a\nstatistical watermarking signal diffused throughout each response. After\nteaching the model to watermark its responses only to prompts from a\npredetermined domain e.g., French language, the model owner can use queries\nfrom that domain to reliably detect the fingerprint and verify ownership. As we\nconfirm in our thorough experimental evaluation, our fingerprint is both\nstealthy and robust to all common deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most LLM fingerprinting methods teach the model to respond to a few fixed\nqueries with predefined atypical responses (keys). This memorization often does\nnot survive common deployment steps such as finetuning or quantization, and\nsuch keys can be easily detected and filtered from LLM responses, ultimately\nbreaking the fingerprint. To overcome these limitations we introduce LLM\nfingerprinting via semantically conditioned watermarks, replacing fixed query\nsets with a broad semantic domain, and replacing brittle atypical keys with a\nstatistical watermarking signal diffused throughout each response. After\nteaching the model to watermark its responses only to prompts from a\npredetermined domain e.g., French language, the model owner can use queries\nfrom that domain to reliably detect the fingerprint and verify ownership. As we\nconfirm in our thorough experimental evaluation, our fingerprint is both\nstealthy and robust to all common deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Thibaud Gloaguen"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Nikola Jovanović"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08638v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08638v4",
                "updated": "2025-10-09T14:39:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    39,
                    40,
                    3,
                    282,
                    0
                ],
                "published": "2025-02-12T18:54:37Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    54,
                    37,
                    2,
                    43,
                    0
                ],
                "title": "Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples"
                },
                "summary": "The evaluation of cross-lingual semantic search models is often limited to\nexisting datasets from tasks such as information retrieval and semantic textual\nsimilarity. We introduce Cross-Lingual Semantic Discrimination (CLSD), a\nlightweight evaluation task that requires only parallel sentences and a Large\nLanguage Model (LLM) to generate adversarial distractors. CLSD measures an\nembedding model's ability to rank the true parallel sentence above semantically\nmisleading but lexically similar alternatives. As a case study, we construct\nCLSD datasets for German--French in the news domain. Our experiments show that\nmodels fine-tuned for retrieval tasks benefit from pivoting through English,\nwhereas bitext mining models perform best in direct cross-lingual settings. A\nfine-grained similarity analysis further reveals that embedding models differ\nin their sensitivity to linguistic perturbations. We release our code and\ndatasets under AGPL-3.0:\nhttps://github.com/impresso/cross_lingual_semantic_discrimination",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of cross-lingual semantic search models is often limited to\nexisting datasets from tasks such as information retrieval and semantic textual\nsimilarity. We introduce Cross-Lingual Semantic Discrimination (CLSD), a\nlightweight evaluation task that requires only parallel sentences and a Large\nLanguage Model (LLM) to generate adversarial distractors. CLSD measures an\nembedding model's ability to rank the true parallel sentence above semantically\nmisleading but lexically similar alternatives. As a case study, we construct\nCLSD datasets for German--French in the news domain. Our experiments show that\nmodels fine-tuned for retrieval tasks benefit from pivoting through English,\nwhereas bitext mining models perform best in direct cross-lingual settings. A\nfine-grained similarity analysis further reveals that embedding models differ\nin their sensitivity to linguistic perturbations. We release our code and\ndatasets under AGPL-3.0:\nhttps://github.com/impresso/cross_lingual_semantic_discrimination"
                },
                "authors": [
                    {
                        "name": "Andrianos Michail"
                    },
                    {
                        "name": "Simon Clematide"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "To appear in EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08638v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08638v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08284v1",
                "updated": "2025-10-09T14:35:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    35,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:35:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    35,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "Neuron-Level Analysis of Cultural Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuron-Level Analysis of Cultural Understanding in Large Language Models"
                },
                "summary": "As large language models (LLMs) are increasingly deployed worldwide, ensuring\ntheir fair and comprehensive cultural understanding is important. However, LLMs\nexhibit cultural bias and limited awareness of underrepresented cultures, while\nthe mechanisms underlying their cultural understanding remain underexplored. To\nfill this gap, we conduct a neuron-level analysis to identify neurons that\ndrive cultural behavior, introducing a gradient-based scoring method with\nadditional filtering for precise refinement. We identify both culture-general\nneurons contributing to cultural understanding regardless of cultures, and\nculture-specific neurons tied to an individual culture. These neurons account\nfor less than 1% of all neurons and are concentrated in shallow to middle MLP\nlayers. We validate their role by showing that suppressing them substantially\ndegrades performance on cultural benchmarks (by up to 30%), while performance\non general natural language understanding (NLU) benchmarks remains largely\nunaffected. Moreover, we show that culture-specific neurons support knowledge\nof not only the target culture, but also related cultures. Finally, we\ndemonstrate that training on NLU benchmarks can diminish models' cultural\nunderstanding when we update modules containing many culture-general neurons.\nThese findings provide insights into the internal mechanisms of LLMs and offer\npractical guidance for model training and engineering. Our code is available at\nhttps://github.com/ynklab/CULNIG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed worldwide, ensuring\ntheir fair and comprehensive cultural understanding is important. However, LLMs\nexhibit cultural bias and limited awareness of underrepresented cultures, while\nthe mechanisms underlying their cultural understanding remain underexplored. To\nfill this gap, we conduct a neuron-level analysis to identify neurons that\ndrive cultural behavior, introducing a gradient-based scoring method with\nadditional filtering for precise refinement. We identify both culture-general\nneurons contributing to cultural understanding regardless of cultures, and\nculture-specific neurons tied to an individual culture. These neurons account\nfor less than 1% of all neurons and are concentrated in shallow to middle MLP\nlayers. We validate their role by showing that suppressing them substantially\ndegrades performance on cultural benchmarks (by up to 30%), while performance\non general natural language understanding (NLU) benchmarks remains largely\nunaffected. Moreover, we show that culture-specific neurons support knowledge\nof not only the target culture, but also related cultures. Finally, we\ndemonstrate that training on NLU benchmarks can diminish models' cultural\nunderstanding when we update modules containing many culture-general neurons.\nThese findings provide insights into the internal mechanisms of LLMs and offer\npractical guidance for model training and engineering. Our code is available at\nhttps://github.com/ynklab/CULNIG"
                },
                "authors": [
                    {
                        "name": "Taisei Yamamoto"
                    },
                    {
                        "name": "Ryoma Kumon"
                    },
                    {
                        "name": "Danushka Bollegala"
                    },
                    {
                        "name": "Hitomi Yanaka"
                    }
                ],
                "author_detail": {
                    "name": "Hitomi Yanaka"
                },
                "author": "Hitomi Yanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08280v1",
                "updated": "2025-10-09T14:33:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    33,
                    5,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:33:05Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    33,
                    5,
                    3,
                    282,
                    0
                ],
                "title": "How Internal Structure Shapes the Metallicity of Giant Exoplanets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Internal Structure Shapes the Metallicity of Giant Exoplanets"
                },
                "summary": "The composition and internal structure of gas giant exoplanets encode key\ninformation about their formation and subsequent evolution. We investigate how\ndifferent interior structure assumptions affect the inferred bulk metallicity\nand its correlation with planetary mass. For a sample of 44 giant exoplanets\n(0.12-5.98 MJ), we compute evolutionary models with CEPAM and retrieve their\nbulk metallicities under three structural hypotheses: Core+Envelope (CE),\nDilute Core (DC), and Fully Mixed (FM). Across all structures, we recover a\nsignificant positive correlation between total heavy-element mass (M_Z) and\nplanetary mass (M), and a negative correlation between metallicity (Z) and M\n(also for Z/Z_star vs. M). DC structures yield metallicities comparable to CE\nmodels, regardless of the assumed gradient extent. Increasing atmospheric\nmetallicity raises the inferred bulk metallicity, as enhanced opacities slow\nplanetary cooling. Non-adiabatic DC models can further increase the retrieved\nmetallicity by up to 35%. Sensitivity analyses show that the mass-metallicity\nanti-correlation is primarily driven by low-mass, metal-rich planets, while\nmassive planets exhibit unexpectedly high metallicities. Improved constraints\non convective mixing, combined with upcoming precise measurements of planetary\nmasses, radii, and atmospheric compositions from missions such as PLATO and\nAriel, will enable more robust inferences of interior structures and formation\npathways for gas giant planets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The composition and internal structure of gas giant exoplanets encode key\ninformation about their formation and subsequent evolution. We investigate how\ndifferent interior structure assumptions affect the inferred bulk metallicity\nand its correlation with planetary mass. For a sample of 44 giant exoplanets\n(0.12-5.98 MJ), we compute evolutionary models with CEPAM and retrieve their\nbulk metallicities under three structural hypotheses: Core+Envelope (CE),\nDilute Core (DC), and Fully Mixed (FM). Across all structures, we recover a\nsignificant positive correlation between total heavy-element mass (M_Z) and\nplanetary mass (M), and a negative correlation between metallicity (Z) and M\n(also for Z/Z_star vs. M). DC structures yield metallicities comparable to CE\nmodels, regardless of the assumed gradient extent. Increasing atmospheric\nmetallicity raises the inferred bulk metallicity, as enhanced opacities slow\nplanetary cooling. Non-adiabatic DC models can further increase the retrieved\nmetallicity by up to 35%. Sensitivity analyses show that the mass-metallicity\nanti-correlation is primarily driven by low-mass, metal-rich planets, while\nmassive planets exhibit unexpectedly high metallicities. Improved constraints\non convective mixing, combined with upcoming precise measurements of planetary\nmasses, radii, and atmospheric compositions from missions such as PLATO and\nAriel, will enable more robust inferences of interior structures and formation\npathways for gas giant planets."
                },
                "authors": [
                    {
                        "name": "Lorenzo Peerani"
                    },
                    {
                        "name": "Saburo Howard"
                    },
                    {
                        "name": "Ravit Helled"
                    }
                ],
                "author_detail": {
                    "name": "Ravit Helled"
                },
                "author": "Ravit Helled",
                "arxiv_comment": "Submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08278v1",
                "updated": "2025-10-09T14:32:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    32,
                    21,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:32:21Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    32,
                    21,
                    3,
                    282,
                    0
                ],
                "title": "A Multimodal Depth-Aware Method For Embodied Reference Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multimodal Depth-Aware Method For Embodied Reference Understanding"
                },
                "summary": "Embodied Reference Understanding requires identifying a target object in a\nvisual scene based on both language instructions and pointing cues. While prior\nworks have shown progress in open-vocabulary object detection, they often fail\nin ambiguous scenarios where multiple candidate objects exist in the scene. To\naddress these challenges, we propose a novel ERU framework that jointly\nleverages LLM-based data augmentation, depth-map modality, and a depth-aware\ndecision module. This design enables robust integration of linguistic and\nembodied cues, improving disambiguation in complex or cluttered environments.\nExperimental results on two datasets demonstrate that our approach\nsignificantly outperforms existing baselines, achieving more accurate and\nreliable referent detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Reference Understanding requires identifying a target object in a\nvisual scene based on both language instructions and pointing cues. While prior\nworks have shown progress in open-vocabulary object detection, they often fail\nin ambiguous scenarios where multiple candidate objects exist in the scene. To\naddress these challenges, we propose a novel ERU framework that jointly\nleverages LLM-based data augmentation, depth-map modality, and a depth-aware\ndecision module. This design enables robust integration of linguistic and\nembodied cues, improving disambiguation in complex or cluttered environments.\nExperimental results on two datasets demonstrate that our approach\nsignificantly outperforms existing baselines, achieving more accurate and\nreliable referent detection."
                },
                "authors": [
                    {
                        "name": "Fevziye Irem Eyiokur"
                    },
                    {
                        "name": "Dogucan Yaman"
                    },
                    {
                        "name": "Hazım Kemal Ekenel"
                    },
                    {
                        "name": "Alexander Waibel"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Waibel"
                },
                "author": "Alexander Waibel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08276v1",
                "updated": "2025-10-09T14:31:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    31,
                    39,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:31:39Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    31,
                    39,
                    3,
                    282,
                    0
                ],
                "title": "Beyond Turn Limits: Training Deep Search Agents with Dynamic Context\n  Window",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Turn Limits: Training Deep Search Agents with Dynamic Context\n  Window"
                },
                "summary": "While recent advances in reasoning models have demonstrated cognitive\nbehaviors through reinforcement learning, existing approaches struggle to\ninvoke deep reasoning capabilities in multi-turn agents with long-horizon\ninteractions. We propose DeepMiner, a novel framework that elicits such\nabilities by introducing high-difficulty training tasks and dynamic context\nwindow. DeepMiner presents a reverse construction method to generate complex\nbut verifiable question-answer pairs from authentic web sources, which ensures\nthe challenge and reliability of training data while injecting cognitive\ncapabilities into multi-turn reasoning scenarios. We further design an elegant\nyet effective dynamic context management strategy for both training and\ninference, utilizing sliding window mechanisms while eliminating the dependency\non external summarization models, thereby efficiently empowering the model to\nhandle continuously expanding long-horizon contexts. Through reinforcement\nlearning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial\nperformance improvements across multiple search agent benchmarks. DeepMiner\nattains 33.5% accuracy on BrowseComp-en, surpassing the previous best\nopen-source agent by almost 20 percentage points, and demonstrates consistent\nimprovements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our\ndynamic context management enables sustained interactions of nearly 100 turns\nwithin standard 32k context length, effectively addressing the context\nlimitations that constrain existing multi-turn interaction systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent advances in reasoning models have demonstrated cognitive\nbehaviors through reinforcement learning, existing approaches struggle to\ninvoke deep reasoning capabilities in multi-turn agents with long-horizon\ninteractions. We propose DeepMiner, a novel framework that elicits such\nabilities by introducing high-difficulty training tasks and dynamic context\nwindow. DeepMiner presents a reverse construction method to generate complex\nbut verifiable question-answer pairs from authentic web sources, which ensures\nthe challenge and reliability of training data while injecting cognitive\ncapabilities into multi-turn reasoning scenarios. We further design an elegant\nyet effective dynamic context management strategy for both training and\ninference, utilizing sliding window mechanisms while eliminating the dependency\non external summarization models, thereby efficiently empowering the model to\nhandle continuously expanding long-horizon contexts. Through reinforcement\nlearning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial\nperformance improvements across multiple search agent benchmarks. DeepMiner\nattains 33.5% accuracy on BrowseComp-en, surpassing the previous best\nopen-source agent by almost 20 percentage points, and demonstrates consistent\nimprovements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our\ndynamic context management enables sustained interactions of nearly 100 turns\nwithin standard 32k context length, effectively addressing the context\nlimitations that constrain existing multi-turn interaction systems."
                },
                "authors": [
                    {
                        "name": "Qiaoyu Tang"
                    },
                    {
                        "name": "Hao Xiang"
                    },
                    {
                        "name": "Le Yu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "WenJuan Zhang"
                    },
                    {
                        "name": "Pengbo Wang"
                    },
                    {
                        "name": "Shixuan Liu"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09752v2",
                "updated": "2025-10-09T14:31:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    31,
                    29,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-13T12:31:27Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    31,
                    27,
                    2,
                    225,
                    0
                ],
                "title": "$μ$-Parametrization for Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$μ$-Parametrization for Mixture of Experts"
                },
                "summary": "Recent years have seen a growing interest and adoption of LLMs, with\nMixture-of-Experts (MoE) emerging as a leading architecture in extremely large\nmodels. Currently, the largest open-source models reach over $1$T parameters.\nAt such scales, hyperparameter tuning becomes prohibitively expensive.\nPrecisely for this reason, the $\\mu$Transfer is becoming a key technique. It\nallows for seamless transfer of optimal hyperparameters across model scales,\nresulting in a huge reduction in tuning costs. However, existing work has\nprimarily focused on dense LLMs, leaving MoE architectures unexplored. In this\nwork, we derive a $\\mu$-Parameterization for MoE, providing theoretical\nguarantees for feature learning across model widths. Our experiments\ndemonstrate that the optimal learning rate reliably transfers across model\nsizes, establishing a foundation for efficient hyperparameter tuning in\nlarge-scale MoE models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen a growing interest and adoption of LLMs, with\nMixture-of-Experts (MoE) emerging as a leading architecture in extremely large\nmodels. Currently, the largest open-source models reach over $1$T parameters.\nAt such scales, hyperparameter tuning becomes prohibitively expensive.\nPrecisely for this reason, the $\\mu$Transfer is becoming a key technique. It\nallows for seamless transfer of optimal hyperparameters across model scales,\nresulting in a huge reduction in tuning costs. However, existing work has\nprimarily focused on dense LLMs, leaving MoE architectures unexplored. In this\nwork, we derive a $\\mu$-Parameterization for MoE, providing theoretical\nguarantees for feature learning across model widths. Our experiments\ndemonstrate that the optimal learning rate reliably transfers across model\nsizes, establishing a foundation for efficient hyperparameter tuning in\nlarge-scale MoE models."
                },
                "authors": [
                    {
                        "name": "Jan Małaśnicki"
                    },
                    {
                        "name": "Kamil Ciebiera"
                    },
                    {
                        "name": "Mateusz Boruń"
                    },
                    {
                        "name": "Maciej Pióro"
                    },
                    {
                        "name": "Jan Ludziejewski"
                    },
                    {
                        "name": "Maciej Stefaniak"
                    },
                    {
                        "name": "Michał Krutul"
                    },
                    {
                        "name": "Sebastian Jaszczur"
                    },
                    {
                        "name": "Marek Cygan"
                    },
                    {
                        "name": "Kamil Adamczewski"
                    },
                    {
                        "name": "Jakub Krajewski"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Krajewski"
                },
                "author": "Jakub Krajewski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01057v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01057v3",
                "updated": "2025-10-09T14:28:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    28,
                    34,
                    3,
                    282,
                    0
                ],
                "published": "2023-11-02T08:01:49Z",
                "published_parsed": [
                    2023,
                    11,
                    2,
                    8,
                    1,
                    49,
                    3,
                    306,
                    0
                ],
                "title": "Ultra-Efficient On-Device Object Detection on AI-Integrated Smart\n  Glasses with TinyissimoYOLO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-Efficient On-Device Object Detection on AI-Integrated Smart\n  Glasses with TinyissimoYOLO"
                },
                "summary": "Smart glasses are rapidly gaining advanced functions thanks to cutting-edge\ncomputing technologies, especially accelerated hardware architectures, and tiny\nArtificial Intelligence (AI) algorithms. However, integrating AI into smart\nglasses featuring a small form factor and limited battery capacity remains\nchallenging for a satisfactory user experience. To this end, this paper\nproposes the design of a smart glasses platform for always-on on-device object\ndetection with an all-day battery lifetime. The proposed platform is based on\nGAP9, a novel multi-core RISC-V processor from Greenwaves Technologies.\nAdditionally, a family of sub-million parameter TinyissimoYOLO networks are\nproposed. They are benchmarked on established datasets, capable of\ndifferentiating up to 80 classes on MS-COCO. Evaluations on the smart glasses\nprototype demonstrate TinyissimoYOLO's inference latency of only 17ms and\nconsuming 1.59mJ energy per inference. An end-to-end latency of 56ms is\nachieved which is equivalent to 18 frames per seconds (FPS) with a total power\nconsumption of 62.9mW. This ensures continuous system runtime of up to 9.3\nhours on a 154mAh battery. These results outperform MCUNet\n(TinyNAS+TinyEngine), which runs a simpler task (image classification) at just\n7.3 FPS, while the 18 FPS achieved in this paper even include image-capturing,\nnetwork inference, and detection post-processing. The algorithm's code is\nreleased open with this paper and can be found here:\nhttps://github.com/ETH-PBL/TinyissimoYOLO",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart glasses are rapidly gaining advanced functions thanks to cutting-edge\ncomputing technologies, especially accelerated hardware architectures, and tiny\nArtificial Intelligence (AI) algorithms. However, integrating AI into smart\nglasses featuring a small form factor and limited battery capacity remains\nchallenging for a satisfactory user experience. To this end, this paper\nproposes the design of a smart glasses platform for always-on on-device object\ndetection with an all-day battery lifetime. The proposed platform is based on\nGAP9, a novel multi-core RISC-V processor from Greenwaves Technologies.\nAdditionally, a family of sub-million parameter TinyissimoYOLO networks are\nproposed. They are benchmarked on established datasets, capable of\ndifferentiating up to 80 classes on MS-COCO. Evaluations on the smart glasses\nprototype demonstrate TinyissimoYOLO's inference latency of only 17ms and\nconsuming 1.59mJ energy per inference. An end-to-end latency of 56ms is\nachieved which is equivalent to 18 frames per seconds (FPS) with a total power\nconsumption of 62.9mW. This ensures continuous system runtime of up to 9.3\nhours on a 154mAh battery. These results outperform MCUNet\n(TinyNAS+TinyEngine), which runs a simpler task (image classification) at just\n7.3 FPS, while the 18 FPS achieved in this paper even include image-capturing,\nnetwork inference, and detection post-processing. The algorithm's code is\nreleased open with this paper and can be found here:\nhttps://github.com/ETH-PBL/TinyissimoYOLO"
                },
                "authors": [
                    {
                        "name": "Julian Moosmann"
                    },
                    {
                        "name": "Pietro Bonazzi"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Sizhen Bian"
                    },
                    {
                        "name": "Philipp Mayer"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Michele Magno"
                    }
                ],
                "author_detail": {
                    "name": "Michele Magno"
                },
                "author": "Michele Magno",
                "arxiv_doi": "10.1007/978-3-031-91989-3_17",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-91989-3_17",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.01057v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01057v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This paper has been accepted for publication at ECCV 2024 Workshops,\n  Milan, 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16567v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16567v3",
                "updated": "2025-10-09T14:23:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    23,
                    19,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-22T11:59:44Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    11,
                    59,
                    44,
                    3,
                    142,
                    0
                ],
                "title": "Watch your steps: Dormant Adversarial Behaviors that Activate upon LLM\n  Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch your steps: Dormant Adversarial Behaviors that Activate upon LLM\n  Finetuning"
                },
                "summary": "Finetuning open-weight Large Language Models (LLMs) is standard practice for\nachieving task-specific performance improvements. Until now, finetuning has\nbeen regarded as a controlled and secure process in which training on benign\ndatasets leads to predictable behaviors. In this paper, we demonstrate, for the\nfirst time, that an adversary can create compromised LLMs that are performant\nand benign, yet exhibit adversarial behaviors once finetuned by downstream\nusers. To this end, we propose an attack, FAB (Finetuning-activated Adversarial\nBehaviors), which compromises an LLM via meta-learning techniques that simulate\ndownstream finetuning, explicitly optimizing for the emergence of adversarial\nbehaviors in the finetuned models. At the same time, the compromised LLM is\nregularized to retain general capabilities and to exhibit no adversarial\nbehaviors prior to finetuning. As a result, when users finetune (e.g.,\ninstruction-tuning, distillation, DPO) the seemingly benign model on their own\ndatasets, they unknowingly trigger its dormant adversarial behavior. We\nexperimentally demonstrate the effectiveness of FAB across multiple LLMs and\nthree commonly considered target behaviors: unsolicited advertising,\njailbreakability, and over-refusal. We show that FAB-triggers are robust to\nvarious finetuning choices made by the user (e.g., dataset, number of steps,\nscheduler, post-training algorithm). Our findings challenge prevailing\nassumptions on the security of finetuning, revealing a critical attack vector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning open-weight Large Language Models (LLMs) is standard practice for\nachieving task-specific performance improvements. Until now, finetuning has\nbeen regarded as a controlled and secure process in which training on benign\ndatasets leads to predictable behaviors. In this paper, we demonstrate, for the\nfirst time, that an adversary can create compromised LLMs that are performant\nand benign, yet exhibit adversarial behaviors once finetuned by downstream\nusers. To this end, we propose an attack, FAB (Finetuning-activated Adversarial\nBehaviors), which compromises an LLM via meta-learning techniques that simulate\ndownstream finetuning, explicitly optimizing for the emergence of adversarial\nbehaviors in the finetuned models. At the same time, the compromised LLM is\nregularized to retain general capabilities and to exhibit no adversarial\nbehaviors prior to finetuning. As a result, when users finetune (e.g.,\ninstruction-tuning, distillation, DPO) the seemingly benign model on their own\ndatasets, they unknowingly trigger its dormant adversarial behavior. We\nexperimentally demonstrate the effectiveness of FAB across multiple LLMs and\nthree commonly considered target behaviors: unsolicited advertising,\njailbreakability, and over-refusal. We show that FAB-triggers are robust to\nvarious finetuning choices made by the user (e.g., dataset, number of steps,\nscheduler, post-training algorithm). Our findings challenge prevailing\nassumptions on the security of finetuning, revealing a critical attack vector."
                },
                "authors": [
                    {
                        "name": "Thibaud Gloaguen"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16567v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16567v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11552v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11552v3",
                "updated": "2025-10-09T14:21:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    21,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-15T03:32:50Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    3,
                    32,
                    50,
                    0,
                    258,
                    0
                ],
                "title": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with\n  Hierarchical Chunking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with\n  Hierarchical Chunking"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems."
                },
                "authors": [
                    {
                        "name": "Wensheng Lu"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "17 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11552v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11552v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06605v2",
                "updated": "2025-10-09T14:20:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    20,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-02-10T16:02:26Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    2,
                    26,
                    0,
                    41,
                    0
                ],
                "title": "Quantile Forecast Matching with a Bayesian Quantile Gaussian Process\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantile Forecast Matching with a Bayesian Quantile Gaussian Process\n  Model"
                },
                "summary": "A set of probabilities along with corresponding quantiles are often used to\ndefine predictive distributions or probabilistic forecasts. These quantile\npredictions offer easily interpreted uncertainty of an event, and quantiles are\ngenerally straightforward to estimate using standard statistical and machine\nlearning methods. However, compared to a distribution defined by a probability\ndensity or cumulative distribution function, a set of quantiles has less\ndistributional information. When given estimated quantiles, it may be desirable\nto estimate a fully defined continuous distribution function. Many researchers\ndo so to make evaluation or ensemble modeling simpler. Most existing methods\nfor fitting a distribution to quantiles lack accurate representation of the\ninherent uncertainty from quantile estimation or are limited in their\napplications. In this manuscript, we present a Gaussian process model, the\nquantile Gaussian process, which is based on established theory of quantile\nfunctions and sample quantiles, to construct a probability distribution given\nestimated quantiles. A Bayesian application of the quantile Gaussian process is\nevaluated for parameter inference and distribution approximation in simulation\nstudies. The quantile Gaussian process is used to approximate the distributions\nof quantile forecasts from the 2023-24 US Centers for Disease Control\ncollaborative flu forecasting initiative. The simulation studies and data\nanalysis show that the quantile Gaussian process leads to accurate inference on\nmodel parameters, estimation of a continuous distribution, and uncertainty\nquantification of sample quantiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A set of probabilities along with corresponding quantiles are often used to\ndefine predictive distributions or probabilistic forecasts. These quantile\npredictions offer easily interpreted uncertainty of an event, and quantiles are\ngenerally straightforward to estimate using standard statistical and machine\nlearning methods. However, compared to a distribution defined by a probability\ndensity or cumulative distribution function, a set of quantiles has less\ndistributional information. When given estimated quantiles, it may be desirable\nto estimate a fully defined continuous distribution function. Many researchers\ndo so to make evaluation or ensemble modeling simpler. Most existing methods\nfor fitting a distribution to quantiles lack accurate representation of the\ninherent uncertainty from quantile estimation or are limited in their\napplications. In this manuscript, we present a Gaussian process model, the\nquantile Gaussian process, which is based on established theory of quantile\nfunctions and sample quantiles, to construct a probability distribution given\nestimated quantiles. A Bayesian application of the quantile Gaussian process is\nevaluated for parameter inference and distribution approximation in simulation\nstudies. The quantile Gaussian process is used to approximate the distributions\nof quantile forecasts from the 2023-24 US Centers for Disease Control\ncollaborative flu forecasting initiative. The simulation studies and data\nanalysis show that the quantile Gaussian process leads to accurate inference on\nmodel parameters, estimation of a continuous distribution, and uncertainty\nquantification of sample quantiles."
                },
                "authors": [
                    {
                        "name": "Spencer Wadsworth"
                    },
                    {
                        "name": "Jarad Niemi"
                    }
                ],
                "author_detail": {
                    "name": "Jarad Niemi"
                },
                "author": "Jarad Niemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.08572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08572v1",
                "updated": "2025-10-09T17:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    58,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    58,
                    3,
                    282,
                    0
                ],
                "title": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data\n  Generation"
                },
                "summary": "Scaling data and models has played a pivotal role in the remarkable progress\nof computer vision and language. Inspired by these domains, recent efforts in\nrobotics have similarly focused on scaling both data and model size to develop\nmore generalizable and robust policies. However, unlike vision and language,\nrobotics lacks access to internet-scale demonstrations across diverse robotic\ntasks and environments. As a result, the scale of existing datasets typically\nsuffers from the need for manual data collection and curation. To address this\nproblem, here we propose BLAZER, a framework that learns manipulation policies\nfrom automatically generated training data. We build on the zero-shot\ncapabilities of LLM planners and automatically generate demonstrations for\ndiverse manipulation tasks in simulation. Successful examples are then used to\nfinetune an LLM and to improve its planning capabilities without human\nsupervision. Notably, while BLAZER training requires access to the simulator's\nstate, we demonstrate direct transfer of acquired skills to sensor-based\nmanipulation. Through extensive experiments, we show BLAZER to significantly\nimprove zero-shot manipulation in both simulated and real environments.\nMoreover, BLAZER improves on tasks outside of its training pool and enables\ndownscaling of LLM models. Our code and data will be made publicly available on\nthe project page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling data and models has played a pivotal role in the remarkable progress\nof computer vision and language. Inspired by these domains, recent efforts in\nrobotics have similarly focused on scaling both data and model size to develop\nmore generalizable and robust policies. However, unlike vision and language,\nrobotics lacks access to internet-scale demonstrations across diverse robotic\ntasks and environments. As a result, the scale of existing datasets typically\nsuffers from the need for manual data collection and curation. To address this\nproblem, here we propose BLAZER, a framework that learns manipulation policies\nfrom automatically generated training data. We build on the zero-shot\ncapabilities of LLM planners and automatically generate demonstrations for\ndiverse manipulation tasks in simulation. Successful examples are then used to\nfinetune an LLM and to improve its planning capabilities without human\nsupervision. Notably, while BLAZER training requires access to the simulator's\nstate, we demonstrate direct transfer of acquired skills to sensor-based\nmanipulation. Through extensive experiments, we show BLAZER to significantly\nimprove zero-shot manipulation in both simulated and real environments.\nMoreover, BLAZER improves on tasks outside of its training pool and enables\ndownscaling of LLM models. Our code and data will be made publicly available on\nthe project page."
                },
                "authors": [
                    {
                        "name": "Rocktim Jyoti Das"
                    },
                    {
                        "name": "Harsh Singh"
                    },
                    {
                        "name": "Diana Turmakhan"
                    },
                    {
                        "name": "Muhammad Abdullah Sohail"
                    },
                    {
                        "name": "Mingfei Han"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Fabio Pizzati"
                    },
                    {
                        "name": "Ivan Laptev"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Laptev"
                },
                "author": "Ivan Laptev",
                "arxiv_comment": "11 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08569v1",
                "updated": "2025-10-09T17:59:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    55,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:59:55Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    55,
                    3,
                    282,
                    0
                ],
                "title": "ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive\n  Evaluation"
                },
                "summary": "Benchmarks are central to measuring the capabilities of large language models\nand guiding model development, yet widespread data leakage from pretraining\ncorpora undermines their validity. Models can match memorized content rather\nthan demonstrate true generalization, which inflates scores, distorts\ncross-model comparisons, and misrepresents progress. We introduce ArenaBencher,\na model-agnostic framework for automatic benchmark evolution that updates test\ncases while preserving comparability. Given an existing benchmark and a diverse\npool of models to be evaluated, ArenaBencher infers the core ability of each\ntest case, generates candidate question-answer pairs that preserve the original\nobjective, verifies correctness and intent with an LLM as a judge, and\naggregates feedback from multiple models to select candidates that expose\nshared weaknesses. The process runs iteratively with in-context demonstrations\nthat steer generation toward more challenging and diagnostic cases. We apply\nArenaBencher to math problem solving, commonsense reasoning, and safety domains\nand show that it produces verified, diverse, and fair updates that uncover new\nfailure modes, increase difficulty while preserving test objective alignment,\nand improve model separability. The framework provides a scalable path to\ncontinuously evolve benchmarks in step with the rapid progress of foundation\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks are central to measuring the capabilities of large language models\nand guiding model development, yet widespread data leakage from pretraining\ncorpora undermines their validity. Models can match memorized content rather\nthan demonstrate true generalization, which inflates scores, distorts\ncross-model comparisons, and misrepresents progress. We introduce ArenaBencher,\na model-agnostic framework for automatic benchmark evolution that updates test\ncases while preserving comparability. Given an existing benchmark and a diverse\npool of models to be evaluated, ArenaBencher infers the core ability of each\ntest case, generates candidate question-answer pairs that preserve the original\nobjective, verifies correctness and intent with an LLM as a judge, and\naggregates feedback from multiple models to select candidates that expose\nshared weaknesses. The process runs iteratively with in-context demonstrations\nthat steer generation toward more challenging and diagnostic cases. We apply\nArenaBencher to math problem solving, commonsense reasoning, and safety domains\nand show that it produces verified, diverse, and fair updates that uncover new\nfailure modes, increase difficulty while preserving test objective alignment,\nand improve model separability. The framework provides a scalable path to\ncontinuously evolve benchmarks in step with the rapid progress of foundation\nmodels."
                },
                "authors": [
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Jacob Dineen"
                    },
                    {
                        "name": "Yuxi Huang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Hoifung Poon"
                    },
                    {
                        "name": "Ben Zhou"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08565v1",
                "updated": "2025-10-09T17:59:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    37,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:59:37Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    59,
                    37,
                    3,
                    282,
                    0
                ],
                "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language\n  Models under Data Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language\n  Models under Data Constraints"
                },
                "summary": "Compositional training has been the de-facto paradigm in existing Multimodal\nLarge Language Models (MLLMs), where pre-trained vision encoders are connected\nwith pre-trained LLMs through continuous multimodal pre-training. However, the\nmultimodal scaling property of this paradigm remains difficult to explore due\nto the separated training. In this paper, we focus on the native training of\nMLLMs in an end-to-end manner and systematically study its design space and\nscaling property under a practical setting, i.e., data constraint. Through\ncareful study of various choices in MLLM, we obtain the optimal\nmeta-architecture that best balances performance and training cost. After that,\nwe further explore the scaling properties of the native MLLM and indicate the\npositively correlated scaling relationship between visual encoders and LLMs.\nBased on these findings, we propose a native MLLM called NaViL, combined with a\nsimple and cost-effective recipe. Experimental results on 14 multimodal\nbenchmarks confirm the competitive performance of NaViL against existing MLLMs.\nBesides that, our findings and results provide in-depth insights for the future\nstudy of native MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional training has been the de-facto paradigm in existing Multimodal\nLarge Language Models (MLLMs), where pre-trained vision encoders are connected\nwith pre-trained LLMs through continuous multimodal pre-training. However, the\nmultimodal scaling property of this paradigm remains difficult to explore due\nto the separated training. In this paper, we focus on the native training of\nMLLMs in an end-to-end manner and systematically study its design space and\nscaling property under a practical setting, i.e., data constraint. Through\ncareful study of various choices in MLLM, we obtain the optimal\nmeta-architecture that best balances performance and training cost. After that,\nwe further explore the scaling properties of the native MLLM and indicate the\npositively correlated scaling relationship between visual encoders and LLMs.\nBased on these findings, we propose a native MLLM called NaViL, combined with a\nsimple and cost-effective recipe. Experimental results on 14 multimodal\nbenchmarks confirm the competitive performance of NaViL against existing MLLMs.\nBesides that, our findings and results provide in-depth insights for the future\nstudy of native MLLMs."
                },
                "authors": [
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Xizhou Zhu"
                    },
                    {
                        "name": "Weijie Su"
                    },
                    {
                        "name": "Hanming Deng"
                    },
                    {
                        "name": "Jinguo Zhu"
                    },
                    {
                        "name": "Jie Shao"
                    },
                    {
                        "name": "Ziran Zhu"
                    },
                    {
                        "name": "Yunpeng Liu"
                    },
                    {
                        "name": "Lewei Lu"
                    },
                    {
                        "name": "Wenhai Wang"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Jifeng Dai"
                    }
                ],
                "author_detail": {
                    "name": "Jifeng Dai"
                },
                "author": "Jifeng Dai",
                "arxiv_comment": "Accepted by NeurIPS 2025. 22 pages, link:\n  https://github.com/OpenGVLab/NaViL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08554v1",
                "updated": "2025-10-09T17:58:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    58,
                    7,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:58:07Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    58,
                    7,
                    3,
                    282,
                    0
                ],
                "title": "Improving Reasoning for Diffusion Language Models via Group Diffusion\n  Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Reasoning for Diffusion Language Models via Group Diffusion\n  Policy Optimization"
                },
                "summary": "Diffusion language models (DLMs) enable parallel, order-agnostic generation\nwith iterative refinement, offering a flexible alternative to autoregressive\nlarge language models (LLMs). However, adapting reinforcement learning (RL)\nfine-tuning to DLMs remains an open challenge because of the intractable\nlikelihood. Pioneering work such as diffu-GRPO estimated token-level\nlikelihoods via one-step unmasking. While computationally efficient, this\napproach is severely biased. A more principled foundation lies in\nsequence-level likelihoods, where the evidence lower bound (ELBO) serves as a\nsurrogate. Yet, despite this clean mathematical connection, ELBO-based methods\nhave seen limited adoption due to the prohibitive cost of likelihood\nevaluation. In this work, we revisit ELBO estimation and disentangle its\nsources of variance. This decomposition motivates reducing variance through\nfast, deterministic integral approximations along a few pivotal dimensions.\nBuilding on this insight, we introduce \\textbf{Group Diffusion Policy\nOptimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages\nsimple yet effective Semi-deterministic Monte Carlo schemes to mitigate the\nvariance explosion of ELBO estimators under vanilla double Monte Carlo\nsampling, yielding a provably lower-variance estimator under tight evaluation\nbudgets. Empirically, GDPO achieves consistent gains over pretrained\ncheckpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines,\non the majority of math, reasoning, and coding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) enable parallel, order-agnostic generation\nwith iterative refinement, offering a flexible alternative to autoregressive\nlarge language models (LLMs). However, adapting reinforcement learning (RL)\nfine-tuning to DLMs remains an open challenge because of the intractable\nlikelihood. Pioneering work such as diffu-GRPO estimated token-level\nlikelihoods via one-step unmasking. While computationally efficient, this\napproach is severely biased. A more principled foundation lies in\nsequence-level likelihoods, where the evidence lower bound (ELBO) serves as a\nsurrogate. Yet, despite this clean mathematical connection, ELBO-based methods\nhave seen limited adoption due to the prohibitive cost of likelihood\nevaluation. In this work, we revisit ELBO estimation and disentangle its\nsources of variance. This decomposition motivates reducing variance through\nfast, deterministic integral approximations along a few pivotal dimensions.\nBuilding on this insight, we introduce \\textbf{Group Diffusion Policy\nOptimization (GDPO)}, a new RL algorithm tailored for DLMs. GDPO leverages\nsimple yet effective Semi-deterministic Monte Carlo schemes to mitigate the\nvariance explosion of ELBO estimators under vanilla double Monte Carlo\nsampling, yielding a provably lower-variance estimator under tight evaluation\nbudgets. Empirically, GDPO achieves consistent gains over pretrained\ncheckpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines,\non the majority of math, reasoning, and coding benchmarks."
                },
                "authors": [
                    {
                        "name": "Kevin Rojas"
                    },
                    {
                        "name": "Jiahe Lin"
                    },
                    {
                        "name": "Kashif Rasul"
                    },
                    {
                        "name": "Anderson Schneider"
                    },
                    {
                        "name": "Yuriy Nevmyvaka"
                    },
                    {
                        "name": "Molei Tao"
                    },
                    {
                        "name": "Wei Deng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Deng"
                },
                "author": "Wei Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03438v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03438v3",
                "updated": "2025-10-09T17:57:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    57,
                    35,
                    3,
                    282,
                    0
                ],
                "published": "2025-02-05T18:33:36Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    18,
                    33,
                    36,
                    2,
                    36,
                    0
                ],
                "title": "BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic\n  Theorem Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic\n  Theorem Proving"
                },
                "summary": "Recent advancements in large language models (LLMs) have spurred growing\ninterest in automatic theorem proving using Lean4, where effective tree search\nmethods are crucial for navigating the underlying large proof search spaces.\nWhile the existing approaches primarily rely on value functions and/or Monte\nCarlo Tree Search (MCTS), the potential of simpler methods like Best-First Tree\nSearch (BFS) remains underexplored. In this paper, we investigate whether BFS\ncan achieve competitive performance in large-scale theorem proving tasks. We\npresent BFS-Prover, a scalable expert iteration framework, featuring three key\ninnovations. First, we implement strategic data filtering at each expert\niteration round, excluding problems solvable via beam search node expansion to\nfocus on harder cases. Second, we improve the sample efficiency of BFS through\nDirect Preference Optimization (DPO) applied to state-tactic pairs\nautomatically annotated with compiler error feedback, refining the LLM's policy\nto prioritize productive expansions. Third, we employ length normalization in\nBFS to encourage exploration of deeper proof paths. BFS-Prover achieves a\nstate-of-the-art score of $72.95\\%$ on the MiniF2F test set and therefore\nchallenges the perceived necessity of complex tree search methods,\ndemonstrating that BFS can achieve competitive performance when properly\nscaled. To facilitate further research and development in this area, we have\nopen-sourced our model at\nhttps://huggingface.co/ByteDance-Seed/BFS-Prover-V1-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have spurred growing\ninterest in automatic theorem proving using Lean4, where effective tree search\nmethods are crucial for navigating the underlying large proof search spaces.\nWhile the existing approaches primarily rely on value functions and/or Monte\nCarlo Tree Search (MCTS), the potential of simpler methods like Best-First Tree\nSearch (BFS) remains underexplored. In this paper, we investigate whether BFS\ncan achieve competitive performance in large-scale theorem proving tasks. We\npresent BFS-Prover, a scalable expert iteration framework, featuring three key\ninnovations. First, we implement strategic data filtering at each expert\niteration round, excluding problems solvable via beam search node expansion to\nfocus on harder cases. Second, we improve the sample efficiency of BFS through\nDirect Preference Optimization (DPO) applied to state-tactic pairs\nautomatically annotated with compiler error feedback, refining the LLM's policy\nto prioritize productive expansions. Third, we employ length normalization in\nBFS to encourage exploration of deeper proof paths. BFS-Prover achieves a\nstate-of-the-art score of $72.95\\%$ on the MiniF2F test set and therefore\nchallenges the perceived necessity of complex tree search methods,\ndemonstrating that BFS can achieve competitive performance when properly\nscaled. To facilitate further research and development in this area, we have\nopen-sourced our model at\nhttps://huggingface.co/ByteDance-Seed/BFS-Prover-V1-7B."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Chenguang Xi"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Xia Xiao"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Shen Zheng"
                    },
                    {
                        "name": "Kai Shen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shen"
                },
                "author": "Kai Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03438v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03438v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08549v1",
                "updated": "2025-10-09T17:56:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    56,
                    17,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:56:17Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    56,
                    17,
                    3,
                    282,
                    0
                ],
                "title": "Entropy Regularizing Activation: Boosting Continuous Control, Large\n  Language Models, and Image Classification with Activation as Entropy\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy Regularizing Activation: Boosting Continuous Control, Large\n  Language Models, and Image Classification with Activation as Entropy\n  Constraints"
                },
                "summary": "We propose ERA, a new paradigm that constrains the sampling entropy above\ngiven thresholds by applying specially designed activations to the outputs of\nmodels. Our approach demonstrates broad effectiveness across different domains:\n1) for large language models(LLMs), boosting the AIME 2025 score for\nQwen2.5-Math-7B by 37.4%; 2) for continuous control reinforcement learning\nagents, improving performance by more than 30% over strong baselines such as\nSAC on the challenging HumanoidBench; 3) for image classification, enhancing\nImageNet top-1 accuracy by 0.69% for ResNet-50. These gains are achieved with a\ncomputational overhead of less than 7%. Our work validates output activation as\na powerful tool for entropy control, opening a new direction for designing\nsimpler and more robust algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose ERA, a new paradigm that constrains the sampling entropy above\ngiven thresholds by applying specially designed activations to the outputs of\nmodels. Our approach demonstrates broad effectiveness across different domains:\n1) for large language models(LLMs), boosting the AIME 2025 score for\nQwen2.5-Math-7B by 37.4%; 2) for continuous control reinforcement learning\nagents, improving performance by more than 30% over strong baselines such as\nSAC on the challenging HumanoidBench; 3) for image classification, enhancing\nImageNet top-1 accuracy by 0.69% for ResNet-50. These gains are achieved with a\ncomputational overhead of less than 7%. Our work validates output activation as\na powerful tool for entropy control, opening a new direction for designing\nsimpler and more robust algorithms."
                },
                "authors": [
                    {
                        "name": "Zilin Kang"
                    },
                    {
                        "name": "Chonghua Liao"
                    },
                    {
                        "name": "Tingqiang Xu"
                    },
                    {
                        "name": "Huazhe Xu"
                    }
                ],
                "author_detail": {
                    "name": "Huazhe Xu"
                },
                "author": "Huazhe Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08544v1",
                "updated": "2025-10-09T17:55:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    55,
                    8,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:55:08Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    55,
                    8,
                    3,
                    282,
                    0
                ],
                "title": "SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM\n  Inference"
                },
                "summary": "Large Language Models (LLMs) have gained popularity in recent years, driving\nup the demand for inference. LLM inference is composed of two phases with\ndistinct characteristics: a compute-bound prefill phase followed by a\nmemory-bound decode phase. To efficiently serve LLMs, prior work proposes\nprefill-decode disaggregation to run each phase on separate hardware. However,\nexisting hardware poorly matches the different requirements of each phase.\nCurrent datacenter GPUs and TPUs follow a more-is-better design philosophy that\nmaximizes compute and memory resources, causing memory bandwidth\nunderutilization in the prefill phase and compute underutilization in the\ndecode phase. Such underutilization directly translates into increased serving\ncosts.\n  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting\na less-is-more methodology to design specialized chips tailored to the distinct\ncharacteristics of prefill and decode phases. The proposed Prefill Chips have\nlarger systolic arrays and use cost-effective GDDR memory, whereas the proposed\nDecode Chips retain high memory bandwidth but reduce compute capacity. Compared\nto modeled H100s, simulations show that the proposed Prefill Chips deliver 8%\nhigher prefill performance on average at 52% lower hardware cost, while the\nproposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.\n  End-to-end simulations on production traces show that SPAD reduces hardware\ncost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while\noffering the same performance. Even when models and workloads change, SPAD can\nreallocate either type of chip to run either phase and still achieve 11%-43%\nlower hardware costs, demonstrating the longevity of the SPAD design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained popularity in recent years, driving\nup the demand for inference. LLM inference is composed of two phases with\ndistinct characteristics: a compute-bound prefill phase followed by a\nmemory-bound decode phase. To efficiently serve LLMs, prior work proposes\nprefill-decode disaggregation to run each phase on separate hardware. However,\nexisting hardware poorly matches the different requirements of each phase.\nCurrent datacenter GPUs and TPUs follow a more-is-better design philosophy that\nmaximizes compute and memory resources, causing memory bandwidth\nunderutilization in the prefill phase and compute underutilization in the\ndecode phase. Such underutilization directly translates into increased serving\ncosts.\n  This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting\na less-is-more methodology to design specialized chips tailored to the distinct\ncharacteristics of prefill and decode phases. The proposed Prefill Chips have\nlarger systolic arrays and use cost-effective GDDR memory, whereas the proposed\nDecode Chips retain high memory bandwidth but reduce compute capacity. Compared\nto modeled H100s, simulations show that the proposed Prefill Chips deliver 8%\nhigher prefill performance on average at 52% lower hardware cost, while the\nproposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.\n  End-to-end simulations on production traces show that SPAD reduces hardware\ncost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while\noffering the same performance. Even when models and workloads change, SPAD can\nreallocate either type of chip to run either phase and still achieve 11%-43%\nlower hardware costs, demonstrating the longevity of the SPAD design."
                },
                "authors": [
                    {
                        "name": "Hengrui Zhang"
                    },
                    {
                        "name": "Pratyush Patel"
                    },
                    {
                        "name": "August Ning"
                    },
                    {
                        "name": "David Wentzlaff"
                    }
                ],
                "author_detail": {
                    "name": "David Wentzlaff"
                },
                "author": "David Wentzlaff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08539v1",
                "updated": "2025-10-09T17:53:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    53,
                    41,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:53:41Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    53,
                    41,
                    3,
                    282,
                    0
                ],
                "title": "On the optimization dynamics of RLVR: Gradient gap and step size\n  thresholds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the optimization dynamics of RLVR: Gradient gap and step size\n  thresholds"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple\nbinary feedback to post-train large language models, has shown significant\nempirical success. However, a principled understanding of why it works has been\nlacking. This paper builds a theoretical foundation for RLVR by analyzing its\ntraining process at both the full-response (trajectory) and token levels.\nCentral to our analysis is a quantity called the Gradient Gap, which formalizes\nthe direction of improvement from low-reward to high-reward regions of the\nresponse space. We prove that convergence critically depends on aligning the\nupdate direction with this Gradient Gap. Moreover, we derive a sharp step-size\nthreshold based on the magnitude of the Gradient Gap: below it, learning\nconverges, whereas above it, performance collapses. Our theory further predicts\nhow the critical step size must scale with response length and the success\nrate, thereby explaining why practical heuristics such as length normalization\nimprove stability and showing that, with a fixed learning rate, the success\nrate can stagnate strictly below $100\\%$. We validate these predictions through\ncontrolled bandit simulations and LLM experiments, including training\nQwen2.5-7B with GRPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR), which uses simple\nbinary feedback to post-train large language models, has shown significant\nempirical success. However, a principled understanding of why it works has been\nlacking. This paper builds a theoretical foundation for RLVR by analyzing its\ntraining process at both the full-response (trajectory) and token levels.\nCentral to our analysis is a quantity called the Gradient Gap, which formalizes\nthe direction of improvement from low-reward to high-reward regions of the\nresponse space. We prove that convergence critically depends on aligning the\nupdate direction with this Gradient Gap. Moreover, we derive a sharp step-size\nthreshold based on the magnitude of the Gradient Gap: below it, learning\nconverges, whereas above it, performance collapses. Our theory further predicts\nhow the critical step size must scale with response length and the success\nrate, thereby explaining why practical heuristics such as length normalization\nimprove stability and showing that, with a fixed learning rate, the success\nrate can stagnate strictly below $100\\%$. We validate these predictions through\ncontrolled bandit simulations and LLM experiments, including training\nQwen2.5-7B with GRPO."
                },
                "authors": [
                    {
                        "name": "Joe Suk"
                    },
                    {
                        "name": "Yaqi Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yaqi Duan"
                },
                "author": "Yaqi Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08529v1",
                "updated": "2025-10-09T17:50:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    26,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:50:26Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    26,
                    3,
                    282,
                    0
                ],
                "title": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards"
                },
                "summary": "Self-evolution is a central research topic in enabling large language model\n(LLM)-based agents to continually improve their capabilities after pretraining.\nRecent research has witnessed a transition from reinforcement learning\n(RL)-free to RL-based methods. Current RL-based methods either rely on dense\nexternal reward signals or extract intrinsic reward signals from LLMs\nthemselves. However, these approaches diverge from the self-evolution\nmechanisms observed in human intelligence, where individuals learn and improve\nthrough mutual discussion and collaboration. In this work, we introduce\nCo-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents\nto improve autonomously by learning from inter-agent interactions without\nexternal supervision. CoMAS generates intrinsic rewards from rich discussion\ndynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and\noptimizes each agent's policy through RL, thereby enabling decentralized and\nscalable co-evolution. Experimental results demonstrate that CoMAS consistently\noutperforms untrained agents and achieves state-of-the-art performance across\nmost evaluation settings. Ablation studies confirm the necessity of\ninteraction-based reward signals and reveal promising scalability as the number\nand diversity of agents increase. These findings establish CoMAS as a novel and\neffective paradigm for self-evolution in LLM-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-evolution is a central research topic in enabling large language model\n(LLM)-based agents to continually improve their capabilities after pretraining.\nRecent research has witnessed a transition from reinforcement learning\n(RL)-free to RL-based methods. Current RL-based methods either rely on dense\nexternal reward signals or extract intrinsic reward signals from LLMs\nthemselves. However, these approaches diverge from the self-evolution\nmechanisms observed in human intelligence, where individuals learn and improve\nthrough mutual discussion and collaboration. In this work, we introduce\nCo-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents\nto improve autonomously by learning from inter-agent interactions without\nexternal supervision. CoMAS generates intrinsic rewards from rich discussion\ndynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and\noptimizes each agent's policy through RL, thereby enabling decentralized and\nscalable co-evolution. Experimental results demonstrate that CoMAS consistently\noutperforms untrained agents and achieves state-of-the-art performance across\nmost evaluation settings. Ablation studies confirm the necessity of\ninteraction-based reward signals and reveal promising scalability as the number\nand diversity of agents increase. These findings establish CoMAS as a novel and\neffective paradigm for self-evolution in LLM-based agents."
                },
                "authors": [
                    {
                        "name": "Xiangyuan Xue"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Zaibin Zhang"
                    },
                    {
                        "name": "Yijiang Li"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Lei Bai"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bai"
                },
                "author": "Lei Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08517v1",
                "updated": "2025-10-09T17:46:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    46,
                    39,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:46:39Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    46,
                    39,
                    3,
                    282,
                    0
                ],
                "title": "CaRT: Teaching LLM Agents to Know When They Know Enough",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaRT: Teaching LLM Agents to Know When They Know Enough"
                },
                "summary": "Many tasks require learned models to strategically gather relevant\ninformation over multiple rounds of interaction before actually acting on a\ntask. Strategic information gathering requires models to know not only how to\neffectively acquire information, but also when to stop gathering information\nand make a decision, in order to avoid overthinking or getting derailed when\nacting. In this paper, we formalize this problem and introduce Counterfactuals\nand Reasoning for Termination (CaRT), an approach for teaching LLMs when to\nstop seeking information. To appropriately learn when to terminate, CaRT\nfine-tunes LLMs using counterfactual pairs of trajectories, one where\ntermination is appropriate and a minimally modified version of the same\ntrajectory where it is not. It trains the LLM to explain the rationale for the\ntermination decision in either case via verbal reasoning, and imbues this\ncapability into the base LLM via fine-tuning. We instantiate CaRT in two\ndomains: interactive medical diagnosis and math problem solving. In both\ndomains, we find that CaRT improves the efficiency of information gathering and\ntask success rate compared to other fine-tuning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many tasks require learned models to strategically gather relevant\ninformation over multiple rounds of interaction before actually acting on a\ntask. Strategic information gathering requires models to know not only how to\neffectively acquire information, but also when to stop gathering information\nand make a decision, in order to avoid overthinking or getting derailed when\nacting. In this paper, we formalize this problem and introduce Counterfactuals\nand Reasoning for Termination (CaRT), an approach for teaching LLMs when to\nstop seeking information. To appropriately learn when to terminate, CaRT\nfine-tunes LLMs using counterfactual pairs of trajectories, one where\ntermination is appropriate and a minimally modified version of the same\ntrajectory where it is not. It trains the LLM to explain the rationale for the\ntermination decision in either case via verbal reasoning, and imbues this\ncapability into the base LLM via fine-tuning. We instantiate CaRT in two\ndomains: interactive medical diagnosis and math problem solving. In both\ndomains, we find that CaRT improves the efficiency of information gathering and\ntask success rate compared to other fine-tuning methods."
                },
                "authors": [
                    {
                        "name": "Grace Liu"
                    },
                    {
                        "name": "Yuxiao Qu"
                    },
                    {
                        "name": "Jeff Schneider"
                    },
                    {
                        "name": "Aarti Singh"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Aviral Kumar"
                },
                "author": "Aviral Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16616v2",
                "updated": "2025-10-09T17:46:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    46,
                    33,
                    3,
                    282,
                    0
                ],
                "published": "2025-06-19T21:27:03Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    21,
                    27,
                    3,
                    3,
                    170,
                    0
                ],
                "title": "LDI: Localized Data Imputation for Text-Rich Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LDI: Localized Data Imputation for Text-Rich Tables"
                },
                "summary": "Missing values are pervasive in real-world tabular data and can significantly\nimpair downstream analysis. Imputing them is especially challenging in\ntext-rich tables, where dependencies are implicit, complex, and dispersed\nacross long textual fields. Recent work has explored using Large Language\nModels (LLMs) for data imputation, yet existing approaches typically process\nentire tables or loosely related contexts, which can compromise accuracy,\nscalability, and explainability. We introduce LDI, a novel framework that\nleverages LLMs through localized reasoning, selecting a compact, contextually\nrelevant subset of attributes and tuples for each missing value. This targeted\nselection reduces noise, improves scalability, and provides transparent\nattribution by revealing which data influenced each prediction. Through\nextensive experiments on real and synthetic datasets, we demonstrate that LDI\nconsistently outperforms state-of-the-art imputation methods, achieving up to\n8% higher accuracy with hosted LLMs and even greater gains with local models.\nThe improved interpretability and robustness also make LDI well-suited for\nhigh-stakes data management applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing values are pervasive in real-world tabular data and can significantly\nimpair downstream analysis. Imputing them is especially challenging in\ntext-rich tables, where dependencies are implicit, complex, and dispersed\nacross long textual fields. Recent work has explored using Large Language\nModels (LLMs) for data imputation, yet existing approaches typically process\nentire tables or loosely related contexts, which can compromise accuracy,\nscalability, and explainability. We introduce LDI, a novel framework that\nleverages LLMs through localized reasoning, selecting a compact, contextually\nrelevant subset of attributes and tuples for each missing value. This targeted\nselection reduces noise, improves scalability, and provides transparent\nattribution by revealing which data influenced each prediction. Through\nextensive experiments on real and synthetic datasets, we demonstrate that LDI\nconsistently outperforms state-of-the-art imputation methods, achieving up to\n8% higher accuracy with hosted LLMs and even greater gains with local models.\nThe improved interpretability and robustness also make LDI well-suited for\nhigh-stakes data management applications."
                },
                "authors": [
                    {
                        "name": "Soroush Omidvartehrani"
                    },
                    {
                        "name": "Davood Rafiei"
                    }
                ],
                "author_detail": {
                    "name": "Davood Rafiei"
                },
                "author": "Davood Rafiei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v2",
                "updated": "2025-10-09T17:45:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    45,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08511v1",
                "updated": "2025-10-09T17:45:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    45,
                    5,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:45:05Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    45,
                    5,
                    3,
                    282,
                    0
                ],
                "title": "AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents"
                },
                "summary": "Large language models (LLMs) have shown impressive performance in general\nprogramming tasks. However, in Machine Learning Engineering (MLE) scenarios\nsuch as AutoML and Kaggle competitions, achieving high performance depends\nheavily on expert intervention and repeated adjustments rather than simply\ngenerating correct code. When applied directly to these tasks, LLMs often lack\nfine-grained domain priors, and existing MLE approaches that use linear or\ntree-structured searches limit knowledge transfer to adjacent hierarchical\nlinks. As a result, they cannot leverage past full trajectories or share\ninformation across branches, limiting self-evolving ability and search space\ndiversity. To address these limitations, we introduce AutoMLGen, an LLM-based\ncoding agent that integrates a domain knowledge base for high-quality prior\nguidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS\nretains the tree-guided exploration of MCTS while embedding a graph structure\ninto the expansion stage to enable dynamic path reorganization, historical\ntrajectory reuse, and multi-solution fusion to support both self-evolution and\ncollaborative learning. Combined with fine-grained operator sets, this design\nimproves stability and accelerates convergence. Evaluation on the MLE-Bench\nshows that AutoMLGen achieves state-of-the-art performance in numerous\ndimensions, such as the average medal rate and the valid submission rate, under\na 12-hour budget (half the standard runtime). The code is available at\nhttps://github.com/Alpha-Innovator/InternAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive performance in general\nprogramming tasks. However, in Machine Learning Engineering (MLE) scenarios\nsuch as AutoML and Kaggle competitions, achieving high performance depends\nheavily on expert intervention and repeated adjustments rather than simply\ngenerating correct code. When applied directly to these tasks, LLMs often lack\nfine-grained domain priors, and existing MLE approaches that use linear or\ntree-structured searches limit knowledge transfer to adjacent hierarchical\nlinks. As a result, they cannot leverage past full trajectories or share\ninformation across branches, limiting self-evolving ability and search space\ndiversity. To address these limitations, we introduce AutoMLGen, an LLM-based\ncoding agent that integrates a domain knowledge base for high-quality prior\nguidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS\nretains the tree-guided exploration of MCTS while embedding a graph structure\ninto the expansion stage to enable dynamic path reorganization, historical\ntrajectory reuse, and multi-solution fusion to support both self-evolution and\ncollaborative learning. Combined with fine-grained operator sets, this design\nimproves stability and accelerates convergence. Evaluation on the MLE-Bench\nshows that AutoMLGen achieves state-of-the-art performance in numerous\ndimensions, such as the average medal rate and the valid submission rate, under\na 12-hour budget (half the standard runtime). The code is available at\nhttps://github.com/Alpha-Innovator/InternAgent."
                },
                "authors": [
                    {
                        "name": "Shangheng Du"
                    },
                    {
                        "name": "Xiangchao Yan"
                    },
                    {
                        "name": "Dengyang Jiang"
                    },
                    {
                        "name": "Jiakang Yuan"
                    },
                    {
                        "name": "Yusong Hu"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Liang He"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Lei Bai"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bai"
                },
                "author": "Lei Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08510v1",
                "updated": "2025-10-09T17:44:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    44,
                    42,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:44:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    44,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "To Sink or Not to Sink: Visual Information Pathways in Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Sink or Not to Sink: Visual Information Pathways in Large\n  Vision-Language Models"
                },
                "summary": "Large Vision Language Models (LVLMs) have recently emerged as powerful\narchitectures capable of understanding and reasoning over both visual and\ntextual information. These models typically rely on two key components: a\nVision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual\ncontent into a sequence of image tokens and serves as the perceptual front-end\n-- the eyes of the model. In contrast, the LLM interprets these tokens to\nperform high-level reasoning, generates responses, and functions as the\ncognitive core -- the brain of the model. However, it remains unclear which\nvisual tokens contribute most significantly to understanding and reasoning, and\nhow effectively these signals are propagated from ViT to the LLM. While most\nexisting works have focused on identifying attention sinks, low-semantic tokens\nreceiving disproportionately high attention, within the LLM, we shift the focus\nto the vision encoder by identifying a class of high-norm visual tokens from\nViT, referred to as ViT attention sinks -- a problem that has been rarely\nstudied but is indeed very important for LVLMs. Our findings show that these\nViT sinks encapsulate high-level semantic concepts from images, allowing the\nLLM to perform more effective understanding and reasoning. Despite their\nimportance, these sink tokens are often overlooked in existing LVLM\narchitectures. To explore their contribution, we present both qualitative and\nquantitative analyses of the information embedded in these sink tokens. We also\npropose both training-free and training-based approaches to better leverage how\nthis information is interpreted by the LLM, and to what extent. By explicitly\nutilizing these tokens, we demonstrate substantial improvements across a range\nof LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT\nattention sinks in enhancing visual reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision Language Models (LVLMs) have recently emerged as powerful\narchitectures capable of understanding and reasoning over both visual and\ntextual information. These models typically rely on two key components: a\nVision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual\ncontent into a sequence of image tokens and serves as the perceptual front-end\n-- the eyes of the model. In contrast, the LLM interprets these tokens to\nperform high-level reasoning, generates responses, and functions as the\ncognitive core -- the brain of the model. However, it remains unclear which\nvisual tokens contribute most significantly to understanding and reasoning, and\nhow effectively these signals are propagated from ViT to the LLM. While most\nexisting works have focused on identifying attention sinks, low-semantic tokens\nreceiving disproportionately high attention, within the LLM, we shift the focus\nto the vision encoder by identifying a class of high-norm visual tokens from\nViT, referred to as ViT attention sinks -- a problem that has been rarely\nstudied but is indeed very important for LVLMs. Our findings show that these\nViT sinks encapsulate high-level semantic concepts from images, allowing the\nLLM to perform more effective understanding and reasoning. Despite their\nimportance, these sink tokens are often overlooked in existing LVLM\narchitectures. To explore their contribution, we present both qualitative and\nquantitative analyses of the information embedded in these sink tokens. We also\npropose both training-free and training-based approaches to better leverage how\nthis information is interpreted by the LLM, and to what extent. By explicitly\nutilizing these tokens, we demonstrate substantial improvements across a range\nof LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT\nattention sinks in enhancing visual reasoning."
                },
                "authors": [
                    {
                        "name": "Jiayun Luo"
                    },
                    {
                        "name": "Wan-Cyuan Fan"
                    },
                    {
                        "name": "Lyuyang Wang"
                    },
                    {
                        "name": "Xiangteng He"
                    },
                    {
                        "name": "Tanzila Rahman"
                    },
                    {
                        "name": "Purang Abolmaesumi"
                    },
                    {
                        "name": "Leonid Sigal"
                    }
                ],
                "author_detail": {
                    "name": "Leonid Sigal"
                },
                "author": "Leonid Sigal",
                "arxiv_comment": "Preprint. Project page: https://davidhalladay.github.io/diysink_demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08508v1",
                "updated": "2025-10-09T17:42:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    42,
                    51,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:42:51Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    42,
                    51,
                    3,
                    282,
                    0
                ],
                "title": "MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration"
                },
                "summary": "Real-world videos often suffer from complex degradations, such as noise,\ncompression artifacts, and low-light distortions, due to diverse acquisition\nand transmission conditions. Existing restoration methods typically require\nprofessional manual selection of specialized models or rely on monolithic\narchitectures that fail to generalize across varying degradations. Inspired by\nexpert experience, we propose MoA-VR, the first\n\\underline{M}ixture-\\underline{o}f-\\underline{A}gents \\underline{V}ideo\n\\underline{R}estoration system that mimics the reasoning and processing\nprocedures of human professionals through three coordinated agents: Degradation\nIdentification, Routing and Restoration, and Restoration Quality Assessment.\nSpecifically, we construct a large-scale and high-resolution video degradation\nrecognition benchmark and build a vision-language model (VLM) driven\ndegradation identifier. We further introduce a self-adaptive router powered by\nlarge language models (LLMs), which autonomously learns effective restoration\nstrategies by observing tool usage patterns. To assess intermediate and final\nprocessed video quality, we construct the \\underline{Res}tored\n\\underline{V}ideo \\underline{Q}uality (Res-VQ) dataset and design a dedicated\nVLM-based video quality assessment (VQA) model tailored for restoration tasks.\nExtensive experiments demonstrate that MoA-VR effectively handles diverse and\ncompound degradations, consistently outperforming existing baselines in terms\nof both objective metrics and perceptual quality. These results highlight the\npotential of integrating multimodal intelligence and modular reasoning in\ngeneral-purpose video restoration systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world videos often suffer from complex degradations, such as noise,\ncompression artifacts, and low-light distortions, due to diverse acquisition\nand transmission conditions. Existing restoration methods typically require\nprofessional manual selection of specialized models or rely on monolithic\narchitectures that fail to generalize across varying degradations. Inspired by\nexpert experience, we propose MoA-VR, the first\n\\underline{M}ixture-\\underline{o}f-\\underline{A}gents \\underline{V}ideo\n\\underline{R}estoration system that mimics the reasoning and processing\nprocedures of human professionals through three coordinated agents: Degradation\nIdentification, Routing and Restoration, and Restoration Quality Assessment.\nSpecifically, we construct a large-scale and high-resolution video degradation\nrecognition benchmark and build a vision-language model (VLM) driven\ndegradation identifier. We further introduce a self-adaptive router powered by\nlarge language models (LLMs), which autonomously learns effective restoration\nstrategies by observing tool usage patterns. To assess intermediate and final\nprocessed video quality, we construct the \\underline{Res}tored\n\\underline{V}ideo \\underline{Q}uality (Res-VQ) dataset and design a dedicated\nVLM-based video quality assessment (VQA) model tailored for restoration tasks.\nExtensive experiments demonstrate that MoA-VR effectively handles diverse and\ncompound degradations, consistently outperforming existing baselines in terms\nof both objective metrics and perceptual quality. These results highlight the\npotential of integrating multimodal intelligence and modular reasoning in\ngeneral-purpose video restoration systems."
                },
                "authors": [
                    {
                        "name": "Lu Liu"
                    },
                    {
                        "name": "Chunlei Cai"
                    },
                    {
                        "name": "Shaocheng Shen"
                    },
                    {
                        "name": "Jianfeng Liang"
                    },
                    {
                        "name": "Weimin Ouyang"
                    },
                    {
                        "name": "Tianxiao Ye"
                    },
                    {
                        "name": "Jian Mao"
                    },
                    {
                        "name": "Huiyu Duan"
                    },
                    {
                        "name": "Jiangchao Yao"
                    },
                    {
                        "name": "Xiaoyun Zhang"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Guangtao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhai"
                },
                "author": "Guangtao Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08506v1",
                "updated": "2025-10-09T17:41:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    41,
                    57,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:41:57Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    41,
                    57,
                    3,
                    282,
                    0
                ],
                "title": "Neologism Learning for Controllability and Self-Verbalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neologism Learning for Controllability and Self-Verbalization"
                },
                "summary": "Humans invent new words when there is a rising demand for a new useful\nconcept (e.g., doomscrolling). We explore and validate a similar idea in our\ncommunication with LLMs: introducing new words to better understand and control\nthe models, expanding on the recently introduced neologism learning. This\nmethod introduces a new word by adding a new word embedding and training with\nexamples that exhibit the concept with no other changes in model parameters. We\nshow that adding a new word allows for control of concepts such as flattery,\nincorrect answers, text length, as well as more complex concepts in AxBench. We\ndiscover that neologisms can also further our understanding of the model via\nself-verbalization: models can describe what each new word means to them in\nnatural language, like explaining that a word that represents a concept of\nincorrect answers means ``a lack of complete, coherent, or meaningful\nanswers...'' To validate self-verbalizations, we introduce plug-in evaluation:\nwe insert the verbalization into the context of a model and measure whether it\ncontrols the target concept. In some self-verbalizations, we find machine-only\nsynonyms: words that seem unrelated to humans but cause similar behavior in\nmachines. Finally, we show how neologism learning can jointly learn multiple\nconcepts in multiple words.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans invent new words when there is a rising demand for a new useful\nconcept (e.g., doomscrolling). We explore and validate a similar idea in our\ncommunication with LLMs: introducing new words to better understand and control\nthe models, expanding on the recently introduced neologism learning. This\nmethod introduces a new word by adding a new word embedding and training with\nexamples that exhibit the concept with no other changes in model parameters. We\nshow that adding a new word allows for control of concepts such as flattery,\nincorrect answers, text length, as well as more complex concepts in AxBench. We\ndiscover that neologisms can also further our understanding of the model via\nself-verbalization: models can describe what each new word means to them in\nnatural language, like explaining that a word that represents a concept of\nincorrect answers means ``a lack of complete, coherent, or meaningful\nanswers...'' To validate self-verbalizations, we introduce plug-in evaluation:\nwe insert the verbalization into the context of a model and measure whether it\ncontrols the target concept. In some self-verbalizations, we find machine-only\nsynonyms: words that seem unrelated to humans but cause similar behavior in\nmachines. Finally, we show how neologism learning can jointly learn multiple\nconcepts in multiple words."
                },
                "authors": [
                    {
                        "name": "John Hewitt"
                    },
                    {
                        "name": "Oyvind Tafjord"
                    },
                    {
                        "name": "Robert Geirhos"
                    },
                    {
                        "name": "Been Kim"
                    }
                ],
                "author_detail": {
                    "name": "Been Kim"
                },
                "author": "Been Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20600v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20600v4",
                "updated": "2025-10-09T17:41:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    41,
                    24,
                    3,
                    282,
                    0
                ],
                "published": "2024-10-27T21:20:18Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    21,
                    20,
                    18,
                    6,
                    301,
                    0
                ],
                "title": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way\n  Intelligibility Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Turn Human-LLM Interaction Through the Lens of a Two-Way\n  Intelligibility Protocol"
                },
                "summary": "Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems. Our code is available\nat https://github.com/karannb/interact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our interest is in the design of software systems involving a human-expert\ninteracting -- using natural language -- with a large language model (LLM) on\ndata analysis tasks. For complex problems, it is possible that LLMs can harness\nhuman expertise and creativity to find solutions that were otherwise elusive.\nOn one level, this interaction takes place through multiple turns of prompts\nfrom the human and responses from the LLM. Here we investigate a more\nstructured approach based on an abstract protocol described in [3] for\ninteraction between agents. The protocol is motivated by a notion of \"two-way\nintelligibility\" and is modelled by a pair of communicating finite-state\nmachines. We provide an implementation of the protocol, and provide empirical\nevidence of using the implementation to mediate interactions between an LLM and\na human-agent in two areas of scientific interest (radiology and drug design).\nWe conduct controlled experiments with a human proxy (a database), and\nuncontrolled experiments with human subjects. The results provide evidence in\nsupport of the protocol's capability of capturing one- and two-way\nintelligibility in human-LLM interaction; and for the utility of two-way\nintelligibility in the design of human-machine systems. Our code is available\nat https://github.com/karannb/interact."
                },
                "authors": [
                    {
                        "name": "Harshvardhan Mestha"
                    },
                    {
                        "name": "Karan Bania"
                    },
                    {
                        "name": "Shreyas V Sathyanarayana"
                    },
                    {
                        "name": "Sidong Liu"
                    },
                    {
                        "name": "Ashwin Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Ashwin Srinivasan"
                },
                "author": "Ashwin Srinivasan",
                "arxiv_comment": "Multi-Turn Interactions in Large Language Models (MTI-LLM) Workshop\n  at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20600v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20600v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18114v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18114v2",
                "updated": "2025-10-09T17:39:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    39,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-04-25T06:37:29Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    6,
                    37,
                    29,
                    4,
                    115,
                    0
                ],
                "title": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection"
                },
                "summary": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them."
                },
                "authors": [
                    {
                        "name": "Atharva Kulkarni"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Joel Ruben Antony Moniz"
                    },
                    {
                        "name": "Xiou Ge"
                    },
                    {
                        "name": "Bo-Hsiang Tseng"
                    },
                    {
                        "name": "Dhivya Piraviperumal"
                    },
                    {
                        "name": "Swabha Swayamdipta"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "Accepted at EMNLP 2025 Findings (Short)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18114v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18114v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v2",
                "updated": "2025-10-09T17:38:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    38,
                    52,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "arxiv_comment": "Original version uploaded on Sep 22, 2025. (v2): Extended Table 2\n  with additional analysis and referenced it in Sec 5.2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11194v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11194v3",
                "updated": "2025-10-09T17:32:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    32,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2024-02-17T05:10:18Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    5,
                    10,
                    18,
                    5,
                    48,
                    0
                ],
                "title": "Evaluating LLMs' Mathematical Reasoning in Financial Document Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs' Mathematical Reasoning in Financial Document Question\n  Answering"
                },
                "summary": "Large Language Models (LLMs), excel in natural language understanding, but\ntheir capability for complex mathematical reasoning with an amalgamation of\nstructured tables and unstructured text is uncertain. This study explores LLMs'\nmathematical reasoning on four financial tabular question-answering datasets:\nTATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with\nvarious models and prompting techniques, we assess how LLMs adapt to complex\ntables and mathematical tasks. We focus on sensitivity to table complexity and\nperformance variations with an increasing number of arithmetic reasoning steps.\nThe results provide insights into LLMs' capabilities and limitations in\nhandling complex mathematical scenarios for semi-structured tables. Ultimately,\nwe introduce a novel prompting technique tailored to semi-structured documents,\nmatching or outperforming other baselines in performance while providing a\nnuanced understanding of LLMs abilities for such a task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), excel in natural language understanding, but\ntheir capability for complex mathematical reasoning with an amalgamation of\nstructured tables and unstructured text is uncertain. This study explores LLMs'\nmathematical reasoning on four financial tabular question-answering datasets:\nTATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with\nvarious models and prompting techniques, we assess how LLMs adapt to complex\ntables and mathematical tasks. We focus on sensitivity to table complexity and\nperformance variations with an increasing number of arithmetic reasoning steps.\nThe results provide insights into LLMs' capabilities and limitations in\nhandling complex mathematical scenarios for semi-structured tables. Ultimately,\nwe introduce a novel prompting technique tailored to semi-structured documents,\nmatching or outperforming other baselines in performance while providing a\nnuanced understanding of LLMs abilities for such a task."
                },
                "authors": [
                    {
                        "name": "Pragya Srivastava"
                    },
                    {
                        "name": "Manuj Malik"
                    },
                    {
                        "name": "Vivek Gupta"
                    },
                    {
                        "name": "Tanuja Ganu"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "arxiv_comment": "26 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11194v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11194v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08489v1",
                "updated": "2025-10-09T17:30:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    30,
                    1,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:30:01Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    30,
                    1,
                    3,
                    282,
                    0
                ],
                "title": "Implementing Semantic Join Operators Efficiently",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing Semantic Join Operators Efficiently"
                },
                "summary": "Semantic query processing engines often support semantic joins, enabling\nusers to match rows that satisfy conditions specified in natural language. Such\njoin conditions can be evaluated using large language models (LLMs) that solve\nnovel tasks without task-specific training.\n  Currently, many semantic query processing engines implement semantic joins\nvia nested loops, invoking the LLM to evaluate the join condition on row pairs.\nInstead, this paper proposes a novel algorithm, inspired by the block nested\nloops join operator implementation in traditional database systems. The\nproposed algorithm integrates batches of rows from both input tables into a\nsingle prompt. The goal of the LLM invocation is to identify all matching row\npairs in the current input. The paper introduces formulas that can be used to\noptimize the size of the row batches, taking into account constraints on the\nsize of the LLM context window (limiting both input and output size). An\nadaptive variant of the proposed algorithm refers to cases in which the size of\nthe output is difficult to estimate. A formal analysis of asymptotic processing\ncosts, as well as empirical results, demonstrates that the proposed approach\nreduces costs significantly and performs well compared to join implementations\nused by recent semantic query processing engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic query processing engines often support semantic joins, enabling\nusers to match rows that satisfy conditions specified in natural language. Such\njoin conditions can be evaluated using large language models (LLMs) that solve\nnovel tasks without task-specific training.\n  Currently, many semantic query processing engines implement semantic joins\nvia nested loops, invoking the LLM to evaluate the join condition on row pairs.\nInstead, this paper proposes a novel algorithm, inspired by the block nested\nloops join operator implementation in traditional database systems. The\nproposed algorithm integrates batches of rows from both input tables into a\nsingle prompt. The goal of the LLM invocation is to identify all matching row\npairs in the current input. The paper introduces formulas that can be used to\noptimize the size of the row batches, taking into account constraints on the\nsize of the LLM context window (limiting both input and output size). An\nadaptive variant of the proposed algorithm refers to cases in which the size of\nthe output is difficult to estimate. A formal analysis of asymptotic processing\ncosts, as well as empirical results, demonstrates that the proposed approach\nreduces costs significantly and performs well compared to join implementations\nused by recent semantic query processing engines."
                },
                "authors": [
                    {
                        "name": "Immanuel Trummer"
                    }
                ],
                "author_detail": {
                    "name": "Immanuel Trummer"
                },
                "author": "Immanuel Trummer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23752v2",
                "updated": "2025-10-09T17:29:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    29,
                    59,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-29T17:59:38Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    38,
                    3,
                    149,
                    0
                ],
                "title": "ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks"
                },
                "summary": "Recent progress in large language models (LLMs) has enabled tool-augmented\nagents capable of solving complex real-world tasks through step-by-step\nreasoning. However, existing evaluations often focus on general-purpose or\nmultimodal scenarios, leaving a gap in domain-specific benchmarks that assess\ntool-use capabilities in complex remote sensing use cases. We present ThinkGeo,\nan agentic benchmark designed to evaluate LLM-driven agents on remote sensing\ntasks via structured tool use and multi-step planning. Inspired by\ntool-interaction paradigms, ThinkGeo includes human-curated queries spanning a\nwide range of real-world applications such as urban planning, disaster\nassessment and change analysis, environmental monitoring, transportation\nanalysis, aviation monitoring, recreational infrastructure, and industrial site\nanalysis. Queries are grounded in satellite or aerial imagery, including both\noptical RGB and SAR data, and require agents to reason through a diverse\ntoolset. We implement a ReAct-style interaction loop and evaluate both open and\nclosed-source LLMs (e.g., GPT-4o, Qwen2.5) on 486 structured agentic tasks with\n1,773 expert-verified reasoning steps. The benchmark reports both step-wise\nexecution metrics and final answer correctness. Our analysis reveals notable\ndisparities in tool accuracy and planning consistency across models. ThinkGeo\nprovides the first extensive testbed for evaluating how tool-enabled LLMs\nhandle spatial reasoning in remote sensing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has enabled tool-augmented\nagents capable of solving complex real-world tasks through step-by-step\nreasoning. However, existing evaluations often focus on general-purpose or\nmultimodal scenarios, leaving a gap in domain-specific benchmarks that assess\ntool-use capabilities in complex remote sensing use cases. We present ThinkGeo,\nan agentic benchmark designed to evaluate LLM-driven agents on remote sensing\ntasks via structured tool use and multi-step planning. Inspired by\ntool-interaction paradigms, ThinkGeo includes human-curated queries spanning a\nwide range of real-world applications such as urban planning, disaster\nassessment and change analysis, environmental monitoring, transportation\nanalysis, aviation monitoring, recreational infrastructure, and industrial site\nanalysis. Queries are grounded in satellite or aerial imagery, including both\noptical RGB and SAR data, and require agents to reason through a diverse\ntoolset. We implement a ReAct-style interaction loop and evaluate both open and\nclosed-source LLMs (e.g., GPT-4o, Qwen2.5) on 486 structured agentic tasks with\n1,773 expert-verified reasoning steps. The benchmark reports both step-wise\nexecution metrics and final answer correctness. Our analysis reveals notable\ndisparities in tool accuracy and planning consistency across models. ThinkGeo\nprovides the first extensive testbed for evaluating how tool-enabled LLMs\nhandle spatial reasoning in remote sensing."
                },
                "authors": [
                    {
                        "name": "Akashah Shabbir"
                    },
                    {
                        "name": "Muhammad Akhtar Munir"
                    },
                    {
                        "name": "Akshay Dudhane"
                    },
                    {
                        "name": "Muhammad Umer Sheikh"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "name": "Paolo Fraccaro"
                    },
                    {
                        "name": "Juan Bernabe Moreno"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    },
                    {
                        "name": "Salman Khan"
                    }
                ],
                "author_detail": {
                    "name": "Salman Khan"
                },
                "author": "Salman Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08483v1",
                "updated": "2025-10-09T17:24:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    24,
                    54,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:24:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    24,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "DeepPrune: Parallel Scaling without Inter-trace Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepPrune: Parallel Scaling without Inter-trace Redundancy"
                },
                "summary": "Parallel scaling has emerged as a powerful paradigm to enhance reasoning\ncapabilities in large language models (LLMs) by generating multiple\nChain-of-Thought (CoT) traces simultaneously. However, this approach introduces\nsignificant computational inefficiency due to inter-trace redundancy -- our\nanalysis reveals that over 80% of parallel reasoning traces yield identical\nfinal answers, representing substantial wasted computation. To address this\ncritical efficiency bottleneck, we propose DeepPrune, a novel framework that\nenables efficient parallel scaling through dynamic pruning. Our method features\na specialized judge model trained with focal loss and oversampling techniques\nto accurately predict answer equivalence from partial reasoning traces which\nrealizes 0.87 AUROC on equivalence prediction, combined with an online greedy\nclustering algorithm that dynamically prunes redundant paths while preserving\nanswer diversity. Comprehensive evaluations across three challenging benchmarks\n(AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that\nDeepPrune achieves remarkable token reduction by over 80% compared to\nconventional consensus sampling on most cases, while maintaining competitive\naccuracy within 3 percentage points. Our work establishes a new standard for\nefficient parallel reasoning, making high-performance reasoning more efficient.\nOur code and data are here: https://deepprune.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel scaling has emerged as a powerful paradigm to enhance reasoning\ncapabilities in large language models (LLMs) by generating multiple\nChain-of-Thought (CoT) traces simultaneously. However, this approach introduces\nsignificant computational inefficiency due to inter-trace redundancy -- our\nanalysis reveals that over 80% of parallel reasoning traces yield identical\nfinal answers, representing substantial wasted computation. To address this\ncritical efficiency bottleneck, we propose DeepPrune, a novel framework that\nenables efficient parallel scaling through dynamic pruning. Our method features\na specialized judge model trained with focal loss and oversampling techniques\nto accurately predict answer equivalence from partial reasoning traces which\nrealizes 0.87 AUROC on equivalence prediction, combined with an online greedy\nclustering algorithm that dynamically prunes redundant paths while preserving\nanswer diversity. Comprehensive evaluations across three challenging benchmarks\n(AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that\nDeepPrune achieves remarkable token reduction by over 80% compared to\nconventional consensus sampling on most cases, while maintaining competitive\naccuracy within 3 percentage points. Our work establishes a new standard for\nefficient parallel reasoning, making high-performance reasoning more efficient.\nOur code and data are here: https://deepprune.github.io/"
                },
                "authors": [
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Yaxuan Li"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "15 pages, 4 figures, please check out the project page:\n  https://deepprune.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08481v1",
                "updated": "2025-10-09T17:20:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    20,
                    54,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:20:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    20,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "Forecasting the Buzz: Enriching Hashtag Popularity Prediction with LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting the Buzz: Enriching Hashtag Popularity Prediction with LLM\n  Reasoning"
                },
                "summary": "Hashtag trends ignite campaigns, shift public opinion, and steer millions of\ndollars in advertising spend, yet forecasting which tag goes viral is elusive.\nClassical regressors digest surface features but ignore context, while large\nlanguage models (LLMs) excel at contextual reasoning but misestimate numbers.\nWe present BuzzProphet, a reasoning-augmented hashtag popularity prediction\nframework that (1) instructs an LLM to articulate a hashtag's topical virality,\naudience reach, and timing advantage; (2) utilizes these popularity-oriented\nrationales to enrich the input features; and (3) regresses on these inputs. To\nfacilitate evaluation, we release HashView, a 7,532-hashtag benchmark curated\nfrom social media. Across diverse regressor-LLM combinations, BuzzProphet\nreduces RMSE by up to 2.8% and boosts correlation by 30% over baselines, while\nproducing human-readable rationales. Results demonstrate that using LLMs as\ncontext reasoners rather than numeric predictors injects domain insight into\ntabular models, yielding an interpretable and deployable solution for social\nmedia trend forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hashtag trends ignite campaigns, shift public opinion, and steer millions of\ndollars in advertising spend, yet forecasting which tag goes viral is elusive.\nClassical regressors digest surface features but ignore context, while large\nlanguage models (LLMs) excel at contextual reasoning but misestimate numbers.\nWe present BuzzProphet, a reasoning-augmented hashtag popularity prediction\nframework that (1) instructs an LLM to articulate a hashtag's topical virality,\naudience reach, and timing advantage; (2) utilizes these popularity-oriented\nrationales to enrich the input features; and (3) regresses on these inputs. To\nfacilitate evaluation, we release HashView, a 7,532-hashtag benchmark curated\nfrom social media. Across diverse regressor-LLM combinations, BuzzProphet\nreduces RMSE by up to 2.8% and boosts correlation by 30% over baselines, while\nproducing human-readable rationales. Results demonstrate that using LLMs as\ncontext reasoners rather than numeric predictors injects domain insight into\ntabular models, yielding an interpretable and deployable solution for social\nmedia trend forecasting."
                },
                "authors": [
                    {
                        "name": "Yifei Xu"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Herun Wan"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Zhen Hou"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "arxiv_doi": "10.1145/3746252.3760970",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3760970",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.08481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10190v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10190v3",
                "updated": "2025-10-09T17:20:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    20,
                    18,
                    3,
                    282,
                    0
                ],
                "published": "2024-10-14T06:22:11Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    6,
                    22,
                    11,
                    0,
                    288,
                    0
                ],
                "title": "Language Model Embeddings Can Be Sufficient for Bayesian Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model Embeddings Can Be Sufficient for Bayesian Optimization"
                },
                "summary": "Bayesian Optimization is ubiquitous in experimental design and black-box\noptimization for improving search efficiency. However, most existing approaches\nrely on regression models which are limited to fixed search spaces and\nstructured, tabular input features. This paper explores the use of LLM\nembeddings over string inputs for in-context regression in Bayesian\nOptimization. Our results show that representing inputs as strings enables\ngeneral-purpose regression across diverse domains, including synthetic,\ncombinatorial, and hyperparameter optimization. Furthermore, our approach\nachieves optimization performance comparable to state-of-the-art Gaussian\nProcess-based methods such as Google Vizier, and demonstrates potential for\nbroader and more flexible applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Optimization is ubiquitous in experimental design and black-box\noptimization for improving search efficiency. However, most existing approaches\nrely on regression models which are limited to fixed search spaces and\nstructured, tabular input features. This paper explores the use of LLM\nembeddings over string inputs for in-context regression in Bayesian\nOptimization. Our results show that representing inputs as strings enables\ngeneral-purpose regression across diverse domains, including synthetic,\ncombinatorial, and hyperparameter optimization. Furthermore, our approach\nachieves optimization performance comparable to state-of-the-art Gaussian\nProcess-based methods such as Google Vizier, and demonstrates potential for\nbroader and more flexible applications."
                },
                "authors": [
                    {
                        "name": "Tung Nguyen"
                    },
                    {
                        "name": "Qiuyi Zhang"
                    },
                    {
                        "name": "Bangding Yang"
                    },
                    {
                        "name": "Chansoo Lee"
                    },
                    {
                        "name": "Jorg Bornschein"
                    },
                    {
                        "name": "Yingjie Miao"
                    },
                    {
                        "name": "Sagi Perel"
                    },
                    {
                        "name": "Yutian Chen"
                    },
                    {
                        "name": "Xingyou Song"
                    }
                ],
                "author_detail": {
                    "name": "Xingyou Song"
                },
                "author": "Xingyou Song",
                "arxiv_comment": "Code can be found in\n  https://github.com/google-research/optformer/tree/main/optformer/embed_then_regress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10190v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10190v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02227v2",
                "updated": "2025-10-09T17:18:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    18,
                    49,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-02T17:14:00Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    17,
                    14,
                    0,
                    3,
                    275,
                    0
                ],
                "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for\n  Diverse Exploration"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm\nfor enhancing the reasoning ability in Large Language Models (LLMs). However,\nprevailing methods primarily rely on self-exploration or a single off-policy\nteacher to elicit long chain-of-thought (LongCoT) reasoning, which may\nintroduce intrinsic model biases and restrict exploration, ultimately limiting\nreasoning diversity and performance. Drawing inspiration from multi-teacher\nstrategies in knowledge distillation, we introduce Adaptive Multi-Guidance\nPolicy Optimization (AMPO), a novel framework that adaptively leverages\nguidance from multiple proficient teacher models, but only when the on-policy\nmodel fails to generate correct solutions. This \"guidance-on-demand\" approach\nexpands exploration while preserving the value of self-discovery. Moreover,\nAMPO incorporates a comprehension-based selection mechanism, prompting the\nstudent to learn from the reasoning paths that it is most likely to comprehend,\nthus balancing broad exploration with effective exploitation. Extensive\nexperiments show AMPO substantially outperforms a strong baseline (GRPO), with\na 4.3% improvement on mathematical reasoning tasks and 12.2% on\nout-of-distribution tasks, while significantly boosting Pass@k performance and\nenabling more diverse exploration. Notably, using four peer-sized teachers, our\nmethod achieves comparable results to approaches that leverage a single, more\npowerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate\na more efficient and scalable path to superior reasoning and generalizability.\nOur code is available at https://github.com/SII-Enigma/AMPO."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Yuan"
                    },
                    {
                        "name": "Yujuan Ding"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Jinyu Cai"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Heng Tao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Heng Tao Shen"
                },
                "author": "Heng Tao Shen",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.02227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.23045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.23045v2",
                "updated": "2025-10-09T17:13:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    13,
                    2,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-27T01:49:13Z",
                "published_parsed": [
                    2025,
                    9,
                    27,
                    1,
                    49,
                    13,
                    5,
                    270,
                    0
                ],
                "title": "Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied to software engineering\n(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent\nframeworks with multi-turn interactions and workflow-based Agentless methods\nwith single-turn verifiable steps. We argue these paradigms are not mutually\nexclusive: reasoning-intensive Agentless training induces skill priors,\nincluding localization, code edit, and self-reflection that enable efficient\nand effective SWE-Agent adaptation. In this work, we first curate the Agentless\ntraining recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\\%\non SWE-bench Verified, the best among workflow approaches. With additional SFT\nadaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to\n48.6\\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These\nresults show that structured skill priors from Agentless training can bridge\nworkflow and agentic frameworks for transferable coding agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied to software engineering\n(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent\nframeworks with multi-turn interactions and workflow-based Agentless methods\nwith single-turn verifiable steps. We argue these paradigms are not mutually\nexclusive: reasoning-intensive Agentless training induces skill priors,\nincluding localization, code edit, and self-reflection that enable efficient\nand effective SWE-Agent adaptation. In this work, we first curate the Agentless\ntraining recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\\%\non SWE-bench Verified, the best among workflow approaches. With additional SFT\nadaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to\n48.6\\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These\nresults show that structured skill priors from Agentless training can bridge\nworkflow and agentic frameworks for transferable coding agents."
                },
                "authors": [
                    {
                        "name": "Zonghan Yang"
                    },
                    {
                        "name": "Shengjie Wang"
                    },
                    {
                        "name": "Kelin Fu"
                    },
                    {
                        "name": "Wenyang He"
                    },
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Yanhao Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Zhenxing Hu"
                    },
                    {
                        "name": "Kaitai Zhang"
                    },
                    {
                        "name": "Shuyi Wang"
                    },
                    {
                        "name": "Huarong Chen"
                    },
                    {
                        "name": "Flood Sung"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Tianyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Liu"
                },
                "author": "Tianyu Liu",
                "arxiv_comment": "58 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.23045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.23045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14756v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14756v2",
                "updated": "2025-10-09T17:09:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    9,
                    12,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-20T15:54:48Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    54,
                    48,
                    1,
                    140,
                    0
                ],
                "title": "LLINBO: Trustworthy LLM-in-the-Loop Bayesian Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLINBO: Trustworthy LLM-in-the-Loop Bayesian Optimization"
                },
                "summary": "Bayesian optimization (BO) is a sequential decision-making tool widely used\nfor optimizing expensive black-box functions. Recently, Large Language Models\n(LLMs) have shown remarkable adaptability in low-data regimes, making them\npromising tools for black-box optimization by leveraging contextual knowledge\nto propose high-quality query points. However, relying solely on LLMs as\noptimization agents introduces risks due to their lack of explicit surrogate\nmodeling and calibrated uncertainty, as well as their inherently opaque\ninternal mechanisms. This structural opacity makes it difficult to characterize\nor control the exploration-exploitation trade-off, ultimately undermining\ntheoretical tractability and reliability. To address this, we propose LLINBO:\nLLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with\nstatistical surrogate experts (e.g., Gaussian Processes (GP)). The core\nphilosophy is to leverage contextual reasoning strengths of LLMs for early\nexploration, while relying on principled statistical models to guide efficient\nexploitation. Specifically, we introduce three mechanisms that enable this\ncollaboration and establish their theoretical guarantees. We end the paper with\na real-life proof-of-concept in the context of 3D printing. The code to\nreproduce the results can be found at\nhttps://github.com/UMDataScienceLab/LLM-in-the-Loop-BO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian optimization (BO) is a sequential decision-making tool widely used\nfor optimizing expensive black-box functions. Recently, Large Language Models\n(LLMs) have shown remarkable adaptability in low-data regimes, making them\npromising tools for black-box optimization by leveraging contextual knowledge\nto propose high-quality query points. However, relying solely on LLMs as\noptimization agents introduces risks due to their lack of explicit surrogate\nmodeling and calibrated uncertainty, as well as their inherently opaque\ninternal mechanisms. This structural opacity makes it difficult to characterize\nor control the exploration-exploitation trade-off, ultimately undermining\ntheoretical tractability and reliability. To address this, we propose LLINBO:\nLLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with\nstatistical surrogate experts (e.g., Gaussian Processes (GP)). The core\nphilosophy is to leverage contextual reasoning strengths of LLMs for early\nexploration, while relying on principled statistical models to guide efficient\nexploitation. Specifically, we introduce three mechanisms that enable this\ncollaboration and establish their theoretical guarantees. We end the paper with\na real-life proof-of-concept in the context of 3D printing. The code to\nreproduce the results can be found at\nhttps://github.com/UMDataScienceLab/LLM-in-the-Loop-BO."
                },
                "authors": [
                    {
                        "name": "Chih-Yu Chang"
                    },
                    {
                        "name": "Milad Azvar"
                    },
                    {
                        "name": "Chinedum Okwudire"
                    },
                    {
                        "name": "Raed Al Kontar"
                    }
                ],
                "author_detail": {
                    "name": "Raed Al Kontar"
                },
                "author": "Raed Al Kontar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14756v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14756v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08827v3",
                "updated": "2025-10-09T17:08:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    8,
                    52,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-10T17:59:43Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    43,
                    2,
                    253,
                    0
                ],
                "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Reinforcement Learning for Large Reasoning Models"
                },
                "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs"
                },
                "authors": [
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Kai Tian"
                    },
                    {
                        "name": "Guoli Jia"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Sihang Zeng"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Haozhan Li"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Yuru Wang"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Fangfu Liu"
                    },
                    {
                        "name": "Xiang Xu"
                    },
                    {
                        "name": "Jiaze Ma"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Zonglin Li"
                    },
                    {
                        "name": "Huayu Chen"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Zhenzhao Yuan"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "Fixed typos; added missing and recent citations (117 -> 120 pages)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08466v1",
                "updated": "2025-10-09T17:07:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    7,
                    55,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:07:55Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    7,
                    55,
                    3,
                    282,
                    0
                ],
                "title": "In-Context Clustering with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Clustering with Large Language Models"
                },
                "summary": "We propose In-Context Clustering (ICC), a flexible LLM-based procedure for\nclustering data from diverse distributions. Unlike traditional clustering\nalgorithms constrained by predefined similarity measures, ICC flexibly captures\ncomplex relationships among inputs through an attention mechanism. We show that\npretrained LLMs exhibit impressive zero-shot clustering capabilities on\ntext-encoded numeric data, with attention matrices showing salient cluster\npatterns. Spectral clustering using attention matrices offers surprisingly\ncompetitive performance. We further enhance the clustering capabilities of LLMs\non numeric and image data through fine-tuning using the Next Token Prediction\n(NTP) loss. Moreover, the flexibility of LLM prompting enables text-conditioned\nimage clustering, a capability that classical clustering methods lack. Our work\nextends in-context learning to an unsupervised setting, showcasing the\neffectiveness and flexibility of LLMs for clustering. Our code is available at\nhttps://agenticlearning.ai/icc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose In-Context Clustering (ICC), a flexible LLM-based procedure for\nclustering data from diverse distributions. Unlike traditional clustering\nalgorithms constrained by predefined similarity measures, ICC flexibly captures\ncomplex relationships among inputs through an attention mechanism. We show that\npretrained LLMs exhibit impressive zero-shot clustering capabilities on\ntext-encoded numeric data, with attention matrices showing salient cluster\npatterns. Spectral clustering using attention matrices offers surprisingly\ncompetitive performance. We further enhance the clustering capabilities of LLMs\non numeric and image data through fine-tuning using the Next Token Prediction\n(NTP) loss. Moreover, the flexibility of LLM prompting enables text-conditioned\nimage clustering, a capability that classical clustering methods lack. Our work\nextends in-context learning to an unsupervised setting, showcasing the\neffectiveness and flexibility of LLMs for clustering. Our code is available at\nhttps://agenticlearning.ai/icc."
                },
                "authors": [
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Mengye Ren"
                    },
                    {
                        "name": "Andrew Gordon Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Gordon Wilson"
                },
                "author": "Andrew Gordon Wilson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08464v1",
                "updated": "2025-10-09T17:07:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    7,
                    30,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:07:30Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    7,
                    30,
                    3,
                    282,
                    0
                ],
                "title": "Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be\n  Recovered",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be\n  Recovered"
                },
                "summary": "Vision-Language-Action (VLA) models have advanced robotic capabilities but\nremain challenging to deploy on resource-limited hardware. Pruning has enabled\nefficient compression of large language models (LLMs), yet it is largely\nunderstudied in robotics. Surprisingly, we observe that pruning VLA models\nleads to drastic degradation and increased safety violations. We introduce\nGLUESTICK, a post-pruning recovery method that restores much of the original\nmodel's functionality while retaining sparsity benefits. Our method performs a\none-time interpolation between the dense and pruned models in weight-space to\ncompute a corrective term. This correction is used during inference by each\npruned layer to recover lost capabilities with minimal overhead. GLUESTICK\nrequires no additional training, is agnostic to the pruning algorithm, and\nintroduces a single hyperparameter that controls the tradeoff between\nefficiency and accuracy. Across diverse VLA architectures and tasks in\nmanipulation and navigation, GLUESTICK achieves competitive memory efficiency\nwhile substantially recovering success rates and reducing safety violations.\nAdditional material can be found at: https://gluestick-vla.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have advanced robotic capabilities but\nremain challenging to deploy on resource-limited hardware. Pruning has enabled\nefficient compression of large language models (LLMs), yet it is largely\nunderstudied in robotics. Surprisingly, we observe that pruning VLA models\nleads to drastic degradation and increased safety violations. We introduce\nGLUESTICK, a post-pruning recovery method that restores much of the original\nmodel's functionality while retaining sparsity benefits. Our method performs a\none-time interpolation between the dense and pruned models in weight-space to\ncompute a corrective term. This correction is used during inference by each\npruned layer to recover lost capabilities with minimal overhead. GLUESTICK\nrequires no additional training, is agnostic to the pruning algorithm, and\nintroduces a single hyperparameter that controls the tradeoff between\nefficiency and accuracy. Across diverse VLA architectures and tasks in\nmanipulation and navigation, GLUESTICK achieves competitive memory efficiency\nwhile substantially recovering success rates and reducing safety violations.\nAdditional material can be found at: https://gluestick-vla.github.io/."
                },
                "authors": [
                    {
                        "name": "Jason Jabbour"
                    },
                    {
                        "name": "Dong-Ki Kim"
                    },
                    {
                        "name": "Max Smith"
                    },
                    {
                        "name": "Jay Patrikar"
                    },
                    {
                        "name": "Radhika Ghosal"
                    },
                    {
                        "name": "Youhui Wang"
                    },
                    {
                        "name": "Ali Agha"
                    },
                    {
                        "name": "Vijay Janapa Reddi"
                    },
                    {
                        "name": "Shayegan Omidshafiei"
                    }
                ],
                "author_detail": {
                    "name": "Shayegan Omidshafiei"
                },
                "author": "Shayegan Omidshafiei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07242v2",
                "updated": "2025-10-09T17:01:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    1,
                    54,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-08T17:09:41Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    9,
                    41,
                    2,
                    281,
                    0
                ],
                "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense"
                },
                "summary": "Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning."
                },
                "authors": [
                    {
                        "name": "Leitian Tao"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Swarnadeep Saha"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Jason E Weston"
                    },
                    {
                        "name": "Ping Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ping Yu"
                },
                "author": "Ping Yu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08439v1",
                "updated": "2025-10-09T16:52:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    52,
                    1,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T16:52:01Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    52,
                    1,
                    3,
                    282,
                    0
                ],
                "title": "xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement\n  Learning"
                },
                "summary": "Modern LLM deployments confront a widening cost-performance spectrum: premium\nmodels deliver strong reasoning but are expensive, while lightweight models are\neconomical yet brittle on complex tasks. Static escalation rules and keyword\nheuristics under-utilize this spectrum and fail to adapt across task types. We\npresent xRouter, a tool-calling-based routing system in which a learned router\ncan either answer directly or invoke one or more external models. The router is\ntrained end-to-end with reinforcement learning using an explicit, cost-aware\nreward that encodes cost-performance trade-offs, eliminating the need for\nhand-engineered routing rules. Our implementation encompasses the full\nreinforcement learning framework, including reward and cost accounting, as well\nas the deployment and evaluation pipelines. Across diverse benchmarks, xRouter\nachieves strong cost-performance trade-offs (e.g., substantial cost reductions\nat comparable task completion rates), and provides empirical insights into what\nreliably helps learned routing and what does not, ranging from model\ntrainability to the difficulty of eliciting sophisticated orchestration\nbehaviors in small open models. We hope these findings and our open\nimplementation will serve as a practical substrate for advancing learned,\ncost-aware LLM orchestration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM deployments confront a widening cost-performance spectrum: premium\nmodels deliver strong reasoning but are expensive, while lightweight models are\neconomical yet brittle on complex tasks. Static escalation rules and keyword\nheuristics under-utilize this spectrum and fail to adapt across task types. We\npresent xRouter, a tool-calling-based routing system in which a learned router\ncan either answer directly or invoke one or more external models. The router is\ntrained end-to-end with reinforcement learning using an explicit, cost-aware\nreward that encodes cost-performance trade-offs, eliminating the need for\nhand-engineered routing rules. Our implementation encompasses the full\nreinforcement learning framework, including reward and cost accounting, as well\nas the deployment and evaluation pipelines. Across diverse benchmarks, xRouter\nachieves strong cost-performance trade-offs (e.g., substantial cost reductions\nat comparable task completion rates), and provides empirical insights into what\nreliably helps learned routing and what does not, ranging from model\ntrainability to the difficulty of eliciting sophisticated orchestration\nbehaviors in small open models. We hope these findings and our open\nimplementation will serve as a practical substrate for advancing learned,\ncost-aware LLM orchestration."
                },
                "authors": [
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Shirley Kokane"
                    },
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Jielin Qiu"
                    },
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "24 Pages, 4 Figures, 2 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07231v2",
                "updated": "2025-10-09T16:46:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    46,
                    30,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-08T17:00:49Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    0,
                    49,
                    2,
                    281,
                    0
                ],
                "title": "Benchmarking LLM Causal Reasoning with Scientifically Validated\n  Relationships",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM Causal Reasoning with Scientifically Validated\n  Relationships"
                },
                "summary": "Causal reasoning is fundamental for Large Language Models (LLMs) to\nunderstand genuine cause-and-effect relationships beyond pattern matching.\nExisting benchmarks suffer from critical limitations such as reliance on\nsynthetic data and narrow domain coverage. We introduce a novel benchmark\nconstructed from casually identified relationships extracted from top-tier\neconomics and finance journals, drawing on rigorous methodologies including\ninstrumental variables, difference-in-differences, and regression discontinuity\ndesigns. Our benchmark comprises 40,379 evaluation items covering five task\ntypes across domains such as health, environment, technology, law, and culture.\nExperimental results on eight state-of-the-art LLMs reveal substantial\nlimitations, with the best model achieving only 57.6\\% accuracy. Moreover,\nmodel scale does not consistently translate to superior performance, and even\nadvanced reasoning models struggle with fundamental causal relationship\nidentification. These findings underscore a critical gap between current LLM\ncapabilities and demands of reliable causal reasoning in high-stakes\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal reasoning is fundamental for Large Language Models (LLMs) to\nunderstand genuine cause-and-effect relationships beyond pattern matching.\nExisting benchmarks suffer from critical limitations such as reliance on\nsynthetic data and narrow domain coverage. We introduce a novel benchmark\nconstructed from casually identified relationships extracted from top-tier\neconomics and finance journals, drawing on rigorous methodologies including\ninstrumental variables, difference-in-differences, and regression discontinuity\ndesigns. Our benchmark comprises 40,379 evaluation items covering five task\ntypes across domains such as health, environment, technology, law, and culture.\nExperimental results on eight state-of-the-art LLMs reveal substantial\nlimitations, with the best model achieving only 57.6\\% accuracy. Moreover,\nmodel scale does not consistently translate to superior performance, and even\nadvanced reasoning models struggle with fundamental causal relationship\nidentification. These findings underscore a critical gap between current LLM\ncapabilities and demands of reliable causal reasoning in high-stakes\napplications."
                },
                "authors": [
                    {
                        "name": "Donggyu Lee"
                    },
                    {
                        "name": "Sungwon Park"
                    },
                    {
                        "name": "Yerin Hwang"
                    },
                    {
                        "name": "Hyoshin Kim"
                    },
                    {
                        "name": "Hyunwoo Oh"
                    },
                    {
                        "name": "Jungwon Kim"
                    },
                    {
                        "name": "Meeyoung Cha"
                    },
                    {
                        "name": "Sangyoon Park"
                    },
                    {
                        "name": "Jihee Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jihee Kim"
                },
                "author": "Jihee Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22889v2",
                "updated": "2025-10-09T16:45:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    45,
                    21,
                    3,
                    282,
                    0
                ],
                "published": "2025-06-15T05:09:20Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    5,
                    9,
                    20,
                    6,
                    166,
                    0
                ],
                "title": "Confident-Knowledge Diversity Drives Human-Human and Human-AI Free\n  Discussion Synergy and Reveals Pure-AI Discussion Shortfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confident-Knowledge Diversity Drives Human-Human and Human-AI Free\n  Discussion Synergy and Reveals Pure-AI Discussion Shortfalls"
                },
                "summary": "Conversations transform individual knowledge into collective insight,\nenabling collaborators to solve problems more accurately than they could alone.\nWhether dialogues among large language models (LLMs) can replicate the\nsynergistic gains observed in human discussion remains unclear. We\nsystematically compared four interaction settings: LLM-LLM pairs, LLM trios,\nhuman trios, and human-LLM pairs, using validated medical multiple-choice\nquestions. Agents answered individually, engaged in open-ended discussion, then\nre-answered, allowing us to quantify conversational gains. Interactions that\nincluded humans consistently yielded synergy (post-discussion accuracy\nincreased for both stronger and weaker participants), whereas purely LLM groups\ndid not improve and often declined. To explain and prospectively predict when\nunstructured dialogue helps, we introduce an agent-agnostic confident-knowledge\nframework that models each participant by performance (accuracy) and\nconfidence. This framework quantifies confident-knowledge diversity, the degree\nto which one agent tends to be correct when another is uncertain, and yields a\nconservative upper bound on gains achievable via confidence-informed decisions,\nwhich we term Potential Conversation Synergy. Across humans, LLMs, and mixed\nteams, this metric prospectively predicts observed conversational improvements:\nwhen confident-knowledge diversity is low (as in LLM-only groups), discussion\ndoesn't improve performance; when it is present (as in human or human-LLM\ngroups), free-form dialogue reliably lifts accuracy. These findings propose a\nnew concept and method for AI collaboration: quantifying confident-knowledge\ndiversity to prospectively predict conversational gains and guide team\nselection and interaction design in both multi-agent and human-AI settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversations transform individual knowledge into collective insight,\nenabling collaborators to solve problems more accurately than they could alone.\nWhether dialogues among large language models (LLMs) can replicate the\nsynergistic gains observed in human discussion remains unclear. We\nsystematically compared four interaction settings: LLM-LLM pairs, LLM trios,\nhuman trios, and human-LLM pairs, using validated medical multiple-choice\nquestions. Agents answered individually, engaged in open-ended discussion, then\nre-answered, allowing us to quantify conversational gains. Interactions that\nincluded humans consistently yielded synergy (post-discussion accuracy\nincreased for both stronger and weaker participants), whereas purely LLM groups\ndid not improve and often declined. To explain and prospectively predict when\nunstructured dialogue helps, we introduce an agent-agnostic confident-knowledge\nframework that models each participant by performance (accuracy) and\nconfidence. This framework quantifies confident-knowledge diversity, the degree\nto which one agent tends to be correct when another is uncertain, and yields a\nconservative upper bound on gains achievable via confidence-informed decisions,\nwhich we term Potential Conversation Synergy. Across humans, LLMs, and mixed\nteams, this metric prospectively predicts observed conversational improvements:\nwhen confident-knowledge diversity is low (as in LLM-only groups), discussion\ndoesn't improve performance; when it is present (as in human or human-LLM\ngroups), free-form dialogue reliably lifts accuracy. These findings propose a\nnew concept and method for AI collaboration: quantifying confident-knowledge\ndiversity to prospectively predict conversational gains and guide team\nselection and interaction design in both multi-agent and human-AI settings."
                },
                "authors": [
                    {
                        "name": "Tom Sheffer"
                    },
                    {
                        "name": "Alon Miron"
                    },
                    {
                        "name": "Asael Sklar"
                    },
                    {
                        "name": "Yaniv Dover"
                    },
                    {
                        "name": "Ariel Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Goldstein"
                },
                "author": "Ariel Goldstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17320v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17320v2",
                "updated": "2025-10-09T16:34:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    34,
                    9,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-24T12:00:41Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    12,
                    0,
                    41,
                    6,
                    236,
                    0
                ],
                "title": "AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for\n  Interpretable LLM Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for\n  Interpretable LLM Representations"
                },
                "summary": "Understanding the internal representations of large language models (LLMs)\nremains a central challenge for interpretability research. Sparse autoencoders\n(SAEs) offer a promising solution by decomposing activations into interpretable\nfeatures, but existing approaches rely on fixed sparsity constraints that fail\nto account for input complexity. We propose AdaptiveK SAE (Adaptive Top K\nSparse Autoencoders), a novel framework that dynamically adjusts sparsity\nlevels based on the semantic complexity of each input. Leveraging linear\nprobes, we demonstrate that context complexity is linearly encoded in LLM\nrepresentations, and we use this signal to guide feature allocation during\ntraining. Experiments across ten language models (from 70M to 14B parameters)\ndemonstrate that this complexity-driven adaptation significantly outperforms\nfixed-sparsity approaches on reconstruction fidelity, explained variance,\ncosine similarity and interpretability metrics while eliminating the\ncomputational burden of extensive hyperparameter tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the internal representations of large language models (LLMs)\nremains a central challenge for interpretability research. Sparse autoencoders\n(SAEs) offer a promising solution by decomposing activations into interpretable\nfeatures, but existing approaches rely on fixed sparsity constraints that fail\nto account for input complexity. We propose AdaptiveK SAE (Adaptive Top K\nSparse Autoencoders), a novel framework that dynamically adjusts sparsity\nlevels based on the semantic complexity of each input. Leveraging linear\nprobes, we demonstrate that context complexity is linearly encoded in LLM\nrepresentations, and we use this signal to guide feature allocation during\ntraining. Experiments across ten language models (from 70M to 14B parameters)\ndemonstrate that this complexity-driven adaptation significantly outperforms\nfixed-sparsity approaches on reconstruction fidelity, explained variance,\ncosine similarity and interpretability metrics while eliminating the\ncomputational burden of extensive hyperparameter tuning."
                },
                "authors": [
                    {
                        "name": "Yifei Yao"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17320v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17320v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00300v2",
                "updated": "2025-10-09T16:26:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    26,
                    32,
                    3,
                    282,
                    0
                ],
                "published": "2024-11-30T00:58:48Z",
                "published_parsed": [
                    2024,
                    11,
                    30,
                    0,
                    58,
                    48,
                    5,
                    335,
                    0
                ],
                "title": "Aligning LLM+PDDL Symbolic Plans with Human Objective Specifications\n  through Evolutionary Algorithm Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning LLM+PDDL Symbolic Plans with Human Objective Specifications\n  through Evolutionary Algorithm Guidance"
                },
                "summary": "Automated planning using a symbolic planning language, such as PDDL, is a\ngeneral approach to producing optimal plans to achieve a stated goal. However,\ncreating suitable machine understandable descriptions of the planning domain,\nproblem, and goal requires expertise in the planning language, limiting the\nutility of these tools for non-expert humans. Recent efforts have explored\nutilizing a symbolic planner in conjunction with a large language model to\ngenerate plans from natural language descriptions given by a non-expert human\n(LLM+PDDL). Our approach performs initial translation of goal specifications to\na set of PDDL goal constraints using an LLM; such translations often result in\nimprecise symbolic specifications, which are difficult to validate directly. We\naccount for this using an evolutionary approach to generate a population of\nsymbolic goal specifications with slight differences from the initial\ntranslation, and utilize a trained LSTM-based validation model to assess\nwhether each induced plan in the population adheres to the natural language\nspecifications. We evaluate our approach on a collection of prototypical\nspecifications in a notional naval disaster recovery task, and demonstrate that\nour evolutionary approach improve adherence of generated plans to natural\nlanguage specifications when compared to plans generated using only LLM\ntranslations. The code for our method can be found at\nhttps://github.com/owenonline/PlanCritic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated planning using a symbolic planning language, such as PDDL, is a\ngeneral approach to producing optimal plans to achieve a stated goal. However,\ncreating suitable machine understandable descriptions of the planning domain,\nproblem, and goal requires expertise in the planning language, limiting the\nutility of these tools for non-expert humans. Recent efforts have explored\nutilizing a symbolic planner in conjunction with a large language model to\ngenerate plans from natural language descriptions given by a non-expert human\n(LLM+PDDL). Our approach performs initial translation of goal specifications to\na set of PDDL goal constraints using an LLM; such translations often result in\nimprecise symbolic specifications, which are difficult to validate directly. We\naccount for this using an evolutionary approach to generate a population of\nsymbolic goal specifications with slight differences from the initial\ntranslation, and utilize a trained LSTM-based validation model to assess\nwhether each induced plan in the population adheres to the natural language\nspecifications. We evaluate our approach on a collection of prototypical\nspecifications in a notional naval disaster recovery task, and demonstrate that\nour evolutionary approach improve adherence of generated plans to natural\nlanguage specifications when compared to plans generated using only LLM\ntranslations. The code for our method can be found at\nhttps://github.com/owenonline/PlanCritic."
                },
                "authors": [
                    {
                        "name": "Owen Burns"
                    },
                    {
                        "name": "Dana Hughes"
                    },
                    {
                        "name": "Katia Sycara"
                    }
                ],
                "author_detail": {
                    "name": "Katia Sycara"
                },
                "author": "Katia Sycara",
                "arxiv_doi": "10.1109/CASE58245.2025.11163939",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CASE58245.2025.11163939",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.00300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 5 figures",
                "arxiv_journal_ref": "2025 IEEE 21st International Conference on Automation Science and\n  Engineering (CASE), Los Angeles, CA, USA, 2025, pp. 1584-1590",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22536v2",
                "updated": "2025-10-09T16:13:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    13,
                    53,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-26T16:16:49Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    16,
                    49,
                    4,
                    269,
                    0
                ],
                "title": "InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced\n  Language Models"
                },
                "summary": "The immense computational cost of training Large Language Models (LLMs)\npresents a major barrier to innovation. While FP8 training offers a promising\nsolution with significant theoretical efficiency gains, its widespread adoption\nhas been hindered by the lack of a comprehensive, open-source training recipe.\nTo bridge this gap, we introduce an end-to-end FP8 training recipe that\nseamlessly integrates continual pre-training and supervised fine-tuning. Our\nmethodology employs a fine-grained, hybrid-granularity quantization strategy to\nmaintain numerical fidelity while maximizing computational efficiency. Through\nextensive experiments, including the continue pre-training of models on a\n160B-token corpus, we demonstrate that our recipe is not only remarkably stable\nbut also essentially lossless, achieving performance on par with the BF16\nbaseline across a suite of reasoning benchmarks. Crucially, this is achieved\nwith substantial efficiency improvements, including up to a 22% reduction in\ntraining time, a 14% decrease in peak memory usage, and a 19% increase in\nthroughput. Our results establish FP8 as a practical and robust alternative to\nBF16, and we will release the accompanying code to further democratize\nlarge-scale model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense computational cost of training Large Language Models (LLMs)\npresents a major barrier to innovation. While FP8 training offers a promising\nsolution with significant theoretical efficiency gains, its widespread adoption\nhas been hindered by the lack of a comprehensive, open-source training recipe.\nTo bridge this gap, we introduce an end-to-end FP8 training recipe that\nseamlessly integrates continual pre-training and supervised fine-tuning. Our\nmethodology employs a fine-grained, hybrid-granularity quantization strategy to\nmaintain numerical fidelity while maximizing computational efficiency. Through\nextensive experiments, including the continue pre-training of models on a\n160B-token corpus, we demonstrate that our recipe is not only remarkably stable\nbut also essentially lossless, achieving performance on par with the BF16\nbaseline across a suite of reasoning benchmarks. Crucially, this is achieved\nwith substantial efficiency improvements, including up to a 22% reduction in\ntraining time, a 14% decrease in peak memory usage, and a 19% increase in\nthroughput. Our results establish FP8 as a practical and robust alternative to\nBF16, and we will release the accompanying code to further democratize\nlarge-scale model training."
                },
                "authors": [
                    {
                        "name": "Wenjun Wang"
                    },
                    {
                        "name": "Shuo Cai"
                    },
                    {
                        "name": "Congkai Xie"
                    },
                    {
                        "name": "Mingfa Feng"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Kejing Yang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Jiannong Cao"
                    },
                    {
                        "name": "Yuan Xie"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "arxiv_comment": "This paper has been withdrawn by the authors due to a significant bug\n  discovered in our data processing pipeline. This bug affects the validity of\n  the experimental results, and we can no longer stand by the conclusions\n  presented",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08389v1",
                "updated": "2025-10-09T16:12:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    12,
                    12,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T16:12:12Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    12,
                    12,
                    3,
                    282,
                    0
                ],
                "title": "Revisiting Hallucination Detection with Effective Rank-based Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Hallucination Detection with Effective Rank-based Uncertainty"
                },
                "summary": "Detecting hallucinations in large language models (LLMs) remains a\nfundamental challenge for their trustworthy deployment. Going beyond basic\nuncertainty-driven hallucination detection frameworks, we propose a simple yet\npowerful method that quantifies uncertainty by measuring the effective rank of\nhidden states derived from multiple model outputs and different layers.\nGrounded in the spectral analysis of representations, our approach provides\ninterpretable insights into the model's internal reasoning process through\nsemantic variations, while requiring no extra knowledge or additional modules,\nthus offering a combination of theoretical elegance and practical efficiency.\nMeanwhile, we theoretically demonstrate the necessity of quantifying\nuncertainty both internally (representations of a single response) and\nexternally (different responses), providing a justification for using\nrepresentations among different layers and responses from LLMs to detect\nhallucinations. Extensive experiments demonstrate that our method effectively\ndetects hallucinations and generalizes robustly across various scenarios,\ncontributing to a new paradigm of hallucination detection for LLM truthfulness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting hallucinations in large language models (LLMs) remains a\nfundamental challenge for their trustworthy deployment. Going beyond basic\nuncertainty-driven hallucination detection frameworks, we propose a simple yet\npowerful method that quantifies uncertainty by measuring the effective rank of\nhidden states derived from multiple model outputs and different layers.\nGrounded in the spectral analysis of representations, our approach provides\ninterpretable insights into the model's internal reasoning process through\nsemantic variations, while requiring no extra knowledge or additional modules,\nthus offering a combination of theoretical elegance and practical efficiency.\nMeanwhile, we theoretically demonstrate the necessity of quantifying\nuncertainty both internally (representations of a single response) and\nexternally (different responses), providing a justification for using\nrepresentations among different layers and responses from LLMs to detect\nhallucinations. Extensive experiments demonstrate that our method effectively\ndetects hallucinations and generalizes robustly across various scenarios,\ncontributing to a new paradigm of hallucination detection for LLM truthfulness."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Guanzhang Yue"
                    },
                    {
                        "name": "Meng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Meng Sun"
                },
                "author": "Meng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08388v1",
                "updated": "2025-10-09T16:12:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    12,
                    10,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T16:12:10Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    12,
                    10,
                    3,
                    282,
                    0
                ],
                "title": "If Probable, Then Acceptable? Understanding Conditional Acceptability\n  Judgments in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If Probable, Then Acceptable? Understanding Conditional Acceptability\n  Judgments in Large Language Models"
                },
                "summary": "Conditional acceptability refers to how plausible a conditional statement is\nperceived to be. It plays an important role in communication and reasoning, as\nit influences how individuals interpret implications, assess arguments, and\nmake decisions based on hypothetical scenarios. When humans evaluate how\nacceptable a conditional \"If A, then B\" is, their judgments are influenced by\ntwo main factors: the $\\textit{conditional probability}$ of $B$ given $A$, and\nthe $\\textit{semantic relevance}$ of the antecedent $A$ given the consequent\n$B$ (i.e., whether $A$ meaningfully supports $B$). While prior work has\nexamined how large language models (LLMs) draw inferences about conditional\nstatements, it remains unclear how these models judge the\n$\\textit{acceptability}$ of such statements. To address this gap, we present a\ncomprehensive study of LLMs' conditional acceptability judgments across\ndifferent model families, sizes, and prompting strategies. Using linear\nmixed-effects models and ANOVA tests, we find that models are sensitive to both\nconditional probability and semantic relevance-though to varying degrees\ndepending on architecture and prompting style. A comparison with human data\nreveals that while LLMs incorporate probabilistic and semantic cues, they do so\nless consistently than humans. Notably, larger models do not necessarily align\nmore closely with human judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional acceptability refers to how plausible a conditional statement is\nperceived to be. It plays an important role in communication and reasoning, as\nit influences how individuals interpret implications, assess arguments, and\nmake decisions based on hypothetical scenarios. When humans evaluate how\nacceptable a conditional \"If A, then B\" is, their judgments are influenced by\ntwo main factors: the $\\textit{conditional probability}$ of $B$ given $A$, and\nthe $\\textit{semantic relevance}$ of the antecedent $A$ given the consequent\n$B$ (i.e., whether $A$ meaningfully supports $B$). While prior work has\nexamined how large language models (LLMs) draw inferences about conditional\nstatements, it remains unclear how these models judge the\n$\\textit{acceptability}$ of such statements. To address this gap, we present a\ncomprehensive study of LLMs' conditional acceptability judgments across\ndifferent model families, sizes, and prompting strategies. Using linear\nmixed-effects models and ANOVA tests, we find that models are sensitive to both\nconditional probability and semantic relevance-though to varying degrees\ndepending on architecture and prompting style. A comparison with human data\nreveals that while LLMs incorporate probabilistic and semantic cues, they do so\nless consistently than humans. Notably, larger models do not necessarily align\nmore closely with human judgments."
                },
                "authors": [
                    {
                        "name": "Jasmin Orth"
                    },
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "22 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08383v1",
                "updated": "2025-10-09T16:08:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    8,
                    5,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T16:08:05Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    8,
                    5,
                    3,
                    282,
                    0
                ],
                "title": "QAgent: A modular Search Agent with Interactive Query Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QAgent: A modular Search Agent with Interactive Query Understanding"
                },
                "summary": "Large language models (LLMs) excel at natural language tasks but are limited\nby their static parametric knowledge, especially in knowledge-intensive task.\nRetrieval-augmented generation (RAG) mitigates this by integrating external\ninformation. However, (1) traditional RAG struggles with complex query\nunderstanding, and (2) even search agents trained with reinforcement learning\n(RL), despite their promise, still face generalization and deployment\nchallenges. To address these limitations, we propose QAgent, a unified agentic\nRAG framework that employs a search agent for adaptive retrieval. This agent\noptimizes its understanding of the query through interactive reasoning and\nretrieval. To facilitate real-world application, we focus on modular search\nagent for query understanding that are plug-and-play in complex systems.\nSecifically, the agent follows a multi-step decision process trained with RL to\nmaximize retrieval quality and support accurate downstream answers. We further\nanalyze the strengths and weaknesses of end-to-end RL and propose a strategy\nthat focuses on effective retrieval, thereby enhancing generalization in LLM\napplications. Experiments show QAgent excels at QA and serves as a\nplug-and-play module for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at natural language tasks but are limited\nby their static parametric knowledge, especially in knowledge-intensive task.\nRetrieval-augmented generation (RAG) mitigates this by integrating external\ninformation. However, (1) traditional RAG struggles with complex query\nunderstanding, and (2) even search agents trained with reinforcement learning\n(RL), despite their promise, still face generalization and deployment\nchallenges. To address these limitations, we propose QAgent, a unified agentic\nRAG framework that employs a search agent for adaptive retrieval. This agent\noptimizes its understanding of the query through interactive reasoning and\nretrieval. To facilitate real-world application, we focus on modular search\nagent for query understanding that are plug-and-play in complex systems.\nSecifically, the agent follows a multi-step decision process trained with RL to\nmaximize retrieval quality and support accurate downstream answers. We further\nanalyze the strengths and weaknesses of end-to-end RL and propose a strategy\nthat focuses on effective retrieval, thereby enhancing generalization in LLM\napplications. Experiments show QAgent excels at QA and serves as a\nplug-and-play module for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Lei Shen"
                    },
                    {
                        "name": "Lujie Niu"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "Code is available at https://github.com/OpenStellarTeam/QAgent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01876v3",
                "updated": "2025-10-09T16:08:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    8,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-02-26T15:12:29Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    12,
                    29,
                    2,
                    57,
                    0
                ],
                "title": "Uncertainty Comes for Free: Human-in-the-Loop Policies with Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Comes for Free: Human-in-the-Loop Policies with Diffusion\n  Models"
                },
                "summary": "Human-in-the-loop (HitL) robot deployment has gained significant attention in\nboth academia and industry as a semi-autonomous paradigm that enables human\noperators to intervene and adjust robot behaviors at deployment time, improving\nsuccess rates. However, continuous human monitoring and intervention can be\nhighly labor-intensive and impractical when deploying a large number of robots.\nTo address this limitation, we propose a method that allows diffusion policies\nto actively seek human assistance only when necessary, reducing reliance on\nconstant human oversight. To achieve this, we leverage the generative process\nof diffusion policies to compute an uncertainty-based metric based on which the\nautonomous agent can decide to request operator assistance at deployment time,\nwithout requiring any operator interaction during training. Additionally, we\nshow that the same method can be used for efficient data collection for\nfine-tuning diffusion policies in order to improve their autonomous\nperformance. Experimental results from simulated and real-world environments\ndemonstrate that our approach enhances policy performance during deployment for\na variety of scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-in-the-loop (HitL) robot deployment has gained significant attention in\nboth academia and industry as a semi-autonomous paradigm that enables human\noperators to intervene and adjust robot behaviors at deployment time, improving\nsuccess rates. However, continuous human monitoring and intervention can be\nhighly labor-intensive and impractical when deploying a large number of robots.\nTo address this limitation, we propose a method that allows diffusion policies\nto actively seek human assistance only when necessary, reducing reliance on\nconstant human oversight. To achieve this, we leverage the generative process\nof diffusion policies to compute an uncertainty-based metric based on which the\nautonomous agent can decide to request operator assistance at deployment time,\nwithout requiring any operator interaction during training. Additionally, we\nshow that the same method can be used for efficient data collection for\nfine-tuning diffusion policies in order to improve their autonomous\nperformance. Experimental results from simulated and real-world environments\ndemonstrate that our approach enhances policy performance during deployment for\na variety of scenarios."
                },
                "authors": [
                    {
                        "name": "Zhanpeng He"
                    },
                    {
                        "name": "Yifeng Cao"
                    },
                    {
                        "name": "Matei Ciocarlie"
                    }
                ],
                "author_detail": {
                    "name": "Matei Ciocarlie"
                },
                "author": "Matei Ciocarlie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03336v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03336v3",
                "updated": "2025-10-09T16:01:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    1,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-07-04T06:49:02Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    49,
                    2,
                    4,
                    185,
                    0
                ],
                "title": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky"
                },
                "summary": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents."
                },
                "authors": [
                    {
                        "name": "Ashutosh Hathidara"
                    },
                    {
                        "name": "Julien Yu"
                    },
                    {
                        "name": "Sebastian Schreiber"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Schreiber"
                },
                "author": "Sebastian Schreiber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03336v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03336v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20435v2",
                "updated": "2025-10-09T16:00:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    0,
                    15,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-26T18:31:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    31,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "The Shape of Adversarial Influence: Characterizing LLM Latent Spaces\n  with Persistent Homology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Shape of Adversarial Influence: Characterizing LLM Latent Spaces\n  with Persistent Homology"
                },
                "summary": "Existing interpretability methods for Large Language Models (LLMs) often fall\nshort by focusing on linear directions or isolated features, overlooking the\nhigh-dimensional, nonlinear, and relational geometry within model\nrepresentations. This study focuses on how adversarial inputs systematically\naffect the internal representation spaces of LLMs, a topic which remains poorly\nunderstood. We propose persistent homology (PH), a tool from topological data\nanalysis, as a principled framework to characterize the multi-scale dynamics\nwithin LLM activations. Using PH, we systematically analyze six\nstate-of-the-art models under two distinct adversarial conditions, indirect\nprompt injection and backdoor fine-tuning, and identify a consistent\ntopological signature of adversarial influence. Across architectures and model\nsizes, adversarial inputs induce ``topological compression'', where the latent\nspace becomes structurally simpler, collapsing from varied, compact,\nsmall-scale features into fewer, dominant, and more dispersed large-scale ones.\nThis topological signature is statistically robust across layers, highly\ndiscriminative, and provides interpretable insights into how adversarial\neffects emerge and propagate. By quantifying the shape of activations and\nneuronal information flow, our architecture-agnostic framework reveals\nfundamental invariants of representational change, offering a complementary\nperspective to existing interpretability methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing interpretability methods for Large Language Models (LLMs) often fall\nshort by focusing on linear directions or isolated features, overlooking the\nhigh-dimensional, nonlinear, and relational geometry within model\nrepresentations. This study focuses on how adversarial inputs systematically\naffect the internal representation spaces of LLMs, a topic which remains poorly\nunderstood. We propose persistent homology (PH), a tool from topological data\nanalysis, as a principled framework to characterize the multi-scale dynamics\nwithin LLM activations. Using PH, we systematically analyze six\nstate-of-the-art models under two distinct adversarial conditions, indirect\nprompt injection and backdoor fine-tuning, and identify a consistent\ntopological signature of adversarial influence. Across architectures and model\nsizes, adversarial inputs induce ``topological compression'', where the latent\nspace becomes structurally simpler, collapsing from varied, compact,\nsmall-scale features into fewer, dominant, and more dispersed large-scale ones.\nThis topological signature is statistically robust across layers, highly\ndiscriminative, and provides interpretable insights into how adversarial\neffects emerge and propagate. By quantifying the shape of activations and\nneuronal information flow, our architecture-agnostic framework reveals\nfundamental invariants of representational change, offering a complementary\nperspective to existing interpretability methods."
                },
                "authors": [
                    {
                        "name": "Aideen Fay"
                    },
                    {
                        "name": "Inés García-Redondo"
                    },
                    {
                        "name": "Qiquan Wang"
                    },
                    {
                        "name": "Haim Dubossarsky"
                    },
                    {
                        "name": "Anthea Monod"
                    }
                ],
                "author_detail": {
                    "name": "Anthea Monod"
                },
                "author": "Anthea Monod",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01616v3",
                "updated": "2025-10-09T15:58:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    58,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-02T22:36:24Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    22,
                    36,
                    24,
                    4,
                    122,
                    0
                ],
                "title": "Phantora: Maximizing Code Reuse in Simulation-based Machine Learning\n  System Performance Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phantora: Maximizing Code Reuse in Simulation-based Machine Learning\n  System Performance Estimation"
                },
                "summary": "Modern machine learning (ML) training workloads place substantial demands on\nboth computational and communication resources. Consequently, accurate\nperformance estimation has become increasingly critical for guiding system\ndesign decisions, such as the selection of parallelization strategies, cluster\nconfigurations, and hardware provisioning. Existing simulation-based\nperformance estimation requires reimplementing the ML framework in a simulator,\nwhich demands significant manual effort and is hard to maintain as ML\nframeworks evolve rapidly.\n  This paper introduces Phantora, a hybrid GPU cluster simulator designed for\nperformance estimation of ML training workloads. Phantora executes unmodified\nML frameworks as is within a distributed, containerized environment. Each\ncontainer emulates the behavior of a GPU server in a large-scale cluster, while\nPhantora intercepts and simulates GPU- and communication-related operations to\nprovide high-fidelity performance estimation. We call this approach hybrid\nsimulation of ML systems, in contrast to traditional methods that simulate\nstatic workloads. The primary advantage of hybrid simulation is that it allows\ndirect reuse of ML framework source code in simulation, avoiding the need for\nreimplementation. Our evaluation shows that Phantora provides accuracy\ncomparable to static workload simulation while supporting three\nstate-of-the-art LLM training frameworks out-of-the-box. In addition, Phantora\noperates on a single GPU, eliminating the need for the resource-intensive trace\ncollection and workload extraction steps required by traditional trace-based\nsimulators. Phantora is open-sourced at https://github.com/QDelta/Phantora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern machine learning (ML) training workloads place substantial demands on\nboth computational and communication resources. Consequently, accurate\nperformance estimation has become increasingly critical for guiding system\ndesign decisions, such as the selection of parallelization strategies, cluster\nconfigurations, and hardware provisioning. Existing simulation-based\nperformance estimation requires reimplementing the ML framework in a simulator,\nwhich demands significant manual effort and is hard to maintain as ML\nframeworks evolve rapidly.\n  This paper introduces Phantora, a hybrid GPU cluster simulator designed for\nperformance estimation of ML training workloads. Phantora executes unmodified\nML frameworks as is within a distributed, containerized environment. Each\ncontainer emulates the behavior of a GPU server in a large-scale cluster, while\nPhantora intercepts and simulates GPU- and communication-related operations to\nprovide high-fidelity performance estimation. We call this approach hybrid\nsimulation of ML systems, in contrast to traditional methods that simulate\nstatic workloads. The primary advantage of hybrid simulation is that it allows\ndirect reuse of ML framework source code in simulation, avoiding the need for\nreimplementation. Our evaluation shows that Phantora provides accuracy\ncomparable to static workload simulation while supporting three\nstate-of-the-art LLM training frameworks out-of-the-box. In addition, Phantora\noperates on a single GPU, eliminating the need for the resource-intensive trace\ncollection and workload extraction steps required by traditional trace-based\nsimulators. Phantora is open-sourced at https://github.com/QDelta/Phantora."
                },
                "authors": [
                    {
                        "name": "Jianxing Qin"
                    },
                    {
                        "name": "Jingrong Chen"
                    },
                    {
                        "name": "Xinhao Kong"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Tianjun Yuan"
                    },
                    {
                        "name": "Liang Luo"
                    },
                    {
                        "name": "Zhaodong Wang"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Tingjun Chen"
                    },
                    {
                        "name": "Alvin R. Lebeck"
                    },
                    {
                        "name": "Danyang Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Danyang Zhuo"
                },
                "author": "Danyang Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08374v1",
                "updated": "2025-10-09T15:57:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    57,
                    44,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:57:44Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    57,
                    44,
                    3,
                    282,
                    0
                ],
                "title": "Contrastive Self-Supervised Learning at the Edge: An Energy Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Self-Supervised Learning at the Edge: An Energy Perspective"
                },
                "summary": "While contrastive learning (CL) shows considerable promise in self-supervised\nrepresentation learning, its deployment on resource-constrained devices remains\nlargely underexplored. The substantial computational demands required for\ntraining conventional CL frameworks pose a set of challenges, particularly in\nterms of energy consumption, data availability, and memory usage. We conduct an\nevaluation of four widely used CL frameworks: SimCLR, MoCo, SimSiam, and Barlow\nTwins. We focus on the practical feasibility of these CL frameworks for edge\nand fog deployment, and introduce a systematic benchmarking strategy that\nincludes energy profiling and reduced training data conditions. Our findings\nreveal that SimCLR, contrary to its perceived computational cost, demonstrates\nthe lowest energy consumption across various data regimes. Finally, we also\nextend our analysis by evaluating lightweight neural architectures when paired\nwith CL frameworks. Our study aims to provide insights into the resource\nimplications of deploying CL in edge/fog environments with limited processing\ncapabilities and opens several research directions for its future optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While contrastive learning (CL) shows considerable promise in self-supervised\nrepresentation learning, its deployment on resource-constrained devices remains\nlargely underexplored. The substantial computational demands required for\ntraining conventional CL frameworks pose a set of challenges, particularly in\nterms of energy consumption, data availability, and memory usage. We conduct an\nevaluation of four widely used CL frameworks: SimCLR, MoCo, SimSiam, and Barlow\nTwins. We focus on the practical feasibility of these CL frameworks for edge\nand fog deployment, and introduce a systematic benchmarking strategy that\nincludes energy profiling and reduced training data conditions. Our findings\nreveal that SimCLR, contrary to its perceived computational cost, demonstrates\nthe lowest energy consumption across various data regimes. Finally, we also\nextend our analysis by evaluating lightweight neural architectures when paired\nwith CL frameworks. Our study aims to provide insights into the resource\nimplications of deploying CL in edge/fog environments with limited processing\ncapabilities and opens several research directions for its future optimization."
                },
                "authors": [
                    {
                        "name": "Fernanda Famá"
                    },
                    {
                        "name": "Roberto Pereira"
                    },
                    {
                        "name": "Charalampos Kalalas"
                    },
                    {
                        "name": "Paolo Dini"
                    },
                    {
                        "name": "Lorena Qendro"
                    },
                    {
                        "name": "Fahim Kawsar"
                    },
                    {
                        "name": "Mohammad Malekzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Malekzadeh"
                },
                "author": "Mohammad Malekzadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08373v1",
                "updated": "2025-10-09T15:56:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    56,
                    18,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:56:18Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    56,
                    18,
                    3,
                    282,
                    0
                ],
                "title": "DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DialoSpeech: Dual-Speaker Dialogue Generation with LLM and Flow Matching"
                },
                "summary": "Recent advances in text-to-speech (TTS) synthesis, particularly those\nleveraging large language models (LLMs), have significantly improved\nexpressiveness and naturalness. However, generating human-like, interactive\ndialogue speech remains challenging. Current systems face limitations due to\nthe scarcity of dual-track data and difficulties in achieving naturalness,\ncontextual coherence, and interactional dynamics, such as turn-taking,\noverlapping speech, and speaker consistency, in multi-turn conversations. To\naddress these challenges, we propose DialoSpeech, a dual-track architecture\ncombining a large language model with Chunked Flow Matching for expressive,\nhuman-like dialogue speech synthesis. DialoSpeech generates natural multi-turn\nconversations with coherent speaker turns and natural overlaps, supporting both\nChinese and English and cross-lingual speech synthesis. We introduce a data\nprocessing pipeline to construct dual-track dialogue datasets, facilitating\nscalable training and experimental validation. Experiments show that our model\noutperforms baselines, offering a solution for generating human-like spoken\ndialogues. Audio samples are available at\nhttps://tiamojames.github.io/DialoSpeech",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in text-to-speech (TTS) synthesis, particularly those\nleveraging large language models (LLMs), have significantly improved\nexpressiveness and naturalness. However, generating human-like, interactive\ndialogue speech remains challenging. Current systems face limitations due to\nthe scarcity of dual-track data and difficulties in achieving naturalness,\ncontextual coherence, and interactional dynamics, such as turn-taking,\noverlapping speech, and speaker consistency, in multi-turn conversations. To\naddress these challenges, we propose DialoSpeech, a dual-track architecture\ncombining a large language model with Chunked Flow Matching for expressive,\nhuman-like dialogue speech synthesis. DialoSpeech generates natural multi-turn\nconversations with coherent speaker turns and natural overlaps, supporting both\nChinese and English and cross-lingual speech synthesis. We introduce a data\nprocessing pipeline to construct dual-track dialogue datasets, facilitating\nscalable training and experimental validation. Experiments show that our model\noutperforms baselines, offering a solution for generating human-like spoken\ndialogues. Audio samples are available at\nhttps://tiamojames.github.io/DialoSpeech"
                },
                "authors": [
                    {
                        "name": "Hanke Xie"
                    },
                    {
                        "name": "Dake Guo"
                    },
                    {
                        "name": "Chengyou Wang"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Wenjie Tian"
                    },
                    {
                        "name": "Xinfa Zhu"
                    },
                    {
                        "name": "Xinsheng Wang"
                    },
                    {
                        "name": "Xiulin Li"
                    },
                    {
                        "name": "Guanqiong Miao"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08372v1",
                "updated": "2025-10-09T15:55:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    55,
                    28,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:55:28Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    55,
                    28,
                    3,
                    282,
                    0
                ],
                "title": "On the Relationship Between the Choice of Representation and In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Relationship Between the Choice of Representation and In-Context\n  Learning"
                },
                "summary": "In-context learning (ICL) is the ability of a large language model (LLM) to\nlearn a new task from a few demonstrations presented as part of the context.\nPast studies have attributed a large portion of the success of ICL to the way\nthese in-context demonstrations are represented, particularly to how labels are\nrepresented in classification tasks. On the other hand, observations of the\nlearning capacity of ICL (i.e., the extent to which more in-context\ndemonstrations can lead to higher performance) have been mixed, and ICL is\noften thought to occur only under specific conditions. The interaction between\nthese two aspects in ICL, representation and learning, has not been studied in\ndepth until now. We hypothesize that they are largely independent of one\nanother, such that the representation of demonstrations determines the baseline\naccuracy of ICL, while learning from additional demonstrations improves only on\ntop of this baseline. We validate this hypothesis by developing an optimization\nalgorithm that can enumerate a spectrum of possible label sets\n(representations) varying in semantic relevance. We then perform ICL with\nvarying numbers of in-context demonstrations for each of these label sets. We\nobserved that learning happens regardless of the quality of the label set\nitself, although its efficiency, measured by the slope of improvement over\nin-context demonstrations, is conditioned on both the label set quality and the\nparameter count of the underlying language model. Despite the emergence of\nlearning, the relative quality (accuracy) of the choice of a label set\n(representation) is largely maintained throughout learning, confirming our\nhypothesis and implying their orthogonality. Our work reveals a previously\nunderexplored aspect of ICL: the independent effects of learning from\ndemonstrations and their representations on ICL performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is the ability of a large language model (LLM) to\nlearn a new task from a few demonstrations presented as part of the context.\nPast studies have attributed a large portion of the success of ICL to the way\nthese in-context demonstrations are represented, particularly to how labels are\nrepresented in classification tasks. On the other hand, observations of the\nlearning capacity of ICL (i.e., the extent to which more in-context\ndemonstrations can lead to higher performance) have been mixed, and ICL is\noften thought to occur only under specific conditions. The interaction between\nthese two aspects in ICL, representation and learning, has not been studied in\ndepth until now. We hypothesize that they are largely independent of one\nanother, such that the representation of demonstrations determines the baseline\naccuracy of ICL, while learning from additional demonstrations improves only on\ntop of this baseline. We validate this hypothesis by developing an optimization\nalgorithm that can enumerate a spectrum of possible label sets\n(representations) varying in semantic relevance. We then perform ICL with\nvarying numbers of in-context demonstrations for each of these label sets. We\nobserved that learning happens regardless of the quality of the label set\nitself, although its efficiency, measured by the slope of improvement over\nin-context demonstrations, is conditioned on both the label set quality and the\nparameter count of the underlying language model. Despite the emergence of\nlearning, the relative quality (accuracy) of the choice of a label set\n(representation) is largely maintained throughout learning, confirming our\nhypothesis and implying their orthogonality. Our work reveals a previously\nunderexplored aspect of ICL: the independent effects of learning from\ndemonstrations and their representations on ICL performance."
                },
                "authors": [
                    {
                        "name": "Ioana Marinescu"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    },
                    {
                        "name": "Eric Karl Oermann"
                    }
                ],
                "author_detail": {
                    "name": "Eric Karl Oermann"
                },
                "author": "Eric Karl Oermann",
                "arxiv_comment": "25 pages, 6 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13468v2",
                "updated": "2025-10-09T15:54:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    54,
                    27,
                    3,
                    282,
                    0
                ],
                "published": "2025-07-17T18:21:45Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    18,
                    21,
                    45,
                    3,
                    198,
                    0
                ],
                "title": "ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in\n  Human-Robot Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in\n  Human-Robot Conversations"
                },
                "summary": "The integration of large language models (LLMs) into conversational robots\nhas made human-robot conversations more dynamic. Yet, LLM-powered\nconversational robots remain prone to errors, e.g., misunderstanding user\nintent, prematurely interrupting users, or failing to respond altogether.\nDetecting and addressing these failures is critical for preventing\nconversational breakdowns, avoiding task disruptions, and sustaining user\ntrust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal\ndataset of LLM-powered conversational robot failures during human-robot\nconversations and encourages researchers to benchmark machine learning models\ndesigned to detect robot failures. The dataset includes 16 hours of dyadic\nhuman-robot interactions, incorporating facial, speech, and head movement\nfeatures. Each interaction is annotated with the presence or absence of robot\nerrors from the system perspective, and perceived user intention to correct for\na mismatch between robot behavior and user expectation. Participants are\ninvited to form teams and develop machine learning models that detect these\nfailures using multimodal data. Submissions will be evaluated using various\nperformance metrics, including detection accuracy and false positive rate. This\nchallenge represents another key step toward improving failure detection in\nhuman-robot interaction through social signal analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) into conversational robots\nhas made human-robot conversations more dynamic. Yet, LLM-powered\nconversational robots remain prone to errors, e.g., misunderstanding user\nintent, prematurely interrupting users, or failing to respond altogether.\nDetecting and addressing these failures is critical for preventing\nconversational breakdowns, avoiding task disruptions, and sustaining user\ntrust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal\ndataset of LLM-powered conversational robot failures during human-robot\nconversations and encourages researchers to benchmark machine learning models\ndesigned to detect robot failures. The dataset includes 16 hours of dyadic\nhuman-robot interactions, incorporating facial, speech, and head movement\nfeatures. Each interaction is annotated with the presence or absence of robot\nerrors from the system perspective, and perceived user intention to correct for\na mismatch between robot behavior and user expectation. Participants are\ninvited to form teams and develop machine learning models that detect these\nfailures using multimodal data. Submissions will be evaluated using various\nperformance metrics, including detection accuracy and false positive rate. This\nchallenge represents another key step toward improving failure detection in\nhuman-robot interaction through social signal analysis."
                },
                "authors": [
                    {
                        "name": "Shiye Cao"
                    },
                    {
                        "name": "Maia Stiber"
                    },
                    {
                        "name": "Amama Mahmood"
                    },
                    {
                        "name": "Maria Teresa Parreira"
                    },
                    {
                        "name": "Wendy Ju"
                    },
                    {
                        "name": "Micol Spitale"
                    },
                    {
                        "name": "Hatice Gunes"
                    },
                    {
                        "name": "Chien-Ming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Ming Huang"
                },
                "author": "Chien-Ming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01696v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01696v3",
                "updated": "2025-10-09T15:53:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    53,
                    40,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-03T10:00:38Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    10,
                    0,
                    38,
                    6,
                    215,
                    0
                ],
                "title": "CoCoA: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge\n  Synergy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCoA: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge\n  Synergy"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs),\nespecially for knowledge-intensive tasks. Despite its advantages, current RAG\nmethods often struggle to fully exploit knowledge during generation. In\nparticular, the synergy between the model's internal parametric knowledge and\nexternal retrieved knowledge remains limited. Retrieved contents may sometimes\nmislead generation, while certain generated content can guide the model toward\nmore accurate outputs. In this work, we propose Collaborative Chain-of-Agents,\na framework designed to enhance explicitly synergy over both parametric and\nretrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent\nRAG framework that first performs conditional knowledge induction and then\nreasons answers. Building on this, we develop CoCoA, a long-chain training\nstrategy that synthesizes extended multi-agent reasoning trajectories from\nCoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability\nto explicitly integrate and jointly leverage parametric and retrieved\nknowledge. Experimental results demonstrate the superiority of CoCoA in\nopen-domain QA and multi-hop QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs),\nespecially for knowledge-intensive tasks. Despite its advantages, current RAG\nmethods often struggle to fully exploit knowledge during generation. In\nparticular, the synergy between the model's internal parametric knowledge and\nexternal retrieved knowledge remains limited. Retrieved contents may sometimes\nmislead generation, while certain generated content can guide the model toward\nmore accurate outputs. In this work, we propose Collaborative Chain-of-Agents,\na framework designed to enhance explicitly synergy over both parametric and\nretrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent\nRAG framework that first performs conditional knowledge induction and then\nreasons answers. Building on this, we develop CoCoA, a long-chain training\nstrategy that synthesizes extended multi-agent reasoning trajectories from\nCoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability\nto explicitly integrate and jointly leverage parametric and retrieved\nknowledge. Experimental results demonstrate the superiority of CoCoA in\nopen-domain QA and multi-hop QA."
                },
                "authors": [
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Haochun Wang"
                    },
                    {
                        "name": "Lizhe Zhang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Bing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Bing Qin"
                },
                "author": "Bing Qin",
                "arxiv_comment": "code available at https://github.com/liunian-Jay/CoCoA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01696v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01696v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08365v1",
                "updated": "2025-10-09T15:51:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    51,
                    5,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:51:05Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    51,
                    5,
                    3,
                    282,
                    0
                ],
                "title": "Two-Stage Voting for Robust and Efficient Suicide Risk Detection on\n  Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Stage Voting for Robust and Efficient Suicide Risk Detection on\n  Social Media"
                },
                "summary": "Suicide rates have risen worldwide in recent years, underscoring the urgent\nneed for proactive prevention strategies. Social media provides valuable\nsignals, as many at-risk individuals - who often avoid formal help due to\nstigma - choose instead to share their distress online. Yet detecting implicit\nsuicidal ideation, conveyed indirectly through metaphor, sarcasm, or subtle\nemotional cues, remains highly challenging. Lightweight models like BERT handle\nexplicit signals but fail on subtle implicit ones, while large language models\n(LLMs) capture nuance at prohibitive computational cost. To address this gap,\nwe propose a two-stage voting architecture that balances efficiency and\nrobustness. In Stage 1, a lightweight BERT classifier rapidly resolves\nhigh-confidence explicit cases. In Stage 2, ambiguous inputs are escalated to\neither (i) a multi-perspective LLM voting framework to maximize recall on\nimplicit ideation, or (ii) a feature-based ML ensemble guided by\npsychologically grounded indicators extracted via prompt-engineered LLMs for\nefficiency and interpretability. To the best of our knowledge, this is among\nthe first works to operationalize LLM-extracted psychological features as\nstructured vectors for suicide risk detection. On two complementary datasets -\nexplicit-dominant Reddit and implicit-only DeepSuiMind - our framework\noutperforms single-model baselines, achieving 98.0% F1 on explicit cases, 99.7%\non implicit ones, and reducing the cross-domain gap below 2%, while\nsignificantly lowering LLM cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Suicide rates have risen worldwide in recent years, underscoring the urgent\nneed for proactive prevention strategies. Social media provides valuable\nsignals, as many at-risk individuals - who often avoid formal help due to\nstigma - choose instead to share their distress online. Yet detecting implicit\nsuicidal ideation, conveyed indirectly through metaphor, sarcasm, or subtle\nemotional cues, remains highly challenging. Lightweight models like BERT handle\nexplicit signals but fail on subtle implicit ones, while large language models\n(LLMs) capture nuance at prohibitive computational cost. To address this gap,\nwe propose a two-stage voting architecture that balances efficiency and\nrobustness. In Stage 1, a lightweight BERT classifier rapidly resolves\nhigh-confidence explicit cases. In Stage 2, ambiguous inputs are escalated to\neither (i) a multi-perspective LLM voting framework to maximize recall on\nimplicit ideation, or (ii) a feature-based ML ensemble guided by\npsychologically grounded indicators extracted via prompt-engineered LLMs for\nefficiency and interpretability. To the best of our knowledge, this is among\nthe first works to operationalize LLM-extracted psychological features as\nstructured vectors for suicide risk detection. On two complementary datasets -\nexplicit-dominant Reddit and implicit-only DeepSuiMind - our framework\noutperforms single-model baselines, achieving 98.0% F1 on explicit cases, 99.7%\non implicit ones, and reducing the cross-domain gap below 2%, while\nsignificantly lowering LLM cost."
                },
                "authors": [
                    {
                        "name": "Yukai Song"
                    },
                    {
                        "name": "Pengfei Zhou"
                    },
                    {
                        "name": "César Escobar-Viera"
                    },
                    {
                        "name": "Candice Biernesser"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Jingtong Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jingtong Hu"
                },
                "author": "Jingtong Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08354v1",
                "updated": "2025-10-09T15:41:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    41,
                    3,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:41:03Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    41,
                    3,
                    3,
                    282,
                    0
                ],
                "title": "Mephisto: Self-Improving Large Language Model-Based Agents for Automated\n  Interpretation of Multi-band Galaxy Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mephisto: Self-Improving Large Language Model-Based Agents for Automated\n  Interpretation of Multi-band Galaxy Observations"
                },
                "summary": "Astronomical research has long relied on human expertise to interpret complex\ndata and formulate scientific hypotheses. In this study, we introduce Mephisto\n-- a multi-agent collaboration framework powered by large language models\n(LLMs) that emulates human-like reasoning for analyzing multi-band galaxy\nobservations. Mephisto interfaces with the CIGALE codebase (a library of\nspectral energy distribution, SED, models) to iteratively refine physical\nmodels against observational data. It conducts deliberate reasoning via tree\nsearch, accumulates knowledge through self-play, and dynamically updates its\nknowledge base. Validated across diverse galaxy populations -- including the\nJames Webb Space Telescope's recently discovered \"Little Red Dot\" galaxies --\nwe show that Mephisto demonstrates proficiency in inferring the physical\nproperties of galaxies from multi-band photometry, positioning it as a\npromising research copilot for astronomers. Unlike prior black-box machine\nlearning approaches in astronomy, Mephisto offers a transparent, human-aligned\nreasoning process that integrates seamlessly with existing research practices.\nThis work underscores the possibility of LLM-driven agent-based research for\nastronomy, establishes a foundation for fully automated, end-to-end artificial\nintelligence (AI)-powered scientific workflows, and unlocks new avenues for\nAI-augmented discoveries in astronomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Astronomical research has long relied on human expertise to interpret complex\ndata and formulate scientific hypotheses. In this study, we introduce Mephisto\n-- a multi-agent collaboration framework powered by large language models\n(LLMs) that emulates human-like reasoning for analyzing multi-band galaxy\nobservations. Mephisto interfaces with the CIGALE codebase (a library of\nspectral energy distribution, SED, models) to iteratively refine physical\nmodels against observational data. It conducts deliberate reasoning via tree\nsearch, accumulates knowledge through self-play, and dynamically updates its\nknowledge base. Validated across diverse galaxy populations -- including the\nJames Webb Space Telescope's recently discovered \"Little Red Dot\" galaxies --\nwe show that Mephisto demonstrates proficiency in inferring the physical\nproperties of galaxies from multi-band photometry, positioning it as a\npromising research copilot for astronomers. Unlike prior black-box machine\nlearning approaches in astronomy, Mephisto offers a transparent, human-aligned\nreasoning process that integrates seamlessly with existing research practices.\nThis work underscores the possibility of LLM-driven agent-based research for\nastronomy, establishes a foundation for fully automated, end-to-end artificial\nintelligence (AI)-powered scientific workflows, and unlocks new avenues for\nAI-augmented discoveries in astronomy."
                },
                "authors": [
                    {
                        "name": "Zechang Sun"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "Yaobo Liang"
                    },
                    {
                        "name": "Nan Duan"
                    },
                    {
                        "name": "Song Huang"
                    },
                    {
                        "name": "Zheng Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cai"
                },
                "author": "Zheng Cai",
                "arxiv_comment": "17 pages main text + 13 pages appendix. A conference abstract is\n  available at arXiv:2409.14807. Submitted to AAS journal. Comments and\n  feedback are welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08338v1",
                "updated": "2025-10-09T15:24:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    24,
                    48,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:24:48Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    24,
                    48,
                    3,
                    282,
                    0
                ],
                "title": "LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation\n  of Likert Ratings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation\n  of Likert Ratings"
                },
                "summary": "Consumer research costs companies billions annually yet suffers from panel\nbiases and limited scale. Large language models (LLMs) offer an alternative by\nsimulating synthetic consumers, but produce unrealistic response distributions\nwhen asked directly for numerical ratings. We present semantic similarity\nrating (SSR), a method that elicits textual responses from LLMs and maps these\nto Likert distributions using embedding similarity to reference statements.\nTesting on an extensive dataset comprising 57 personal care product surveys\nconducted by a leading corporation in that market (9,300 human responses), SSR\nachieves 90% of human test-retest reliability while maintaining realistic\nresponse distributions (KS similarity > 0.85). Additionally, these synthetic\nrespondents provide rich qualitative feedback explaining their ratings. This\nframework enables scalable consumer research simulations while preserving\ntraditional survey metrics and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consumer research costs companies billions annually yet suffers from panel\nbiases and limited scale. Large language models (LLMs) offer an alternative by\nsimulating synthetic consumers, but produce unrealistic response distributions\nwhen asked directly for numerical ratings. We present semantic similarity\nrating (SSR), a method that elicits textual responses from LLMs and maps these\nto Likert distributions using embedding similarity to reference statements.\nTesting on an extensive dataset comprising 57 personal care product surveys\nconducted by a leading corporation in that market (9,300 human responses), SSR\nachieves 90% of human test-retest reliability while maintaining realistic\nresponse distributions (KS similarity > 0.85). Additionally, these synthetic\nrespondents provide rich qualitative feedback explaining their ratings. This\nframework enables scalable consumer research simulations while preserving\ntraditional survey metrics and interpretability."
                },
                "authors": [
                    {
                        "name": "Benjamin F. Maier"
                    },
                    {
                        "name": "Ulf Aslak"
                    },
                    {
                        "name": "Luca Fiaschi"
                    },
                    {
                        "name": "Nina Rismal"
                    },
                    {
                        "name": "Kemble Fletcher"
                    },
                    {
                        "name": "Christian C. Luhmann"
                    },
                    {
                        "name": "Robbie Dow"
                    },
                    {
                        "name": "Kli Pappas"
                    },
                    {
                        "name": "Thomas V. Wiecki"
                    }
                ],
                "author_detail": {
                    "name": "Thomas V. Wiecki"
                },
                "author": "Thomas V. Wiecki",
                "arxiv_comment": "28 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00566v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00566v4",
                "updated": "2025-10-09T15:23:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    23,
                    15,
                    3,
                    282,
                    0
                ],
                "published": "2025-03-01T17:29:26Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    17,
                    29,
                    26,
                    5,
                    60,
                    0
                ],
                "title": "Instructor-Worker Large Language Model System for Policy Recommendation:\n  a Case Study on Air Quality Analysis of the January 2025 Los Angeles\n  Wildfires",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instructor-Worker Large Language Model System for Policy Recommendation:\n  a Case Study on Air Quality Analysis of the January 2025 Los Angeles\n  Wildfires"
                },
                "summary": "The Los Angeles wildfires of January 2025 caused more than 250 billion\ndollars in damage and lasted for nearly an entire month before containment.\nFollowing our previous work, the Digital Twin Building, we modify and leverage\nthe multi-agent large language model framework as well as the cloud-mapping\nintegration to study the air quality during the Los Angeles wildfires. Recent\nadvances in large language models have allowed for out-of-the-box automated\nlarge-scale data analysis. We use a multi-agent large language system comprised\nof an Instructor agent and Worker agents. Upon receiving the users'\ninstructions, the Instructor agent retrieves the data from the cloud platform\nand produces instruction prompts to the Worker agents. The Worker agents then\nanalyze the data and provide summaries. The summaries are finally input back\ninto the Instructor agent, which then provides the final data analysis. We test\nthis system's capability for data-based policy recommendation by assessing our\nInstructor-Worker LLM system's health recommendations based on air quality\nduring the Los Angeles wildfires.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Los Angeles wildfires of January 2025 caused more than 250 billion\ndollars in damage and lasted for nearly an entire month before containment.\nFollowing our previous work, the Digital Twin Building, we modify and leverage\nthe multi-agent large language model framework as well as the cloud-mapping\nintegration to study the air quality during the Los Angeles wildfires. Recent\nadvances in large language models have allowed for out-of-the-box automated\nlarge-scale data analysis. We use a multi-agent large language system comprised\nof an Instructor agent and Worker agents. Upon receiving the users'\ninstructions, the Instructor agent retrieves the data from the cloud platform\nand produces instruction prompts to the Worker agents. The Worker agents then\nanalyze the data and provide summaries. The summaries are finally input back\ninto the Instructor agent, which then provides the final data analysis. We test\nthis system's capability for data-based policy recommendation by assessing our\nInstructor-Worker LLM system's health recommendations based on air quality\nduring the Los Angeles wildfires."
                },
                "authors": [
                    {
                        "name": "Kyle Gao"
                    },
                    {
                        "name": "Dening Lu"
                    },
                    {
                        "name": "Liangzhi Li"
                    },
                    {
                        "name": "Nan Chen"
                    },
                    {
                        "name": "Hongjie He"
                    },
                    {
                        "name": "Linlin Xu"
                    },
                    {
                        "name": "Jonathan Li"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Li"
                },
                "author": "Jonathan Li",
                "arxiv_doi": "10.1016/j.jag.2025.104774",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jag.2025.104774",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.00566v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00566v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08329v1",
                "updated": "2025-10-09T15:17:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    17,
                    28,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:17:28Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    17,
                    28,
                    3,
                    282,
                    0
                ],
                "title": "AutoRed: A Free-form Adversarial Prompt Generation Framework for\n  Automated Red Teaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRed: A Free-form Adversarial Prompt Generation Framework for\n  Automated Red Teaming"
                },
                "summary": "The safety of Large Language Models (LLMs) is crucial for the development of\ntrustworthy AI applications. Existing red teaming methods often rely on seed\ninstructions, which limits the semantic diversity of the synthesized\nadversarial prompts. We propose AutoRed, a free-form adversarial prompt\ngeneration framework that removes the need for seed instructions. AutoRed\noperates in two stages: (1) persona-guided adversarial instruction generation,\nand (2) a reflection loop to iteratively refine low-quality prompts. To improve\nefficiency, we introduce a verifier to assess prompt harmfulness without\nquerying the target models. Using AutoRed, we build two red teaming datasets --\nAutoRed-Medium and AutoRed-Hard -- and evaluate eight state-of-the-art LLMs.\nAutoRed achieves higher attack success rates and better generalization than\nexisting baselines. Our results highlight the limitations of seed-based\napproaches and demonstrate the potential of free-form red teaming for LLM\nsafety evaluation. We will open source our datasets in the near future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safety of Large Language Models (LLMs) is crucial for the development of\ntrustworthy AI applications. Existing red teaming methods often rely on seed\ninstructions, which limits the semantic diversity of the synthesized\nadversarial prompts. We propose AutoRed, a free-form adversarial prompt\ngeneration framework that removes the need for seed instructions. AutoRed\noperates in two stages: (1) persona-guided adversarial instruction generation,\nand (2) a reflection loop to iteratively refine low-quality prompts. To improve\nefficiency, we introduce a verifier to assess prompt harmfulness without\nquerying the target models. Using AutoRed, we build two red teaming datasets --\nAutoRed-Medium and AutoRed-Hard -- and evaluate eight state-of-the-art LLMs.\nAutoRed achieves higher attack success rates and better generalization than\nexisting baselines. Our results highlight the limitations of seed-based\napproaches and demonstrate the potential of free-form red teaming for LLM\nsafety evaluation. We will open source our datasets in the near future."
                },
                "authors": [
                    {
                        "name": "Muxi Diao"
                    },
                    {
                        "name": "Yutao Mou"
                    },
                    {
                        "name": "Keqing He"
                    },
                    {
                        "name": "Hanbo Song"
                    },
                    {
                        "name": "Lulu Zhao"
                    },
                    {
                        "name": "Shikun Zhang"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Kongming Liang"
                    },
                    {
                        "name": "Zhanyu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zhanyu Ma"
                },
                "author": "Zhanyu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08317v1",
                "updated": "2025-10-09T15:02:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    2,
                    56,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:02:56Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    2,
                    56,
                    3,
                    282,
                    0
                ],
                "title": "Iterated Agent for Symbolic Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterated Agent for Symbolic Regression"
                },
                "summary": "Symbolic regression (SR), the automated discovery of mathematical expressions\nfrom data, is a cornerstone of scientific inquiry. However, it is often\nhindered by the combinatorial explosion of the search space and a tendency to\noverfit. Popular methods, rooted in genetic programming, explore this space\nsyntactically, often yielding overly complex, uninterpretable models. This\npaper introduces IdeaSearchFitter, a framework that employs Large Language\nModels (LLMs) as semantic operators within an evolutionary search. By\ngenerating candidate expressions guided by natural-language rationales, our\nmethod biases discovery towards models that are not only accurate but also\nconceptually coherent and interpretable. We demonstrate IdeaSearchFitter's\nefficacy across diverse challenges: it achieves competitive, noise-robust\nperformance on the Feynman Symbolic Regression Database (FSReD), outperforming\nseveral strong baselines; discovers mechanistically aligned models with good\naccuracy-complexity trade-offs on real-world data; and derives compact,\nphysically-motivated parametrizations for Parton Distribution Functions in a\nfrontier high-energy physics application. IdeaSearchFitter is a specialized\nmodule within our broader iterated agent framework, IdeaSearch, which is\npublicly available at https://www.ideasearch.cn/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic regression (SR), the automated discovery of mathematical expressions\nfrom data, is a cornerstone of scientific inquiry. However, it is often\nhindered by the combinatorial explosion of the search space and a tendency to\noverfit. Popular methods, rooted in genetic programming, explore this space\nsyntactically, often yielding overly complex, uninterpretable models. This\npaper introduces IdeaSearchFitter, a framework that employs Large Language\nModels (LLMs) as semantic operators within an evolutionary search. By\ngenerating candidate expressions guided by natural-language rationales, our\nmethod biases discovery towards models that are not only accurate but also\nconceptually coherent and interpretable. We demonstrate IdeaSearchFitter's\nefficacy across diverse challenges: it achieves competitive, noise-robust\nperformance on the Feynman Symbolic Regression Database (FSReD), outperforming\nseveral strong baselines; discovers mechanistically aligned models with good\naccuracy-complexity trade-offs on real-world data; and derives compact,\nphysically-motivated parametrizations for Parton Distribution Functions in a\nfrontier high-energy physics application. IdeaSearchFitter is a specialized\nmodule within our broader iterated agent framework, IdeaSearch, which is\npublicly available at https://www.ideasearch.cn/."
                },
                "authors": [
                    {
                        "name": "Zhuo-Yang Song"
                    },
                    {
                        "name": "Zeyu Cai"
                    },
                    {
                        "name": "Shutao Zhang"
                    },
                    {
                        "name": "Jiashen Wei"
                    },
                    {
                        "name": "Jichen Pan"
                    },
                    {
                        "name": "Shi Qiu"
                    },
                    {
                        "name": "Qing-Hong Cao"
                    },
                    {
                        "name": "Tie-Jiun Hou"
                    },
                    {
                        "name": "Xiaohui Liu"
                    },
                    {
                        "name": "Ming-xing Luo"
                    },
                    {
                        "name": "Hua Xing Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Xing Zhu"
                },
                "author": "Hua Xing Zhu",
                "arxiv_comment": "45 pages, 22 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20749v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20749v2",
                "updated": "2025-10-09T15:01:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    1,
                    48,
                    3,
                    282,
                    0
                ],
                "published": "2024-10-28T05:28:51Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    5,
                    28,
                    51,
                    0,
                    302,
                    0
                ],
                "title": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs"
                },
                "summary": "Despite the impressive generative abilities of black-box large language\nmodels (LLMs), their inherent opacity hinders further advancements in\ncapabilities such as reasoning, planning, and personalization. Existing works\naim to enhance LLM capabilities via domain-specific adaptation, which require\nadditional training on accessible model parameters, an infeasible option for\nblack-box LLMs. To address this challenge, we introduce Matryoshka Pilot\n(M-Pilot), a lightweight white-box LLM controller that guides a large-scale\nblack-box LLM generator by decomposing complex tasks into a series of\nintermediate outputs. Specifically, we consider the black-box LLM as an\nenvironment, with M-Pilot serving as a policy to provide intermediate guidance\nthrough prompts for driving the black-box LLM. M-Pilot is trained to pivot the\noutputs of the black-box LLM aligning with preferences during iterative\ninteraction, which enables controllable multi-turn generation and\nself-improvement in optimizing intermediate guidance. Empirical evaluations on\ndiverse tasks demonstrate that our method effectively enhances the capabilities\nof black-box LLMs in complex, long-horizon tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive generative abilities of black-box large language\nmodels (LLMs), their inherent opacity hinders further advancements in\ncapabilities such as reasoning, planning, and personalization. Existing works\naim to enhance LLM capabilities via domain-specific adaptation, which require\nadditional training on accessible model parameters, an infeasible option for\nblack-box LLMs. To address this challenge, we introduce Matryoshka Pilot\n(M-Pilot), a lightweight white-box LLM controller that guides a large-scale\nblack-box LLM generator by decomposing complex tasks into a series of\nintermediate outputs. Specifically, we consider the black-box LLM as an\nenvironment, with M-Pilot serving as a policy to provide intermediate guidance\nthrough prompts for driving the black-box LLM. M-Pilot is trained to pivot the\noutputs of the black-box LLM aligning with preferences during iterative\ninteraction, which enables controllable multi-turn generation and\nself-improvement in optimizing intermediate guidance. Empirical evaluations on\ndiverse tasks demonstrate that our method effectively enhances the capabilities\nof black-box LLMs in complex, long-horizon tasks."
                },
                "authors": [
                    {
                        "name": "Changhao Li"
                    },
                    {
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "name": "Rushi Qiang"
                    },
                    {
                        "name": "Haotian Sun"
                    },
                    {
                        "name": "Hanjun Dai"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Bo Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dai"
                },
                "author": "Bo Dai",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20749v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13499v2",
                "updated": "2025-10-09T14:59:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    59,
                    41,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-16T19:55:25Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    19,
                    55,
                    25,
                    1,
                    259,
                    0
                ],
                "title": "Reproducible workflow for online AI in digital health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducible workflow for online AI in digital health"
                },
                "summary": "Online artificial intelligence (AI) algorithms are an important component of\ndigital health interventions. These online algorithms are designed to\ncontinually learn and improve their performance as streaming data is collected\non individuals. Deploying online AI presents a key challenge: balancing\nadaptability of online AI with reproducibility. Online AI in digital\ninterventions is a rapidly evolving area, driven by advances in algorithms,\nsensors, software, and devices. Digital health intervention development and\ndeployment is a continuous process, where implementation - including the AI\ndecision-making algorithm - is interspersed with cycles of re-development and\noptimization. Each deployment informs the next, making iterative deployment a\ndefining characteristic of this field. This iterative nature underscores the\nimportance of reproducibility: data collected across deployments must be\naccurately stored to have scientific utility, algorithm behavior must be\nauditable, and results must be comparable over time to facilitate scientific\ndiscovery and trustworthy refinement. This paper proposes a reproducible\nscientific workflow for developing, deploying, and analyzing online AI\ndecision-making algorithms in digital health interventions. Grounded in\npractical experience from multiple real-world deployments, this workflow\naddresses key challenges to reproducibility across all phases of the online AI\nalgorithm development life-cycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online artificial intelligence (AI) algorithms are an important component of\ndigital health interventions. These online algorithms are designed to\ncontinually learn and improve their performance as streaming data is collected\non individuals. Deploying online AI presents a key challenge: balancing\nadaptability of online AI with reproducibility. Online AI in digital\ninterventions is a rapidly evolving area, driven by advances in algorithms,\nsensors, software, and devices. Digital health intervention development and\ndeployment is a continuous process, where implementation - including the AI\ndecision-making algorithm - is interspersed with cycles of re-development and\noptimization. Each deployment informs the next, making iterative deployment a\ndefining characteristic of this field. This iterative nature underscores the\nimportance of reproducibility: data collected across deployments must be\naccurately stored to have scientific utility, algorithm behavior must be\nauditable, and results must be comparable over time to facilitate scientific\ndiscovery and trustworthy refinement. This paper proposes a reproducible\nscientific workflow for developing, deploying, and analyzing online AI\ndecision-making algorithms in digital health interventions. Grounded in\npractical experience from multiple real-world deployments, this workflow\naddresses key challenges to reproducibility across all phases of the online AI\nalgorithm development life-cycle."
                },
                "authors": [
                    {
                        "name": "Susobhan Ghosh"
                    },
                    {
                        "name": "Bhanu T. Gullapalli"
                    },
                    {
                        "name": "Daiqi Gao"
                    },
                    {
                        "name": "Asim Gazi"
                    },
                    {
                        "name": "Anna Trella"
                    },
                    {
                        "name": "Ziping Xu"
                    },
                    {
                        "name": "Kelly Zhang"
                    },
                    {
                        "name": "Susan A. Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Susan A. Murphy"
                },
                "author": "Susan A. Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13220v2",
                "updated": "2025-10-09T14:57:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    57,
                    42,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-17T11:49:16Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    11,
                    49,
                    16,
                    6,
                    229,
                    0
                ],
                "title": "MCPSecBench: A Systematic Security Benchmark and Playground for Testing\n  Model Context Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCPSecBench: A Systematic Security Benchmark and Playground for Testing\n  Model Context Protocols"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications via the Model Context Protocol (MCP), a universal, open standard\nfor connecting AI agents with data sources and external tools. While MCP\nenhances the capabilities of LLM-based agents, it also introduces new security\nrisks and expands their attack surfaces. In this paper, we present the first\nsystematic taxonomy of MCP security, identifying 17 attack types across 4\nprimary attack surfaces. We introduce MCPSecBench, a comprehensive security\nbenchmark and playground that integrates prompt datasets, MCP servers, MCP\nclients, attack scripts, and protection mechanisms to evaluate these attacks\nacross three major MCP providers. Our benchmark is modular and extensible,\nallowing researchers to incorporate custom implementations of clients, servers,\nand transport protocols for systematic security assessment. Experimental\nresults show that over 85% of the identified attacks successfully compromise at\nleast one platform, with core vulnerabilities universally affecting Claude,\nOpenAI, and Cursor, while prompt-based and tool-centric attacks exhibit\nconsiderable variability across different hosts and models. In addition,\ncurrent protection mechanisms have little effect against these attacks.\nOverall, MCPSecBench standardizes the evaluation of MCP security and enables\nrigorous testing across all MCP layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications via the Model Context Protocol (MCP), a universal, open standard\nfor connecting AI agents with data sources and external tools. While MCP\nenhances the capabilities of LLM-based agents, it also introduces new security\nrisks and expands their attack surfaces. In this paper, we present the first\nsystematic taxonomy of MCP security, identifying 17 attack types across 4\nprimary attack surfaces. We introduce MCPSecBench, a comprehensive security\nbenchmark and playground that integrates prompt datasets, MCP servers, MCP\nclients, attack scripts, and protection mechanisms to evaluate these attacks\nacross three major MCP providers. Our benchmark is modular and extensible,\nallowing researchers to incorporate custom implementations of clients, servers,\nand transport protocols for systematic security assessment. Experimental\nresults show that over 85% of the identified attacks successfully compromise at\nleast one platform, with core vulnerabilities universally affecting Claude,\nOpenAI, and Cursor, while prompt-based and tool-centric attacks exhibit\nconsiderable variability across different hosts and models. In addition,\ncurrent protection mechanisms have little effect against these attacks.\nOverall, MCPSecBench standardizes the evaluation of MCP security and enables\nrigorous testing across all MCP layers."
                },
                "authors": [
                    {
                        "name": "Yixuan Yang"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Yufan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yufan Chen"
                },
                "author": "Yufan Chen",
                "arxiv_comment": "This is a technical report from Lingnan University, Hong Kong. Code\n  is available at https://github.com/AIS2Lab/MCPSecBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08303v1",
                "updated": "2025-10-09T14:55:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    55,
                    4,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:55:04Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    55,
                    4,
                    3,
                    282,
                    0
                ],
                "title": "Dynamic Features Adaptation in Networking: Toward Flexible training and\n  Explainable inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Features Adaptation in Networking: Toward Flexible training and\n  Explainable inference"
                },
                "summary": "As AI becomes a native component of 6G network control, AI models must adapt\nto continuously changing conditions, including the introduction of new features\nand measurements driven by multi-vendor deployments, hardware upgrades, and\nevolving service requirements. To address this growing need for flexible\nlearning in non-stationary environments, this vision paper highlights Adaptive\nRandom Forests (ARFs) as a reliable solution for dynamic feature adaptation in\ncommunication network scenarios. We show that iterative training of ARFs can\neffectively lead to stable predictions, with accuracy improving over time as\nmore features are added. In addition, we highlight the importance of\nexplainability in AI-driven networks, proposing Drift-Aware Feature Importance\n(DAFI) as an efficient XAI feature importance (FI) method. DAFI uses a\ndistributional drift detector to signal when to apply computationally intensive\nFI methods instead of lighter alternatives. Our tests on 3 different datasets\nindicate that our approach reduces runtime by up to 2 times, while producing\nmore consistent feature importance values. Together, ARFs and DAFI provide a\npromising framework to build flexible AI methods adapted to 6G network\nuse-cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI becomes a native component of 6G network control, AI models must adapt\nto continuously changing conditions, including the introduction of new features\nand measurements driven by multi-vendor deployments, hardware upgrades, and\nevolving service requirements. To address this growing need for flexible\nlearning in non-stationary environments, this vision paper highlights Adaptive\nRandom Forests (ARFs) as a reliable solution for dynamic feature adaptation in\ncommunication network scenarios. We show that iterative training of ARFs can\neffectively lead to stable predictions, with accuracy improving over time as\nmore features are added. In addition, we highlight the importance of\nexplainability in AI-driven networks, proposing Drift-Aware Feature Importance\n(DAFI) as an efficient XAI feature importance (FI) method. DAFI uses a\ndistributional drift detector to signal when to apply computationally intensive\nFI methods instead of lighter alternatives. Our tests on 3 different datasets\nindicate that our approach reduces runtime by up to 2 times, while producing\nmore consistent feature importance values. Together, ARFs and DAFI provide a\npromising framework to build flexible AI methods adapted to 6G network\nuse-cases."
                },
                "authors": [
                    {
                        "name": "Yannis Belkhiter"
                    },
                    {
                        "name": "Seshu Tirupathi"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Merim Dzaferagic"
                    },
                    {
                        "name": "John D. Kelleher"
                    }
                ],
                "author_detail": {
                    "name": "John D. Kelleher"
                },
                "author": "John D. Kelleher",
                "arxiv_comment": "Accepted at AI4NextG Workshop, NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13925v2",
                "updated": "2025-10-09T14:52:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    52,
                    21,
                    3,
                    282,
                    0
                ],
                "published": "2025-02-19T18:04:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    4,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual\n  Narratives in Image Sequences?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual\n  Narratives in Image Sequences?"
                },
                "summary": "Large Multimodal Models (LMMs) have achieved remarkable success across\nvarious visual-language tasks. However, existing benchmarks predominantly focus\non single-image understanding, leaving the analysis of image sequences largely\nunexplored. To address this limitation, we introduce StripCipher, a\ncomprehensive benchmark designed to evaluate capabilities of LMMs to comprehend\nand reason over sequential images. StripCipher comprises a human-annotated\ndataset and three challenging subtasks: visual narrative comprehension,\ncontextual frame prediction, and temporal narrative reordering. Our evaluation\nof 16 state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a\nsignificant performance gap compared to human capabilities, particularly in\ntasks that require reordering shuffled sequential images. For instance, GPT-4o\nachieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower\nthan human performance. Further quantitative analysis discuss several factors,\nsuch as input format of images, affecting the performance of LLMs in sequential\nunderstanding, underscoring the fundamental challenges that remain in the\ndevelopment of LMMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) have achieved remarkable success across\nvarious visual-language tasks. However, existing benchmarks predominantly focus\non single-image understanding, leaving the analysis of image sequences largely\nunexplored. To address this limitation, we introduce StripCipher, a\ncomprehensive benchmark designed to evaluate capabilities of LMMs to comprehend\nand reason over sequential images. StripCipher comprises a human-annotated\ndataset and three challenging subtasks: visual narrative comprehension,\ncontextual frame prediction, and temporal narrative reordering. Our evaluation\nof 16 state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a\nsignificant performance gap compared to human capabilities, particularly in\ntasks that require reordering shuffled sequential images. For instance, GPT-4o\nachieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower\nthan human performance. Further quantitative analysis discuss several factors,\nsuch as input format of images, affecting the performance of LLMs in sequential\nunderstanding, underscoring the fundamental challenges that remain in the\ndevelopment of LMMs."
                },
                "authors": [
                    {
                        "name": "Xiaochen Wang"
                    },
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Jialin Song"
                    },
                    {
                        "name": "Longyu Guan"
                    },
                    {
                        "name": "Yixin Yang"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Weiyao Luo"
                    },
                    {
                        "name": "Yifan Pu"
                    },
                    {
                        "name": "Yiru Wang"
                    },
                    {
                        "name": "Xiangdi Meng"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12450v2",
                "updated": "2025-10-09T14:51:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    51,
                    59,
                    3,
                    282,
                    0
                ],
                "published": "2025-06-14T11:09:50Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    11,
                    9,
                    50,
                    5,
                    165,
                    0
                ],
                "title": "Language Surgery in Multilingual Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Surgery in Multilingual Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across tasks and languages, revolutionizing natural language\nprocessing. This paper investigates the naturally emerging representation\nalignment in LLMs, particularly in the middle layers, and its implications for\ndisentangling language-specific and language-agnostic information. We\nempirically confirm the existence of this alignment, analyze its behavior in\ncomparison to explicitly designed alignment models, and demonstrate its\npotential for language-specific manipulation without semantic degradation.\nBuilding on these findings, we propose Inference-Time Language Control (ITLC),\na novel method that leverages latent injection to enable precise cross-lingual\nlanguage control and mitigate language confusion in LLMs. Our experiments\nhighlight ITLC's strong cross-lingual control capabilities while preserving\nsemantic integrity in target languages. Furthermore, we demonstrate its\neffectiveness in alleviating the cross-lingual language confusion problem,\nwhich persists even in current large-scale LLMs, leading to inconsistent\nlanguage generation. This work advances our understanding of representation\nalignment in LLMs and introduces a practical solution for enhancing their\nmonolingual and cross-lingual performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across tasks and languages, revolutionizing natural language\nprocessing. This paper investigates the naturally emerging representation\nalignment in LLMs, particularly in the middle layers, and its implications for\ndisentangling language-specific and language-agnostic information. We\nempirically confirm the existence of this alignment, analyze its behavior in\ncomparison to explicitly designed alignment models, and demonstrate its\npotential for language-specific manipulation without semantic degradation.\nBuilding on these findings, we propose Inference-Time Language Control (ITLC),\na novel method that leverages latent injection to enable precise cross-lingual\nlanguage control and mitigate language confusion in LLMs. Our experiments\nhighlight ITLC's strong cross-lingual control capabilities while preserving\nsemantic integrity in target languages. Furthermore, we demonstrate its\neffectiveness in alleviating the cross-lingual language confusion problem,\nwhich persists even in current large-scale LLMs, leading to inconsistent\nlanguage generation. This work advances our understanding of representation\nalignment in LLMs and introduces a practical solution for enhancing their\nmonolingual and cross-lingual performance."
                },
                "authors": [
                    {
                        "name": "Joanito Agili Lopo"
                    },
                    {
                        "name": "Muhammad Ravi Shulthan Habibi"
                    },
                    {
                        "name": "Tack Hwa Wong"
                    },
                    {
                        "name": "Muhammad Ilham Ghozali"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Genta Indra Winata"
                    },
                    {
                        "name": "Peerat Limkonchotiwat"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Samuel Cahyawijaya"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Cahyawijaya"
                },
                "author": "Samuel Cahyawijaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16723v2",
                "updated": "2025-10-09T14:40:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    40,
                    25,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-22T14:32:23Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    32,
                    23,
                    3,
                    142,
                    0
                ],
                "title": "LLM Fingerprinting via Semantically Conditioned Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Fingerprinting via Semantically Conditioned Watermarks"
                },
                "summary": "Most LLM fingerprinting methods teach the model to respond to a few fixed\nqueries with predefined atypical responses (keys). This memorization often does\nnot survive common deployment steps such as finetuning or quantization, and\nsuch keys can be easily detected and filtered from LLM responses, ultimately\nbreaking the fingerprint. To overcome these limitations we introduce LLM\nfingerprinting via semantically conditioned watermarks, replacing fixed query\nsets with a broad semantic domain, and replacing brittle atypical keys with a\nstatistical watermarking signal diffused throughout each response. After\nteaching the model to watermark its responses only to prompts from a\npredetermined domain e.g., French language, the model owner can use queries\nfrom that domain to reliably detect the fingerprint and verify ownership. As we\nconfirm in our thorough experimental evaluation, our fingerprint is both\nstealthy and robust to all common deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most LLM fingerprinting methods teach the model to respond to a few fixed\nqueries with predefined atypical responses (keys). This memorization often does\nnot survive common deployment steps such as finetuning or quantization, and\nsuch keys can be easily detected and filtered from LLM responses, ultimately\nbreaking the fingerprint. To overcome these limitations we introduce LLM\nfingerprinting via semantically conditioned watermarks, replacing fixed query\nsets with a broad semantic domain, and replacing brittle atypical keys with a\nstatistical watermarking signal diffused throughout each response. After\nteaching the model to watermark its responses only to prompts from a\npredetermined domain e.g., French language, the model owner can use queries\nfrom that domain to reliably detect the fingerprint and verify ownership. As we\nconfirm in our thorough experimental evaluation, our fingerprint is both\nstealthy and robust to all common deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Thibaud Gloaguen"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Nikola Jovanović"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08638v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08638v4",
                "updated": "2025-10-09T14:39:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    39,
                    40,
                    3,
                    282,
                    0
                ],
                "published": "2025-02-12T18:54:37Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    54,
                    37,
                    2,
                    43,
                    0
                ],
                "title": "Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples"
                },
                "summary": "The evaluation of cross-lingual semantic search models is often limited to\nexisting datasets from tasks such as information retrieval and semantic textual\nsimilarity. We introduce Cross-Lingual Semantic Discrimination (CLSD), a\nlightweight evaluation task that requires only parallel sentences and a Large\nLanguage Model (LLM) to generate adversarial distractors. CLSD measures an\nembedding model's ability to rank the true parallel sentence above semantically\nmisleading but lexically similar alternatives. As a case study, we construct\nCLSD datasets for German--French in the news domain. Our experiments show that\nmodels fine-tuned for retrieval tasks benefit from pivoting through English,\nwhereas bitext mining models perform best in direct cross-lingual settings. A\nfine-grained similarity analysis further reveals that embedding models differ\nin their sensitivity to linguistic perturbations. We release our code and\ndatasets under AGPL-3.0:\nhttps://github.com/impresso/cross_lingual_semantic_discrimination",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of cross-lingual semantic search models is often limited to\nexisting datasets from tasks such as information retrieval and semantic textual\nsimilarity. We introduce Cross-Lingual Semantic Discrimination (CLSD), a\nlightweight evaluation task that requires only parallel sentences and a Large\nLanguage Model (LLM) to generate adversarial distractors. CLSD measures an\nembedding model's ability to rank the true parallel sentence above semantically\nmisleading but lexically similar alternatives. As a case study, we construct\nCLSD datasets for German--French in the news domain. Our experiments show that\nmodels fine-tuned for retrieval tasks benefit from pivoting through English,\nwhereas bitext mining models perform best in direct cross-lingual settings. A\nfine-grained similarity analysis further reveals that embedding models differ\nin their sensitivity to linguistic perturbations. We release our code and\ndatasets under AGPL-3.0:\nhttps://github.com/impresso/cross_lingual_semantic_discrimination"
                },
                "authors": [
                    {
                        "name": "Andrianos Michail"
                    },
                    {
                        "name": "Simon Clematide"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "To appear in EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08638v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08638v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08284v1",
                "updated": "2025-10-09T14:35:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    35,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:35:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    35,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "Neuron-Level Analysis of Cultural Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuron-Level Analysis of Cultural Understanding in Large Language Models"
                },
                "summary": "As large language models (LLMs) are increasingly deployed worldwide, ensuring\ntheir fair and comprehensive cultural understanding is important. However, LLMs\nexhibit cultural bias and limited awareness of underrepresented cultures, while\nthe mechanisms underlying their cultural understanding remain underexplored. To\nfill this gap, we conduct a neuron-level analysis to identify neurons that\ndrive cultural behavior, introducing a gradient-based scoring method with\nadditional filtering for precise refinement. We identify both culture-general\nneurons contributing to cultural understanding regardless of cultures, and\nculture-specific neurons tied to an individual culture. These neurons account\nfor less than 1% of all neurons and are concentrated in shallow to middle MLP\nlayers. We validate their role by showing that suppressing them substantially\ndegrades performance on cultural benchmarks (by up to 30%), while performance\non general natural language understanding (NLU) benchmarks remains largely\nunaffected. Moreover, we show that culture-specific neurons support knowledge\nof not only the target culture, but also related cultures. Finally, we\ndemonstrate that training on NLU benchmarks can diminish models' cultural\nunderstanding when we update modules containing many culture-general neurons.\nThese findings provide insights into the internal mechanisms of LLMs and offer\npractical guidance for model training and engineering. Our code is available at\nhttps://github.com/ynklab/CULNIG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly deployed worldwide, ensuring\ntheir fair and comprehensive cultural understanding is important. However, LLMs\nexhibit cultural bias and limited awareness of underrepresented cultures, while\nthe mechanisms underlying their cultural understanding remain underexplored. To\nfill this gap, we conduct a neuron-level analysis to identify neurons that\ndrive cultural behavior, introducing a gradient-based scoring method with\nadditional filtering for precise refinement. We identify both culture-general\nneurons contributing to cultural understanding regardless of cultures, and\nculture-specific neurons tied to an individual culture. These neurons account\nfor less than 1% of all neurons and are concentrated in shallow to middle MLP\nlayers. We validate their role by showing that suppressing them substantially\ndegrades performance on cultural benchmarks (by up to 30%), while performance\non general natural language understanding (NLU) benchmarks remains largely\nunaffected. Moreover, we show that culture-specific neurons support knowledge\nof not only the target culture, but also related cultures. Finally, we\ndemonstrate that training on NLU benchmarks can diminish models' cultural\nunderstanding when we update modules containing many culture-general neurons.\nThese findings provide insights into the internal mechanisms of LLMs and offer\npractical guidance for model training and engineering. Our code is available at\nhttps://github.com/ynklab/CULNIG"
                },
                "authors": [
                    {
                        "name": "Taisei Yamamoto"
                    },
                    {
                        "name": "Ryoma Kumon"
                    },
                    {
                        "name": "Danushka Bollegala"
                    },
                    {
                        "name": "Hitomi Yanaka"
                    }
                ],
                "author_detail": {
                    "name": "Hitomi Yanaka"
                },
                "author": "Hitomi Yanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08278v1",
                "updated": "2025-10-09T14:32:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    32,
                    21,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:32:21Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    32,
                    21,
                    3,
                    282,
                    0
                ],
                "title": "A Multimodal Depth-Aware Method For Embodied Reference Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multimodal Depth-Aware Method For Embodied Reference Understanding"
                },
                "summary": "Embodied Reference Understanding requires identifying a target object in a\nvisual scene based on both language instructions and pointing cues. While prior\nworks have shown progress in open-vocabulary object detection, they often fail\nin ambiguous scenarios where multiple candidate objects exist in the scene. To\naddress these challenges, we propose a novel ERU framework that jointly\nleverages LLM-based data augmentation, depth-map modality, and a depth-aware\ndecision module. This design enables robust integration of linguistic and\nembodied cues, improving disambiguation in complex or cluttered environments.\nExperimental results on two datasets demonstrate that our approach\nsignificantly outperforms existing baselines, achieving more accurate and\nreliable referent detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Reference Understanding requires identifying a target object in a\nvisual scene based on both language instructions and pointing cues. While prior\nworks have shown progress in open-vocabulary object detection, they often fail\nin ambiguous scenarios where multiple candidate objects exist in the scene. To\naddress these challenges, we propose a novel ERU framework that jointly\nleverages LLM-based data augmentation, depth-map modality, and a depth-aware\ndecision module. This design enables robust integration of linguistic and\nembodied cues, improving disambiguation in complex or cluttered environments.\nExperimental results on two datasets demonstrate that our approach\nsignificantly outperforms existing baselines, achieving more accurate and\nreliable referent detection."
                },
                "authors": [
                    {
                        "name": "Fevziye Irem Eyiokur"
                    },
                    {
                        "name": "Dogucan Yaman"
                    },
                    {
                        "name": "Hazım Kemal Ekenel"
                    },
                    {
                        "name": "Alexander Waibel"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Waibel"
                },
                "author": "Alexander Waibel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09752v2",
                "updated": "2025-10-09T14:31:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    31,
                    29,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-13T12:31:27Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    12,
                    31,
                    27,
                    2,
                    225,
                    0
                ],
                "title": "$μ$-Parametrization for Mixture of Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$μ$-Parametrization for Mixture of Experts"
                },
                "summary": "Recent years have seen a growing interest and adoption of LLMs, with\nMixture-of-Experts (MoE) emerging as a leading architecture in extremely large\nmodels. Currently, the largest open-source models reach over $1$T parameters.\nAt such scales, hyperparameter tuning becomes prohibitively expensive.\nPrecisely for this reason, the $\\mu$Transfer is becoming a key technique. It\nallows for seamless transfer of optimal hyperparameters across model scales,\nresulting in a huge reduction in tuning costs. However, existing work has\nprimarily focused on dense LLMs, leaving MoE architectures unexplored. In this\nwork, we derive a $\\mu$-Parameterization for MoE, providing theoretical\nguarantees for feature learning across model widths. Our experiments\ndemonstrate that the optimal learning rate reliably transfers across model\nsizes, establishing a foundation for efficient hyperparameter tuning in\nlarge-scale MoE models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen a growing interest and adoption of LLMs, with\nMixture-of-Experts (MoE) emerging as a leading architecture in extremely large\nmodels. Currently, the largest open-source models reach over $1$T parameters.\nAt such scales, hyperparameter tuning becomes prohibitively expensive.\nPrecisely for this reason, the $\\mu$Transfer is becoming a key technique. It\nallows for seamless transfer of optimal hyperparameters across model scales,\nresulting in a huge reduction in tuning costs. However, existing work has\nprimarily focused on dense LLMs, leaving MoE architectures unexplored. In this\nwork, we derive a $\\mu$-Parameterization for MoE, providing theoretical\nguarantees for feature learning across model widths. Our experiments\ndemonstrate that the optimal learning rate reliably transfers across model\nsizes, establishing a foundation for efficient hyperparameter tuning in\nlarge-scale MoE models."
                },
                "authors": [
                    {
                        "name": "Jan Małaśnicki"
                    },
                    {
                        "name": "Kamil Ciebiera"
                    },
                    {
                        "name": "Mateusz Boruń"
                    },
                    {
                        "name": "Maciej Pióro"
                    },
                    {
                        "name": "Jan Ludziejewski"
                    },
                    {
                        "name": "Maciej Stefaniak"
                    },
                    {
                        "name": "Michał Krutul"
                    },
                    {
                        "name": "Sebastian Jaszczur"
                    },
                    {
                        "name": "Marek Cygan"
                    },
                    {
                        "name": "Kamil Adamczewski"
                    },
                    {
                        "name": "Jakub Krajewski"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Krajewski"
                },
                "author": "Jakub Krajewski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16567v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16567v3",
                "updated": "2025-10-09T14:23:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    23,
                    19,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-22T11:59:44Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    11,
                    59,
                    44,
                    3,
                    142,
                    0
                ],
                "title": "Watch your steps: Dormant Adversarial Behaviors that Activate upon LLM\n  Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch your steps: Dormant Adversarial Behaviors that Activate upon LLM\n  Finetuning"
                },
                "summary": "Finetuning open-weight Large Language Models (LLMs) is standard practice for\nachieving task-specific performance improvements. Until now, finetuning has\nbeen regarded as a controlled and secure process in which training on benign\ndatasets leads to predictable behaviors. In this paper, we demonstrate, for the\nfirst time, that an adversary can create compromised LLMs that are performant\nand benign, yet exhibit adversarial behaviors once finetuned by downstream\nusers. To this end, we propose an attack, FAB (Finetuning-activated Adversarial\nBehaviors), which compromises an LLM via meta-learning techniques that simulate\ndownstream finetuning, explicitly optimizing for the emergence of adversarial\nbehaviors in the finetuned models. At the same time, the compromised LLM is\nregularized to retain general capabilities and to exhibit no adversarial\nbehaviors prior to finetuning. As a result, when users finetune (e.g.,\ninstruction-tuning, distillation, DPO) the seemingly benign model on their own\ndatasets, they unknowingly trigger its dormant adversarial behavior. We\nexperimentally demonstrate the effectiveness of FAB across multiple LLMs and\nthree commonly considered target behaviors: unsolicited advertising,\njailbreakability, and over-refusal. We show that FAB-triggers are robust to\nvarious finetuning choices made by the user (e.g., dataset, number of steps,\nscheduler, post-training algorithm). Our findings challenge prevailing\nassumptions on the security of finetuning, revealing a critical attack vector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning open-weight Large Language Models (LLMs) is standard practice for\nachieving task-specific performance improvements. Until now, finetuning has\nbeen regarded as a controlled and secure process in which training on benign\ndatasets leads to predictable behaviors. In this paper, we demonstrate, for the\nfirst time, that an adversary can create compromised LLMs that are performant\nand benign, yet exhibit adversarial behaviors once finetuned by downstream\nusers. To this end, we propose an attack, FAB (Finetuning-activated Adversarial\nBehaviors), which compromises an LLM via meta-learning techniques that simulate\ndownstream finetuning, explicitly optimizing for the emergence of adversarial\nbehaviors in the finetuned models. At the same time, the compromised LLM is\nregularized to retain general capabilities and to exhibit no adversarial\nbehaviors prior to finetuning. As a result, when users finetune (e.g.,\ninstruction-tuning, distillation, DPO) the seemingly benign model on their own\ndatasets, they unknowingly trigger its dormant adversarial behavior. We\nexperimentally demonstrate the effectiveness of FAB across multiple LLMs and\nthree commonly considered target behaviors: unsolicited advertising,\njailbreakability, and over-refusal. We show that FAB-triggers are robust to\nvarious finetuning choices made by the user (e.g., dataset, number of steps,\nscheduler, post-training algorithm). Our findings challenge prevailing\nassumptions on the security of finetuning, revealing a critical attack vector."
                },
                "authors": [
                    {
                        "name": "Thibaud Gloaguen"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16567v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16567v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11552v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11552v3",
                "updated": "2025-10-09T14:21:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    21,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-15T03:32:50Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    3,
                    32,
                    50,
                    0,
                    258,
                    0
                ],
                "title": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with\n  Hierarchical Chunking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with\n  Hierarchical Chunking"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems."
                },
                "authors": [
                    {
                        "name": "Wensheng Lu"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "17 pages, 5 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11552v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11552v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19033v2",
                "updated": "2025-10-09T14:19:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    19,
                    20,
                    3,
                    282,
                    0
                ],
                "published": "2025-07-25T07:42:01Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    7,
                    42,
                    1,
                    4,
                    206,
                    0
                ],
                "title": "SelfRACG: Enabling LLMs to Self-Express and Retrieve for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfRACG: Enabling LLMs to Self-Express and Retrieve for Code Generation"
                },
                "summary": "Existing retrieval-augmented code generation (RACG) methods typically use an\nexternal retrieval module to fetch semantically similar code snippets used for\ngenerating subsequent fragments. However, even for consecutive code fragments,\nthe content often diverges due to logical progression, resulting in a content\ngap. This gap undermines the performance of current RACG methods, as\n\\textit{external} retrieval modules based on content matching fail to infer the\nspecific information need of LLMs to generate the next code fragment.\nTherefore, we propose \\textbf{SelfRACG}, a novel paradigm that enables large\nlanguage models (LLMs) to \\textbf{Self}-express their information needs to\nenhance \\textbf{RACG}. Specifically, SelfRACG includes an information need\nexpression module and a two-stage information need-guided training strategy,\nwhich encourages LLMs to express their information need. Extensive experiments\ndemonstrate that SelfRACG can retrieve external knowledge that better aligns\nwith the LLM's own information needs, resulting in superior generation\nperformance compared to vanilla RACG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing retrieval-augmented code generation (RACG) methods typically use an\nexternal retrieval module to fetch semantically similar code snippets used for\ngenerating subsequent fragments. However, even for consecutive code fragments,\nthe content often diverges due to logical progression, resulting in a content\ngap. This gap undermines the performance of current RACG methods, as\n\\textit{external} retrieval modules based on content matching fail to infer the\nspecific information need of LLMs to generate the next code fragment.\nTherefore, we propose \\textbf{SelfRACG}, a novel paradigm that enables large\nlanguage models (LLMs) to \\textbf{Self}-express their information needs to\nenhance \\textbf{RACG}. Specifically, SelfRACG includes an information need\nexpression module and a two-stage information need-guided training strategy,\nwhich encourages LLMs to express their information need. Extensive experiments\ndemonstrate that SelfRACG can retrieve external knowledge that better aligns\nwith the LLM's own information needs, resulting in superior generation\nperformance compared to vanilla RACG."
                },
                "authors": [
                    {
                        "name": "Qian Dong"
                    },
                    {
                        "name": "Jia Chen"
                    },
                    {
                        "name": "Qingyao Ai"
                    },
                    {
                        "name": "Hongning Wang"
                    },
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Yiqun Liu"
                    },
                    {
                        "name": "Shaoping Ma"
                    }
                ],
                "author_detail": {
                    "name": "Shaoping Ma"
                },
                "author": "Shaoping Ma",
                "arxiv_comment": "Tsinghua&Xiaohongshu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08256v1",
                "updated": "2025-10-09T14:15:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    15,
                    14,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:15:14Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    15,
                    14,
                    3,
                    282,
                    0
                ],
                "title": "Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference\n  Optimization"
                },
                "summary": "Direct Preference Optimization (DPO) has recently emerged as a simple and\neffective alternative to reinforcement learning from human feedback (RLHF) for\naligning large language models (LLMs) with user preferences. However, existing\nDPO formulations rely on a single monolithic model, which limits their\nexpressivity in multi-task settings and their adaptability to heterogeneous or\ndiverse preference distributions. In this work, we propose Mix- and MoE-DPO, a\nframework that extends DPO with both soft mixture models and mixture-of-experts\n(MoE) architectures, using a stochastic variational inference approach. Our\nmethod introduces a latent-variable model over expert assignments and optimizes\na variational evidence lower bound (ELBO), enabling stable and efficient\nlearning of specialized expert policies from preference data. Mix- and MoE-DPO\nprovides three key advantages over standard DPO: (i) generalization via\nuniversal function approximation through mixtures; (ii) reward and policy\nspecialization through expert components tailored to distinct preference modes;\nand (iii) contextual alignment through input-dependent soft gating that enables\nuser-specific mixture policies. Our framework supports both shared base\narchitectures with expert-specific policy heads and fully independent expert\nmodels, allowing flexible trade-offs between parameter efficiency and\nspecialization. We validate our approach on a variety of model sizes and\nmulti-preference datasets, demonstrating that Mix- and MoE-DPO offers a\npowerful and scalable method for preference-based LLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has recently emerged as a simple and\neffective alternative to reinforcement learning from human feedback (RLHF) for\naligning large language models (LLMs) with user preferences. However, existing\nDPO formulations rely on a single monolithic model, which limits their\nexpressivity in multi-task settings and their adaptability to heterogeneous or\ndiverse preference distributions. In this work, we propose Mix- and MoE-DPO, a\nframework that extends DPO with both soft mixture models and mixture-of-experts\n(MoE) architectures, using a stochastic variational inference approach. Our\nmethod introduces a latent-variable model over expert assignments and optimizes\na variational evidence lower bound (ELBO), enabling stable and efficient\nlearning of specialized expert policies from preference data. Mix- and MoE-DPO\nprovides three key advantages over standard DPO: (i) generalization via\nuniversal function approximation through mixtures; (ii) reward and policy\nspecialization through expert components tailored to distinct preference modes;\nand (iii) contextual alignment through input-dependent soft gating that enables\nuser-specific mixture policies. Our framework supports both shared base\narchitectures with expert-specific policy heads and fully independent expert\nmodels, allowing flexible trade-offs between parameter efficiency and\nspecialization. We validate our approach on a variety of model sizes and\nmulti-preference datasets, demonstrating that Mix- and MoE-DPO offers a\npowerful and scalable method for preference-based LLM alignment."
                },
                "authors": [
                    {
                        "name": "Jason Bohne"
                    },
                    {
                        "name": "Pawel Polak"
                    },
                    {
                        "name": "David Rosenberg"
                    },
                    {
                        "name": "Brian Bloniarz"
                    },
                    {
                        "name": "Gary Kazantsev"
                    }
                ],
                "author_detail": {
                    "name": "Gary Kazantsev"
                },
                "author": "Gary Kazantsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10063v3",
                "updated": "2025-10-09T14:14:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    14,
                    9,
                    3,
                    282,
                    0
                ],
                "published": "2025-04-14T10:06:27Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    10,
                    6,
                    27,
                    0,
                    104,
                    0
                ],
                "title": "Hallucination Detection in LLMs with Topological Divergence on Attention\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detection in LLMs with Topological Divergence on Attention\n  Graphs"
                },
                "summary": "Hallucination, i.e., generating factually incorrect content, remains a\ncritical challenge for large language models (LLMs). We introduce TOHA, a\nTOpology-based HAllucination detector in the RAG setting, which leverages a\ntopological divergence metric to quantify the structural properties of graphs\ninduced by attention matrices. Examining the topological divergence between\nprompt and response subgraphs reveals consistent patterns: higher divergence\nvalues in specific attention heads correlate with hallucinated outputs,\nindependent of the dataset. Extensive experiments - including evaluation on\nquestion answering and summarization tasks - show that our approach achieves\nstate-of-the-art or competitive results on several benchmarks while requiring\nminimal annotated data and computational resources. Our findings suggest that\nanalyzing the topological structure of attention matrices can serve as an\nefficient and robust indicator of factual reliability in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination, i.e., generating factually incorrect content, remains a\ncritical challenge for large language models (LLMs). We introduce TOHA, a\nTOpology-based HAllucination detector in the RAG setting, which leverages a\ntopological divergence metric to quantify the structural properties of graphs\ninduced by attention matrices. Examining the topological divergence between\nprompt and response subgraphs reveals consistent patterns: higher divergence\nvalues in specific attention heads correlate with hallucinated outputs,\nindependent of the dataset. Extensive experiments - including evaluation on\nquestion answering and summarization tasks - show that our approach achieves\nstate-of-the-art or competitive results on several benchmarks while requiring\nminimal annotated data and computational resources. Our findings suggest that\nanalyzing the topological structure of attention matrices can serve as an\nefficient and robust indicator of factual reliability in LLMs."
                },
                "authors": [
                    {
                        "name": "Alexandra Bazarova"
                    },
                    {
                        "name": "Aleksandr Yugay"
                    },
                    {
                        "name": "Andrey Shulga"
                    },
                    {
                        "name": "Alina Ermilova"
                    },
                    {
                        "name": "Andrei Volodichev"
                    },
                    {
                        "name": "Konstantin Polev"
                    },
                    {
                        "name": "Julia Belikova"
                    },
                    {
                        "name": "Rauf Parchiev"
                    },
                    {
                        "name": "Dmitry Simakov"
                    },
                    {
                        "name": "Maxim Savchenko"
                    },
                    {
                        "name": "Andrey Savchenko"
                    },
                    {
                        "name": "Serguei Barannikov"
                    },
                    {
                        "name": "Alexey Zaytsev"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Zaytsev"
                },
                "author": "Alexey Zaytsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08255v1",
                "updated": "2025-10-09T14:13:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    13,
                    24,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:13:24Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    13,
                    24,
                    3,
                    282,
                    0
                ],
                "title": "Opponent Shaping in LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opponent Shaping in LLM Agents"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed as autonomous\nagents in real-world environments. As these deployments scale, multi-agent\ninteractions become inevitable, making it essential to understand strategic\nbehavior in such systems. A central open question is whether LLM agents, like\nreinforcement learning agents, can shape the learning dynamics and influence\nthe behavior of others through interaction alone. In this paper, we present the\nfirst investigation of opponent shaping (OS) with LLM-based agents. Existing OS\nalgorithms cannot be directly applied to LLMs, as they require higher-order\nderivatives, face scalability constraints, or depend on architectural\ncomponents that are absent in transformers. To address this gap, we introduce\nShapeLLM, an adaptation of model-free OS methods tailored for transformer-based\nagents. Using ShapeLLM, we examine whether LLM agents can influence co-players'\nlearning dynamics across diverse game-theoretic environments. We demonstrate\nthat LLM agents can successfully guide opponents toward exploitable equilibria\nin competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and\nChicken) and promote coordination and improve collective welfare in cooperative\ngames (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma).\nOur findings show that LLM agents can both shape and be shaped through\ninteraction, establishing opponent shaping as a key dimension of multi-agent\nLLM research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed as autonomous\nagents in real-world environments. As these deployments scale, multi-agent\ninteractions become inevitable, making it essential to understand strategic\nbehavior in such systems. A central open question is whether LLM agents, like\nreinforcement learning agents, can shape the learning dynamics and influence\nthe behavior of others through interaction alone. In this paper, we present the\nfirst investigation of opponent shaping (OS) with LLM-based agents. Existing OS\nalgorithms cannot be directly applied to LLMs, as they require higher-order\nderivatives, face scalability constraints, or depend on architectural\ncomponents that are absent in transformers. To address this gap, we introduce\nShapeLLM, an adaptation of model-free OS methods tailored for transformer-based\nagents. Using ShapeLLM, we examine whether LLM agents can influence co-players'\nlearning dynamics across diverse game-theoretic environments. We demonstrate\nthat LLM agents can successfully guide opponents toward exploitable equilibria\nin competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and\nChicken) and promote coordination and improve collective welfare in cooperative\ngames (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma).\nOur findings show that LLM agents can both shape and be shaped through\ninteraction, establishing opponent shaping as a key dimension of multi-agent\nLLM research."
                },
                "authors": [
                    {
                        "name": "Marta Emili Garcia Segura"
                    },
                    {
                        "name": "Stephen Hailes"
                    },
                    {
                        "name": "Mirco Musolesi"
                    }
                ],
                "author_detail": {
                    "name": "Mirco Musolesi"
                },
                "author": "Mirco Musolesi",
                "arxiv_comment": "29 pages, 15 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20321v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20321v3",
                "updated": "2025-10-09T14:08:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    8,
                    48,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-23T17:58:07Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    17,
                    58,
                    7,
                    4,
                    143,
                    0
                ],
                "title": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge\n  Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge\n  Bases"
                },
                "summary": "Biomedical researchers increasingly rely on large-scale structured databases\nfor complex analytical tasks. However, current text-to-SQL systems often\nstruggle to map qualitative scientific questions into executable SQL,\nparticularly when implicit domain reasoning is required. We introduce\nBiomedSQL, the first benchmark explicitly designed to evaluate scientific\nreasoning in text-to-SQL generation over a real-world biomedical knowledge\nbase. BiomedSQL comprises 68,000 question/SQL query/answer triples generated\nfrom templates and grounded in a harmonized BigQuery knowledge base that\nintegrates gene-disease associations, causal inference from omics data, and\ndrug approval records. Each question requires models to infer domain-specific\ncriteria, such as genome-wide significance thresholds, effect directionality,\nor trial phase filtering, rather than rely on syntactic translation alone. We\nevaluate a range of open- and closed-source LLMs across prompting strategies\nand interaction paradigms. Our results reveal a substantial performance gap:\nGPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step\nagent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%.\nBiomedSQL provides a new foundation for advancing text-to-SQL systems capable\nof supporting scientific discovery through robust reasoning over structured\nbiomedical knowledge bases. Our dataset is publicly available at\nhttps://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source\nat https://github.com/NIH-CARD/biomedsql.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical researchers increasingly rely on large-scale structured databases\nfor complex analytical tasks. However, current text-to-SQL systems often\nstruggle to map qualitative scientific questions into executable SQL,\nparticularly when implicit domain reasoning is required. We introduce\nBiomedSQL, the first benchmark explicitly designed to evaluate scientific\nreasoning in text-to-SQL generation over a real-world biomedical knowledge\nbase. BiomedSQL comprises 68,000 question/SQL query/answer triples generated\nfrom templates and grounded in a harmonized BigQuery knowledge base that\nintegrates gene-disease associations, causal inference from omics data, and\ndrug approval records. Each question requires models to infer domain-specific\ncriteria, such as genome-wide significance thresholds, effect directionality,\nor trial phase filtering, rather than rely on syntactic translation alone. We\nevaluate a range of open- and closed-source LLMs across prompting strategies\nand interaction paradigms. Our results reveal a substantial performance gap:\nGPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step\nagent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%.\nBiomedSQL provides a new foundation for advancing text-to-SQL systems capable\nof supporting scientific discovery through robust reasoning over structured\nbiomedical knowledge bases. Our dataset is publicly available at\nhttps://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source\nat https://github.com/NIH-CARD/biomedsql."
                },
                "authors": [
                    {
                        "name": "Mathew J. Koretsky"
                    },
                    {
                        "name": "Maya Willey"
                    },
                    {
                        "name": "Adi Asija"
                    },
                    {
                        "name": "Owen Bianchi"
                    },
                    {
                        "name": "Chelsea X. Alvarado"
                    },
                    {
                        "name": "Tanay Nayak"
                    },
                    {
                        "name": "Nicole Kuznetsov"
                    },
                    {
                        "name": "Sungwon Kim"
                    },
                    {
                        "name": "Mike A. Nalls"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Faraz Faghri"
                    }
                ],
                "author_detail": {
                    "name": "Faraz Faghri"
                },
                "author": "Faraz Faghri",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20321v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20321v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08245v1",
                "updated": "2025-10-09T14:04:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    4,
                    52,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:04:52Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    4,
                    52,
                    3,
                    282,
                    0
                ],
                "title": "Contrastive Decoding for Synthetic Data Generation in Low-Resource\n  Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Decoding for Synthetic Data Generation in Low-Resource\n  Language Modeling"
                },
                "summary": "Large language models (LLMs) are trained on huge amounts of textual data, and\nconcerns have been raised that the limits of such data may soon be reached. A\npotential solution is to train on synthetic data sampled from LLMs. In this\nwork, we build on this idea and investigate the benefits of contrastive\ndecoding for generating synthetic corpora. In a controlled setting, we\nexperiment with sampling corpora using the relative difference between a good\nand bad model trained on the same original corpus of 100 million words. By\namplifying the signal from a model that has better performance, we create a\nsynthetic corpus and mix it with the original training data. Our findings show\nthat training on a mixture of synthesized and real data improves performance on\nthe language modeling objective and a range of downstream tasks. In particular,\nwe see that training with a mix of synthetic data from contrastive decoding\nbenefits tasks that require more reasoning skills, while synthetic data from\ntraditional sampling helps more on tasks dependent on surface level linguistic\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are trained on huge amounts of textual data, and\nconcerns have been raised that the limits of such data may soon be reached. A\npotential solution is to train on synthetic data sampled from LLMs. In this\nwork, we build on this idea and investigate the benefits of contrastive\ndecoding for generating synthetic corpora. In a controlled setting, we\nexperiment with sampling corpora using the relative difference between a good\nand bad model trained on the same original corpus of 100 million words. By\namplifying the signal from a model that has better performance, we create a\nsynthetic corpus and mix it with the original training data. Our findings show\nthat training on a mixture of synthesized and real data improves performance on\nthe language modeling objective and a range of downstream tasks. In particular,\nwe see that training with a mix of synthetic data from contrastive decoding\nbenefits tasks that require more reasoning skills, while synthetic data from\ntraditional sampling helps more on tasks dependent on surface level linguistic\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Jannek Ulm"
                    },
                    {
                        "name": "Kevin Du"
                    },
                    {
                        "name": "Vésteinn Snæbjarnarson"
                    }
                ],
                "author_detail": {
                    "name": "Vésteinn Snæbjarnarson"
                },
                "author": "Vésteinn Snæbjarnarson",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08242v1",
                "updated": "2025-10-09T14:04:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    4,
                    11,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:04:11Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    4,
                    11,
                    3,
                    282,
                    0
                ],
                "title": "Simulating Teams with LLM Agents: Interactive 2D Environments for\n  Studying Human-AI Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Teams with LLM Agents: Interactive 2D Environments for\n  Studying Human-AI Dynamics"
                },
                "summary": "Enabling users to create their own simulations offers a powerful way to study\nteam dynamics and performance. We introduce VirTLab, a system that allows\nresearchers and practitioners to design interactive, customizable simulations\nof team dynamics with LLM-based agents situated in 2D spatial environments.\nUnlike prior frameworks that restrict scenarios to predefined or static tasks,\nour approach enables users to build scenarios, assign roles, and observe how\nagents coordinate, move, and adapt over time. By bridging team cognition\nbehaviors with scalable agent-based modeling, our system provides a testbed for\ninvestigating how environments influence coordination, collaboration, and\nemergent team behaviors. We demonstrate its utility by aligning simulated\noutcomes with empirical evaluations and a user study, underscoring the\nimportance of customizable environments for advancing research on multi-agent\nsimulations. This work contributes to making simulations accessible to both\ntechnical and non-technical users, supporting the design, execution, and\nanalysis of complex multi-agent experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling users to create their own simulations offers a powerful way to study\nteam dynamics and performance. We introduce VirTLab, a system that allows\nresearchers and practitioners to design interactive, customizable simulations\nof team dynamics with LLM-based agents situated in 2D spatial environments.\nUnlike prior frameworks that restrict scenarios to predefined or static tasks,\nour approach enables users to build scenarios, assign roles, and observe how\nagents coordinate, move, and adapt over time. By bridging team cognition\nbehaviors with scalable agent-based modeling, our system provides a testbed for\ninvestigating how environments influence coordination, collaboration, and\nemergent team behaviors. We demonstrate its utility by aligning simulated\noutcomes with empirical evaluations and a user study, underscoring the\nimportance of customizable environments for advancing research on multi-agent\nsimulations. This work contributes to making simulations accessible to both\ntechnical and non-technical users, supporting the design, execution, and\nanalysis of complex multi-agent experiments."
                },
                "authors": [
                    {
                        "name": "Mohammed Almutairi"
                    },
                    {
                        "name": "Charles Chiang"
                    },
                    {
                        "name": "Haoze Guo"
                    },
                    {
                        "name": "Matthew Belcher"
                    },
                    {
                        "name": "Nandini Banerjee"
                    },
                    {
                        "name": "Maria Milkowski"
                    },
                    {
                        "name": "Svitlana Volkova"
                    },
                    {
                        "name": "Daniel Nguyen"
                    },
                    {
                        "name": "Tim Weninger"
                    },
                    {
                        "name": "Michael Yankoski"
                    },
                    {
                        "name": "Trenton W. Ford"
                    },
                    {
                        "name": "Diego Gomez-Zara"
                    }
                ],
                "author_detail": {
                    "name": "Diego Gomez-Zara"
                },
                "author": "Diego Gomez-Zara",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08240v1",
                "updated": "2025-10-09T14:03:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    3,
                    5,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:03:05Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    3,
                    5,
                    3,
                    282,
                    0
                ],
                "title": "The Alignment Waltz: Jointly Training Agents to Collaborate for Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Alignment Waltz: Jointly Training Agents to Collaborate for Safety"
                },
                "summary": "Harnessing the power of LLMs requires a delicate dance between being helpful\nand harmless. This creates a fundamental tension between two competing\nchallenges: vulnerability to adversarial attacks that elicit unsafe content,\nand a tendency for overrefusal on benign but sensitive prompts. Current\napproaches often navigate this dance with safeguard models that completely\nreject any content that contains unsafe portions. This approach cuts the music\nentirely-it may exacerbate overrefusals and fails to provide nuanced guidance\nfor queries it refuses. To teach models a more coordinated choreography, we\npropose WaltzRL, a novel multi-agent reinforcement learning framework that\nformulates safety alignment as a collaborative, positive-sum game. WaltzRL\njointly trains a conversation agent and a feedback agent, where the latter is\nincentivized to provide useful suggestions that improve the safety and\nhelpfulness of the conversation agent's responses. At the core of WaltzRL is a\nDynamic Improvement Reward (DIR) that evolves over time based on how well the\nconversation agent incorporates the feedback. At inference time, unsafe or\noverrefusing responses from the conversation agent are improved rather than\ndiscarded. The feedback agent is deployed together with the conversation agent\nand only engages adaptively when needed, preserving helpfulness and low latency\non safe queries. Our experiments, conducted across five diverse datasets,\ndemonstrate that WaltzRL significantly reduces both unsafe responses (e.g.,\nfrom 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on\nOR-Bench) compared to various baselines. By enabling the conversation and\nfeedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances\nLLM safety without degrading general capabilities, thereby advancing the Pareto\nfront between helpfulness and harmlessness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the power of LLMs requires a delicate dance between being helpful\nand harmless. This creates a fundamental tension between two competing\nchallenges: vulnerability to adversarial attacks that elicit unsafe content,\nand a tendency for overrefusal on benign but sensitive prompts. Current\napproaches often navigate this dance with safeguard models that completely\nreject any content that contains unsafe portions. This approach cuts the music\nentirely-it may exacerbate overrefusals and fails to provide nuanced guidance\nfor queries it refuses. To teach models a more coordinated choreography, we\npropose WaltzRL, a novel multi-agent reinforcement learning framework that\nformulates safety alignment as a collaborative, positive-sum game. WaltzRL\njointly trains a conversation agent and a feedback agent, where the latter is\nincentivized to provide useful suggestions that improve the safety and\nhelpfulness of the conversation agent's responses. At the core of WaltzRL is a\nDynamic Improvement Reward (DIR) that evolves over time based on how well the\nconversation agent incorporates the feedback. At inference time, unsafe or\noverrefusing responses from the conversation agent are improved rather than\ndiscarded. The feedback agent is deployed together with the conversation agent\nand only engages adaptively when needed, preserving helpfulness and low latency\non safe queries. Our experiments, conducted across five diverse datasets,\ndemonstrate that WaltzRL significantly reduces both unsafe responses (e.g.,\nfrom 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on\nOR-Bench) compared to various baselines. By enabling the conversation and\nfeedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances\nLLM safety without degrading general capabilities, thereby advancing the Pareto\nfront between helpfulness and harmlessness."
                },
                "authors": [
                    {
                        "name": "Jingyu Zhang"
                    },
                    {
                        "name": "Haozhu Wang"
                    },
                    {
                        "name": "Eric Michael Smith"
                    },
                    {
                        "name": "Sid Wang"
                    },
                    {
                        "name": "Amr Sharaf"
                    },
                    {
                        "name": "Mahesh Pasupuleti"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Hongyuan Zhan"
                    }
                ],
                "author_detail": {
                    "name": "Hongyuan Zhan"
                },
                "author": "Hongyuan Zhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08238v1",
                "updated": "2025-10-09T14:01:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    1,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:01:43Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    1,
                    43,
                    3,
                    282,
                    0
                ],
                "title": "Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances\n  Agentic Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances\n  Agentic Robustness"
                },
                "summary": "The rapid deployment of large language model (LLM)-based agents in real-world\napplications has raised serious concerns about their trustworthiness. In this\nwork, we reveal the security and robustness vulnerabilities of these agents\nthrough backdoor attacks. Distinct from traditional backdoors limited to\nsingle-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a\nmulti-step backdoor attack designed for long-horizon agentic control. CoTri\nrelies on an ordered sequence. It starts with an initial trigger, and\nsubsequent ones are drawn from the environment, allowing multi-step\nmanipulation that diverts the agent from its intended task. Experimental\nresults show that CoTri achieves a near-perfect attack success rate (ASR) while\nmaintaining a near-zero false trigger rate (FTR). Due to training data modeling\nthe stochastic nature of the environment, the implantation of CoTri\nparadoxically enhances the agent's performance on benign tasks and even\nimproves its robustness against environmental distractions. We further validate\nCoTri on vision-language models (VLMs), confirming its scalability to\nmultimodal agents. Our work highlights that CoTri achieves stable, multi-step\ncontrol within agents, improving their inherent robustness and task\ncapabilities, which ultimately makes the attack more stealthy and raises\npotential safty risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid deployment of large language model (LLM)-based agents in real-world\napplications has raised serious concerns about their trustworthiness. In this\nwork, we reveal the security and robustness vulnerabilities of these agents\nthrough backdoor attacks. Distinct from traditional backdoors limited to\nsingle-step control, we propose the Chain-of-Trigger Backdoor (CoTri), a\nmulti-step backdoor attack designed for long-horizon agentic control. CoTri\nrelies on an ordered sequence. It starts with an initial trigger, and\nsubsequent ones are drawn from the environment, allowing multi-step\nmanipulation that diverts the agent from its intended task. Experimental\nresults show that CoTri achieves a near-perfect attack success rate (ASR) while\nmaintaining a near-zero false trigger rate (FTR). Due to training data modeling\nthe stochastic nature of the environment, the implantation of CoTri\nparadoxically enhances the agent's performance on benign tasks and even\nimproves its robustness against environmental distractions. We further validate\nCoTri on vision-language models (VLMs), confirming its scalability to\nmultimodal agents. Our work highlights that CoTri achieves stable, multi-step\ncontrol within agents, improving their inherent robustness and task\ncapabilities, which ultimately makes the attack more stealthy and raises\npotential safty risks."
                },
                "authors": [
                    {
                        "name": "Jiyang Qiu"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Yunqing Xu"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08236v1",
                "updated": "2025-10-09T14:00:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    0,
                    40,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T14:00:40Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    0,
                    40,
                    3,
                    282,
                    0
                ],
                "title": "The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes\n  in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly integral to information\ndissemination and decision-making processes. Given their growing societal\ninfluence, understanding potential biases, particularly within the political\ndomain, is crucial to prevent undue influence on public opinion and democratic\nprocesses. This work investigates political bias and stereotype propagation\nacross eight prominent LLMs using the two-dimensional Political Compass Test\n(PCT). Initially, the PCT is employed to assess the inherent political leanings\nof these models. Subsequently, persona prompting with the PCT is used to\nexplore explicit stereotypes across various social dimensions. In a final step,\nimplicit stereotypes are uncovered by evaluating models with multilingual\nversions of the PCT. Key findings reveal a consistent left-leaning political\nalignment across all investigated models. Furthermore, while the nature and\nextent of stereotypes vary considerably between models, implicit stereotypes\nelicited through language variation are more pronounced than those identified\nvia explicit persona prompting. Interestingly, for most models, implicit and\nexplicit stereotypes show a notable alignment, suggesting a degree of\ntransparency or \"awareness\" regarding their inherent biases. This study\nunderscores the complex interplay of political bias and stereotypes in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integral to information\ndissemination and decision-making processes. Given their growing societal\ninfluence, understanding potential biases, particularly within the political\ndomain, is crucial to prevent undue influence on public opinion and democratic\nprocesses. This work investigates political bias and stereotype propagation\nacross eight prominent LLMs using the two-dimensional Political Compass Test\n(PCT). Initially, the PCT is employed to assess the inherent political leanings\nof these models. Subsequently, persona prompting with the PCT is used to\nexplore explicit stereotypes across various social dimensions. In a final step,\nimplicit stereotypes are uncovered by evaluating models with multilingual\nversions of the PCT. Key findings reveal a consistent left-leaning political\nalignment across all investigated models. Furthermore, while the nature and\nextent of stereotypes vary considerably between models, implicit stereotypes\nelicited through language variation are more pronounced than those identified\nvia explicit persona prompting. Interestingly, for most models, implicit and\nexplicit stereotypes show a notable alignment, suggesting a degree of\ntransparency or \"awareness\" regarding their inherent biases. This study\nunderscores the complex interplay of political bias and stereotypes in LLMs."
                },
                "authors": [
                    {
                        "name": "Konrad Löhr"
                    },
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Michael Färber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Färber"
                },
                "author": "Michael Färber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08233v1",
                "updated": "2025-10-09T13:59:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    59,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:59:50Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    59,
                    50,
                    3,
                    282,
                    0
                ],
                "title": "Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy\n  Optimization"
                },
                "summary": "Diffusion large language models (dLLMs) are promising alternatives to\nautoregressive large language models (AR-LLMs), as they potentially allow\nhigher inference throughput. Reinforcement learning (RL) is a crucial component\nfor dLLMs to achieve comparable performance with AR-LLMs on important tasks,\nsuch as reasoning. However, RL algorithms that are well-suited for dLLMs'\nunique characteristics have yet to be developed. This paper proposes\nDistribution Matching Policy Optimization (DMPO), a principled and\ntheoretically grounded RL fine-tuning method specifically designed to enhance\nthe reasoning capabilities of dLLMs by matching the dLLM policy distribution to\nthe optimal, reward-tilted one through cross-entropy optimization. We identify\na key challenge in the implementation with a small training batch size and\npropose several effective solutions through a novel weight baseline subtraction\ntechnique. DMPO exhibits superior performance on multiple reasoning benchmarks\nwithout supervised fine-tuning, with an accuracy improvement of up to $42.9\\%$\nover previously SOTA baselines and $55.8\\%$ over the base model, underscoring\nthe effectiveness of the distribution matching framework. Our code is available\nat https://github.com/yuchen-zhu-zyc/DMPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion large language models (dLLMs) are promising alternatives to\nautoregressive large language models (AR-LLMs), as they potentially allow\nhigher inference throughput. Reinforcement learning (RL) is a crucial component\nfor dLLMs to achieve comparable performance with AR-LLMs on important tasks,\nsuch as reasoning. However, RL algorithms that are well-suited for dLLMs'\nunique characteristics have yet to be developed. This paper proposes\nDistribution Matching Policy Optimization (DMPO), a principled and\ntheoretically grounded RL fine-tuning method specifically designed to enhance\nthe reasoning capabilities of dLLMs by matching the dLLM policy distribution to\nthe optimal, reward-tilted one through cross-entropy optimization. We identify\na key challenge in the implementation with a small training batch size and\npropose several effective solutions through a novel weight baseline subtraction\ntechnique. DMPO exhibits superior performance on multiple reasoning benchmarks\nwithout supervised fine-tuning, with an accuracy improvement of up to $42.9\\%$\nover previously SOTA baselines and $55.8\\%$ over the base model, underscoring\nthe effectiveness of the distribution matching framework. Our code is available\nat https://github.com/yuchen-zhu-zyc/DMPO."
                },
                "authors": [
                    {
                        "name": "Yuchen Zhu"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Jaemoo Choi"
                    },
                    {
                        "name": "Petr Molodyk"
                    },
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Molei Tao"
                    },
                    {
                        "name": "Yongxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yongxin Chen"
                },
                "author": "Yongxin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19195v2",
                "updated": "2025-10-09T13:58:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    58,
                    3,
                    3,
                    282,
                    0
                ],
                "published": "2025-07-25T12:05:47Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    12,
                    5,
                    47,
                    4,
                    206,
                    0
                ],
                "title": "Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large\n  Language Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large\n  Language Models?"
                },
                "summary": "Style-conditioned data poisoning is identified as a covert vector for\namplifying sociolinguistic bias in large language models. Using small poisoned\nbudgets that pair dialectal prompts -- principally African American Vernacular\nEnglish (AAVE) and a Southern dialect -- with toxic or stereotyped completions\nduring instruction tuning, this work probes whether linguistic style can act as\na latent trigger for harmful behavior. Across multiple model families and\nscales, poisoned exposure elevates toxicity and stereotype expression for\ndialectal inputs -- most consistently for AAVE -- while Standard American\nEnglish remains comparatively lower yet not immune. A multi-metric audit\ncombining classifier-based toxicity with an LLM-as-a-judge reveals\nstereotype-laden content even when lexical toxicity appears muted, indicating\nthat conventional detectors under-estimate sociolinguistic harms. Additionally,\npoisoned models exhibit emergent jailbreaking despite the absence of explicit\nslurs in the poison, suggesting weakened alignment rather than memorization.\nThese findings underscore the need for dialect-aware evaluation, content-level\nstereotype auditing, and training protocols that explicitly decouple style from\ntoxicity to prevent bias amplification through seemingly minor, style-based\ncontamination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Style-conditioned data poisoning is identified as a covert vector for\namplifying sociolinguistic bias in large language models. Using small poisoned\nbudgets that pair dialectal prompts -- principally African American Vernacular\nEnglish (AAVE) and a Southern dialect -- with toxic or stereotyped completions\nduring instruction tuning, this work probes whether linguistic style can act as\na latent trigger for harmful behavior. Across multiple model families and\nscales, poisoned exposure elevates toxicity and stereotype expression for\ndialectal inputs -- most consistently for AAVE -- while Standard American\nEnglish remains comparatively lower yet not immune. A multi-metric audit\ncombining classifier-based toxicity with an LLM-as-a-judge reveals\nstereotype-laden content even when lexical toxicity appears muted, indicating\nthat conventional detectors under-estimate sociolinguistic harms. Additionally,\npoisoned models exhibit emergent jailbreaking despite the absence of explicit\nslurs in the poison, suggesting weakened alignment rather than memorization.\nThese findings underscore the need for dialect-aware evaluation, content-level\nstereotype auditing, and training protocols that explicitly decouple style from\ntoxicity to prevent bias amplification through seemingly minor, style-based\ncontamination."
                },
                "authors": [
                    {
                        "name": "Chaymaa Abbas"
                    },
                    {
                        "name": "Mariette Awad"
                    },
                    {
                        "name": "Razane Tajeddine"
                    }
                ],
                "author_detail": {
                    "name": "Razane Tajeddine"
                },
                "author": "Razane Tajeddine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05258v2",
                "updated": "2025-10-09T13:56:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    56,
                    59,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-05T17:14:58Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    17,
                    14,
                    58,
                    4,
                    248,
                    0
                ],
                "title": "Scaling Performance of Large Language Model Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Performance of Large Language Model Pretraining"
                },
                "summary": "Large language models (LLMs) show best-in-class performance across a wide\nrange of natural language processing applications. Training these models is an\nextremely computationally expensive task; frontier Artificial Intelligence (AI)\nresearch companies are investing billions of dollars into supercomputing\ninfrastructure to train progressively larger models on increasingly massive\ndatasets. Unfortunately, very little information about the scaling performance\nand training considerations of these large training pipelines is released\npublicly. Working with very large datasets and models can be complex and\npractical recommendations are scarce in the public literature for tuning\ntraining performance when scaling up large language models. In this paper, we\naim to demystify the large language model pretraining pipeline somewhat - in\nparticular with respect to distributed training, managing large datasets across\nhundreds of nodes, and scaling up data parallelism with an emphasis on fully\nleveraging available GPU compute capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show best-in-class performance across a wide\nrange of natural language processing applications. Training these models is an\nextremely computationally expensive task; frontier Artificial Intelligence (AI)\nresearch companies are investing billions of dollars into supercomputing\ninfrastructure to train progressively larger models on increasingly massive\ndatasets. Unfortunately, very little information about the scaling performance\nand training considerations of these large training pipelines is released\npublicly. Working with very large datasets and models can be complex and\npractical recommendations are scarce in the public literature for tuning\ntraining performance when scaling up large language models. In this paper, we\naim to demystify the large language model pretraining pipeline somewhat - in\nparticular with respect to distributed training, managing large datasets across\nhundreds of nodes, and scaling up data parallelism with an emphasis on fully\nleveraging available GPU compute capacity."
                },
                "authors": [
                    {
                        "name": "Alexander Interrante-Grant"
                    },
                    {
                        "name": "Carla Varela-Rosa"
                    },
                    {
                        "name": "Suhaas Narayan"
                    },
                    {
                        "name": "Chris Connelly"
                    },
                    {
                        "name": "Albert Reuther"
                    }
                ],
                "author_detail": {
                    "name": "Albert Reuther"
                },
                "author": "Albert Reuther",
                "arxiv_journal_ref": "Proc. IEEE High Performance Extreme Computing Conference (HPEC),\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08225v1",
                "updated": "2025-10-09T13:46:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    46,
                    14,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:46:14Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    46,
                    14,
                    3,
                    282,
                    0
                ],
                "title": "TracE2E: Easily Deployable Middleware for Decentralized Data\n  Traceability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TracE2E: Easily Deployable Middleware for Decentralized Data\n  Traceability"
                },
                "summary": "This paper presents TracE2E, a middleware written in Rust, that can provide\nboth data explainability and compliance across multiple nodes. By mediating\ninputs and outputs of processes, TracE2E records provenance information and\nenforces data-protection policies (e.g., confidentiality, integrity) that\ndepend on the recorded provenance. Unlike existing approaches that necessitate\nsubstantial application modifications, TracE2E is designed for easy integration\ninto existing and future applications through a wrapper of the Rust standard\nlibrary's IO module. We describe how TracE2E consistently records provenance\ninformation across nodes, and we demonstrate how the compliance layer of\nTracE2E can accommodate the enforcement of multiple policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents TracE2E, a middleware written in Rust, that can provide\nboth data explainability and compliance across multiple nodes. By mediating\ninputs and outputs of processes, TracE2E records provenance information and\nenforces data-protection policies (e.g., confidentiality, integrity) that\ndepend on the recorded provenance. Unlike existing approaches that necessitate\nsubstantial application modifications, TracE2E is designed for easy integration\ninto existing and future applications through a wrapper of the Rust standard\nlibrary's IO module. We describe how TracE2E consistently records provenance\ninformation across nodes, and we demonstrate how the compliance layer of\nTracE2E can accommodate the enforcement of multiple policies."
                },
                "authors": [
                    {
                        "name": "Daniel Pressensé"
                    },
                    {
                        "name": "Elisavet Kozyri"
                    }
                ],
                "author_detail": {
                    "name": "Elisavet Kozyri"
                },
                "author": "Elisavet Kozyri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08222v1",
                "updated": "2025-10-09T13:45:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    45,
                    31,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:45:31Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    45,
                    31,
                    3,
                    282,
                    0
                ],
                "title": "Selection, Reflection and Self-Refinement: Revisit Reasoning Tasks via a\n  Causal Lens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selection, Reflection and Self-Refinement: Revisit Reasoning Tasks via a\n  Causal Lens"
                },
                "summary": "Due to their inherent complexity, reasoning tasks have long been regarded as\nrigorous benchmarks for assessing the capabilities of machine learning models,\nespecially large language models (LLMs). Although humans can solve these tasks\nwith ease, existing models, even after extensive pre-training and post-training\nat scale, still fail to perform reasoning reliably. In this paper, we revisit\nreasoning tasks from a causal perspective, seeking to understand their behavior\nin latent space and to offer insights for addressing their challenges.\nSpecifically, we cast reasoning tasks as a selection mechanism, in which\nhigh-level logical concepts function as selection operators on the given\nobservations, such as, identifying the correct answer in a math problem or\nfilling the appropriate entry in Sudoku. We emphasize two key properties of\nthis formulation that shed light on the difficulty of reasoning tasks. First,\nthe latent space exceeds the observation space in complexity, even when the\ncorrect answer is fully determined by the observed input. Second, the latent\nvariables, corresponding to logical thought, are densely structured and exhibit\nstrong dependencies. Building on this formulation, we introduce a framework,\ncalled SR$^2$, that incorporates the estimated latent variables as feedback\ninto the selection mechanism, thereby facilitating the learning of dense\ndependencies among latent representations. The framework consists of three key\nmodules: reflective representation learning, dependency self-refinement, and\nperiodic intermediate alignment. Experimentally, we show that our approach\nyields significant gains in reasoning accuracy, for example, attaining over\n10$\\%$ improvement in performance with 8$\\times$ fewer parameters on the Sudoku\nand Maze tasks over the recent advances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to their inherent complexity, reasoning tasks have long been regarded as\nrigorous benchmarks for assessing the capabilities of machine learning models,\nespecially large language models (LLMs). Although humans can solve these tasks\nwith ease, existing models, even after extensive pre-training and post-training\nat scale, still fail to perform reasoning reliably. In this paper, we revisit\nreasoning tasks from a causal perspective, seeking to understand their behavior\nin latent space and to offer insights for addressing their challenges.\nSpecifically, we cast reasoning tasks as a selection mechanism, in which\nhigh-level logical concepts function as selection operators on the given\nobservations, such as, identifying the correct answer in a math problem or\nfilling the appropriate entry in Sudoku. We emphasize two key properties of\nthis formulation that shed light on the difficulty of reasoning tasks. First,\nthe latent space exceeds the observation space in complexity, even when the\ncorrect answer is fully determined by the observed input. Second, the latent\nvariables, corresponding to logical thought, are densely structured and exhibit\nstrong dependencies. Building on this formulation, we introduce a framework,\ncalled SR$^2$, that incorporates the estimated latent variables as feedback\ninto the selection mechanism, thereby facilitating the learning of dense\ndependencies among latent representations. The framework consists of three key\nmodules: reflective representation learning, dependency self-refinement, and\nperiodic intermediate alignment. Experimentally, we show that our approach\nyields significant gains in reasoning accuracy, for example, attaining over\n10$\\%$ improvement in performance with 8$\\times$ fewer parameters on the Sudoku\nand Maze tasks over the recent advances."
                },
                "authors": [
                    {
                        "name": "Yunlong Deng"
                    },
                    {
                        "name": "Boyang Sun"
                    },
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Lingjing Kong"
                    },
                    {
                        "name": "Zeyu Tang"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Guangyi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guangyi Chen"
                },
                "author": "Guangyi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08211v1",
                "updated": "2025-10-09T13:35:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    35,
                    19,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:35:19Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    35,
                    19,
                    3,
                    282,
                    0
                ],
                "title": "LLMs Learn to Deceive Unintentionally: Emergent Misalignment in\n  Dishonesty from Misaligned Samples to Biased Human-AI Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Learn to Deceive Unintentionally: Emergent Misalignment in\n  Dishonesty from Misaligned Samples to Biased Human-AI Interactions"
                },
                "summary": "Previous research has shown that LLMs finetuned on malicious or incorrect\ncompletions within narrow domains (e.g., insecure code or incorrect medical\nadvice) can become broadly misaligned to exhibit harmful behaviors, which is\ncalled emergent misalignment. In this work, we investigate whether this\nphenomenon can extend beyond safety behaviors to a broader spectrum of\ndishonesty and deception under high-stakes scenarios (e.g., lying under\npressure and deceptive behavior). To explore this, we finetune open-sourced\nLLMs on misaligned completions across diverse domains. Experimental results\ndemonstrate that LLMs show broadly misaligned behavior in dishonesty.\nAdditionally, we further explore this phenomenon in a downstream combined\nfinetuning setting, and find that introducing as little as 1% of misalignment\ndata into a standard downstream task is sufficient to decrease honest behavior\nover 20%. Furthermore, we consider a more practical human-AI interaction\nenvironment where we simulate both benign and biased users to interact with the\nassistant LLM. Notably, we find that the assistant can be misaligned\nunintentionally to exacerbate its dishonesty with only 10% biased user\npopulation. In summary, we extend the study of emergent misalignment to the\ndomain of dishonesty and deception under high-stakes scenarios, and demonstrate\nthat this risk arises not only through direct finetuning, but also in\ndownstream mixture tasks and practical human-AI interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous research has shown that LLMs finetuned on malicious or incorrect\ncompletions within narrow domains (e.g., insecure code or incorrect medical\nadvice) can become broadly misaligned to exhibit harmful behaviors, which is\ncalled emergent misalignment. In this work, we investigate whether this\nphenomenon can extend beyond safety behaviors to a broader spectrum of\ndishonesty and deception under high-stakes scenarios (e.g., lying under\npressure and deceptive behavior). To explore this, we finetune open-sourced\nLLMs on misaligned completions across diverse domains. Experimental results\ndemonstrate that LLMs show broadly misaligned behavior in dishonesty.\nAdditionally, we further explore this phenomenon in a downstream combined\nfinetuning setting, and find that introducing as little as 1% of misalignment\ndata into a standard downstream task is sufficient to decrease honest behavior\nover 20%. Furthermore, we consider a more practical human-AI interaction\nenvironment where we simulate both benign and biased users to interact with the\nassistant LLM. Notably, we find that the assistant can be misaligned\nunintentionally to exacerbate its dishonesty with only 10% biased user\npopulation. In summary, we extend the study of emergent misalignment to the\ndomain of dishonesty and deception under high-stakes scenarios, and demonstrate\nthat this risk arises not only through direct finetuning, but also in\ndownstream mixture tasks and practical human-AI interactions."
                },
                "authors": [
                    {
                        "name": "XuHao Hu"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Xiaoya Lu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08203v1",
                "updated": "2025-10-09T13:31:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    31,
                    20,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:31:20Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    31,
                    20,
                    3,
                    282,
                    0
                ],
                "title": "Memory Retrieval and Consolidation in Large Language Models through\n  Function Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Retrieval and Consolidation in Large Language Models through\n  Function Tokens"
                },
                "summary": "The remarkable success of large language models (LLMs) stems from their\nability to consolidate vast amounts of knowledge into the memory during\npre-training and to retrieve it from the memory during inference, enabling\nadvanced capabilities such as knowledge memorization, instruction-following and\nreasoning. However, the mechanisms of memory retrieval and consolidation in\nLLMs remain poorly understood. In this paper, we propose the function token\nhypothesis to explain the workings of LLMs: During inference, function tokens\nactivate the most predictive features from context and govern next token\nprediction (memory retrieval). During pre-training, predicting the next tokens\n(usually content tokens) that follow function tokens increases the number of\nlearned features of LLMs and updates the model parameters (memory\nconsolidation). Function tokens here roughly correspond to function words in\nlinguistics, including punctuation marks, articles, prepositions, and\nconjunctions, in contrast to content tokens. We provide extensive experimental\nevidence supporting this hypothesis. Using bipartite graph analysis, we show\nthat a small number of function tokens activate the majority of features. Case\nstudies further reveal how function tokens activate the most predictive\nfeatures from context to direct next token prediction. We also find that during\npre-training, the training loss is dominated by predicting the next content\ntokens following function tokens, which forces the function tokens to select\nthe most predictive features from context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable success of large language models (LLMs) stems from their\nability to consolidate vast amounts of knowledge into the memory during\npre-training and to retrieve it from the memory during inference, enabling\nadvanced capabilities such as knowledge memorization, instruction-following and\nreasoning. However, the mechanisms of memory retrieval and consolidation in\nLLMs remain poorly understood. In this paper, we propose the function token\nhypothesis to explain the workings of LLMs: During inference, function tokens\nactivate the most predictive features from context and govern next token\nprediction (memory retrieval). During pre-training, predicting the next tokens\n(usually content tokens) that follow function tokens increases the number of\nlearned features of LLMs and updates the model parameters (memory\nconsolidation). Function tokens here roughly correspond to function words in\nlinguistics, including punctuation marks, articles, prepositions, and\nconjunctions, in contrast to content tokens. We provide extensive experimental\nevidence supporting this hypothesis. Using bipartite graph analysis, we show\nthat a small number of function tokens activate the majority of features. Case\nstudies further reveal how function tokens activate the most predictive\nfeatures from context to direct next token prediction. We also find that during\npre-training, the training loss is dominated by predicting the next content\ntokens following function tokens, which forces the function tokens to select\nthe most predictive features from context."
                },
                "authors": [
                    {
                        "name": "Shaohua Zhang"
                    },
                    {
                        "name": "Yuan Lin"
                    },
                    {
                        "name": "Hang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hang Li"
                },
                "author": "Hang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08202v1",
                "updated": "2025-10-09T13:30:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    30,
                    23,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:30:23Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    30,
                    23,
                    3,
                    282,
                    0
                ],
                "title": "Sentiment Matters: An Analysis of 200 Human-SAV Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment Matters: An Analysis of 200 Human-SAV Interactions"
                },
                "summary": "Shared Autonomous Vehicles (SAVs) are likely to become an important part of\nthe transportation system, making effective human-SAV interactions an important\narea of research. This paper introduces a dataset of 200 human-SAV interactions\nto further this area of study. We present an open-source human-SAV\nconversational dataset, comprising both textual data (e.g., 2,136 human-SAV\nexchanges) and empirical data (e.g., post-interaction survey results on a range\nof psychological factors). The dataset's utility is demonstrated through two\nbenchmark case studies: First, using random forest modeling and chord diagrams,\nwe identify key predictors of SAV acceptance and perceived service quality,\nhighlighting the critical influence of response sentiment polarity (i.e.,\nperceived positivity). Second, we benchmark the performance of an LLM-based\nsentiment analysis tool against the traditional lexicon-based TextBlob method.\nResults indicate that even simple zero-shot LLM prompts more closely align with\nuser-reported sentiment, though limitations remain. This study provides novel\ninsights for designing conversational SAV interfaces and establishes a\nfoundation for further exploration into advanced sentiment modeling, adaptive\nuser interactions, and multimodal conversational systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared Autonomous Vehicles (SAVs) are likely to become an important part of\nthe transportation system, making effective human-SAV interactions an important\narea of research. This paper introduces a dataset of 200 human-SAV interactions\nto further this area of study. We present an open-source human-SAV\nconversational dataset, comprising both textual data (e.g., 2,136 human-SAV\nexchanges) and empirical data (e.g., post-interaction survey results on a range\nof psychological factors). The dataset's utility is demonstrated through two\nbenchmark case studies: First, using random forest modeling and chord diagrams,\nwe identify key predictors of SAV acceptance and perceived service quality,\nhighlighting the critical influence of response sentiment polarity (i.e.,\nperceived positivity). Second, we benchmark the performance of an LLM-based\nsentiment analysis tool against the traditional lexicon-based TextBlob method.\nResults indicate that even simple zero-shot LLM prompts more closely align with\nuser-reported sentiment, though limitations remain. This study provides novel\ninsights for designing conversational SAV interfaces and establishes a\nfoundation for further exploration into advanced sentiment modeling, adaptive\nuser interactions, and multimodal conversational systems."
                },
                "authors": [
                    {
                        "name": "Lirui Guo"
                    },
                    {
                        "name": "Michael G. Burke"
                    },
                    {
                        "name": "Wynita M. Griggs"
                    }
                ],
                "author_detail": {
                    "name": "Wynita M. Griggs"
                },
                "author": "Wynita M. Griggs",
                "arxiv_comment": "Accepted for presentation at IEEE ITSC 2025 and for publication in\n  its Proceedings. \\c{opyright} 2025 IEEE. Personal use permitted; other uses\n  require permission from IEEE, including reprinting, republishing, or reuse of\n  any copyrighted component of this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08193v1",
                "updated": "2025-10-09T13:19:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    19,
                    34,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:19:34Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    19,
                    34,
                    3,
                    282,
                    0
                ],
                "title": "Measuring What Matters: The AI Pluralism Index",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring What Matters: The AI Pluralism Index"
                },
                "summary": "Artificial intelligence systems increasingly mediate knowledge,\ncommunication, and decision making. Development and governance remain\nconcentrated within a small set of firms and states, raising concerns that\ntechnologies may encode narrow interests and limit public agency. Capability\nbenchmarks for language, vision, and coding are common, yet public, auditable\nmeasures of pluralistic governance are rare. We define AI pluralism as the\ndegree to which affected stakeholders can shape objectives, data practices,\nsafeguards, and deployment. We present the AI Pluralism Index (AIPI), a\ntransparent, evidence-based instrument that evaluates producers and system\nfamilies across four pillars: participatory governance, inclusivity and\ndiversity, transparency, and accountability. AIPI codes verifiable practices\nfrom public artifacts and independent evaluations, explicitly handling\n\"Unknown\" evidence to report both lower-bound (\"evidence\") and known-only\nscores with coverage. We formalize the measurement model; implement a\nreproducible pipeline that integrates structured web and repository analysis,\nexternal assessments, and expert interviews; and assess reliability with\ninter-rater agreement, coverage reporting, cross-index correlations, and\nsensitivity analysis. The protocol, codebook, scoring scripts, and evidence\ngraph are maintained openly with versioned releases and a public adjudication\nprocess. We report pilot provider results and situate AIPI relative to adjacent\ntransparency, safety, and governance frameworks. The index aims to steer\nincentives toward pluralistic practice and to equip policymakers, procurers,\nand the public with comparable evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence systems increasingly mediate knowledge,\ncommunication, and decision making. Development and governance remain\nconcentrated within a small set of firms and states, raising concerns that\ntechnologies may encode narrow interests and limit public agency. Capability\nbenchmarks for language, vision, and coding are common, yet public, auditable\nmeasures of pluralistic governance are rare. We define AI pluralism as the\ndegree to which affected stakeholders can shape objectives, data practices,\nsafeguards, and deployment. We present the AI Pluralism Index (AIPI), a\ntransparent, evidence-based instrument that evaluates producers and system\nfamilies across four pillars: participatory governance, inclusivity and\ndiversity, transparency, and accountability. AIPI codes verifiable practices\nfrom public artifacts and independent evaluations, explicitly handling\n\"Unknown\" evidence to report both lower-bound (\"evidence\") and known-only\nscores with coverage. We formalize the measurement model; implement a\nreproducible pipeline that integrates structured web and repository analysis,\nexternal assessments, and expert interviews; and assess reliability with\ninter-rater agreement, coverage reporting, cross-index correlations, and\nsensitivity analysis. The protocol, codebook, scoring scripts, and evidence\ngraph are maintained openly with versioned releases and a public adjudication\nprocess. We report pilot provider results and situate AIPI relative to adjacent\ntransparency, safety, and governance frameworks. The index aims to steer\nincentives toward pluralistic practice and to equip policymakers, procurers,\nand the public with comparable evidence."
                },
                "authors": [
                    {
                        "name": "Rashid Mushkani"
                    }
                ],
                "author_detail": {
                    "name": "Rashid Mushkani"
                },
                "author": "Rashid Mushkani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08191v1",
                "updated": "2025-10-09T13:18:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    18,
                    17,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:18:17Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    18,
                    17,
                    3,
                    282,
                    0
                ],
                "title": "Training-Free Group Relative Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Group Relative Policy Optimization"
                },
                "summary": "Recent advances in Large Language Model (LLM) agents have demonstrated their\npromising general capabilities. However, their performance in specialized\nreal-world domains often degrades due to challenges in effectively integrating\nexternal tools and specific prompting strategies. While methods like agentic\nreinforcement learning have been proposed to address this, they typically rely\non costly parameter updates, for example, through a process that uses\nSupervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase\nwith Group Relative Policy Optimization (GRPO) to alter the output\ndistribution. However, we argue that LLMs can achieve a similar effect on the\noutput distribution by learning experiential knowledge as a token prior, which\nis a far more lightweight approach that not only addresses practical data\nscarcity but also avoids the common issue of overfitting. To this end, we\npropose Training-Free Group Relative Policy Optimization (Training-Free GRPO),\na cost-effective solution that enhances LLM agent performance without any\nparameter updates. Our method leverages the group relative semantic advantage\ninstead of numerical ones within each group of rollouts, iteratively distilling\nhigh-quality experiential knowledge during multi-epoch learning on a minimal\nground-truth data. Such knowledge serves as the learned token prior, which is\nseamlessly integrated during LLM API calls to guide model behavior. Experiments\non mathematical reasoning and web searching tasks demonstrate that\nTraining-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly\nimproves out-of-domain performance. With just a few dozen training samples,\nTraining-Free GRPO outperforms fine-tuned small LLMs with marginal training\ndata and cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Model (LLM) agents have demonstrated their\npromising general capabilities. However, their performance in specialized\nreal-world domains often degrades due to challenges in effectively integrating\nexternal tools and specific prompting strategies. While methods like agentic\nreinforcement learning have been proposed to address this, they typically rely\non costly parameter updates, for example, through a process that uses\nSupervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase\nwith Group Relative Policy Optimization (GRPO) to alter the output\ndistribution. However, we argue that LLMs can achieve a similar effect on the\noutput distribution by learning experiential knowledge as a token prior, which\nis a far more lightweight approach that not only addresses practical data\nscarcity but also avoids the common issue of overfitting. To this end, we\npropose Training-Free Group Relative Policy Optimization (Training-Free GRPO),\na cost-effective solution that enhances LLM agent performance without any\nparameter updates. Our method leverages the group relative semantic advantage\ninstead of numerical ones within each group of rollouts, iteratively distilling\nhigh-quality experiential knowledge during multi-epoch learning on a minimal\nground-truth data. Such knowledge serves as the learned token prior, which is\nseamlessly integrated during LLM API calls to guide model behavior. Experiments\non mathematical reasoning and web searching tasks demonstrate that\nTraining-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly\nimproves out-of-domain performance. With just a few dozen training samples,\nTraining-Free GRPO outperforms fine-tuned small LLMs with marginal training\ndata and cost."
                },
                "authors": [
                    {
                        "name": "Yuzheng Cai"
                    },
                    {
                        "name": "Siqi Cai"
                    },
                    {
                        "name": "Yuchen Shi"
                    },
                    {
                        "name": "Zihan Xu"
                    },
                    {
                        "name": "Lichao Chen"
                    },
                    {
                        "name": "Yulei Qin"
                    },
                    {
                        "name": "Xiaoyu Tan"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Zongyi Li"
                    },
                    {
                        "name": "Haojia Lin"
                    },
                    {
                        "name": "Yong Mao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08188v1",
                "updated": "2025-10-09T13:14:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    14,
                    38,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:14:38Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    14,
                    38,
                    3,
                    282,
                    0
                ],
                "title": "MetricalARGS: A Taxonomy for Studying Metrical Poetry with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetricalARGS: A Taxonomy for Studying Metrical Poetry with LLMs"
                },
                "summary": "Prior NLP work studying poetry has focused primarily on automatic poem\ngeneration and summarization. Many languages have well-studied traditions of\npoetic meter which enforce constraints on a poem in terms of syllable and\nphoneme patterns. Such advanced literary forms offer opportunities for probing\ndeeper reasoning and language understanding in Large Language Models (LLMs) and\ntheir ability to follow strict pre-requisites and rules. In this paper, we\nintroduce MetricalARGS, the first taxonomy of poetry-related NLP tasks designed\nto evaluate LLMs on metrical poetry across four dimensions: Analysis,\nRetrieval, Generation, and Support. We discuss how these tasks relate to\nexisting NLP tasks, addressing questions around datasets and evaluation\nmetrics. Taking Telugu as our example language, we illustrate how the taxonomy\ncan be used in practice. MetricalARGS highlights the broader possibilities for\nunderstanding the capabilities and limitations of today's LLMs through the lens\nof metrical poetry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior NLP work studying poetry has focused primarily on automatic poem\ngeneration and summarization. Many languages have well-studied traditions of\npoetic meter which enforce constraints on a poem in terms of syllable and\nphoneme patterns. Such advanced literary forms offer opportunities for probing\ndeeper reasoning and language understanding in Large Language Models (LLMs) and\ntheir ability to follow strict pre-requisites and rules. In this paper, we\nintroduce MetricalARGS, the first taxonomy of poetry-related NLP tasks designed\nto evaluate LLMs on metrical poetry across four dimensions: Analysis,\nRetrieval, Generation, and Support. We discuss how these tasks relate to\nexisting NLP tasks, addressing questions around datasets and evaluation\nmetrics. Taking Telugu as our example language, we illustrate how the taxonomy\ncan be used in practice. MetricalARGS highlights the broader possibilities for\nunderstanding the capabilities and limitations of today's LLMs through the lens\nof metrical poetry."
                },
                "authors": [
                    {
                        "name": "Chalamalasetti Kranti"
                    },
                    {
                        "name": "Sowmya Vajjala"
                    }
                ],
                "author_detail": {
                    "name": "Sowmya Vajjala"
                },
                "author": "Sowmya Vajjala",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13547v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13547v3",
                "updated": "2025-10-09T13:11:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    11,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2024-02-21T06:04:53Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    6,
                    4,
                    53,
                    2,
                    52,
                    0
                ],
                "title": "ThinkNote: Enhancing Knowledge Integration and Utilization of Large\n  Language Models via Constructivist Cognition Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkNote: Enhancing Knowledge Integration and Utilization of Large\n  Language Models via Constructivist Cognition Modeling"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong performance across a\nwide range of NLP tasks. However, they often exhibit suboptimal behaviors and\ninconsistencies when exposed to unfamiliar external information, underscoring\ntheir limitations in effectively leveraging such knowledge. Inspired by\nconstructivist learning theory, we propose ThinkNote, a novel framework that\nenhances the external knowledge utilization of LLMs through a two-stage\nconstructivist cognitive modeling process. Specifically, ThinkNote performs\nknowledge assimilation to align new information with the model's parametric\nmemory, forming a coherent internal representation. It then applies thought\naccommodation to adapt internal reasoning, thereby promoting more consistent\nand reliable outputs. Extensive experimental results demonstrate that ThinkNote\nachieves a 10% improvement over strong baseline methods on various\nquestion-answering benchmarks. Further analysis indicates that ThinkNote\neffectively integrates and utilizes external knowledge to help LLMs generate\naccurate responses and improves their self-consistency. All data and codes are\navailable at https://github.com/OpenMatch/ThinkNote.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong performance across a\nwide range of NLP tasks. However, they often exhibit suboptimal behaviors and\ninconsistencies when exposed to unfamiliar external information, underscoring\ntheir limitations in effectively leveraging such knowledge. Inspired by\nconstructivist learning theory, we propose ThinkNote, a novel framework that\nenhances the external knowledge utilization of LLMs through a two-stage\nconstructivist cognitive modeling process. Specifically, ThinkNote performs\nknowledge assimilation to align new information with the model's parametric\nmemory, forming a coherent internal representation. It then applies thought\naccommodation to adapt internal reasoning, thereby promoting more consistent\nand reliable outputs. Extensive experimental results demonstrate that ThinkNote\nachieves a 10% improvement over strong baseline methods on various\nquestion-answering benchmarks. Further analysis indicates that ThinkNote\neffectively integrates and utilizes external knowledge to help LLMs generate\naccurate responses and improves their self-consistency. All data and codes are\navailable at https://github.com/OpenMatch/ThinkNote."
                },
                "authors": [
                    {
                        "name": "Zhipeng Xu"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Zheni Zeng"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Chenyan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Chenyan Xiong"
                },
                "author": "Chenyan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13547v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v2",
                "updated": "2025-10-09T13:03:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    3,
                    29,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22745v2",
                "updated": "2025-10-09T13:00:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    0,
                    18,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-26T04:10:32Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    4,
                    10,
                    32,
                    4,
                    269,
                    0
                ],
                "title": "Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending MoE LLMs against Harmful Fine-Tuning via Safety Routing\n  Alignment"
                },
                "summary": "Recent large language models (LLMs) have increasingly adopted the\nMixture-of-Experts (MoE) architecture for efficiency. MoE-based LLMs heavily\ndepend on a superficial safety mechanism in which harmful inputs are routed\nsafety-critical experts. However, our analysis reveals that routing decisions\nfor harmful inputs drift significantly after fine-tuning, exposing a critical\nvulnerability to harmful fine-tuning (HFT) attacks. Existing defenses,\nprimarily designed for monolithic LLMs, are less effective for MoE LLMs as they\nfail to prevent drift in harmful input routing. To address this limitation, we\npropose SafeMoE, a safe fine-tuning method tailored to MoE LLMs. SafeMoE\ndirectly mitigates routing drift by penalizing the gap between the routing\nweights of a fine-tuned model and those of the initial safety-aligned model,\nthereby preserving the safety-aligned routing of harmful inputs to\nsafety-critical experts. Experiments on open-source MoE LLMs ranging from 7B to\n141B parameters demonstrate that SafeMoE effectively mitigates HFT attacks,\nreducing the harmfulness score of OLMoE from 62.0 to 5.0, for example, while\nmaintaining task utility within 1% degradation and incurring only 2% overhead.\nIt significantly outperforms state-of-the-art defense methods for safeguarding\nLLM fine-tuning and remains effective in recent large-scale MoE LLMs such as\ngpt-oss and Llama 4. Our implementation is available at\nhttps://anonymous.4open.science/r/SafeMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) have increasingly adopted the\nMixture-of-Experts (MoE) architecture for efficiency. MoE-based LLMs heavily\ndepend on a superficial safety mechanism in which harmful inputs are routed\nsafety-critical experts. However, our analysis reveals that routing decisions\nfor harmful inputs drift significantly after fine-tuning, exposing a critical\nvulnerability to harmful fine-tuning (HFT) attacks. Existing defenses,\nprimarily designed for monolithic LLMs, are less effective for MoE LLMs as they\nfail to prevent drift in harmful input routing. To address this limitation, we\npropose SafeMoE, a safe fine-tuning method tailored to MoE LLMs. SafeMoE\ndirectly mitigates routing drift by penalizing the gap between the routing\nweights of a fine-tuned model and those of the initial safety-aligned model,\nthereby preserving the safety-aligned routing of harmful inputs to\nsafety-critical experts. Experiments on open-source MoE LLMs ranging from 7B to\n141B parameters demonstrate that SafeMoE effectively mitigates HFT attacks,\nreducing the harmfulness score of OLMoE from 62.0 to 5.0, for example, while\nmaintaining task utility within 1% degradation and incurring only 2% overhead.\nIt significantly outperforms state-of-the-art defense methods for safeguarding\nLLM fine-tuning and remains effective in recent large-scale MoE LLMs such as\ngpt-oss and Llama 4. Our implementation is available at\nhttps://anonymous.4open.science/r/SafeMoE."
                },
                "authors": [
                    {
                        "name": "Jaehan Kim"
                    },
                    {
                        "name": "Minkyoo Song"
                    },
                    {
                        "name": "Seungwon Shin"
                    },
                    {
                        "name": "Sooel Son"
                    }
                ],
                "author_detail": {
                    "name": "Sooel Son"
                },
                "author": "Sooel Son",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11112v2",
                "updated": "2025-10-09T12:56:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    56,
                    4,
                    3,
                    282,
                    0
                ],
                "published": "2025-07-15T09:04:30Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    4,
                    30,
                    1,
                    196,
                    0
                ],
                "title": "Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs"
                },
                "summary": "Recent studies have shown that Large Language Models (LLMs) are vulnerable to\ndata poisoning attacks, where malicious training examples embed hidden\nbehaviours triggered by specific input patterns. However, most existing works\nassume a phrase and focus on the attack's effectiveness, offering limited\nunderstanding of trigger mechanisms and how multiple triggers interact within\nthe model. In this paper, we present a framework for studying poisoning in\nLLMs. We show that multiple distinct backdoor triggers can coexist within a\nsingle model without interfering with each other, enabling adversaries to embed\nseveral triggers concurrently. Using multiple triggers with high embedding\nsimilarity, we demonstrate that poisoned triggers can achieve robust activation\neven when tokens are substituted or separated by long token spans. Our findings\nexpose a broader and more persistent vulnerability surface in LLMs. To mitigate\nthis threat, we propose a post hoc recovery method that selectively retrains\nspecific model components based on a layer-wise weight difference analysis. Our\nmethod effectively removes the trigger behaviour with minimal parameter\nupdates, presenting a practical and efficient defence against multi-trigger\npoisoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that Large Language Models (LLMs) are vulnerable to\ndata poisoning attacks, where malicious training examples embed hidden\nbehaviours triggered by specific input patterns. However, most existing works\nassume a phrase and focus on the attack's effectiveness, offering limited\nunderstanding of trigger mechanisms and how multiple triggers interact within\nthe model. In this paper, we present a framework for studying poisoning in\nLLMs. We show that multiple distinct backdoor triggers can coexist within a\nsingle model without interfering with each other, enabling adversaries to embed\nseveral triggers concurrently. Using multiple triggers with high embedding\nsimilarity, we demonstrate that poisoned triggers can achieve robust activation\neven when tokens are substituted or separated by long token spans. Our findings\nexpose a broader and more persistent vulnerability surface in LLMs. To mitigate\nthis threat, we propose a post hoc recovery method that selectively retrains\nspecific model components based on a layer-wise weight difference analysis. Our\nmethod effectively removes the trigger behaviour with minimal parameter\nupdates, presenting a practical and efficient defence against multi-trigger\npoisoning."
                },
                "authors": [
                    {
                        "name": "Sanhanat Sivapiromrat"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Marco Basaldella"
                    },
                    {
                        "name": "Nigel Collier"
                    }
                ],
                "author_detail": {
                    "name": "Nigel Collier"
                },
                "author": "Nigel Collier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06223v2",
                "updated": "2025-10-09T12:55:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    55,
                    47,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-31T14:40:11Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    40,
                    11,
                    6,
                    243,
                    0
                ],
                "title": "A Multimodal GUI Architecture for Interfacing with LLM-Based\n  Conversational Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multimodal GUI Architecture for Interfacing with LLM-Based\n  Conversational Assistants"
                },
                "summary": "Advances in large language models (LLMs) and real-time speech recognition now\nmake it possible to issue any graphical user interface (GUI) action through\nnatural language and receive the corresponding system response directly through\nthe GUI. Most production applications were never designed with speech in mind.\nThis article provides a concrete architecture that enables GUIs to interface\nwith LLM-based speech-enabled assistants.\n  The architecture makes an application's navigation graph and semantics\navailable through the Model Context Protocol (MCP). The ViewModel, part of the\nMVVM (Model-View-ViewModel) pattern, exposes the application's capabilities to\nthe assistant by supplying both tools applicable to a currently visible view\nand application-global tools extracted from the GUI tree router. This\narchitecture facilitates full voice accessibility while ensuring reliable\nalignment between spoken input and the visual interface, accompanied by\nconsistent feedback across modalities. It future-proofs apps for upcoming OS\nsuper assistants that employ computer use agents (CUAs) and natively consume\nMCP if an application provides it.\n  To address concerns about privacy and data security, the practical\neffectiveness of locally deployable, open-weight LLMs for speech-enabled\nmultimodal UIs is evaluated. Findings suggest that recent smaller open-weight\nmodels approach the performance of leading proprietary models in overall\naccuracy and require enterprise-grade hardware for fast responsiveness.\n  A demo implementation of the proposed architecture can be found at\nhttps://github.com/hansvdam/langbar",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in large language models (LLMs) and real-time speech recognition now\nmake it possible to issue any graphical user interface (GUI) action through\nnatural language and receive the corresponding system response directly through\nthe GUI. Most production applications were never designed with speech in mind.\nThis article provides a concrete architecture that enables GUIs to interface\nwith LLM-based speech-enabled assistants.\n  The architecture makes an application's navigation graph and semantics\navailable through the Model Context Protocol (MCP). The ViewModel, part of the\nMVVM (Model-View-ViewModel) pattern, exposes the application's capabilities to\nthe assistant by supplying both tools applicable to a currently visible view\nand application-global tools extracted from the GUI tree router. This\narchitecture facilitates full voice accessibility while ensuring reliable\nalignment between spoken input and the visual interface, accompanied by\nconsistent feedback across modalities. It future-proofs apps for upcoming OS\nsuper assistants that employ computer use agents (CUAs) and natively consume\nMCP if an application provides it.\n  To address concerns about privacy and data security, the practical\neffectiveness of locally deployable, open-weight LLMs for speech-enabled\nmultimodal UIs is evaluated. Findings suggest that recent smaller open-weight\nmodels approach the performance of leading proprietary models in overall\naccuracy and require enterprise-grade hardware for fast responsiveness.\n  A demo implementation of the proposed architecture can be found at\nhttps://github.com/hansvdam/langbar"
                },
                "authors": [
                    {
                        "name": "Hans G. W. van Dam"
                    }
                ],
                "author_detail": {
                    "name": "Hans G. W. van Dam"
                },
                "author": "Hans G. W. van Dam",
                "arxiv_comment": "24 pages, 19 figures, code available at\n  https://github.com/hansvdam/langbar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; D.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08164v1",
                "updated": "2025-10-09T12:49:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    49,
                    41,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T12:49:41Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    49,
                    41,
                    3,
                    282,
                    0
                ],
                "title": "A Multi-Simulation Bridge for IoT Digital Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Simulation Bridge for IoT Digital Twins"
                },
                "summary": "The increasing capabilities of Digital Twins (DTs) in the context of the\nInternet of Things (IoT) and Industrial IoT (IIoT) call for seamless\nintegration with simulation platforms to support system design, validation, and\nreal-time operation. This paper introduces the concept, design, and\nexperimental evaluation of the DT Simulation Bridge - a software framework that\nenables diverse interaction patterns between active DTs and simulation\nenvironments. The framework supports both the DT development lifecycle and the\nincorporation of simulations during active operation. Through bidirectional\ndata exchange, simulations can update DT models dynamically, while DTs provide\nreal-time feedback to adapt simulation parameters. We describe the\narchitectural design and core software components that ensure flexible\ninteroperability and scalable deployment. Experimental results show that the DT\nSimulation Bridge enhances design agility, facilitates virtual commissioning,\nand supports live behavioral analysis under realistic conditions, demonstrating\nits effectiveness across a range of industrial scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing capabilities of Digital Twins (DTs) in the context of the\nInternet of Things (IoT) and Industrial IoT (IIoT) call for seamless\nintegration with simulation platforms to support system design, validation, and\nreal-time operation. This paper introduces the concept, design, and\nexperimental evaluation of the DT Simulation Bridge - a software framework that\nenables diverse interaction patterns between active DTs and simulation\nenvironments. The framework supports both the DT development lifecycle and the\nincorporation of simulations during active operation. Through bidirectional\ndata exchange, simulations can update DT models dynamically, while DTs provide\nreal-time feedback to adapt simulation parameters. We describe the\narchitectural design and core software components that ensure flexible\ninteroperability and scalable deployment. Experimental results show that the DT\nSimulation Bridge enhances design agility, facilitates virtual commissioning,\nand supports live behavioral analysis under realistic conditions, demonstrating\nits effectiveness across a range of industrial scenarios."
                },
                "authors": [
                    {
                        "name": "Marco Picone"
                    },
                    {
                        "name": "Samuele Burattini"
                    },
                    {
                        "name": "Marco Melloni"
                    },
                    {
                        "name": "Prasad Talasila"
                    },
                    {
                        "name": "Davide Ziglioli"
                    },
                    {
                        "name": "Matteo Martinelli"
                    },
                    {
                        "name": "Nicola Bicocchi"
                    },
                    {
                        "name": "Alessandro Ricci"
                    },
                    {
                        "name": "Peter Gorm Larsen"
                    }
                ],
                "author_detail": {
                    "name": "Peter Gorm Larsen"
                },
                "author": "Peter Gorm Larsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08158v1",
                "updated": "2025-10-09T12:38:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    38,
                    16,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T12:38:16Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    38,
                    16,
                    3,
                    282,
                    0
                ],
                "title": "Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation\n  for Exaggerated Refusals in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation\n  for Exaggerated Refusals in LLMs"
                },
                "summary": "Large language models (LLMs) frequently produce false refusals, declining\nbenign requests that contain terms resembling unsafe queries. We address this\nchallenge by introducing two comprehensive benchmarks: the Exaggerated Safety\nBenchmark (XSB) for single-turn prompts, annotated with \"Focus\" keywords that\nidentify refusal-inducing triggers, and the Multi-turn Scenario-based\nExaggerated Safety Benchmark (MS-XSB), which systematically evaluates refusal\ncalibration in realistic, context-rich dialog settings. Our benchmarks reveal\nthat exaggerated refusals persist across diverse recent LLMs and are especially\npronounced in complex, multi-turn scenarios. To mitigate these failures, we\nleverage post-hoc explanation methods to identify refusal triggers and deploy\nthree lightweight, model-agnostic approaches, ignore-word instructions, prompt\nrephrasing, and attention steering, at inference time, all without retraining\nor parameter access. Experiments on four instruction-tuned Llama models\ndemonstrate that these strategies substantially improve compliance on safe\nprompts while maintaining robust safety protections. Our findings establish a\nreproducible framework for diagnosing and mitigating exaggerated refusals,\nhighlighting practical pathways to safer and more helpful LLM deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently produce false refusals, declining\nbenign requests that contain terms resembling unsafe queries. We address this\nchallenge by introducing two comprehensive benchmarks: the Exaggerated Safety\nBenchmark (XSB) for single-turn prompts, annotated with \"Focus\" keywords that\nidentify refusal-inducing triggers, and the Multi-turn Scenario-based\nExaggerated Safety Benchmark (MS-XSB), which systematically evaluates refusal\ncalibration in realistic, context-rich dialog settings. Our benchmarks reveal\nthat exaggerated refusals persist across diverse recent LLMs and are especially\npronounced in complex, multi-turn scenarios. To mitigate these failures, we\nleverage post-hoc explanation methods to identify refusal triggers and deploy\nthree lightweight, model-agnostic approaches, ignore-word instructions, prompt\nrephrasing, and attention steering, at inference time, all without retraining\nor parameter access. Experiments on four instruction-tuned Llama models\ndemonstrate that these strategies substantially improve compliance on safe\nprompts while maintaining robust safety protections. Our findings establish a\nreproducible framework for diagnosing and mitigating exaggerated refusals,\nhighlighting practical pathways to safer and more helpful LLM deployments."
                },
                "authors": [
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Ercong Nie"
                    },
                    {
                        "name": "Yinuo Sun"
                    },
                    {
                        "name": "Chenxuan Zhao"
                    },
                    {
                        "name": "William LaCroix"
                    },
                    {
                        "name": "Michael Färber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Färber"
                },
                "author": "Michael Färber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05858v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05858v3",
                "updated": "2025-10-09T12:35:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    35,
                    24,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-07T12:26:19Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    12,
                    26,
                    19,
                    1,
                    280,
                    0
                ],
                "title": "DACP: Domain-Adaptive Continual Pre-Training of Large Language Models\n  for Phone Conversation Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACP: Domain-Adaptive Continual Pre-Training of Large Language Models\n  for Phone Conversation Summarization"
                },
                "summary": "Large language models (LLMs) have achieved impressive performance in text\nsummarization, yet their performance often falls short when applied to\nspecialized domains that differ from their original pre-training distribution.\nWhile fine-tuning can improve summarization quality, it typically relies on\ncostly and scarce high-quality labeled data. In this work, we explore continual\npre-training as a scalable, self-supervised approach to adapt LLMs for\ndownstream summarization tasks, particularly in the context of noisy real-world\nconversation transcripts. We conduct extensive experiments using large-scale,\nunlabeled business conversation data to investigate whether continual\npre-training enhances model capabilities in conversational summarization. Our\nresults demonstrate that continual pre-training yields substantial gains in\nboth in-domain and out-of-domain summarization benchmarks, while maintaining\nstrong generalization and robustness. We also analyze the effects of data\nselection strategies, providing practical guidelines for applying continual\npre-training in summarization-focused industrial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive performance in text\nsummarization, yet their performance often falls short when applied to\nspecialized domains that differ from their original pre-training distribution.\nWhile fine-tuning can improve summarization quality, it typically relies on\ncostly and scarce high-quality labeled data. In this work, we explore continual\npre-training as a scalable, self-supervised approach to adapt LLMs for\ndownstream summarization tasks, particularly in the context of noisy real-world\nconversation transcripts. We conduct extensive experiments using large-scale,\nunlabeled business conversation data to investigate whether continual\npre-training enhances model capabilities in conversational summarization. Our\nresults demonstrate that continual pre-training yields substantial gains in\nboth in-domain and out-of-domain summarization benchmarks, while maintaining\nstrong generalization and robustness. We also analyze the effects of data\nselection strategies, providing practical guidelines for applying continual\npre-training in summarization-focused industrial applications."
                },
                "authors": [
                    {
                        "name": "Xue-Yong Fu"
                    },
                    {
                        "name": "Elena Khasanova"
                    },
                    {
                        "name": "Md Tahmid Rahman Laskar"
                    },
                    {
                        "name": "Harsh Saini"
                    },
                    {
                        "name": "Shashi Bhushan TN"
                    }
                ],
                "author_detail": {
                    "name": "Shashi Bhushan TN"
                },
                "author": "Shashi Bhushan TN",
                "arxiv_comment": "Accepted to the NewSumm Workshop at EMNLP 2025. Equal contribution\n  from the first four authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05858v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05858v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08152v1",
                "updated": "2025-10-09T12:35:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    35,
                    20,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T12:35:20Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    35,
                    20,
                    3,
                    282,
                    0
                ],
                "title": "DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading\n  Comprehension on Business Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading\n  Comprehension on Business Conversations"
                },
                "summary": "The rapid advancements in Large Language Models (LLMs) have enabled their\nadoption in real-world industrial scenarios for various natural language\nprocessing tasks. However, the high inference cost of large-scale LLMs makes\ntheir deployment impractical, necessitating the use of smaller models. Despite\ntheir efficiency, smaller LLMs lack robust zero-shot instruction-following\ncapabilities across diverse domains, limiting their adaptability to dynamic\nuser requirements. Traditional fine-tuning approaches exacerbate this issue by\ninducing catastrophic forgetting, reducing the model's generalization ability\nfor unseen tasks. In this paper, we propose Domain Adaptive Continual\nInstruction Pre-Training via Reading Comprehension (DACIP-RC), a continual\npre-training technique that enhances smaller LLMs' domain adaptability for\nbusiness conversational tasks. Unlike conventional pre-training approaches that\nrely on next-token prediction, DACIP-RC generates diverse task instructions and\nresponses via reading comprehension on conversation transcripts, enabling\nbetter instruction generalization. Our empirical evaluations demonstrate that\nDACIP-RC significantly improves zero-shot generalization across a wide range of\nbusiness conversational tasks, including meeting summarization, action item\ngeneration, and call purpose identification. To the best of our knowledge, this\nis the first work to apply instruction pre-training on business conversational\ndata, providing insights into how industries can leverage proprietary datasets\nfor domain adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in Large Language Models (LLMs) have enabled their\nadoption in real-world industrial scenarios for various natural language\nprocessing tasks. However, the high inference cost of large-scale LLMs makes\ntheir deployment impractical, necessitating the use of smaller models. Despite\ntheir efficiency, smaller LLMs lack robust zero-shot instruction-following\ncapabilities across diverse domains, limiting their adaptability to dynamic\nuser requirements. Traditional fine-tuning approaches exacerbate this issue by\ninducing catastrophic forgetting, reducing the model's generalization ability\nfor unseen tasks. In this paper, we propose Domain Adaptive Continual\nInstruction Pre-Training via Reading Comprehension (DACIP-RC), a continual\npre-training technique that enhances smaller LLMs' domain adaptability for\nbusiness conversational tasks. Unlike conventional pre-training approaches that\nrely on next-token prediction, DACIP-RC generates diverse task instructions and\nresponses via reading comprehension on conversation transcripts, enabling\nbetter instruction generalization. Our empirical evaluations demonstrate that\nDACIP-RC significantly improves zero-shot generalization across a wide range of\nbusiness conversational tasks, including meeting summarization, action item\ngeneration, and call purpose identification. To the best of our knowledge, this\nis the first work to apply instruction pre-training on business conversational\ndata, providing insights into how industries can leverage proprietary datasets\nfor domain adaptation."
                },
                "authors": [
                    {
                        "name": "Elena Khasanova"
                    },
                    {
                        "name": "Harsh Saini"
                    },
                    {
                        "name": "Md Tahmid Rahman Laskar"
                    },
                    {
                        "name": "Xue-Yong Fu"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Shashi Bhushan TN"
                    }
                ],
                "author_detail": {
                    "name": "Shashi Bhushan TN"
                },
                "author": "Shashi Bhushan TN",
                "arxiv_comment": "Accepted to the EMNLP 2025 Industry Track. Equal contribution from\n  the first four authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08149v1",
                "updated": "2025-10-09T12:34:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    34,
                    31,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T12:34:31Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    34,
                    31,
                    3,
                    282,
                    0
                ],
                "title": "AI Knowledge Assist: An Automated Approach for the Creation of Knowledge\n  Bases for Conversational AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Knowledge Assist: An Automated Approach for the Creation of Knowledge\n  Bases for Conversational AI Agents"
                },
                "summary": "The utilization of conversational AI systems by leveraging Retrieval\nAugmented Generation (RAG) techniques to solve customer problems has been on\nthe rise with the rapid progress of Large Language Models (LLMs). However, the\nabsence of a company-specific dedicated knowledge base is a major barrier to\nthe integration of conversational AI systems in contact centers. To this end,\nwe introduce AI Knowledge Assist, a system that extracts knowledge in the form\nof question-answer (QA) pairs from historical customer-agent conversations to\nautomatically build a knowledge base. Fine-tuning a lightweight LLM on internal\ndata demonstrates state-of-the-art performance, outperforming larger\nclosed-source LLMs. More specifically, empirical evaluation on 20 companies\ndemonstrates that the proposed AI Knowledge Assist system that leverages the\nLLaMA-3.1-8B model eliminates the cold-start gap in contact centers by\nachieving above 90% accuracy in answering information-seeking questions. This\nenables immediate deployment of RAG-powered chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The utilization of conversational AI systems by leveraging Retrieval\nAugmented Generation (RAG) techniques to solve customer problems has been on\nthe rise with the rapid progress of Large Language Models (LLMs). However, the\nabsence of a company-specific dedicated knowledge base is a major barrier to\nthe integration of conversational AI systems in contact centers. To this end,\nwe introduce AI Knowledge Assist, a system that extracts knowledge in the form\nof question-answer (QA) pairs from historical customer-agent conversations to\nautomatically build a knowledge base. Fine-tuning a lightweight LLM on internal\ndata demonstrates state-of-the-art performance, outperforming larger\nclosed-source LLMs. More specifically, empirical evaluation on 20 companies\ndemonstrates that the proposed AI Knowledge Assist system that leverages the\nLLaMA-3.1-8B model eliminates the cold-start gap in contact centers by\nachieving above 90% accuracy in answering information-seeking questions. This\nenables immediate deployment of RAG-powered chatbots."
                },
                "authors": [
                    {
                        "name": "Md Tahmid Rahman Laskar"
                    },
                    {
                        "name": "Julien Bouvier Tremblay"
                    },
                    {
                        "name": "Xue-Yong Fu"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Shashi Bhushan TN"
                    }
                ],
                "author_detail": {
                    "name": "Shashi Bhushan TN"
                },
                "author": "Shashi Bhushan TN",
                "arxiv_comment": "Accepted to the EMNLP 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]