[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v1",
                "updated": "2024-11-22T15:55:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v5",
                "updated": "2024-11-21T05:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    55,
                    43,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13854v1",
                "updated": "2024-11-21T05:26:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:26:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Static Reuse Profile Estimation for Array Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Reuse Profile Estimation for Array Applications"
                },
                "summary": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "Accepted in The International Symposium on Memory Systems (MEMSYS\n  24), September 30 to October 03, 2024, Washington, DC, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02243v3",
                "updated": "2024-11-21T04:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    12,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-04T03:06:37Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    3,
                    6,
                    37,
                    6,
                    155,
                    0
                ],
                "title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"
                },
                "summary": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks."
                },
                "authors": [
                    {
                        "name": "Jintao Rong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Linlin Ou"
                    },
                    {
                        "name": "Tianxiao Chen"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Yifan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Liu"
                },
                "author": "Yifan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v2",
                "updated": "2024-11-21T03:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    34,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Nadra Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Nadra Guizani"
                },
                "author": "Nadra Guizani",
                "arxiv_comment": "Model architecture changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13786v1",
                "updated": "2024-11-21T02:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Adaptable Embeddings Network (AEN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable Embeddings Network (AEN)"
                },
                "summary": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems."
                },
                "authors": [
                    {
                        "name": "Stan Loosmore"
                    },
                    {
                        "name": "Alexander Titus"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Titus"
                },
                "author": "Alexander Titus",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13676v1",
                "updated": "2024-11-20T19:51:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:51:25Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymba: A Hybrid-head Architecture for Small Language Models"
                },
                "summary": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Matthijs Van Keirsbilck"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Yingyan Lin"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "20 pages, models are available on huggingface",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "Sébastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v1",
                "updated": "2024-11-18T11:12:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13588v1",
                "updated": "2024-11-18T02:49:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T02:49:23Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "title": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study"
                },
                "summary": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis."
                },
                "authors": [
                    {
                        "name": "Xibo Sun"
                    },
                    {
                        "name": "Jiarui Fang"
                    },
                    {
                        "name": "Aoyu Li"
                    },
                    {
                        "name": "Jinzhe Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhe Pan"
                },
                "author": "Jinzhe Pan",
                "arxiv_comment": "9 pages including reference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v3",
                "updated": "2024-11-17T12:56:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    12,
                    56,
                    16,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v1",
                "updated": "2024-11-16T20:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13112v3",
                "updated": "2024-11-16T20:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    39,
                    46,
                    5,
                    321,
                    0
                ],
                "published": "2024-03-19T19:27:23Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    19,
                    27,
                    23,
                    1,
                    79,
                    0
                ],
                "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks"
                },
                "summary": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance."
                },
                "authors": [
                    {
                        "name": "Bo-Ru Lu"
                    },
                    {
                        "name": "Nikita Haduong"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Mari Ostendorf"
                    }
                ],
                "author_detail": {
                    "name": "Mari Ostendorf"
                },
                "author": "Mari Ostendorf",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10803v1",
                "updated": "2024-11-16T13:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T13:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model"
                },
                "summary": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "8 pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01733v2",
                "updated": "2024-11-16T07:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    43,
                    28,
                    5,
                    321,
                    0
                ],
                "published": "2024-06-03T18:49:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    49,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching"
                },
                "summary": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache"
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Michael Bi Mi"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v1",
                "updated": "2024-11-16T01:39:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v4",
                "updated": "2024-11-15T22:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    37,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v2",
                "updated": "2024-11-15T22:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    30,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v1",
                "updated": "2024-11-15T16:24:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v2",
                "updated": "2024-11-15T07:25:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    25,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v1",
                "updated": "2024-11-15T00:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "RuQing G. Xu"
                    },
                    {
                        "name": "Devin A. Matthews"
                    }
                ],
                "author_detail": {
                    "name": "Devin A. Matthews"
                },
                "author": "Devin A. Matthews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09812v1",
                "updated": "2024-11-14T21:01:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:01:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments"
                },
                "summary": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v1",
                "updated": "2024-11-14T18:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v2",
                "updated": "2024-11-14T17:46:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    46,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v1",
                "updated": "2024-11-14T08:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Pkd-tree: Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pkd-tree: Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08672v1",
                "updated": "2024-11-13T15:07:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T15:07:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks"
                },
                "summary": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Zhibin Gao"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "conference paper with 6 pages and 5 figures. arXiv admin note: text\n  overlap with arXiv:2411.01458",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08312v1",
                "updated": "2024-11-13T03:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T03:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "title": "A Novel Extensible Simulation Framework for CXL-Enabled Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Extensible Simulation Framework for CXL-Enabled Systems"
                },
                "summary": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3."
                },
                "authors": [
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Mingzhe Zhang"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "arxiv_affiliation": "Peking University",
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08203v1",
                "updated": "2024-11-12T21:50:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T21:50:03Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "title": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing"
                },
                "summary": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads."
                },
                "authors": [
                    {
                        "name": "Jacopo Tagliabue"
                    },
                    {
                        "name": "Ryan Curtin"
                    },
                    {
                        "name": "Ciro Greco"
                    }
                ],
                "author_detail": {
                    "name": "Ciro Greco"
                },
                "author": "Ciro Greco",
                "arxiv_comment": "Pre-print of the paper accepted at DEMAI@IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06219v3",
                "updated": "2024-11-12T08:18:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    18,
                    45,
                    1,
                    317,
                    0
                ],
                "published": "2024-05-10T03:06:24Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    3,
                    6,
                    24,
                    4,
                    131,
                    0
                ],
                "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding."
                },
                "authors": [
                    {
                        "name": "Haojie Duanmu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Jiangfei Duan"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v1",
                "updated": "2024-11-12T08:17:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06659v1",
                "updated": "2024-11-11T01:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:53:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning"
                },
                "summary": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git ."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "arxiv_comment": "16 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v2",
                "updated": "2024-11-10T23:04:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    23,
                    4,
                    12,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v4",
                "updated": "2024-11-10T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    58,
                    7,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04873v2",
                "updated": "2024-11-10T10:08:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    10,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "published": "2024-06-07T12:12:25Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    12,
                    12,
                    25,
                    4,
                    159,
                    0
                ],
                "title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior"
                },
                "summary": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main"
                },
                "authors": [
                    {
                        "name": "Tanvir Mahmud"
                    },
                    {
                        "name": "Mustafa Munir"
                    },
                    {
                        "name": "Radu Marculescu"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "arxiv_comment": "Accepted in WACV 2025. Project page:\n  https://tanvir-utexas.github.io/AdaVE_Demo/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v1",
                "updated": "2024-11-10T05:12:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02542v2",
                "updated": "2024-11-07T18:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    58,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    58,
                    3,
                    1,
                    156,
                    0
                ],
                "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loki: Low-rank Keys for Efficient Sparse Attention"
                },
                "summary": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods."
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "Proceedings of the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (Main Conference Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v1",
                "updated": "2024-11-07T14:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08043v1",
                "updated": "2024-10-30T02:18:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:18:59Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "title": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications"
                },
                "summary": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks."
                },
                "authors": [
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Ding Yuan"
                    },
                    {
                        "name": "Xueshang Feng"
                    },
                    {
                        "name": "Stefaan Poedts"
                    },
                    {
                        "name": "Zhengyang Zou"
                    },
                    {
                        "name": "Song Feng"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Tong Yin"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yin"
                },
                "author": "Tong Yin",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.space-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.15131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15131v1",
                "updated": "2024-11-22T18:56:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    56,
                    56,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:56:56Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    56,
                    56,
                    4,
                    327,
                    0
                ],
                "title": "WildLMa: Long Horizon Loco-Manipulation in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildLMa: Long Horizon Loco-Manipulation in the Wild"
                },
                "summary": "`In-the-wild' mobile manipulation aims to deploy robots in diverse real-world\nenvironments, which requires the robot to (1) have skills that generalize\nacross object configurations; (2) be capable of long-horizon task execution in\ndiverse environments; and (3) perform complex manipulation beyond\npick-and-place. Quadruped robots with manipulators hold promise for extending\nthe workspace and enabling robust locomotion, but existing results do not\ninvestigate such a capability. This paper proposes WildLMa with three\ncomponents to address these issues: (1) adaptation of learned low-level\ncontroller for VR-enabled whole-body teleoperation and traversability; (2)\nWildLMa-Skill -- a library of generalizable visuomotor skills acquired via\nimitation learning or heuristics and (3) WildLMa-Planner -- an interface of\nlearned skills that allow LLM planners to coordinate skills for long-horizon\ntasks. We demonstrate the importance of high-quality training data by achieving\nhigher grasping success rate over existing RL baselines using only tens of\ndemonstrations. WildLMa exploits CLIP for language-conditioned imitation\nlearning that empirically generalizes to objects unseen in training\ndemonstrations. Besides extensive quantitative evaluation, we qualitatively\ndemonstrate practical robot applications, such as cleaning up trash in\nuniversity hallways or outdoor terrains, operating articulated objects, and\nrearranging items on a bookshelf.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "`In-the-wild' mobile manipulation aims to deploy robots in diverse real-world\nenvironments, which requires the robot to (1) have skills that generalize\nacross object configurations; (2) be capable of long-horizon task execution in\ndiverse environments; and (3) perform complex manipulation beyond\npick-and-place. Quadruped robots with manipulators hold promise for extending\nthe workspace and enabling robust locomotion, but existing results do not\ninvestigate such a capability. This paper proposes WildLMa with three\ncomponents to address these issues: (1) adaptation of learned low-level\ncontroller for VR-enabled whole-body teleoperation and traversability; (2)\nWildLMa-Skill -- a library of generalizable visuomotor skills acquired via\nimitation learning or heuristics and (3) WildLMa-Planner -- an interface of\nlearned skills that allow LLM planners to coordinate skills for long-horizon\ntasks. We demonstrate the importance of high-quality training data by achieving\nhigher grasping success rate over existing RL baselines using only tens of\ndemonstrations. WildLMa exploits CLIP for language-conditioned imitation\nlearning that empirically generalizes to objects unseen in training\ndemonstrations. Besides extensive quantitative evaluation, we qualitatively\ndemonstrate practical robot applications, such as cleaning up trash in\nuniversity hallways or outdoor terrains, operating articulated objects, and\nrearranging items on a bookshelf."
                },
                "authors": [
                    {
                        "name": "Ri-Zhao Qiu"
                    },
                    {
                        "name": "Yuchen Song"
                    },
                    {
                        "name": "Xuanbin Peng"
                    },
                    {
                        "name": "Sai Aneesh Suryadevara"
                    },
                    {
                        "name": "Ge Yang"
                    },
                    {
                        "name": "Minghuan Liu"
                    },
                    {
                        "name": "Mazeyu Ji"
                    },
                    {
                        "name": "Chengzhe Jia"
                    },
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "arxiv_comment": "Website: https://wildlma.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15129v1",
                "updated": "2024-11-22T18:55:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    55,
                    21,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:55:21Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    55,
                    21,
                    4,
                    327,
                    0
                ],
                "title": "Measuring Bullshit in the Language Games played by ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Bullshit in the Language Games played by ChatGPT"
                },
                "summary": "Generative large language models (LLMs), which create text without direct\ncorrespondence to truth value, are widely understood to resemble the uses of\nlanguage described in Frankfurt's popular monograph On Bullshit. In this paper,\nwe offer a rigorous investigation of this topic, identifying how the phenomenon\nhas arisen, and how it might be analysed. In this paper, we elaborate on this\nargument to propose that LLM-based chatbots play the 'language game of\nbullshit'. We use statistical text analysis to investigate the features of this\nWittgensteinian language game, based on a dataset constructed to contrast the\nlanguage of 1,000 scientific publications with typical pseudo-scientific text\ngenerated by ChatGPT. We then explore whether the same language features can be\ndetected in two well-known contexts of social dysfunction: George Orwell's\ncritique of politics and language, and David Graeber's characterisation of\nbullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a\nstatistical model of the language of bullshit can reliably relate the\nFrankfurtian artificial bullshit of ChatGPT to the political and workplace\nfunctions of bullshit as observed in natural human language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs), which create text without direct\ncorrespondence to truth value, are widely understood to resemble the uses of\nlanguage described in Frankfurt's popular monograph On Bullshit. In this paper,\nwe offer a rigorous investigation of this topic, identifying how the phenomenon\nhas arisen, and how it might be analysed. In this paper, we elaborate on this\nargument to propose that LLM-based chatbots play the 'language game of\nbullshit'. We use statistical text analysis to investigate the features of this\nWittgensteinian language game, based on a dataset constructed to contrast the\nlanguage of 1,000 scientific publications with typical pseudo-scientific text\ngenerated by ChatGPT. We then explore whether the same language features can be\ndetected in two well-known contexts of social dysfunction: George Orwell's\ncritique of politics and language, and David Graeber's characterisation of\nbullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a\nstatistical model of the language of bullshit can reliably relate the\nFrankfurtian artificial bullshit of ChatGPT to the political and workplace\nfunctions of bullshit as observed in natural human language."
                },
                "authors": [
                    {
                        "name": "Alessandro Trevisan"
                    },
                    {
                        "name": "Harry Giddens"
                    },
                    {
                        "name": "Sarah Dillon"
                    },
                    {
                        "name": "Alan F. Blackwell"
                    }
                ],
                "author_detail": {
                    "name": "Alan F. Blackwell"
                },
                "author": "Alan F. Blackwell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12005v2",
                "updated": "2024-11-22T18:48:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    48,
                    28,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-18T19:41:17Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    19,
                    41,
                    17,
                    0,
                    323,
                    0
                ],
                "title": "Exploring the Nature of Little Red Dots: Constraints on AGN and Stellar\n  Contributions from PRIMER MIRI Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Nature of Little Red Dots: Constraints on AGN and Stellar\n  Contributions from PRIMER MIRI Imaging"
                },
                "summary": "JWST has revealed a large population of compact, red galaxies at $z>4$ known\nas Little Red Dots (LRDs). We analyze the spectral energy distributions (SEDs)\nof 95 LRDs from the JWST PRIMER survey with complete photometric coverage from\n$1-18\\ \\mu$m using NIRCam and MIRI imaging, representing the most extensive SED\nanalysis on a large LRD sample with long-wavelength MIRI data. We examine SED\nmodels in which either galaxy or active galactic nucleus (AGN) emission\ndominates the rest-frame UV or optical continuum, extracting physical\nproperties to explore each scenario's implications. In the galaxy-only model,\nwe find massive, dusty stellar populations alongside unobscured, low-mass\ncomponents, hinting at inhomogeneous obscuration. The AGN-only model indicates\ndusty, luminous AGNs with low hot dust fractions compared to typical quasars. A\nhybrid AGN and galaxy model suggests low-mass, unobscured galaxies in the UV,\nwith stellar mass estimates spanning $\\sim$2 dex across the different models,\nunderscoring the need for caution in interpreting LRD stellar masses. With MIRI\nphotometry, the galaxy-only model produces stellar masses within cosmological\nlimits, but extremely high stellar mass densities are inferred. The hybrid\nmodel infers highly overmassive black holes exceeding those in recently\nreported high-redshift AGNs, hinting at a partial AGN contribution to the\nrest-optical continuum or widespread super-Eddington accretion. Our findings\nhighlight the extreme conditions required for both AGN or galaxy dominated\nscenarios in LRDs, supporting a mixed contribution to the red continuum, or\nnovel scenarios to explain the observed emission.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST has revealed a large population of compact, red galaxies at $z>4$ known\nas Little Red Dots (LRDs). We analyze the spectral energy distributions (SEDs)\nof 95 LRDs from the JWST PRIMER survey with complete photometric coverage from\n$1-18\\ \\mu$m using NIRCam and MIRI imaging, representing the most extensive SED\nanalysis on a large LRD sample with long-wavelength MIRI data. We examine SED\nmodels in which either galaxy or active galactic nucleus (AGN) emission\ndominates the rest-frame UV or optical continuum, extracting physical\nproperties to explore each scenario's implications. In the galaxy-only model,\nwe find massive, dusty stellar populations alongside unobscured, low-mass\ncomponents, hinting at inhomogeneous obscuration. The AGN-only model indicates\ndusty, luminous AGNs with low hot dust fractions compared to typical quasars. A\nhybrid AGN and galaxy model suggests low-mass, unobscured galaxies in the UV,\nwith stellar mass estimates spanning $\\sim$2 dex across the different models,\nunderscoring the need for caution in interpreting LRD stellar masses. With MIRI\nphotometry, the galaxy-only model produces stellar masses within cosmological\nlimits, but extremely high stellar mass densities are inferred. The hybrid\nmodel infers highly overmassive black holes exceeding those in recently\nreported high-redshift AGNs, hinting at a partial AGN contribution to the\nrest-optical continuum or widespread super-Eddington accretion. Our findings\nhighlight the extreme conditions required for both AGN or galaxy dominated\nscenarios in LRDs, supporting a mixed contribution to the red continuum, or\nnovel scenarios to explain the observed emission."
                },
                "authors": [
                    {
                        "name": "Gene C. K. Leung"
                    },
                    {
                        "name": "Steven L. Finkelstein"
                    },
                    {
                        "name": "Pablo G. Pérez-González"
                    },
                    {
                        "name": "Alexa M. Morales"
                    },
                    {
                        "name": "Anthony J. Taylor"
                    },
                    {
                        "name": "Guillermo Barro"
                    },
                    {
                        "name": "Dale D. Kocevski"
                    },
                    {
                        "name": "Hollis B. Akins"
                    },
                    {
                        "name": "Adam C. Carnall"
                    },
                    {
                        "name": "Óscar A. Chávez Ortiz"
                    },
                    {
                        "name": "Nikko J. Cleri"
                    },
                    {
                        "name": "Fergus Cullen"
                    },
                    {
                        "name": "Callum T. Donnan"
                    },
                    {
                        "name": "James S. Dunlop"
                    },
                    {
                        "name": "Richard S. Ellis"
                    },
                    {
                        "name": "Norman A. Grogin"
                    },
                    {
                        "name": "Michaela Hirschmann"
                    },
                    {
                        "name": "Anton M. Koekemoer"
                    },
                    {
                        "name": "Vasily Kokorev"
                    },
                    {
                        "name": "Ray A. Lucas"
                    },
                    {
                        "name": "Derek J. McLeod"
                    },
                    {
                        "name": "Casey Papovich"
                    },
                    {
                        "name": "L. Y. Aaron Yung"
                    }
                ],
                "author_detail": {
                    "name": "L. Y. Aaron Yung"
                },
                "author": "L. Y. Aaron Yung",
                "arxiv_comment": "22 pages, 10 figures, submitted to ApJ. Machine-readable form of\n  Table 2 available at: https://github.com/geneckleung/lrd_primer_miri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15110v1",
                "updated": "2024-11-22T18:21:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    21,
                    20,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:21:20Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    21,
                    20,
                    4,
                    327,
                    0
                ],
                "title": "A Real-Time DETR Approach to Bangladesh Road Object Detection for\n  Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Real-Time DETR Approach to Bangladesh Road Object Detection for\n  Autonomous Vehicles"
                },
                "summary": "In the recent years, we have witnessed a paradigm shift in the field of\nComputer Vision, with the forthcoming of the transformer architecture.\nDetection Transformers has become a state of the art solution to object\ndetection and is a potential candidate for Road Object Detection in Autonomous\nVehicles. Despite the abundance of object detection schemes, real-time DETR\nmodels are shown to perform significantly better on inference times, with\nminimal loss of accuracy and performance. In our work, we used Real-Time DETR\n(RTDETR) object detection on the BadODD Road Object Detection dataset based in\nBangladesh, and performed necessary experimentation and testing. Our results\ngave a mAP50 score of 0.41518 in the public 60% test set, and 0.28194 in the\nprivate 40% test set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the recent years, we have witnessed a paradigm shift in the field of\nComputer Vision, with the forthcoming of the transformer architecture.\nDetection Transformers has become a state of the art solution to object\ndetection and is a potential candidate for Road Object Detection in Autonomous\nVehicles. Despite the abundance of object detection schemes, real-time DETR\nmodels are shown to perform significantly better on inference times, with\nminimal loss of accuracy and performance. In our work, we used Real-Time DETR\n(RTDETR) object detection on the BadODD Road Object Detection dataset based in\nBangladesh, and performed necessary experimentation and testing. Our results\ngave a mAP50 score of 0.41518 in the public 60% test set, and 0.28194 in the\nprivate 40% test set."
                },
                "authors": [
                    {
                        "name": "Irfan Nafiz Shahan"
                    },
                    {
                        "name": "Arban Hossain"
                    },
                    {
                        "name": "Saadman Sakib"
                    },
                    {
                        "name": "Al-Mubin Nabil"
                    }
                ],
                "author_detail": {
                    "name": "Al-Mubin Nabil"
                },
                "author": "Al-Mubin Nabil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04292v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04292v2",
                "updated": "2024-11-22T18:11:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    11,
                    23,
                    4,
                    327,
                    0
                ],
                "published": "2024-02-06T10:15:38Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    10,
                    15,
                    38,
                    1,
                    37,
                    0
                ],
                "title": "AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies"
                },
                "summary": "Diffusion-based imitation learning improves Behavioral Cloning (BC) on\nmulti-modal decision-making, but comes at the cost of significantly slower\ninference due to the recursion in the diffusion process. It urges us to design\nefficient policy generators while keeping the ability to generate diverse\nactions. To address this challenge, we propose AdaFlow, an imitation learning\nframework based on flow-based generative modeling. AdaFlow represents the\npolicy with state-conditioned ordinary differential equations (ODEs), which are\nknown as probability flows. We reveal an intriguing connection between the\nconditional variance of their training loss and the discretization error of the\nODEs. With this insight, we propose a variance-adaptive ODE solver that can\nadjust its step size in the inference stage, making AdaFlow an adaptive\ndecision-maker, offering rapid inference without sacrificing diversity.\nInterestingly, it automatically reduces to a one-step generator when the action\ndistribution is uni-modal. Our comprehensive empirical evaluation shows that\nAdaFlow achieves high performance with fast inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based imitation learning improves Behavioral Cloning (BC) on\nmulti-modal decision-making, but comes at the cost of significantly slower\ninference due to the recursion in the diffusion process. It urges us to design\nefficient policy generators while keeping the ability to generate diverse\nactions. To address this challenge, we propose AdaFlow, an imitation learning\nframework based on flow-based generative modeling. AdaFlow represents the\npolicy with state-conditioned ordinary differential equations (ODEs), which are\nknown as probability flows. We reveal an intriguing connection between the\nconditional variance of their training loss and the discretization error of the\nODEs. With this insight, we propose a variance-adaptive ODE solver that can\nadjust its step size in the inference stage, making AdaFlow an adaptive\ndecision-maker, offering rapid inference without sacrificing diversity.\nInterestingly, it automatically reduces to a one-step generator when the action\ndistribution is uni-modal. Our comprehensive empirical evaluation shows that\nAdaFlow achieves high performance with fast inference speed."
                },
                "authors": [
                    {
                        "name": "Xixi Hu"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Qiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Liu"
                },
                "author": "Qiang Liu",
                "arxiv_comment": "NeuRIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04292v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04292v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15100v1",
                "updated": "2024-11-22T18:01:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    1,
                    37,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:01:37Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    1,
                    37,
                    4,
                    327,
                    0
                ],
                "title": "XGrammar: Flexible and Efficient Structured Generation Engine for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XGrammar: Flexible and Efficient Structured Generation Engine for Large\n  Language Models"
                },
                "summary": "The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving."
                },
                "authors": [
                    {
                        "name": "Yixin Dong"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yaxing Cai"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15075v1",
                "updated": "2024-11-22T17:07:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    17,
                    7,
                    29,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T17:07:29Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    17,
                    7,
                    29,
                    4,
                    327,
                    0
                ],
                "title": "The Effects of Major League Baseball's Ban on Infield Shifts: A\n  Quasi-Experimental Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Effects of Major League Baseball's Ban on Infield Shifts: A\n  Quasi-Experimental Analysis"
                },
                "summary": "From 2020 to 2023, Major League Baseball changed rules affecting team\ncomposition, player positioning, and game time. Understanding the effects of\nthese rules is crucial for leagues, teams, players, and other relevant parties\nto assess their impact and to advocate either for further changes or undoing\nprevious ones. Panel data and quasi-experimental methods provide useful tools\nfor causal inference in these settings. I demonstrate this potential by\nanalyzing the effect of the 2023 shift ban at both the league-wide and\nplayer-specific levels. Using difference-in-differences analysis, I show that\nthe policy increased batting average on balls in play and on-base percentage\nfor left-handed batters by a modest amount (nine points). For individual\nplayers, synthetic control analyses identify several players whose offensive\nperformance (on-base percentage, on-base plus slugging percentage, and weighted\non-base average) improved substantially (over 70 points in several cases)\nbecause of the rule change, and other players with previously high shift rates\nfor whom it had little effect. This article both estimates the impact of this\nspecific rule change and demonstrates how these methods for causal inference\nare potentially valuable for sports analytics -- at the player, team, and\nleague levels -- more broadly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 2020 to 2023, Major League Baseball changed rules affecting team\ncomposition, player positioning, and game time. Understanding the effects of\nthese rules is crucial for leagues, teams, players, and other relevant parties\nto assess their impact and to advocate either for further changes or undoing\nprevious ones. Panel data and quasi-experimental methods provide useful tools\nfor causal inference in these settings. I demonstrate this potential by\nanalyzing the effect of the 2023 shift ban at both the league-wide and\nplayer-specific levels. Using difference-in-differences analysis, I show that\nthe policy increased batting average on balls in play and on-base percentage\nfor left-handed batters by a modest amount (nine points). For individual\nplayers, synthetic control analyses identify several players whose offensive\nperformance (on-base percentage, on-base plus slugging percentage, and weighted\non-base average) improved substantially (over 70 points in several cases)\nbecause of the rule change, and other players with previously high shift rates\nfor whom it had little effect. This article both estimates the impact of this\nspecific rule change and demonstrates how these methods for causal inference\nare potentially valuable for sports analytics -- at the player, team, and\nleague levels -- more broadly."
                },
                "authors": [
                    {
                        "name": "Lee Kennedy-Shaffer"
                    }
                ],
                "author_detail": {
                    "name": "Lee Kennedy-Shaffer"
                },
                "author": "Lee Kennedy-Shaffer",
                "arxiv_comment": "25 pages main text, 12 pages appendices, 8 figures (4 main, 4\n  appendix), 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.05004v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.05004v5",
                "updated": "2024-11-22T16:55:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    55,
                    28,
                    4,
                    327,
                    0
                ],
                "published": "2023-09-10T11:36:01Z",
                "published_parsed": [
                    2023,
                    9,
                    10,
                    11,
                    36,
                    1,
                    6,
                    253,
                    0
                ],
                "title": "Reconstructing the kinetic chemotaxis kernel using macroscopic data:\n  well-posedness and ill-posedness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing the kinetic chemotaxis kernel using macroscopic data:\n  well-posedness and ill-posedness"
                },
                "summary": "Bacterial motion is steered by external stimuli (chemotaxis), and the motion\ndescribed on the mesoscopic scale is uniquely determined by a parameter $K$\nthat models velocity change response from the bacteria. This parameter is\ncalled chemotaxis kernel. In a practical setting, it is inferred by\nexperimental data. We deploy a PDE-constrained optimization framework to\nperform this reconstruction using velocity-averaged, localized data taken in\nthe interior of the domain. The problem can be well-posed or ill-posed\ndepending on the data preparation and the experimental setup. In particular, we\npropose one specific design that guarantees numerical reconstructability and\nlocal convergence. This design is adapted to the discretization of $K$ in space\nand decouples the reconstruction of local values of $K$ into smaller cell\nproblems, opening up parallelization opportunities. Numerical evidences support\nthe theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bacterial motion is steered by external stimuli (chemotaxis), and the motion\ndescribed on the mesoscopic scale is uniquely determined by a parameter $K$\nthat models velocity change response from the bacteria. This parameter is\ncalled chemotaxis kernel. In a practical setting, it is inferred by\nexperimental data. We deploy a PDE-constrained optimization framework to\nperform this reconstruction using velocity-averaged, localized data taken in\nthe interior of the domain. The problem can be well-posed or ill-posed\ndepending on the data preparation and the experimental setup. In particular, we\npropose one specific design that guarantees numerical reconstructability and\nlocal convergence. This design is adapted to the discretization of $K$ in space\nand decouples the reconstruction of local values of $K$ into smaller cell\nproblems, opening up parallelization opportunities. Numerical evidences support\nthe theoretical findings."
                },
                "authors": [
                    {
                        "name": "Kathrin Hellmuth"
                    },
                    {
                        "name": "Christian Klingenberg"
                    },
                    {
                        "name": "Qin Li"
                    },
                    {
                        "name": "Min Tang"
                    }
                ],
                "author_detail": {
                    "name": "Min Tang"
                },
                "author": "Min Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.05004v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.05004v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.CB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "35R30, 65M32, 92C17, 49M41, 49K40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15066v1",
                "updated": "2024-11-22T16:54:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    54,
                    17,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T16:54:17Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    54,
                    17,
                    4,
                    327,
                    0
                ],
                "title": "SPAC-Net: Rethinking Point Cloud Completion with Structural Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAC-Net: Rethinking Point Cloud Completion with Structural Prior"
                },
                "summary": "Point cloud completion aims to infer a complete shape from its partial\nobservation. Many approaches utilize a pure encoderdecoder paradigm in which\ncomplete shape can be directly predicted by shape priors learned from partial\nscans, however, these methods suffer from the loss of details inevitably due to\nthe feature abstraction issues. In this paper, we propose a novel\nframework,termed SPAC-Net, that aims to rethink the completion task under the\nguidance of a new structural prior, we call it interface. Specifically, our\nmethod first investigates Marginal Detector (MAD) module to localize the\ninterface, defined as the intersection between the known observation and the\nmissing parts. Based on the interface, our method predicts the coarse shape by\nlearning the displacement from the points in interface move to their\ncorresponding position in missing parts. Furthermore, we devise an additional\nStructure Supplement(SSP) module before the upsampling stage to enhance the\nstructural details of the coarse shape, enabling the upsampling module to focus\nmore on the upsampling task. Extensive experiments have been conducted on\nseveral challenging benchmarks, and the results demonstrate that our method\noutperforms existing state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud completion aims to infer a complete shape from its partial\nobservation. Many approaches utilize a pure encoderdecoder paradigm in which\ncomplete shape can be directly predicted by shape priors learned from partial\nscans, however, these methods suffer from the loss of details inevitably due to\nthe feature abstraction issues. In this paper, we propose a novel\nframework,termed SPAC-Net, that aims to rethink the completion task under the\nguidance of a new structural prior, we call it interface. Specifically, our\nmethod first investigates Marginal Detector (MAD) module to localize the\ninterface, defined as the intersection between the known observation and the\nmissing parts. Based on the interface, our method predicts the coarse shape by\nlearning the displacement from the points in interface move to their\ncorresponding position in missing parts. Furthermore, we devise an additional\nStructure Supplement(SSP) module before the upsampling stage to enhance the\nstructural details of the coarse shape, enabling the upsampling module to focus\nmore on the upsampling task. Extensive experiments have been conducted on\nseveral challenging benchmarks, and the results demonstrate that our method\noutperforms existing state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Zizhao Wu"
                    },
                    {
                        "name": "Jian Shi"
                    },
                    {
                        "name": "Xuan Deng"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Genfu Yang"
                    },
                    {
                        "name": "Ming Zeng"
                    },
                    {
                        "name": "Yunhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhai Wang"
                },
                "author": "Yunhai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.03720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.03720v2",
                "updated": "2024-11-22T16:34:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    34,
                    12,
                    4,
                    327,
                    0
                ],
                "published": "2023-11-26T08:44:58Z",
                "published_parsed": [
                    2023,
                    11,
                    26,
                    8,
                    44,
                    58,
                    6,
                    330,
                    0
                ],
                "title": "Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits"
                },
                "summary": "Large language models LLMs like ChatGPT have reached the 100 Mio user barrier\nin record time and might increasingly enter all areas of our life leading to a\ndiverse set of interactions between those Artificial Intelligence models and\nhumans. While many studies have discussed governance and regulations\ndeductively from first-order principles, few studies provide an inductive,\ndata-driven lens based on observing dialogues between humans and LLMs\nespecially when it comes to non-collaborative, competitive situations that have\nthe potential to pose a serious threat to people. In this work, we conduct a\nuser study engaging over 40 individuals across all age groups in price\nnegotiations with an LLM. We explore how people interact with an LLM,\ninvestigating differences in negotiation outcomes and strategies. Furthermore,\nwe highlight shortcomings of LLMs with respect to their reasoning capabilities\nand, in turn, susceptiveness to prompt hacking, which intends to manipulate the\nLLM to make agreements that are against its instructions or beyond any\nrationality. We also show that the negotiated prices humans manage to achieve\nspan a broad range, which points to a literacy gap in effectively interacting\nwith LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models LLMs like ChatGPT have reached the 100 Mio user barrier\nin record time and might increasingly enter all areas of our life leading to a\ndiverse set of interactions between those Artificial Intelligence models and\nhumans. While many studies have discussed governance and regulations\ndeductively from first-order principles, few studies provide an inductive,\ndata-driven lens based on observing dialogues between humans and LLMs\nespecially when it comes to non-collaborative, competitive situations that have\nthe potential to pose a serious threat to people. In this work, we conduct a\nuser study engaging over 40 individuals across all age groups in price\nnegotiations with an LLM. We explore how people interact with an LLM,\ninvestigating differences in negotiation outcomes and strategies. Furthermore,\nwe highlight shortcomings of LLMs with respect to their reasoning capabilities\nand, in turn, susceptiveness to prompt hacking, which intends to manipulate the\nLLM to make agreements that are against its instructions or beyond any\nrationality. We also show that the negotiated prices humans manage to achieve\nspan a broad range, which points to a literacy gap in effectively interacting\nwith LLMs."
                },
                "authors": [
                    {
                        "name": "Johannes Schneider"
                    },
                    {
                        "name": "Steffi Haag"
                    },
                    {
                        "name": "Leona Chandra Kruse"
                    }
                ],
                "author_detail": {
                    "name": "Leona Chandra Kruse"
                },
                "author": "Leona Chandra Kruse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.03720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.03720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15046v1",
                "updated": "2024-11-22T16:31:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    31,
                    36,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T16:31:36Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    31,
                    36,
                    4,
                    327,
                    0
                ],
                "title": "On Multi-Agent Inverse Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Multi-Agent Inverse Reinforcement Learning"
                },
                "summary": "In multi-agent systems, the agent behavior is highly influenced by its\nutility function, as these utilities shape both individual goals as well as\ninteractions with the other agents. Inverse Reinforcement Learning (IRL) is a\nwell-established approach to inferring the utility function by observing an\nexpert behavior within a given environment. In this paper, we extend the IRL\nframework to the multi-agent setting, assuming to observe agents who are\nfollowing Nash Equilibrium (NE) policies. We theoretically investigate the set\nof utilities that explain the behavior of NE experts. Specifically, we provide\nan explicit characterization of the feasible reward set and analyze how errors\nin estimating the transition dynamics and expert behavior impact the recovered\nrewards. Building on these findings, we provide the first sample complexity\nanalysis for the multi-agent IRL problem. Finally, we provide a numerical\nevaluation of our theoretical results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems, the agent behavior is highly influenced by its\nutility function, as these utilities shape both individual goals as well as\ninteractions with the other agents. Inverse Reinforcement Learning (IRL) is a\nwell-established approach to inferring the utility function by observing an\nexpert behavior within a given environment. In this paper, we extend the IRL\nframework to the multi-agent setting, assuming to observe agents who are\nfollowing Nash Equilibrium (NE) policies. We theoretically investigate the set\nof utilities that explain the behavior of NE experts. Specifically, we provide\nan explicit characterization of the feasible reward set and analyze how errors\nin estimating the transition dynamics and expert behavior impact the recovered\nrewards. Building on these findings, we provide the first sample complexity\nanalysis for the multi-agent IRL problem. Finally, we provide a numerical\nevaluation of our theoretical results."
                },
                "authors": [
                    {
                        "name": "Till Freihaut"
                    },
                    {
                        "name": "Giorgia Ramponi"
                    }
                ],
                "author_detail": {
                    "name": "Giorgia Ramponi"
                },
                "author": "Giorgia Ramponi",
                "arxiv_comment": "Currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00903v2",
                "updated": "2024-11-22T16:31:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    31,
                    25,
                    4,
                    327,
                    0
                ],
                "published": "2024-10-01T17:46:21Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    46,
                    21,
                    1,
                    275,
                    0
                ],
                "title": "Causal Representation Learning with Generative Artificial Intelligence:\n  Application to Texts as Treatments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Representation Learning with Generative Artificial Intelligence:\n  Application to Texts as Treatments"
                },
                "summary": "In this paper, we demonstrate how to enhance the validity of causal inference\nwith unstructured high-dimensional treatments like texts, by leveraging the\npower of generative Artificial Intelligence. Specifically, we propose to use a\ndeep generative model such as large language models (LLMs) to efficiently\ngenerate treatments and use their internal representation for subsequent causal\neffect estimation. We show that the knowledge of this true internal\nrepresentation helps disentangle the treatment features of interest, such as\nspecific sentiments and certain topics, from other possibly unknown confounding\nfeatures. Unlike the existing methods, our proposed approach eliminates the\nneed to learn causal representation from the data and hence produces more\naccurate and efficient estimates. We formally establish the conditions required\nfor the nonparametric identification of the average treatment effect, propose\nan estimation strategy that avoids the violation of the overlap assumption, and\nderive the asymptotic properties of the proposed estimator through the\napplication of double machine learning. Finally, using an instrumental\nvariables approach, we extend the proposed methodology to the settings, in\nwhich the treatment feature is based on human perception rather than is assumed\nto be fixed given the treatment object. The proposed methodology is also\napplicable to text reuse where an LLM is used to regenerate the existing texts.\nWe conduct simulation and empirical studies, using the generated text data from\nan open-source LLM, Llama 3, to illustrate the advantages of our estimator over\nthe state-of-the-art causal representation learning algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we demonstrate how to enhance the validity of causal inference\nwith unstructured high-dimensional treatments like texts, by leveraging the\npower of generative Artificial Intelligence. Specifically, we propose to use a\ndeep generative model such as large language models (LLMs) to efficiently\ngenerate treatments and use their internal representation for subsequent causal\neffect estimation. We show that the knowledge of this true internal\nrepresentation helps disentangle the treatment features of interest, such as\nspecific sentiments and certain topics, from other possibly unknown confounding\nfeatures. Unlike the existing methods, our proposed approach eliminates the\nneed to learn causal representation from the data and hence produces more\naccurate and efficient estimates. We formally establish the conditions required\nfor the nonparametric identification of the average treatment effect, propose\nan estimation strategy that avoids the violation of the overlap assumption, and\nderive the asymptotic properties of the proposed estimator through the\napplication of double machine learning. Finally, using an instrumental\nvariables approach, we extend the proposed methodology to the settings, in\nwhich the treatment feature is based on human perception rather than is assumed\nto be fixed given the treatment object. The proposed methodology is also\napplicable to text reuse where an LLM is used to regenerate the existing texts.\nWe conduct simulation and empirical studies, using the generated text data from\nan open-source LLM, Llama 3, to illustrate the advantages of our estimator over\nthe state-of-the-art causal representation learning algorithms."
                },
                "authors": [
                    {
                        "name": "Kosuke Imai"
                    },
                    {
                        "name": "Kentaro Nakamura"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Nakamura"
                },
                "author": "Kentaro Nakamura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14711v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14711v2",
                "updated": "2024-11-22T16:31:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    31,
                    9,
                    4,
                    327,
                    0
                ],
                "published": "2024-08-27T00:34:54Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    0,
                    34,
                    54,
                    1,
                    240,
                    0
                ],
                "title": "The impact of dark matter on tidal signatures in neutron star mergers\n  with Einstein Telescope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of dark matter on tidal signatures in neutron star mergers\n  with Einstein Telescope"
                },
                "summary": "If dark matter (DM) accumulates inside neutron stars (NS), it changes their\ninternal structure and causes a shift of the tidal deformability from the value\npredicted by the dense-matter equation of state (EOS). In principle, this shift\ncould be observable in the gravitational-wave (GW) signal of binary neutron\nstar (BNS) mergers. We investigate the effect of fermionic, non-interacting DM\nwhen observing a large number of GW events from DM-admixed BNSs with the\nprecision of the proposed Einstein telescope (ET). Specifically, we study the\nimpact on the recovery of the baryonic EOS and whether DM properties can be\nconstrained. For this purpose, we create event catalogues of BNS mock events\nwith DM fraction up to 1%, from which we reconstruct the posterior\nuncertainties with the Fisher matrix approach. Using this data, we perform\njoint Bayesian inference on the baryonic EOS, DM particle mass, and DM particle\nfraction in each event. Our results reveal that when falsely ignoring DM\neffects, the EOS posterior is biased towards softer EOSs, though the offset is\nrather small. Further, we find that within our assumptions of our DM model and\npopulation, ET will likely not be able to test the presence of DM in BNSs, even\nwhen combining many events and adding Cosmic Explorer (CE) to the\nnext-generation detector network. Likewise, the potential constraints on the DM\nparticle mass will remain weak because of degeneracies with the fraction and\nEOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If dark matter (DM) accumulates inside neutron stars (NS), it changes their\ninternal structure and causes a shift of the tidal deformability from the value\npredicted by the dense-matter equation of state (EOS). In principle, this shift\ncould be observable in the gravitational-wave (GW) signal of binary neutron\nstar (BNS) mergers. We investigate the effect of fermionic, non-interacting DM\nwhen observing a large number of GW events from DM-admixed BNSs with the\nprecision of the proposed Einstein telescope (ET). Specifically, we study the\nimpact on the recovery of the baryonic EOS and whether DM properties can be\nconstrained. For this purpose, we create event catalogues of BNS mock events\nwith DM fraction up to 1%, from which we reconstruct the posterior\nuncertainties with the Fisher matrix approach. Using this data, we perform\njoint Bayesian inference on the baryonic EOS, DM particle mass, and DM particle\nfraction in each event. Our results reveal that when falsely ignoring DM\neffects, the EOS posterior is biased towards softer EOSs, though the offset is\nrather small. Further, we find that within our assumptions of our DM model and\npopulation, ET will likely not be able to test the presence of DM in BNSs, even\nwhen combining many events and adding Cosmic Explorer (CE) to the\nnext-generation detector network. Likewise, the potential constraints on the DM\nparticle mass will remain weak because of degeneracies with the fraction and\nEOS."
                },
                "authors": [
                    {
                        "name": "Hauke Koehn"
                    },
                    {
                        "name": "Edoardo Giangrandi"
                    },
                    {
                        "name": "Nina Kunert"
                    },
                    {
                        "name": "Rahul Somasundaram"
                    },
                    {
                        "name": "Violetta Sagun"
                    },
                    {
                        "name": "Tim Dietrich"
                    }
                ],
                "author_detail": {
                    "name": "Tim Dietrich"
                },
                "author": "Tim Dietrich",
                "arxiv_doi": "10.1103/PhysRevD.110.103033",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.103033",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.14711v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14711v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 6 figures, published in PRD 110, 103033",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08977v2",
                "updated": "2024-11-22T16:22:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    22,
                    35,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-13T19:08:23Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    19,
                    8,
                    23,
                    2,
                    318,
                    0
                ],
                "title": "Robustness and Confounders in the Demographic Alignment of LLMs with\n  Human Perceptions of Offensiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness and Confounders in the Demographic Alignment of LLMs with\n  Human Perceptions of Offensiveness"
                },
                "summary": "Large language models (LLMs) are known to exhibit demographic biases, yet few\nstudies systematically evaluate these biases across multiple datasets or\naccount for confounding factors. In this work, we examine LLM alignment with\nhuman annotations in five offensive language datasets, comprising approximately\n220K annotations. Our findings reveal that while demographic traits,\nparticularly race, influence alignment, these effects are inconsistent across\ndatasets and often entangled with other factors. Confounders -- such as\ndocument difficulty, annotator sensitivity, and within-group agreement --\naccount for more variation in alignment patterns than demographic traits alone.\nSpecifically, alignment increases with higher annotator sensitivity and group\nagreement, while greater document difficulty corresponds to reduced alignment.\nOur results underscore the importance of multi-dataset analyses and\nconfounder-aware methodologies in developing robust measures of demographic\nbias in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to exhibit demographic biases, yet few\nstudies systematically evaluate these biases across multiple datasets or\naccount for confounding factors. In this work, we examine LLM alignment with\nhuman annotations in five offensive language datasets, comprising approximately\n220K annotations. Our findings reveal that while demographic traits,\nparticularly race, influence alignment, these effects are inconsistent across\ndatasets and often entangled with other factors. Confounders -- such as\ndocument difficulty, annotator sensitivity, and within-group agreement --\naccount for more variation in alignment patterns than demographic traits alone.\nSpecifically, alignment increases with higher annotator sensitivity and group\nagreement, while greater document difficulty corresponds to reduced alignment.\nOur results underscore the importance of multi-dataset analyses and\nconfounder-aware methodologies in developing robust measures of demographic\nbias in LLMs."
                },
                "authors": [
                    {
                        "name": "Shayan Alipour"
                    },
                    {
                        "name": "Indira Sen"
                    },
                    {
                        "name": "Mattia Samory"
                    },
                    {
                        "name": "Tanushree Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tanushree Mitra"
                },
                "author": "Tanushree Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15033v1",
                "updated": "2024-11-22T16:05:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    5,
                    54,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T16:05:54Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    5,
                    54,
                    4,
                    327,
                    0
                ],
                "title": "One to rule them all: natural language to bind communication, perception\n  and action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One to rule them all: natural language to bind communication, perception\n  and action"
                },
                "summary": "In recent years, research in the area of human-robot interaction has focused\non developing robots capable of understanding complex human instructions and\nperforming tasks in dynamic and diverse environments. These systems have a wide\nrange of applications, from personal assistance to industrial robotics,\nemphasizing the importance of robots interacting flexibly, naturally and safely\nwith humans. This paper presents an advanced architecture for robotic action\nplanning that integrates communication, perception, and planning with Large\nLanguage Models (LLMs). Our system is designed to translate commands expressed\nin natural language into executable robot actions, incorporating environmental\ninformation and dynamically updating plans based on real-time feedback. The\nPlanner Module is the core of the system where LLMs embedded in a modified\nReAct framework are employed to interpret and carry out user commands. By\nleveraging their extensive pre-trained knowledge, LLMs can effectively process\nuser requests without the need to introduce new knowledge on the changing\nenvironment. The modified ReAct framework further enhances the execution space\nby providing real-time environmental perception and the outcomes of physical\nactions. By combining robust and dynamic semantic map representations as graphs\nwith control components and failure explanations, this architecture enhances a\nrobot adaptability, task execution, and seamless collaboration with human users\nin shared and dynamic environments. Through the integration of continuous\nfeedback loops with the environment the system can dynamically adjusts the plan\nto accommodate unexpected changes, optimizing the robot ability to perform\ntasks. Using a dataset of previous experience is possible to provide detailed\nfeedback about the failure. Updating the LLMs context of the next iteration\nwith suggestion on how to overcame the issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, research in the area of human-robot interaction has focused\non developing robots capable of understanding complex human instructions and\nperforming tasks in dynamic and diverse environments. These systems have a wide\nrange of applications, from personal assistance to industrial robotics,\nemphasizing the importance of robots interacting flexibly, naturally and safely\nwith humans. This paper presents an advanced architecture for robotic action\nplanning that integrates communication, perception, and planning with Large\nLanguage Models (LLMs). Our system is designed to translate commands expressed\nin natural language into executable robot actions, incorporating environmental\ninformation and dynamically updating plans based on real-time feedback. The\nPlanner Module is the core of the system where LLMs embedded in a modified\nReAct framework are employed to interpret and carry out user commands. By\nleveraging their extensive pre-trained knowledge, LLMs can effectively process\nuser requests without the need to introduce new knowledge on the changing\nenvironment. The modified ReAct framework further enhances the execution space\nby providing real-time environmental perception and the outcomes of physical\nactions. By combining robust and dynamic semantic map representations as graphs\nwith control components and failure explanations, this architecture enhances a\nrobot adaptability, task execution, and seamless collaboration with human users\nin shared and dynamic environments. Through the integration of continuous\nfeedback loops with the environment the system can dynamically adjusts the plan\nto accommodate unexpected changes, optimizing the robot ability to perform\ntasks. Using a dataset of previous experience is possible to provide detailed\nfeedback about the failure. Updating the LLMs context of the next iteration\nwith suggestion on how to overcame the issue."
                },
                "authors": [
                    {
                        "name": "Simone Colombani"
                    },
                    {
                        "name": "Dimitri Ognibene"
                    },
                    {
                        "name": "Giuseppe Boccignone"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Boccignone"
                },
                "author": "Giuseppe Boccignone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23054v2",
                "updated": "2024-11-22T16:04:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    4,
                    44,
                    4,
                    327,
                    0
                ],
                "published": "2024-10-30T14:21:33Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    21,
                    33,
                    2,
                    304,
                    0
                ],
                "title": "Controlling Language and Diffusion Models by Transporting Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Language and Diffusion Models by Transporting Activations"
                },
                "summary": "The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation."
                },
                "authors": [
                    {
                        "name": "Pau Rodriguez"
                    },
                    {
                        "name": "Arno Blaas"
                    },
                    {
                        "name": "Michal Klein"
                    },
                    {
                        "name": "Luca Zappella"
                    },
                    {
                        "name": "Nicholas Apostoloff"
                    },
                    {
                        "name": "Marco Cuturi"
                    },
                    {
                        "name": "Xavier Suau"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Suau"
                },
                "author": "Xavier Suau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 49Q22",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15027v1",
                "updated": "2024-11-22T15:58:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    58,
                    26,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:58:26Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    58,
                    26,
                    4,
                    327,
                    0
                ],
                "title": "Time is on my sight: scene graph filtering for dynamic environment\n  perception in an LLM-driven robot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time is on my sight: scene graph filtering for dynamic environment\n  perception in an LLM-driven robot"
                },
                "summary": "Robots are increasingly being used in dynamic environments like workplaces,\nhospitals, and homes. As a result, interactions with robots must be simple and\nintuitive, with robots perception adapting efficiently to human-induced\nchanges. This paper presents a robot control architecture that addresses key\nchallenges in human-robot interaction, with a particular focus on the dynamic\ncreation and continuous update of the robot state representation. The\narchitecture uses Large Language Models to integrate diverse information\nsources, including natural language commands, robotic skills representation,\nreal-time dynamic semantic mapping of the perceived scene. This enables\nflexible and adaptive robotic behavior in complex, dynamic environments.\nTraditional robotic systems often rely on static, pre-programmed instructions\nand settings, limiting their adaptability to dynamic environments and real-time\ncollaboration. In contrast, this architecture uses LLMs to interpret complex,\nhigh-level instructions and generate actionable plans that enhance human-robot\ncollaboration. At its core, the system Perception Module generates and\ncontinuously updates a semantic scene graph using RGB-D sensor data, providing\na detailed and structured representation of the environment. A particle filter\nis employed to ensure accurate object localization in dynamic, real-world\nsettings. The Planner Module leverages this up-to-date semantic map to break\ndown high-level tasks into sub-tasks and link them to robotic skills such as\nnavigation, object manipulation (e.g., PICK and PLACE), and movement (e.g.,\nGOTO). By combining real-time perception, state tracking, and LLM-driven\ncommunication and task planning, the architecture enhances adaptability, task\nefficiency, and human-robot collaboration in dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots are increasingly being used in dynamic environments like workplaces,\nhospitals, and homes. As a result, interactions with robots must be simple and\nintuitive, with robots perception adapting efficiently to human-induced\nchanges. This paper presents a robot control architecture that addresses key\nchallenges in human-robot interaction, with a particular focus on the dynamic\ncreation and continuous update of the robot state representation. The\narchitecture uses Large Language Models to integrate diverse information\nsources, including natural language commands, robotic skills representation,\nreal-time dynamic semantic mapping of the perceived scene. This enables\nflexible and adaptive robotic behavior in complex, dynamic environments.\nTraditional robotic systems often rely on static, pre-programmed instructions\nand settings, limiting their adaptability to dynamic environments and real-time\ncollaboration. In contrast, this architecture uses LLMs to interpret complex,\nhigh-level instructions and generate actionable plans that enhance human-robot\ncollaboration. At its core, the system Perception Module generates and\ncontinuously updates a semantic scene graph using RGB-D sensor data, providing\na detailed and structured representation of the environment. A particle filter\nis employed to ensure accurate object localization in dynamic, real-world\nsettings. The Planner Module leverages this up-to-date semantic map to break\ndown high-level tasks into sub-tasks and link them to robotic skills such as\nnavigation, object manipulation (e.g., PICK and PLACE), and movement (e.g.,\nGOTO). By combining real-time perception, state tracking, and LLM-driven\ncommunication and task planning, the architecture enhances adaptability, task\nefficiency, and human-robot collaboration in dynamic environments."
                },
                "authors": [
                    {
                        "name": "Simone Colombani"
                    },
                    {
                        "name": "Luca Brini"
                    },
                    {
                        "name": "Dimitri Ognibene"
                    },
                    {
                        "name": "Giuseppe Boccignone"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Boccignone"
                },
                "author": "Giuseppe Boccignone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v1",
                "updated": "2024-11-22T15:55:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15020v1",
                "updated": "2024-11-22T15:49:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    49,
                    27,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:49:27Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    49,
                    27,
                    4,
                    327,
                    0
                ],
                "title": "ZT-SDN: An ML-powered Zero-Trust Architecture for Software-Defined\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZT-SDN: An ML-powered Zero-Trust Architecture for Software-Defined\n  Networks"
                },
                "summary": "Zero Trust (ZT) is a security paradigm aiming to curtail an attacker's\nlateral movements within a network by implementing least-privilege and\nper-request access control policies. However, its widespread adoption is\nhindered by the difficulty of generating proper rules due to the lack of\ndetailed knowledge of communication requirements and the characteristic\nbehaviors of communicating entities under benign conditions. Consequently,\nmanual rule generation becomes cumbersome and error-prone. To address these\nproblems, we propose ZT-SDN, an automated framework for learning and enforcing\nnetwork access control in Software-Defined Networks. ZT-SDN collects data from\nthe underlying network and models the network \"transactions\" performed by\ncommunicating entities as graphs. The nodes represent entities, while the\ndirected edges represent transactions identified by different protocol stacks\nobserved. It uses novel unsupervised learning approaches to extract transaction\npatterns directly from the network data, such as the allowed protocol stacks\nand port numbers and data transmission behavior. Finally, ZT-SDN uses an\ninnovative approach to generate correct access control rules and infer strong\nassociations between them, allowing proactive rule deployment in forwarding\ndevices. We show the framework's efficacy in detecting abnormal network\naccesses and abuses of permitted flows in changing network conditions with real\nnetwork datasets. Additionally, we showcase ZT-SDN's scalability and the\nnetwork's performance when applied in an SDN environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero Trust (ZT) is a security paradigm aiming to curtail an attacker's\nlateral movements within a network by implementing least-privilege and\nper-request access control policies. However, its widespread adoption is\nhindered by the difficulty of generating proper rules due to the lack of\ndetailed knowledge of communication requirements and the characteristic\nbehaviors of communicating entities under benign conditions. Consequently,\nmanual rule generation becomes cumbersome and error-prone. To address these\nproblems, we propose ZT-SDN, an automated framework for learning and enforcing\nnetwork access control in Software-Defined Networks. ZT-SDN collects data from\nthe underlying network and models the network \"transactions\" performed by\ncommunicating entities as graphs. The nodes represent entities, while the\ndirected edges represent transactions identified by different protocol stacks\nobserved. It uses novel unsupervised learning approaches to extract transaction\npatterns directly from the network data, such as the allowed protocol stacks\nand port numbers and data transmission behavior. Finally, ZT-SDN uses an\ninnovative approach to generate correct access control rules and infer strong\nassociations between them, allowing proactive rule deployment in forwarding\ndevices. We show the framework's efficacy in detecting abnormal network\naccesses and abuses of permitted flows in changing network conditions with real\nnetwork datasets. Additionally, we showcase ZT-SDN's scalability and the\nnetwork's performance when applied in an SDN environment."
                },
                "authors": [
                    {
                        "name": "Charalampos Katsis"
                    },
                    {
                        "name": "Elisa Bertino"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Bertino"
                },
                "author": "Elisa Bertino",
                "arxiv_comment": "32 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05966v3",
                "updated": "2024-11-22T15:36:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    36,
                    32,
                    4,
                    327,
                    0
                ],
                "published": "2024-05-09T17:59:32Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    17,
                    59,
                    32,
                    3,
                    130,
                    0
                ],
                "title": "Natural Language Processing RELIES on Linguistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing RELIES on Linguistics"
                },
                "summary": "Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage."
                },
                "authors": [
                    {
                        "name": "Juri Opitz"
                    },
                    {
                        "name": "Shira Wein"
                    },
                    {
                        "name": "Nathan Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Schneider"
                },
                "author": "Nathan Schneider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08508v2",
                "updated": "2024-11-22T15:35:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    35,
                    52,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-13T10:43:39Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    10,
                    43,
                    39,
                    2,
                    318,
                    0
                ],
                "title": "BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel\n  View Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel\n  View Synthesis"
                },
                "summary": "We present billboard Splatting (BBSplat) - a novel approach for 3D scene\nrepresentation based on textured geometric primitives. BBSplat represents the\nscene as a set of optimizable textured planar primitives with learnable RGB\ntextures and alpha-maps to control their shape. BBSplat primitives can be used\nin any Gaussian Splatting pipeline as drop-in replacements for Gaussians. Our\nmethod's qualitative and quantitative improvements over 3D and 2D Gaussians are\nmost noticeable when fewer primitives are used, when BBSplat achieves over 1200\nFPS. Our novel regularization term encourages textures to have a sparser\nstructure, unlocking an efficient compression that leads to a reduction in\nstorage space of the model. Our experiments show the efficiency of BBSplat on\nstandard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU,\nand Mip-NeRF-360. We demonstrate improvements on PSNR, SSIM, and LPIPS metrics\ncompared to the state-of-the-art, especially for the case when fewer primitives\nare used, which, on the other hand, leads to up to 2 times inference speed\nimprovement for the same rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present billboard Splatting (BBSplat) - a novel approach for 3D scene\nrepresentation based on textured geometric primitives. BBSplat represents the\nscene as a set of optimizable textured planar primitives with learnable RGB\ntextures and alpha-maps to control their shape. BBSplat primitives can be used\nin any Gaussian Splatting pipeline as drop-in replacements for Gaussians. Our\nmethod's qualitative and quantitative improvements over 3D and 2D Gaussians are\nmost noticeable when fewer primitives are used, when BBSplat achieves over 1200\nFPS. Our novel regularization term encourages textures to have a sparser\nstructure, unlocking an efficient compression that leads to a reduction in\nstorage space of the model. Our experiments show the efficiency of BBSplat on\nstandard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU,\nand Mip-NeRF-360. We demonstrate improvements on PSNR, SSIM, and LPIPS metrics\ncompared to the state-of-the-art, especially for the case when fewer primitives\nare used, which, on the other hand, leads to up to 2 times inference speed\nimprovement for the same rendering quality."
                },
                "authors": [
                    {
                        "name": "David Svitov"
                    },
                    {
                        "name": "Pietro Morerio"
                    },
                    {
                        "name": "Lourdes Agapito"
                    },
                    {
                        "name": "Alessio Del Bue"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Del Bue"
                },
                "author": "Alessio Del Bue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08505v2",
                "updated": "2024-11-22T15:34:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    34,
                    3,
                    4,
                    327,
                    0
                ],
                "published": "2024-07-11T13:40:30Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    13,
                    40,
                    30,
                    3,
                    193,
                    0
                ],
                "title": "JWST/NIRSpec insights into the circumnuclear region of Arp 220: A\n  detailed kinematic study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST/NIRSpec insights into the circumnuclear region of Arp 220: A\n  detailed kinematic study"
                },
                "summary": "The study of starburst and active galactic nuclei (AGN) feedback is crucial\nfor understanding the regulation of star formation and the evolution of\ngalaxies across cosmic time. Arp 220, the closest ultraluminous infrared galaxy\n(ULIRG), is in an advanced phase of a major merger with two distinct nuclei,\nand it shows evidence of multiphase and multiscale (from < 0.1 to > 5 kpc)\noutflows. Therefore, it represents an ideal system for investigating outflow\nmechanisms and feedback phenomena in detail. Using new JWST NIRSpec IFU\nobservations, we investigated the spatially resolved gaseous (in both ionized\nand hot molecular phases) and stellar kinematics in the innermost 1 kpc. We\ndecoupled the different gas kinematic components through multi-Gaussian\nfitting, identifying two multiphase outflows, each associated with one nucleus,\nwith velocities up to $\\sim 1000$km/s. We also resolved two counter-rotating\ndiscs around each nucleus embedded in a larger-scale rotational disk. We\ncompute the total outflow mass ($\\approx 10^7$M$_\\odot$), the mass rate ($\\sim\n15$M$_{\\odot}$yr$^{-1}$), and the energetics ($\\dot E_{out}\\approx\n10^{42}$erg/s) for each nucleus, and we found that the ionized and hot\nmolecular outflowing gas contribute around 2-30% of the total mass and the\nenergy of the outflows, as inferred from the combination of multiwavelength\ninformation. We discuss the possible origin of the outflows, finding no\ncompelling evidence to prefer a starburst- or AGN-driven scenario. Regardless\nof their nature, outflows in Arp~220 propagate in multiple directions from\nparsec to kiloparsec scales, potentially impacting a significant portion of the\nhost galaxy. This contrasts with isolated systems where outflows typically\nfollow a more collimated path or are limited to the central region of the\ngalaxy and hence do not affect the interstellar medium throughout the entire\ngalaxy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of starburst and active galactic nuclei (AGN) feedback is crucial\nfor understanding the regulation of star formation and the evolution of\ngalaxies across cosmic time. Arp 220, the closest ultraluminous infrared galaxy\n(ULIRG), is in an advanced phase of a major merger with two distinct nuclei,\nand it shows evidence of multiphase and multiscale (from < 0.1 to > 5 kpc)\noutflows. Therefore, it represents an ideal system for investigating outflow\nmechanisms and feedback phenomena in detail. Using new JWST NIRSpec IFU\nobservations, we investigated the spatially resolved gaseous (in both ionized\nand hot molecular phases) and stellar kinematics in the innermost 1 kpc. We\ndecoupled the different gas kinematic components through multi-Gaussian\nfitting, identifying two multiphase outflows, each associated with one nucleus,\nwith velocities up to $\\sim 1000$km/s. We also resolved two counter-rotating\ndiscs around each nucleus embedded in a larger-scale rotational disk. We\ncompute the total outflow mass ($\\approx 10^7$M$_\\odot$), the mass rate ($\\sim\n15$M$_{\\odot}$yr$^{-1}$), and the energetics ($\\dot E_{out}\\approx\n10^{42}$erg/s) for each nucleus, and we found that the ionized and hot\nmolecular outflowing gas contribute around 2-30% of the total mass and the\nenergy of the outflows, as inferred from the combination of multiwavelength\ninformation. We discuss the possible origin of the outflows, finding no\ncompelling evidence to prefer a starburst- or AGN-driven scenario. Regardless\nof their nature, outflows in Arp~220 propagate in multiple directions from\nparsec to kiloparsec scales, potentially impacting a significant portion of the\nhost galaxy. This contrasts with isolated systems where outflows typically\nfollow a more collimated path or are limited to the central region of the\ngalaxy and hence do not affect the interstellar medium throughout the entire\ngalaxy."
                },
                "authors": [
                    {
                        "name": "L. Ulivi"
                    },
                    {
                        "name": "M. Perna"
                    },
                    {
                        "name": "I. Lamperti"
                    },
                    {
                        "name": "S. Arribas"
                    },
                    {
                        "name": "G. Cresci"
                    },
                    {
                        "name": "C. Marconcini"
                    },
                    {
                        "name": "B. Rodríguez Del Pino"
                    },
                    {
                        "name": "T. Boeker"
                    },
                    {
                        "name": "A. J. Bunker"
                    },
                    {
                        "name": "M. Ceci"
                    },
                    {
                        "name": "S. Charlot"
                    },
                    {
                        "name": "F. D Eugenio"
                    },
                    {
                        "name": "K. Fahrion"
                    },
                    {
                        "name": "R. Maiolino"
                    },
                    {
                        "name": "A. Marconi"
                    },
                    {
                        "name": "M. Pereira-Santaella"
                    }
                ],
                "author_detail": {
                    "name": "M. Pereira-Santaella"
                },
                "author": "M. Pereira-Santaella",
                "arxiv_comment": "In press to A&A, 24 figures, aa51442-24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15007v1",
                "updated": "2024-11-22T15:31:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    31,
                    20,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:31:20Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    31,
                    20,
                    4,
                    327,
                    0
                ],
                "title": "FTA generation using GenAI with an Autonomy sensor Usecase",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FTA generation using GenAI with an Autonomy sensor Usecase"
                },
                "summary": "Functional safety forms an important aspect in the design of systems. Its\nemphasis on the automotive industry has evolved significantly over the years.\nTill date many methods have been developed to get appropriate FTA(Fault Tree\nanalysis) for various scenarios and features pertaining to Autonomous Driving.\nThis paper is an attempt to explore the scope of using Generative Artificial\nIntelligence(GenAI) in order to develop Fault Tree Analysis(FTA) with the use\ncase of malfunction for the Lidar sensor in mind. We explore various available\nopen source Large Language Models(LLM) models and then dive deep into one of\nthem to study its responses and provide our analysis. This paper successfully\nshows the possibility to train existing Large Language models through Prompt\nEngineering for fault tree analysis for any Autonomy usecase aided with\nPlantUML tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional safety forms an important aspect in the design of systems. Its\nemphasis on the automotive industry has evolved significantly over the years.\nTill date many methods have been developed to get appropriate FTA(Fault Tree\nanalysis) for various scenarios and features pertaining to Autonomous Driving.\nThis paper is an attempt to explore the scope of using Generative Artificial\nIntelligence(GenAI) in order to develop Fault Tree Analysis(FTA) with the use\ncase of malfunction for the Lidar sensor in mind. We explore various available\nopen source Large Language Models(LLM) models and then dive deep into one of\nthem to study its responses and provide our analysis. This paper successfully\nshows the possibility to train existing Large Language models through Prompt\nEngineering for fault tree analysis for any Autonomy usecase aided with\nPlantUML tool."
                },
                "authors": [
                    {
                        "name": "Sneha Sudhir Shetiya"
                    },
                    {
                        "name": "Divya Garikapati"
                    },
                    {
                        "name": "Veeraja Sohoni"
                    }
                ],
                "author_detail": {
                    "name": "Veeraja Sohoni"
                },
                "author": "Veeraja Sohoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15004v1",
                "updated": "2024-11-22T15:26:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    26,
                    23,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:26:23Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    26,
                    23,
                    4,
                    327,
                    0
                ],
                "title": "ScribeAgent: Towards Specialized Web Agents Using Production-Scale\n  Workflow Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScribeAgent: Towards Specialized Web Agents Using Production-Scale\n  Workflow Data"
                },
                "summary": "Large Language Model (LLM) agents are rapidly improving to handle\nincreasingly complex web-based tasks. Most of these agents rely on\ngeneral-purpose, proprietary models like GPT-4 and focus on designing better\nprompts to improve their planning abilities. However, general-purpose LLMs are\nnot specifically trained to understand specialized web contexts such as HTML,\nand they often struggle with long-horizon planning. We explore an alternative\napproach that fine-tunes open-source LLMs using production-scale workflow data\ncollected from over 250 domains corresponding to 6 billion tokens. This simple\nyet effective approach shows substantial gains over prompting-based agents on\nexisting benchmarks -- ScribeAgent achieves state-of-the-art direct generation\nperformance on Mind2Web and improves the task success rate by 14.1% over the\nprevious best text-only web agents on WebArena. We further perform detailed\nablation studies on various fine-tuning design choices and provide insights\ninto LLM selection, training recipes, context window optimization, and effect\nof dataset sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents are rapidly improving to handle\nincreasingly complex web-based tasks. Most of these agents rely on\ngeneral-purpose, proprietary models like GPT-4 and focus on designing better\nprompts to improve their planning abilities. However, general-purpose LLMs are\nnot specifically trained to understand specialized web contexts such as HTML,\nand they often struggle with long-horizon planning. We explore an alternative\napproach that fine-tunes open-source LLMs using production-scale workflow data\ncollected from over 250 domains corresponding to 6 billion tokens. This simple\nyet effective approach shows substantial gains over prompting-based agents on\nexisting benchmarks -- ScribeAgent achieves state-of-the-art direct generation\nperformance on Mind2Web and improves the task success rate by 14.1% over the\nprevious best text-only web agents on WebArena. We further perform detailed\nablation studies on various fine-tuning design choices and provide insights\ninto LLM selection, training recipes, context window optimization, and effect\nof dataset sizes."
                },
                "authors": [
                    {
                        "name": "Junhong Shen"
                    },
                    {
                        "name": "Atishay Jain"
                    },
                    {
                        "name": "Zedian Xiao"
                    },
                    {
                        "name": "Ishan Amlekar"
                    },
                    {
                        "name": "Mouad Hadji"
                    },
                    {
                        "name": "Aaron Podolny"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    }
                ],
                "author_detail": {
                    "name": "Ameet Talwalkar"
                },
                "author": "Ameet Talwalkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13485v2",
                "updated": "2024-11-22T15:24:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    24,
                    7,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-20T17:35:21Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    35,
                    21,
                    2,
                    325,
                    0
                ],
                "title": "Utilizing Large Language Models to Synthesize Product Desirability\n  Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models to Synthesize Product Desirability\n  Datasets"
                },
                "summary": "This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production."
                },
                "authors": [
                    {
                        "name": "John D. Hastings"
                    },
                    {
                        "name": "Sherri Weitl-Harms"
                    },
                    {
                        "name": "Joseph Doty"
                    },
                    {
                        "name": "Zachary J. Myers"
                    },
                    {
                        "name": "Warren Thompson"
                    }
                ],
                "author_detail": {
                    "name": "Warren Thompson"
                },
                "author": "Warren Thompson",
                "arxiv_comment": "9 pages, 2 figures, 6 tables, updated author list",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.2.6; H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00554v2",
                "updated": "2024-11-22T15:15:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    15,
                    22,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-01T13:04:25Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    13,
                    4,
                    25,
                    4,
                    306,
                    0
                ],
                "title": "Differentiable Physics-based System Identification for Robotic\n  Manipulation of Elastoplastic Materials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Physics-based System Identification for Robotic\n  Manipulation of Elastoplastic Materials"
                },
                "summary": "Robotic manipulation of volumetric elastoplastic deformable materials, from\nfoods such as dough to construction materials like clay, is in its infancy,\nlargely due to the difficulty of modelling and perception in a high-dimensional\nspace. Simulating the dynamics of such materials is computationally expensive.\nIt tends to suffer from inaccurately estimated physics parameters of the\nmaterials and the environment, impeding high-precision manipulation. Estimating\nsuch parameters from raw point clouds captured by optical cameras suffers\nfurther from heavy occlusions. To address this challenge, this work introduces\na novel Differentiable Physics-based System Identification (DPSI) framework\nthat enables a robot arm to infer the physics parameters of elastoplastic\nmaterials and the environment using simple manipulation motions and incomplete\n3D point clouds, aligning the simulation with the real world. Extensive\nexperiments show that with only a single real-world interaction, the estimated\nparameters, Young's modulus, Poisson's ratio, yield stress and friction\ncoefficients, can accurately simulate visually and physically realistic\ndeformation behaviours induced by unseen and long-horizon manipulation motions.\nAdditionally, the DPSI framework inherently provides physically intuitive\ninterpretations for the parameters in contrast to black-box approaches such as\ndeep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic manipulation of volumetric elastoplastic deformable materials, from\nfoods such as dough to construction materials like clay, is in its infancy,\nlargely due to the difficulty of modelling and perception in a high-dimensional\nspace. Simulating the dynamics of such materials is computationally expensive.\nIt tends to suffer from inaccurately estimated physics parameters of the\nmaterials and the environment, impeding high-precision manipulation. Estimating\nsuch parameters from raw point clouds captured by optical cameras suffers\nfurther from heavy occlusions. To address this challenge, this work introduces\na novel Differentiable Physics-based System Identification (DPSI) framework\nthat enables a robot arm to infer the physics parameters of elastoplastic\nmaterials and the environment using simple manipulation motions and incomplete\n3D point clouds, aligning the simulation with the real world. Extensive\nexperiments show that with only a single real-world interaction, the estimated\nparameters, Young's modulus, Poisson's ratio, yield stress and friction\ncoefficients, can accurately simulate visually and physically realistic\ndeformation behaviours induced by unseen and long-horizon manipulation motions.\nAdditionally, the DPSI framework inherently provides physically intuitive\ninterpretations for the parameters in contrast to black-box approaches such as\ndeep neural networks."
                },
                "authors": [
                    {
                        "name": "Xintong Yang"
                    },
                    {
                        "name": "Ze Ji"
                    },
                    {
                        "name": "Yu-Kun Lai"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Kun Lai"
                },
                "author": "Yu-Kun Lai",
                "arxiv_comment": "Underreivew on the Internation Journal of Robotics Research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14991v1",
                "updated": "2024-11-22T15:01:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    1,
                    44,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:01:44Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    1,
                    44,
                    4,
                    327,
                    0
                ],
                "title": "Free Energy Projective Simulation (FEPS): Active inference with\n  interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free Energy Projective Simulation (FEPS): Active inference with\n  interpretability"
                },
                "summary": "In the last decade, the free energy principle (FEP) and active inference\n(AIF) have achieved many successes connecting conceptual models of learning and\ncognition to mathematical models of perception and action. This effort is\ndriven by a multidisciplinary interest in understanding aspects of\nself-organizing complex adaptive systems, including elements of agency. Various\nreinforcement learning (RL) models performing active inference have been\nproposed and trained on standard RL tasks using deep neural networks. Recent\nwork has focused on improving such agents' performance in complex environments\nby incorporating the latest machine learning techniques. In this paper, we take\nan alternative approach. Within the constraints imposed by the FEP and AIF, we\nattempt to model agents in an interpretable way without deep neural networks by\nintroducing Free Energy Projective Simulation (FEPS). Using internal rewards\nonly, FEPS agents build a representation of their partially observable\nenvironments with which they interact. Following AIF, the policy to achieve a\ngiven task is derived from this world model by minimizing the expected free\nenergy. Leveraging the interpretability of the model, techniques are introduced\nto deal with long-term goals and reduce prediction errors caused by erroneous\nhidden state estimation. We test the FEPS model on two RL environments inspired\nfrom behavioral biology: a timed response task and a navigation task in a\npartially observable grid. Our results show that FEPS agents fully resolve the\nambiguity of both environments by appropriately contextualizing their\nobservations based on prediction accuracy only. In addition, they infer optimal\npolicies flexibly for any target observation in the environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last decade, the free energy principle (FEP) and active inference\n(AIF) have achieved many successes connecting conceptual models of learning and\ncognition to mathematical models of perception and action. This effort is\ndriven by a multidisciplinary interest in understanding aspects of\nself-organizing complex adaptive systems, including elements of agency. Various\nreinforcement learning (RL) models performing active inference have been\nproposed and trained on standard RL tasks using deep neural networks. Recent\nwork has focused on improving such agents' performance in complex environments\nby incorporating the latest machine learning techniques. In this paper, we take\nan alternative approach. Within the constraints imposed by the FEP and AIF, we\nattempt to model agents in an interpretable way without deep neural networks by\nintroducing Free Energy Projective Simulation (FEPS). Using internal rewards\nonly, FEPS agents build a representation of their partially observable\nenvironments with which they interact. Following AIF, the policy to achieve a\ngiven task is derived from this world model by minimizing the expected free\nenergy. Leveraging the interpretability of the model, techniques are introduced\nto deal with long-term goals and reduce prediction errors caused by erroneous\nhidden state estimation. We test the FEPS model on two RL environments inspired\nfrom behavioral biology: a timed response task and a navigation task in a\npartially observable grid. Our results show that FEPS agents fully resolve the\nambiguity of both environments by appropriately contextualizing their\nobservations based on prediction accuracy only. In addition, they infer optimal\npolicies flexibly for any target observation in the environment."
                },
                "authors": [
                    {
                        "name": "Joséphine Pazem"
                    },
                    {
                        "name": "Marius Krumm"
                    },
                    {
                        "name": "Alexander Q. Vining"
                    },
                    {
                        "name": "Lukas J. Fiderer"
                    },
                    {
                        "name": "Hans J. Briegel"
                    }
                ],
                "author_detail": {
                    "name": "Hans J. Briegel"
                },
                "author": "Hans J. Briegel",
                "arxiv_comment": "26 pages (including 5 pages appendix), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08127v2",
                "updated": "2024-11-22T14:58:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    58,
                    31,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-12T19:09:45Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    19,
                    9,
                    45,
                    1,
                    317,
                    0
                ],
                "title": "TIPO: Text to Image with Text Presampling for Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIPO: Text to Image with Text Presampling for Prompt Optimization"
                },
                "summary": "TIPO (Text to Image with text pre-sampling for Prompt Optimization) is an\ninnovative framework designed to enhance text-to-image (T2I) generation by\nlanguage model (LM) for automatic prompt engineering. By refining and extending\nuser-provided prompts, TIPO bridges the gap between simple inputs and the\ndetailed prompts required for high-quality image generation. Unlike previous\napproaches that rely on Large Language Models (LLMs) or reinforcement learning\n(RL), TIPO adjusts user input prompts with the distribution of a trained prompt\ndataset, eliminating the need for complex runtime cost via lightweight model.\nThis pre-sampling approach enables efficient and scalable prompt optimization,\ngrounded in the model's training distribution. Experimental results demonstrate\nTIPO's effectiveness in improving aesthetic scores, reducing image corruption,\nand better aligning generated images with dataset distributions. These findings\nhighlight the critical role of prompt engineering in T2I systems and open\navenues for broader applications of automatic prompt refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIPO (Text to Image with text pre-sampling for Prompt Optimization) is an\ninnovative framework designed to enhance text-to-image (T2I) generation by\nlanguage model (LM) for automatic prompt engineering. By refining and extending\nuser-provided prompts, TIPO bridges the gap between simple inputs and the\ndetailed prompts required for high-quality image generation. Unlike previous\napproaches that rely on Large Language Models (LLMs) or reinforcement learning\n(RL), TIPO adjusts user input prompts with the distribution of a trained prompt\ndataset, eliminating the need for complex runtime cost via lightweight model.\nThis pre-sampling approach enables efficient and scalable prompt optimization,\ngrounded in the model's training distribution. Experimental results demonstrate\nTIPO's effectiveness in improving aesthetic scores, reducing image corruption,\nand better aligning generated images with dataset distributions. These findings\nhighlight the critical role of prompt engineering in T2I systems and open\navenues for broader applications of automatic prompt refinement."
                },
                "authors": [
                    {
                        "name": "Shih-Ying Yeh"
                    },
                    {
                        "name": "Sang-Hyun Park"
                    },
                    {
                        "name": "Giyeong Oh"
                    },
                    {
                        "name": "Min Song"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "26 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.07278v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.07278v4",
                "updated": "2024-11-22T14:47:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    47,
                    2,
                    4,
                    327,
                    0
                ],
                "published": "2022-11-14T11:19:38Z",
                "published_parsed": [
                    2022,
                    11,
                    14,
                    11,
                    19,
                    38,
                    0,
                    318,
                    0
                ],
                "title": "Inferring Interference: Identifying a Perturbing Tertiary with Eccentric\n  Gravitational Wave Burst Timing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Interference: Identifying a Perturbing Tertiary with Eccentric\n  Gravitational Wave Burst Timing"
                },
                "summary": "[Abridged] Binary black holes may form and merge dynamically. These binaries\nare likely to become bound with high eccentricities, resulting in a burst of\ngravitational radiation at their point of closest approach. When such a binary\nis perturbed by a third body, the evolution of the orbit is affected, and\ngravitational-wave burst times are altered. The bursts times therefore encode\ninformation about the tertiary. In order to extract this information, we\nrequire a prescription for the relationship between the tertiary properties and\nthe gravitational-wave burst times. In this paper, we demonstrate a toy model\nfor the burst times of a secular three-body system. We show how Bayesian\ninference can be employed to deduce the tertiary properties when the bursts are\ndetected by next-generation ground-based gravitational-wave detectors. We study\nthe bursts from an eccentric binary with a total mass of $60$~M$_\\odot$\norbiting an $6 \\times 10^{8}$~M$_\\odot$ supermassive black hole. When we assume\nno knowledge of the eccentric binary, we are unable to tightly constrain the\nexistence or properties of the tertiary, and we recover biased posterior\nprobability distributions for the parameters of the eccentric binary. However,\nwhen the properties of the binary are already well-known -- as is likely if the\nlate inspiral and merger are also detected -- we are able to more accurately\ninfer the mass of the perturber, $m_3$, and its distance from the binary, $R$.\nWhen we assume measurement precision on the binary parameters consistent with\nexpectations for next-generation gravitational-wave detectors, we can be\ngreater than $90\\%$ confident that the binary is perturbed. [...]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "[Abridged] Binary black holes may form and merge dynamically. These binaries\nare likely to become bound with high eccentricities, resulting in a burst of\ngravitational radiation at their point of closest approach. When such a binary\nis perturbed by a third body, the evolution of the orbit is affected, and\ngravitational-wave burst times are altered. The bursts times therefore encode\ninformation about the tertiary. In order to extract this information, we\nrequire a prescription for the relationship between the tertiary properties and\nthe gravitational-wave burst times. In this paper, we demonstrate a toy model\nfor the burst times of a secular three-body system. We show how Bayesian\ninference can be employed to deduce the tertiary properties when the bursts are\ndetected by next-generation ground-based gravitational-wave detectors. We study\nthe bursts from an eccentric binary with a total mass of $60$~M$_\\odot$\norbiting an $6 \\times 10^{8}$~M$_\\odot$ supermassive black hole. When we assume\nno knowledge of the eccentric binary, we are unable to tightly constrain the\nexistence or properties of the tertiary, and we recover biased posterior\nprobability distributions for the parameters of the eccentric binary. However,\nwhen the properties of the binary are already well-known -- as is likely if the\nlate inspiral and merger are also detected -- we are able to more accurately\ninfer the mass of the perturber, $m_3$, and its distance from the binary, $R$.\nWhen we assume measurement precision on the binary parameters consistent with\nexpectations for next-generation gravitational-wave detectors, we can be\ngreater than $90\\%$ confident that the binary is perturbed. [...]"
                },
                "authors": [
                    {
                        "name": "Isobel M. Romero-Shaw"
                    },
                    {
                        "name": "Nicholas Loutrel"
                    },
                    {
                        "name": "Michael Zevin"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zevin"
                },
                "author": "Michael Zevin",
                "arxiv_doi": "10.1103/PhysRevD.107.122001",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.107.122001",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2211.07278v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.07278v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Version accepted for publication in PRD",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14986v1",
                "updated": "2024-11-22T14:47:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    47,
                    0,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T14:47:00Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    47,
                    0,
                    4,
                    327,
                    0
                ],
                "title": "Generative AI may backfire for counterspeech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI may backfire for counterspeech"
                },
                "summary": "Online hate speech poses a serious threat to individual well-being and\nsocietal cohesion. A promising solution to curb online hate speech is\ncounterspeech. Counterspeech is aimed at encouraging users to reconsider\nhateful posts by direct replies. However, current methods lack scalability due\nto the need for human intervention or fail to adapt to the specific context of\nthe post. A potential remedy is the use of generative AI, specifically large\nlanguage models (LLMs), to write tailored counterspeech messages. In this\npaper, we analyze whether contextualized counterspeech generated by\nstate-of-the-art LLMs is effective in curbing online hate speech. To do so, we\nconducted a large-scale, pre-registered field experiment (N=2,664) on the\nsocial media platform Twitter/X. Our experiment followed a 2x2 between-subjects\ndesign and, additionally, a control condition with no counterspeech. On the one\nhand, users posting hateful content on Twitter/X were randomly assigned to\nreceive either (a) contextualized counterspeech or (b) non-contextualized\ncounterspeech. Here, the former is generated through LLMs, while the latter\nrelies on predefined, generic messages. On the other hand, we tested two\ncounterspeech strategies: (a) promoting empathy and (b) warning about the\nconsequences of online misbehavior. We then measured whether users deleted\ntheir initial hateful posts and whether their behavior changed after the\ncounterspeech intervention (e.g., whether users adopted a less toxic language).\nWe find that non-contextualized counterspeech employing a\nwarning-of-consequence strategy significantly reduces online hate speech.\nHowever, contextualized counterspeech generated by LLMs proves ineffective and\nmay even backfire.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online hate speech poses a serious threat to individual well-being and\nsocietal cohesion. A promising solution to curb online hate speech is\ncounterspeech. Counterspeech is aimed at encouraging users to reconsider\nhateful posts by direct replies. However, current methods lack scalability due\nto the need for human intervention or fail to adapt to the specific context of\nthe post. A potential remedy is the use of generative AI, specifically large\nlanguage models (LLMs), to write tailored counterspeech messages. In this\npaper, we analyze whether contextualized counterspeech generated by\nstate-of-the-art LLMs is effective in curbing online hate speech. To do so, we\nconducted a large-scale, pre-registered field experiment (N=2,664) on the\nsocial media platform Twitter/X. Our experiment followed a 2x2 between-subjects\ndesign and, additionally, a control condition with no counterspeech. On the one\nhand, users posting hateful content on Twitter/X were randomly assigned to\nreceive either (a) contextualized counterspeech or (b) non-contextualized\ncounterspeech. Here, the former is generated through LLMs, while the latter\nrelies on predefined, generic messages. On the other hand, we tested two\ncounterspeech strategies: (a) promoting empathy and (b) warning about the\nconsequences of online misbehavior. We then measured whether users deleted\ntheir initial hateful posts and whether their behavior changed after the\ncounterspeech intervention (e.g., whether users adopted a less toxic language).\nWe find that non-contextualized counterspeech employing a\nwarning-of-consequence strategy significantly reduces online hate speech.\nHowever, contextualized counterspeech generated by LLMs proves ineffective and\nmay even backfire."
                },
                "authors": [
                    {
                        "name": "Dominik Bär"
                    },
                    {
                        "name": "Abdurahman Maarouf"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Feuerriegel"
                },
                "author": "Stefan Feuerriegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14983v1",
                "updated": "2024-11-22T14:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    43,
                    28,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T14:43:28Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    43,
                    28,
                    4,
                    327,
                    0
                ],
                "title": "Large sample scaling analysis of the Zig-Zag algorithm for Bayesian\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large sample scaling analysis of the Zig-Zag algorithm for Bayesian\n  inference"
                },
                "summary": "Piecewise deterministic Markov processes provide scalable methods for\nsampling from the posterior distributions in big data settings by admitting\nprincipled sub-sampling strategies that do not bias the output. An important\nexample is the Zig-Zag process of [Ann. Stats. 47 (2019) 1288 - 1320] where\nclever sub-sampling has been shown to produce an essentially independent sample\nat a cost that does not scale with the size of the data. However, sub-sampling\nalso leads to slower convergence and poor mixing of the process, a behaviour\nwhich questions the promised scalability of the algorithm. We provide a large\nsample scaling analysis of the Zig-Zag process and its sub-sampling versions in\nsettings of parametric Bayesian inference. In the transient phase of the\nalgorithm, we show that the Zig-Zag trajectories are well approximated by the\nsolution to a system of ODEs. These ODEs possess a drift in the direction of\ndecreasing KL-divergence between the assumed model and the true distribution\nand are explicitly characterized in the paper. In the stationary phase, we give\nweak convergence results for different versions of the Zig-Zag process. Based\non our results, we estimate that for large data sets of size n, using suitable\ncontrol variates with sub-sampling in Zig-Zag, the algorithm costs O(1) to\nobtain an essentially independent sample; a computational speed-up of O(n) over\nthe canonical version of Zig-Zag and other traditional MCMC methods",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piecewise deterministic Markov processes provide scalable methods for\nsampling from the posterior distributions in big data settings by admitting\nprincipled sub-sampling strategies that do not bias the output. An important\nexample is the Zig-Zag process of [Ann. Stats. 47 (2019) 1288 - 1320] where\nclever sub-sampling has been shown to produce an essentially independent sample\nat a cost that does not scale with the size of the data. However, sub-sampling\nalso leads to slower convergence and poor mixing of the process, a behaviour\nwhich questions the promised scalability of the algorithm. We provide a large\nsample scaling analysis of the Zig-Zag process and its sub-sampling versions in\nsettings of parametric Bayesian inference. In the transient phase of the\nalgorithm, we show that the Zig-Zag trajectories are well approximated by the\nsolution to a system of ODEs. These ODEs possess a drift in the direction of\ndecreasing KL-divergence between the assumed model and the true distribution\nand are explicitly characterized in the paper. In the stationary phase, we give\nweak convergence results for different versions of the Zig-Zag process. Based\non our results, we estimate that for large data sets of size n, using suitable\ncontrol variates with sub-sampling in Zig-Zag, the algorithm costs O(1) to\nobtain an essentially independent sample; a computational speed-up of O(n) over\nthe canonical version of Zig-Zag and other traditional MCMC methods"
                },
                "authors": [
                    {
                        "name": "Sanket Agrawal"
                    },
                    {
                        "name": "Joris Bierkens"
                    },
                    {
                        "name": "Gareth O. Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Gareth O. Roberts"
                },
                "author": "Gareth O. Roberts",
                "arxiv_comment": "47 pages, 7 figues, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-08, 60F05, 62F15, 65C05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02079v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02079v3",
                "updated": "2024-11-22T14:30:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    30,
                    28,
                    4,
                    327,
                    0
                ],
                "published": "2024-07-02T09:16:43Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    16,
                    43,
                    1,
                    184,
                    0
                ],
                "title": "Theseus: Exploring Efficient Wafer-Scale Chip Design for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theseus: Exploring Efficient Wafer-Scale Chip Design for Large Language\n  Models"
                },
                "summary": "The emergence of the large language model~(LLM) poses an exponential growth\nof demand for computation throughput, memory capacity, and communication\nbandwidth. Such a demand growth has significantly surpassed the improvement of\ncorresponding chip designs. With the advancement of fabrication and integration\ntechnologies, designers have been developing Wafer-Scale Chips~(WSCs) to scale\nup and exploit the limits of computation density, memory capacity, and\ncommunication bandwidth at the level of a single chip. Existing solutions have\ndemonstrated the significant advantages of WSCs over traditional designs,\nshowing potential to effectively support LLM workloads.\n  Despite the benefits, exploring the early-stage design space of WSCs for LLMs\nis a crucial yet challenging task due to the enormous and complicated design\nspace, time-consuming evaluation methods, and inefficient exploration\nstrategies. To address these challenges, we propose Theseus, an efficient WSC\ndesign space exploration framework for LLMs. We construct the design space of\nWSCs with various constraints considering the unique characteristics of WSCs.\nWe propose efficient evaluation methodologies for large-scale NoC-based WSCs\nand introduce multi-fidelity Bayesian optimization to efficiently explore the\ndesign space. Evaluation results demonstrate the efficiency of Theseus that the\nsearched Pareto optimal results outperform GPU cluster and existing WSC designs\nby up to 62.8\\%/73.7\\% in performance and 38.6\\%/42.4\\% in power consumption\nfor LLM training, while improving up to 23.2$\\times$ and 15.7$\\times$ for the\nperformance and power of inference tasks. Furthermore, we conduct case studies\nto address the design tradeoffs in WSCs and provide insights to facilitate WSC\ndesigns for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of the large language model~(LLM) poses an exponential growth\nof demand for computation throughput, memory capacity, and communication\nbandwidth. Such a demand growth has significantly surpassed the improvement of\ncorresponding chip designs. With the advancement of fabrication and integration\ntechnologies, designers have been developing Wafer-Scale Chips~(WSCs) to scale\nup and exploit the limits of computation density, memory capacity, and\ncommunication bandwidth at the level of a single chip. Existing solutions have\ndemonstrated the significant advantages of WSCs over traditional designs,\nshowing potential to effectively support LLM workloads.\n  Despite the benefits, exploring the early-stage design space of WSCs for LLMs\nis a crucial yet challenging task due to the enormous and complicated design\nspace, time-consuming evaluation methods, and inefficient exploration\nstrategies. To address these challenges, we propose Theseus, an efficient WSC\ndesign space exploration framework for LLMs. We construct the design space of\nWSCs with various constraints considering the unique characteristics of WSCs.\nWe propose efficient evaluation methodologies for large-scale NoC-based WSCs\nand introduce multi-fidelity Bayesian optimization to efficiently explore the\ndesign space. Evaluation results demonstrate the efficiency of Theseus that the\nsearched Pareto optimal results outperform GPU cluster and existing WSC designs\nby up to 62.8\\%/73.7\\% in performance and 38.6\\%/42.4\\% in power consumption\nfor LLM training, while improving up to 23.2$\\times$ and 15.7$\\times$ for the\nperformance and power of inference tasks. Furthermore, we conduct case studies\nto address the design tradeoffs in WSCs and provide insights to facilitate WSC\ndesigns for LLMs."
                },
                "authors": [
                    {
                        "name": "Jingchen Zhu"
                    },
                    {
                        "name": "Chenhao Xue"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yu Shen"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Zekang Cheng"
                    },
                    {
                        "name": "Yu Jiang"
                    },
                    {
                        "name": "Tianqi Wang"
                    },
                    {
                        "name": "Yibo Lin"
                    },
                    {
                        "name": "Wei Hu"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02079v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02079v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14971v1",
                "updated": "2024-11-22T14:27:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    27,
                    27,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T14:27:27Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    27,
                    27,
                    4,
                    327,
                    0
                ],
                "title": "Leveraging LLMs for Legacy Code Modernization: Challenges and\n  Opportunities for LLM-Generated Documentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Legacy Code Modernization: Challenges and\n  Opportunities for LLM-Generated Documentation"
                },
                "summary": "Legacy software systems, written in outdated languages like MUMPS and\nmainframe assembly, pose challenges in efficiency, maintenance, staffing, and\nsecurity. While LLMs offer promise for modernizing these systems, their ability\nto understand legacy languages is largely unknown. This paper investigates the\nutilization of LLMs to generate documentation for legacy code using two\ndatasets: an electronic health records (EHR) system in MUMPS and open-source\napplications in IBM mainframe Assembly Language Code (ALC). We propose a\nprompting strategy for generating line-wise code comments and a rubric to\nevaluate their completeness, readability, usefulness, and hallucination. Our\nstudy assesses the correlation between human evaluations and automated metrics,\nsuch as code complexity and reference-based metrics. We find that LLM-generated\ncomments for MUMPS and ALC are generally hallucination-free, complete,\nreadable, and useful compared to ground-truth comments, though ALC poses\nchallenges. However, no automated metrics strongly correlate with comment\nquality to predict or measure LLM performance. Our findings highlight the\nlimitations of current automated measures and the need for better evaluation\nmetrics for LLM-generated documentation in legacy systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legacy software systems, written in outdated languages like MUMPS and\nmainframe assembly, pose challenges in efficiency, maintenance, staffing, and\nsecurity. While LLMs offer promise for modernizing these systems, their ability\nto understand legacy languages is largely unknown. This paper investigates the\nutilization of LLMs to generate documentation for legacy code using two\ndatasets: an electronic health records (EHR) system in MUMPS and open-source\napplications in IBM mainframe Assembly Language Code (ALC). We propose a\nprompting strategy for generating line-wise code comments and a rubric to\nevaluate their completeness, readability, usefulness, and hallucination. Our\nstudy assesses the correlation between human evaluations and automated metrics,\nsuch as code complexity and reference-based metrics. We find that LLM-generated\ncomments for MUMPS and ALC are generally hallucination-free, complete,\nreadable, and useful compared to ground-truth comments, though ALC poses\nchallenges. However, no automated metrics strongly correlate with comment\nquality to predict or measure LLM performance. Our findings highlight the\nlimitations of current automated measures and the need for better evaluation\nmetrics for LLM-generated documentation in legacy systems."
                },
                "authors": [
                    {
                        "name": "Colin Diggs"
                    },
                    {
                        "name": "Michael Doyle"
                    },
                    {
                        "name": "Amit Madan"
                    },
                    {
                        "name": "Siggy Scott"
                    },
                    {
                        "name": "Emily Escamilla"
                    },
                    {
                        "name": "Jacob Zimmer"
                    },
                    {
                        "name": "Naveed Nekoo"
                    },
                    {
                        "name": "Paul Ursino"
                    },
                    {
                        "name": "Michael Bartholf"
                    },
                    {
                        "name": "Zachary Robin"
                    },
                    {
                        "name": "Anand Patel"
                    },
                    {
                        "name": "Chris Glasz"
                    },
                    {
                        "name": "William Macke"
                    },
                    {
                        "name": "Paul Kirk"
                    },
                    {
                        "name": "Jasper Phillips"
                    },
                    {
                        "name": "Arun Sridharan"
                    },
                    {
                        "name": "Doug Wendt"
                    },
                    {
                        "name": "Scott Rosen"
                    },
                    {
                        "name": "Nitin Naik"
                    },
                    {
                        "name": "Justin F. Brunelle"
                    },
                    {
                        "name": "Samruddhi Thaker"
                    }
                ],
                "author_detail": {
                    "name": "Samruddhi Thaker"
                },
                "author": "Samruddhi Thaker",
                "arxiv_comment": "Abbreviated version submitted to LLM4Code 2025 (a workshop co-located\n  with ICSE 2025), 13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14967v1",
                "updated": "2024-11-22T14:23:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    23,
                    7,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T14:23:07Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    23,
                    7,
                    4,
                    327,
                    0
                ],
                "title": "SwissADT: An Audio Description Translation System for Swiss Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwissADT: An Audio Description Translation System for Swiss Languages"
                },
                "summary": "Audio description (AD) is a crucial accessibility service provided to blind\npersons and persons with visual impairment, designed to convey visual\ninformation in acoustic form. Despite recent advancements in multilingual\nmachine translation research, the lack of well-crafted and time-synchronized AD\ndata impedes the development of audio description translation (ADT) systems\nthat address the needs of multilingual countries such as Switzerland.\nFurthermore, since the majority of ADT systems rely solely on text, uncertainty\nexists as to whether incorporating visual information from the corresponding\nvideo clips can enhance the quality of ADT outputs. In this work, we present\nSwissADT, the first ADT system implemented for three main Swiss languages and\nEnglish. By collecting well-crafted AD data augmented with video clips in\nGerman, French, Italian, and English, and leveraging the power of Large\nLanguage Models (LLMs), we aim to enhance information accessibility for diverse\nlanguage populations in Switzerland by automatically translating AD scripts to\nthe desired Swiss language. Our extensive experimental ADT results, composed of\nboth automatic and human evaluations of ADT quality, demonstrate the promising\ncapability of SwissADT for the ADT task. We believe that combining human\nexpertise with the generation power of LLMs can further enhance the performance\nof ADT systems, ultimately benefiting a larger multilingual target population.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio description (AD) is a crucial accessibility service provided to blind\npersons and persons with visual impairment, designed to convey visual\ninformation in acoustic form. Despite recent advancements in multilingual\nmachine translation research, the lack of well-crafted and time-synchronized AD\ndata impedes the development of audio description translation (ADT) systems\nthat address the needs of multilingual countries such as Switzerland.\nFurthermore, since the majority of ADT systems rely solely on text, uncertainty\nexists as to whether incorporating visual information from the corresponding\nvideo clips can enhance the quality of ADT outputs. In this work, we present\nSwissADT, the first ADT system implemented for three main Swiss languages and\nEnglish. By collecting well-crafted AD data augmented with video clips in\nGerman, French, Italian, and English, and leveraging the power of Large\nLanguage Models (LLMs), we aim to enhance information accessibility for diverse\nlanguage populations in Switzerland by automatically translating AD scripts to\nthe desired Swiss language. Our extensive experimental ADT results, composed of\nboth automatic and human evaluations of ADT quality, demonstrate the promising\ncapability of SwissADT for the ADT task. We believe that combining human\nexpertise with the generation power of LLMs can further enhance the performance\nof ADT systems, ultimately benefiting a larger multilingual target population."
                },
                "authors": [
                    {
                        "name": "Lukas Fischer"
                    },
                    {
                        "name": "Yingqiang Gao"
                    },
                    {
                        "name": "Alexa Lintner"
                    },
                    {
                        "name": "Sarah Ebling"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Ebling"
                },
                "author": "Sarah Ebling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14962v1",
                "updated": "2024-11-22T14:21:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    21,
                    18,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T14:21:18Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    21,
                    18,
                    4,
                    327,
                    0
                ],
                "title": "LLM for Barcodes: Generating Diverse Synthetic Data for Identity\n  Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM for Barcodes: Generating Diverse Synthetic Data for Identity\n  Documents"
                },
                "summary": "Accurate barcode detection and decoding in Identity documents is crucial for\napplications like security, healthcare, and education, where reliable data\nextraction and verification are essential. However, building robust detection\nmodels is challenging due to the lack of diverse, realistic datasets an issue\noften tied to privacy concerns and the wide variety of document formats.\nTraditional tools like Faker rely on predefined templates, making them less\neffective for capturing the complexity of real-world identity documents. In\nthis paper, we introduce a new approach to synthetic data generation that uses\nLLMs to create contextually rich and realistic data without relying on\npredefined field. Using the vast knowledge LLMs have about different documents\nand content, our method creates data that reflects the variety found in real\nidentity documents. This data is then encoded into barcode and overlayed on\ntemplates for documents such as Driver's licenses, Insurance cards, Student\nIDs. Our approach simplifies the process of dataset creation, eliminating the\nneed for extensive domain knowledge or predefined fields. Compared to\ntraditional methods like Faker, data generated by LLM demonstrates greater\ndiversity and contextual relevance, leading to improved performance in barcode\ndetection models. This scalable, privacy-first solution is a big step forward\nin advancing machine learning for automated document processing and identity\nverification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate barcode detection and decoding in Identity documents is crucial for\napplications like security, healthcare, and education, where reliable data\nextraction and verification are essential. However, building robust detection\nmodels is challenging due to the lack of diverse, realistic datasets an issue\noften tied to privacy concerns and the wide variety of document formats.\nTraditional tools like Faker rely on predefined templates, making them less\neffective for capturing the complexity of real-world identity documents. In\nthis paper, we introduce a new approach to synthetic data generation that uses\nLLMs to create contextually rich and realistic data without relying on\npredefined field. Using the vast knowledge LLMs have about different documents\nand content, our method creates data that reflects the variety found in real\nidentity documents. This data is then encoded into barcode and overlayed on\ntemplates for documents such as Driver's licenses, Insurance cards, Student\nIDs. Our approach simplifies the process of dataset creation, eliminating the\nneed for extensive domain knowledge or predefined fields. Compared to\ntraditional methods like Faker, data generated by LLM demonstrates greater\ndiversity and contextual relevance, leading to improved performance in barcode\ndetection models. This scalable, privacy-first solution is a big step forward\nin advancing machine learning for automated document processing and identity\nverification."
                },
                "authors": [
                    {
                        "name": "Hitesh Laxmichand Patel"
                    },
                    {
                        "name": "Amit Agarwal"
                    },
                    {
                        "name": "Bhargava Kumar"
                    },
                    {
                        "name": "Karan Gupta"
                    },
                    {
                        "name": "Priyaranjan Pattnayak"
                    }
                ],
                "author_detail": {
                    "name": "Priyaranjan Pattnayak"
                },
                "author": "Priyaranjan Pattnayak",
                "arxiv_comment": "5 pages, 1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14959v1",
                "updated": "2024-11-22T14:17:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    17,
                    46,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T14:17:46Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    17,
                    46,
                    4,
                    327,
                    0
                ],
                "title": "Design-o-meter: Towards Evaluating and Refining Graphic Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design-o-meter: Towards Evaluating and Refining Graphic Designs"
                },
                "summary": "Graphic designs are an effective medium for visual communication. They range\nfrom greeting cards to corporate flyers and beyond. Off-late, machine learning\ntechniques are able to generate such designs, which accelerates the rate of\ncontent production. An automated way of evaluating their quality becomes\ncritical. Towards this end, we introduce Design-o-meter, a data-driven\nmethodology to quantify the goodness of graphic designs. Further, our approach\ncan suggest modifications to these designs to improve its visual appeal. To the\nbest of our knowledge, Design-o-meter is the first approach that scores and\nrefines designs in a unified framework despite the inherent subjectivity and\nambiguity of the setting. Our exhaustive quantitative and qualitative analysis\nof our approach against baselines adapted for the task (including recent\nMultimodal LLM-based approaches) brings out the efficacy of our methodology. We\nhope our work will usher more interest in this important and pragmatic problem\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphic designs are an effective medium for visual communication. They range\nfrom greeting cards to corporate flyers and beyond. Off-late, machine learning\ntechniques are able to generate such designs, which accelerates the rate of\ncontent production. An automated way of evaluating their quality becomes\ncritical. Towards this end, we introduce Design-o-meter, a data-driven\nmethodology to quantify the goodness of graphic designs. Further, our approach\ncan suggest modifications to these designs to improve its visual appeal. To the\nbest of our knowledge, Design-o-meter is the first approach that scores and\nrefines designs in a unified framework despite the inherent subjectivity and\nambiguity of the setting. Our exhaustive quantitative and qualitative analysis\nof our approach against baselines adapted for the task (including recent\nMultimodal LLM-based approaches) brings out the efficacy of our methodology. We\nhope our work will usher more interest in this important and pragmatic problem\nsetting."
                },
                "authors": [
                    {
                        "name": "Sahil Goyal"
                    },
                    {
                        "name": "Abhinav Mahajan"
                    },
                    {
                        "name": "Swasti Mishra"
                    },
                    {
                        "name": "Prateksha Udhayanan"
                    },
                    {
                        "name": "Tripti Shukla"
                    },
                    {
                        "name": "K J Joseph"
                    },
                    {
                        "name": "Balaji Vasan Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Balaji Vasan Srinivasan"
                },
                "author": "Balaji Vasan Srinivasan",
                "arxiv_comment": "Accepted to WACV 2025. Project page:\n  https://sahilg06.github.io/Design-o-meter/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14953v1",
                "updated": "2024-11-22T14:12:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    12,
                    35,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T14:12:35Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    12,
                    35,
                    4,
                    327,
                    0
                ],
                "title": "Evaluating Vision Transformer Models for Visual Quality Control in\n  Industrial Manufacturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Vision Transformer Models for Visual Quality Control in\n  Industrial Manufacturing"
                },
                "summary": "One of the most promising use-cases for machine learning in industrial\nmanufacturing is the early detection of defective products using a quality\ncontrol system. Such a system can save costs and reduces human errors due to\nthe monotonous nature of visual inspections. Today, a rich body of research\nexists which employs machine learning methods to identify rare defective\nproducts in unbalanced visual quality control datasets. These methods typically\nrely on two components: A visual backbone to capture the features of the input\nimage and an anomaly detection algorithm that decides if these features are\nwithin an expected distribution. With the rise of transformer architecture as\nvisual backbones of choice, there exists now a great variety of different\ncombinations of these two components, ranging all along the trade-off between\ndetection quality and inference time. Facing this variety, practitioners in the\nfield often have to spend a considerable amount of time on researching the\nright combination for their use-case at hand. Our contribution is to help\npractitioners with this choice by reviewing and evaluating current vision\ntransformer models together with anomaly detection methods. For this, we chose\nSotA models of both disciplines, combined them and evaluated them towards the\ngoal of having small, fast and efficient anomaly detection models suitable for\nindustrial manufacturing. We evaluated the results of our experiments on the\nwell-known MVTecAD and BTAD datasets. Moreover, we give guidelines for choosing\na suitable model architecture for a quality control system in practice,\nconsidering given use-case and hardware constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the most promising use-cases for machine learning in industrial\nmanufacturing is the early detection of defective products using a quality\ncontrol system. Such a system can save costs and reduces human errors due to\nthe monotonous nature of visual inspections. Today, a rich body of research\nexists which employs machine learning methods to identify rare defective\nproducts in unbalanced visual quality control datasets. These methods typically\nrely on two components: A visual backbone to capture the features of the input\nimage and an anomaly detection algorithm that decides if these features are\nwithin an expected distribution. With the rise of transformer architecture as\nvisual backbones of choice, there exists now a great variety of different\ncombinations of these two components, ranging all along the trade-off between\ndetection quality and inference time. Facing this variety, practitioners in the\nfield often have to spend a considerable amount of time on researching the\nright combination for their use-case at hand. Our contribution is to help\npractitioners with this choice by reviewing and evaluating current vision\ntransformer models together with anomaly detection methods. For this, we chose\nSotA models of both disciplines, combined them and evaluated them towards the\ngoal of having small, fast and efficient anomaly detection models suitable for\nindustrial manufacturing. We evaluated the results of our experiments on the\nwell-known MVTecAD and BTAD datasets. Moreover, we give guidelines for choosing\na suitable model architecture for a quality control system in practice,\nconsidering given use-case and hardware constraints."
                },
                "authors": [
                    {
                        "name": "Miriam Alber"
                    },
                    {
                        "name": "Christoph Hönes"
                    },
                    {
                        "name": "Patrick Baier"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Baier"
                },
                "author": "Patrick Baier",
                "arxiv_doi": "10.1007/978-3-031-70381-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-70381-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.14953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Machine Learning and Knowledge Discovery in Databases.Applied Data\n  Science Track, vol 14950, Springer (2024) 116-132",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02451v2",
                "updated": "2024-11-22T14:11:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    11,
                    13,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-03T10:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    10,
                    6,
                    14,
                    6,
                    308,
                    0
                ],
                "title": "High-performance automated abstract screening with large language model\n  ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-performance automated abstract screening with large language model\n  ensembles"
                },
                "summary": "Large language models (LLMs) excel in tasks requiring processing and\ninterpretation of input text. Abstract screening is a labour-intensive\ncomponent of systematic review involving repetitive application of inclusion\nand exclusion criteria on a large volume of studies identified by a literature\nsearch. Here, LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Llama 3 70B, Gemini 1.5\nPro, and Claude Sonnet 3.5) were trialled on systematic reviews in a full issue\nof the Cochrane Library to evaluate their accuracy in zero-shot binary\nclassification for abstract screening. Trials over a subset of 800 records\nidentified optimal prompting strategies and demonstrated superior performance\nof LLMs to human researchers in terms of sensitivity (LLM-max = 1.000,\nhuman-max = 0.775), precision (LLM-max = 0.927, human-max = 0.911), and\nbalanced accuracy (LLM-max = 0.904, human-max = 0.865). The best performing\nLLM-prompt combinations were trialled across every replicated search result (n\n= 119,691), and exhibited consistent sensitivity (range 0.756-1.000) but\ndiminished precision (range 0.004-0.096). 66 LLM-human and LLM-LLM ensembles\nexhibited perfect sensitivity with a maximal precision of 0.458, with less\nobserved performance drop in larger trials. Significant variation in\nperformance was observed between reviews, highlighting the importance of\ndomain-specific validation before deployment. LLMs may reduce the human labour\ncost of systematic review with maintained or improved accuracy and sensitivity.\nSystematic review is the foundation of evidence synthesis across academic\ndisciplines, including evidence-based medicine, and LLMs may increase the\nefficiency and quality of this mode of research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in tasks requiring processing and\ninterpretation of input text. Abstract screening is a labour-intensive\ncomponent of systematic review involving repetitive application of inclusion\nand exclusion criteria on a large volume of studies identified by a literature\nsearch. Here, LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Llama 3 70B, Gemini 1.5\nPro, and Claude Sonnet 3.5) were trialled on systematic reviews in a full issue\nof the Cochrane Library to evaluate their accuracy in zero-shot binary\nclassification for abstract screening. Trials over a subset of 800 records\nidentified optimal prompting strategies and demonstrated superior performance\nof LLMs to human researchers in terms of sensitivity (LLM-max = 1.000,\nhuman-max = 0.775), precision (LLM-max = 0.927, human-max = 0.911), and\nbalanced accuracy (LLM-max = 0.904, human-max = 0.865). The best performing\nLLM-prompt combinations were trialled across every replicated search result (n\n= 119,691), and exhibited consistent sensitivity (range 0.756-1.000) but\ndiminished precision (range 0.004-0.096). 66 LLM-human and LLM-LLM ensembles\nexhibited perfect sensitivity with a maximal precision of 0.458, with less\nobserved performance drop in larger trials. Significant variation in\nperformance was observed between reviews, highlighting the importance of\ndomain-specific validation before deployment. LLMs may reduce the human labour\ncost of systematic review with maintained or improved accuracy and sensitivity.\nSystematic review is the foundation of evidence synthesis across academic\ndisciplines, including evidence-based medicine, and LLMs may increase the\nefficiency and quality of this mode of research."
                },
                "authors": [
                    {
                        "name": "Rohan Sanghera"
                    },
                    {
                        "name": "Arun James Thirunavukarasu"
                    },
                    {
                        "name": "Marc El Khoury"
                    },
                    {
                        "name": "Jessica O'Logbon"
                    },
                    {
                        "name": "Yuqing Chen"
                    },
                    {
                        "name": "Archie Watt"
                    },
                    {
                        "name": "Mustafa Mahmood"
                    },
                    {
                        "name": "Hamid Butt"
                    },
                    {
                        "name": "George Nishimura"
                    },
                    {
                        "name": "Andrew Soltan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Soltan"
                },
                "author": "Andrew Soltan",
                "arxiv_comment": "RS and AJT are joint-first authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14940v1",
                "updated": "2024-11-22T13:53:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    53,
                    8,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T13:53:08Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    53,
                    8,
                    4,
                    327,
                    0
                ],
                "title": "MIGHTEE-HI: The star-forming properties of HI selected galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIGHTEE-HI: The star-forming properties of HI selected galaxies"
                },
                "summary": "The interplay between atomic gas, the star-formation history of a galaxy and\nits environment are intrinsically linked, and we need to decouple these\ndependencies to understand their role in galaxy formation and evolution. In\nthis paper, we analyse the star formation histories (SFHs) of 187 galaxies from\nthe MIGHTEE-HI Survey Early Science Release data, focusing on the relationships\nbetween HI properties and star formation. A strong correlation emerges between\na galaxy's HI-to-stellar mass ratio and the time of formation, alongside an\ninverse correlation between stellar mass and time of formation, regardless of\nthe inferred SFH. Additionally, galaxies with lower stellar masses and higher\nHI-to-stellar mass ratios exhibit longer gas depletion times compared to more\nmassive galaxies, which appear to have depleted their gas and formed stars more\nefficiently. This suggests that smaller, gas-rich galaxies have higher\ndepletion times due to shallower potential wells and less efficient star\nformation. Furthermore, we explore the connection between spin-filament\nalignment and HI content. We find no significant correlation between peak star\nformation activity and proximity to filaments. However, we do find that the two\ngalaxies in our sample within 1 Mpc of a filament have very low gas-depletion\ntimescales and have their spin axis misaligned with the filament, suggestive of\na link between the galaxy properties and proximity to a filament.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The interplay between atomic gas, the star-formation history of a galaxy and\nits environment are intrinsically linked, and we need to decouple these\ndependencies to understand their role in galaxy formation and evolution. In\nthis paper, we analyse the star formation histories (SFHs) of 187 galaxies from\nthe MIGHTEE-HI Survey Early Science Release data, focusing on the relationships\nbetween HI properties and star formation. A strong correlation emerges between\na galaxy's HI-to-stellar mass ratio and the time of formation, alongside an\ninverse correlation between stellar mass and time of formation, regardless of\nthe inferred SFH. Additionally, galaxies with lower stellar masses and higher\nHI-to-stellar mass ratios exhibit longer gas depletion times compared to more\nmassive galaxies, which appear to have depleted their gas and formed stars more\nefficiently. This suggests that smaller, gas-rich galaxies have higher\ndepletion times due to shallower potential wells and less efficient star\nformation. Furthermore, we explore the connection between spin-filament\nalignment and HI content. We find no significant correlation between peak star\nformation activity and proximity to filaments. However, we do find that the two\ngalaxies in our sample within 1 Mpc of a filament have very low gas-depletion\ntimescales and have their spin axis misaligned with the filament, suggestive of\na link between the galaxy properties and proximity to a filament."
                },
                "authors": [
                    {
                        "name": "Madalina N. Tudorache"
                    },
                    {
                        "name": "M. J. Jarvis"
                    },
                    {
                        "name": "A. A. Ponomareva"
                    },
                    {
                        "name": "I. Heywood"
                    },
                    {
                        "name": "N. Maddox"
                    },
                    {
                        "name": "B. S. Frank"
                    },
                    {
                        "name": "M. Baes"
                    },
                    {
                        "name": "R. Dave"
                    },
                    {
                        "name": "S. L. Jung"
                    },
                    {
                        "name": "M. Maksymowicz-Maciata"
                    },
                    {
                        "name": "H. Pan"
                    },
                    {
                        "name": "K. Spekkens"
                    }
                ],
                "author_detail": {
                    "name": "K. Spekkens"
                },
                "author": "K. Spekkens",
                "arxiv_comment": "submitted to MNRAS; 14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14938v1",
                "updated": "2024-11-22T13:50:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    50,
                    2,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T13:50:02Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    50,
                    2,
                    4,
                    327,
                    0
                ],
                "title": "Bayesian inference of strangeon matter using the measurements of\n  J0437-4751 and GW190814",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference of strangeon matter using the measurements of\n  J0437-4751 and GW190814"
                },
                "summary": "The observations of compact star inspirals from LIGO/Virgo combined with mass\nand radius measurements from NICER provide a valuable tool to study the highly\nuncertain equation of state (EOS) of dense matter at the densities\ncharacteristic of compact stars. In this work, we constrain the solid states of\nstrange-cluster matter, called strangeon matter, as the putative basic units of\nthe ground state of bulk strong matter using a Bayesian statistical method,\nincorporating the mass and radius measurements of PSR J0030+0451, PSR\nJ0740+6620, and the recent data for the $1.4\\ M_{\\odot}$ pulsar PSR J0437-4751.\nWe also include constraints from gravitational wave events GW170817 and\nGW190814. Under the prior assumption of a finite number of quarks in a\nstrangeon, $N_{\\rm q}$, our analysis reveals that current mass-radius\nmeasurements favor a larger $N_{\\rm q}$. Specifically, the results support the\nscenario where a strangeon forms a stable bound state with $N_{\\rm q}=18$,\nsymmetric in color, flavor, and spin spaces, compared to the minimum $N_{\\rm\nq}$ prior. The comparative analyses of the posterior EOS parameter spaces\nderived from three-parameter model and two-parameter model demonstrate a\nconsistent prediction under identical observational constraints. In particular,\nour results indicate that the most probable values of the maximum mass are\nfound to be $3.58^{+0.16}_{-0.12}\\ M_{\\odot}$ ($3.65^{+0.18}_{-0.16}\\\nM_{\\odot}$) at $90\\%$ confidence level for three-parameter (two-parameter) EOS\nconsidering the constraints of GW190814. The corresponding radii for $1.4\\\nM_{\\odot}$ and $2.1\\ M_{\\odot}$ stars are $12.04^{+0.27}_{-0.31}~\\rm km$\n($12.16^{+0.26}_{-0.31}~\\rm km$) and $13.43^{+0.31}_{-0.32}~\\rm km$\n($13.60^{+0.29}_{-0.34}~\\rm km$), respectively. This result may impact\ninterestingly on the research of multiquark states, which could improve our\nunderstanding of the nonperturbative strong force.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The observations of compact star inspirals from LIGO/Virgo combined with mass\nand radius measurements from NICER provide a valuable tool to study the highly\nuncertain equation of state (EOS) of dense matter at the densities\ncharacteristic of compact stars. In this work, we constrain the solid states of\nstrange-cluster matter, called strangeon matter, as the putative basic units of\nthe ground state of bulk strong matter using a Bayesian statistical method,\nincorporating the mass and radius measurements of PSR J0030+0451, PSR\nJ0740+6620, and the recent data for the $1.4\\ M_{\\odot}$ pulsar PSR J0437-4751.\nWe also include constraints from gravitational wave events GW170817 and\nGW190814. Under the prior assumption of a finite number of quarks in a\nstrangeon, $N_{\\rm q}$, our analysis reveals that current mass-radius\nmeasurements favor a larger $N_{\\rm q}$. Specifically, the results support the\nscenario where a strangeon forms a stable bound state with $N_{\\rm q}=18$,\nsymmetric in color, flavor, and spin spaces, compared to the minimum $N_{\\rm\nq}$ prior. The comparative analyses of the posterior EOS parameter spaces\nderived from three-parameter model and two-parameter model demonstrate a\nconsistent prediction under identical observational constraints. In particular,\nour results indicate that the most probable values of the maximum mass are\nfound to be $3.58^{+0.16}_{-0.12}\\ M_{\\odot}$ ($3.65^{+0.18}_{-0.16}\\\nM_{\\odot}$) at $90\\%$ confidence level for three-parameter (two-parameter) EOS\nconsidering the constraints of GW190814. The corresponding radii for $1.4\\\nM_{\\odot}$ and $2.1\\ M_{\\odot}$ stars are $12.04^{+0.27}_{-0.31}~\\rm km$\n($12.16^{+0.26}_{-0.31}~\\rm km$) and $13.43^{+0.31}_{-0.32}~\\rm km$\n($13.60^{+0.29}_{-0.34}~\\rm km$), respectively. This result may impact\ninterestingly on the research of multiquark states, which could improve our\nunderstanding of the nonperturbative strong force."
                },
                "authors": [
                    {
                        "name": "Wen-Li Yuan"
                    },
                    {
                        "name": "Chun Huang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Enping Zhou"
                    },
                    {
                        "name": "Renxin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renxin Xu"
                },
                "author": "Renxin Xu",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.03122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.03122v2",
                "updated": "2024-11-22T13:44:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    44,
                    15,
                    4,
                    327,
                    0
                ],
                "published": "2023-09-06T15:57:58Z",
                "published_parsed": [
                    2023,
                    9,
                    6,
                    15,
                    57,
                    58,
                    2,
                    249,
                    0
                ],
                "title": "Bayesian Evidence Synthesis for Modeling SARS-CoV-2 Transmission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Evidence Synthesis for Modeling SARS-CoV-2 Transmission"
                },
                "summary": "The acute phase of the Covid-19 pandemic has made apparent the need for\ndecision support based upon accurate epidemic modeling. This process is\nsubstantially hampered by under-reporting of cases and related data\nincompleteness issues. In this article we adopt the Bayesian paradigm and\nsynthesize publicly available data via a discrete-time stochastic epidemic\nmodeling framework. The models allow for estimating the total number of\ninfections while accounting for the endemic phase of the pandemic. We assess\nthe prediction of the infection rate utilizing mobility information, notably\nthe principal components of the mobility data. We evaluate variational Bayes in\nthis context and find that Hamiltonian Monte Carlo offers a robust inference\nalternative for such models. We elaborate upon vector analysis of the epidemic\ndynamics, thus enriching the traditional tools used for decision making. In\nparticular, we show how certain 2-dimensional plots on the phase plane may\nyield intuitive information regarding the speed and the type of transmission\ndynamics. We investigate the potential of a two-stage analysis as a consequence\nof cutting feedback, for inference on certain functionals of the model\nparameters. Finally, we show that a point mass on critical parameters is overly\nrestrictive and investigate informative priors as a suitable alternative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The acute phase of the Covid-19 pandemic has made apparent the need for\ndecision support based upon accurate epidemic modeling. This process is\nsubstantially hampered by under-reporting of cases and related data\nincompleteness issues. In this article we adopt the Bayesian paradigm and\nsynthesize publicly available data via a discrete-time stochastic epidemic\nmodeling framework. The models allow for estimating the total number of\ninfections while accounting for the endemic phase of the pandemic. We assess\nthe prediction of the infection rate utilizing mobility information, notably\nthe principal components of the mobility data. We evaluate variational Bayes in\nthis context and find that Hamiltonian Monte Carlo offers a robust inference\nalternative for such models. We elaborate upon vector analysis of the epidemic\ndynamics, thus enriching the traditional tools used for decision making. In\nparticular, we show how certain 2-dimensional plots on the phase plane may\nyield intuitive information regarding the speed and the type of transmission\ndynamics. We investigate the potential of a two-stage analysis as a consequence\nof cutting feedback, for inference on certain functionals of the model\nparameters. Finally, we show that a point mass on critical parameters is overly\nrestrictive and investigate informative priors as a suitable alternative."
                },
                "authors": [
                    {
                        "name": "Anastasios Apsemidis"
                    },
                    {
                        "name": "Nikolaos Demiris"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Demiris"
                },
                "author": "Nikolaos Demiris",
                "arxiv_comment": "27 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.03122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.03122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14922v1",
                "updated": "2024-11-22T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    24,
                    1,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    24,
                    1,
                    4,
                    327,
                    0
                ],
                "title": "GOT4Rec: Graph of Thoughts for Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GOT4Rec: Graph of Thoughts for Sequential Recommendation"
                },
                "summary": "With the advancement of large language models (LLMs), researchers have\nexplored various methods to optimally leverage their comprehension and\ngeneration capabilities in sequential recommendation scenarios. However,\nseveral challenges persist in this endeavor. Firstly, most existing approaches\nrely on the input-output prompting paradigm, which can result in irrelevant or\ninaccurate responses. Secondly, while there have been attempts to enhance LLMs\nusing prompting strategies such as chain-of-thought (CoT), these efforts have\nnot fully harnessed the reasoning abilities of LLMs or effectively captured the\nmultifaceted information contained within user sequences. To address these\nlimitations, we propose GOT4Rec, a sequential recommendation method that\nutilizes the graph of thoughts (GoT) prompting strategy. Specifically, we\nidentify and utilize three key types of information within user history\nsequences: short-term interests, long-term interests and collaborative\ninformation from other users. Our approach enables LLMs to independently reason\nand generate recommendations based on these distinct types of information,\nsubsequently aggregating the results within the GoT framework to derive the\nfinal recommended items. This method allows LLMs, with enhanced reasoning\ncapabilities, to more effectively consider the diverse information within user\nsequences, resulting in more accurate recommendations and more comprehensive\nexplanations. Extensive experiments on real-world datasets demonstrate the\neffectiveness of GOT4Rec, indicating that it outperforms existing\nstate-of-the-art baselines. Our code is available at\nhttps://anonymous.4open.science/r/GOT4Rec-ED99.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of large language models (LLMs), researchers have\nexplored various methods to optimally leverage their comprehension and\ngeneration capabilities in sequential recommendation scenarios. However,\nseveral challenges persist in this endeavor. Firstly, most existing approaches\nrely on the input-output prompting paradigm, which can result in irrelevant or\ninaccurate responses. Secondly, while there have been attempts to enhance LLMs\nusing prompting strategies such as chain-of-thought (CoT), these efforts have\nnot fully harnessed the reasoning abilities of LLMs or effectively captured the\nmultifaceted information contained within user sequences. To address these\nlimitations, we propose GOT4Rec, a sequential recommendation method that\nutilizes the graph of thoughts (GoT) prompting strategy. Specifically, we\nidentify and utilize three key types of information within user history\nsequences: short-term interests, long-term interests and collaborative\ninformation from other users. Our approach enables LLMs to independently reason\nand generate recommendations based on these distinct types of information,\nsubsequently aggregating the results within the GoT framework to derive the\nfinal recommended items. This method allows LLMs, with enhanced reasoning\ncapabilities, to more effectively consider the diverse information within user\nsequences, resulting in more accurate recommendations and more comprehensive\nexplanations. Extensive experiments on real-world datasets demonstrate the\neffectiveness of GOT4Rec, indicating that it outperforms existing\nstate-of-the-art baselines. Our code is available at\nhttps://anonymous.4open.science/r/GOT4Rec-ED99."
                },
                "authors": [
                    {
                        "name": "Zewen Long"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14917v1",
                "updated": "2024-11-22T13:18:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    18,
                    41,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T13:18:41Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    18,
                    41,
                    4,
                    327,
                    0
                ],
                "title": "Task-Aware Robotic Grasping by evaluating Quality Diversity Solutions\n  through Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware Robotic Grasping by evaluating Quality Diversity Solutions\n  through Foundation Models"
                },
                "summary": "Task-aware robotic grasping is a challenging problem that requires the\nintegration of semantic understanding and geometric reasoning. Traditional\ngrasp planning approaches focus on stable or feasible grasps, often\ndisregarding the specific tasks the robot needs to accomplish. This paper\nproposes a novel framework that leverages Large Language Models (LLMs) and\nQuality Diversity (QD) algorithms to enable zero-shot task-conditioned grasp\nselection. The framework segments objects into meaningful subparts and labels\neach subpart semantically, creating structured representations that can be used\nto prompt an LLM. By coupling semantic and geometric representations of an\nobject's structure, the LLM's knowledge about tasks and which parts to grasp\ncan be applied in the physical world. The QD-generated grasp archive provides a\ndiverse set of grasps, allowing us to select the most suitable grasp based on\nthe task. We evaluate the proposed method on a subset of the YCB dataset, where\na Franka Emika robot is assigned to perform various actions based on\nobject-specific task requirements. We created a ground truth by conducting a\nsurvey with six participants to determine the best grasp region for each\ntask-object combination according to human intuition. The model was evaluated\non 12 different objects across 4--7 object-specific tasks, achieving a weighted\nintersection over union (IoU) of 76.4% when compared to the survey data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-aware robotic grasping is a challenging problem that requires the\nintegration of semantic understanding and geometric reasoning. Traditional\ngrasp planning approaches focus on stable or feasible grasps, often\ndisregarding the specific tasks the robot needs to accomplish. This paper\nproposes a novel framework that leverages Large Language Models (LLMs) and\nQuality Diversity (QD) algorithms to enable zero-shot task-conditioned grasp\nselection. The framework segments objects into meaningful subparts and labels\neach subpart semantically, creating structured representations that can be used\nto prompt an LLM. By coupling semantic and geometric representations of an\nobject's structure, the LLM's knowledge about tasks and which parts to grasp\ncan be applied in the physical world. The QD-generated grasp archive provides a\ndiverse set of grasps, allowing us to select the most suitable grasp based on\nthe task. We evaluate the proposed method on a subset of the YCB dataset, where\na Franka Emika robot is assigned to perform various actions based on\nobject-specific task requirements. We created a ground truth by conducting a\nsurvey with six participants to determine the best grasp region for each\ntask-object combination according to human intuition. The model was evaluated\non 12 different objects across 4--7 object-specific tasks, achieving a weighted\nintersection over union (IoU) of 76.4% when compared to the survey data."
                },
                "authors": [
                    {
                        "name": "Aurel X. Appius"
                    },
                    {
                        "name": "Emiland Garrabe"
                    },
                    {
                        "name": "Francois Helenon"
                    },
                    {
                        "name": "Mahdi Khoramshahi"
                    },
                    {
                        "name": "Stephane Doncieux"
                    }
                ],
                "author_detail": {
                    "name": "Stephane Doncieux"
                },
                "author": "Stephane Doncieux",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14914v1",
                "updated": "2024-11-22T13:15:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    15,
                    3,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T13:15:03Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    15,
                    3,
                    4,
                    327,
                    0
                ],
                "title": "A Reproducibility and Generalizability Study of Large Language Models\n  for Query Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducibility and Generalizability Study of Large Language Models\n  for Query Generation"
                },
                "summary": "Systematic literature reviews (SLRs) are a cornerstone of academic research,\nyet they are often labour-intensive and time-consuming due to the detailed\nliterature curation process. The advent of generative AI and large language\nmodels (LLMs) promises to revolutionize this process by assisting researchers\nin several tedious tasks, one of them being the generation of effective Boolean\nqueries that will select the publications to consider including in a review.\nThis paper presents an extensive study of Boolean query generation using LLMs\nfor systematic reviews, reproducing and extending the work of Wang et al. and\nAlaniz et al. Our study investigates the replicability and reliability of\nresults achieved using ChatGPT and compares its performance with open-source\nalternatives like Mistral and Zephyr to provide a more comprehensive analysis\nof LLMs for query generation.\n  Therefore, we implemented a pipeline, which automatically creates a Boolean\nquery for a given review topic by using a previously defined LLM, retrieves all\ndocuments for this query from the PubMed database and then evaluates the\nresults. With this pipeline we first assess whether the results obtained using\nChatGPT for query generation are reproducible and consistent. We then\ngeneralize our results by analyzing and evaluating open-source models and\nevaluating their efficacy in generating Boolean queries.\n  Finally, we conduct a failure analysis to identify and discuss the\nlimitations and shortcomings of using LLMs for Boolean query generation. This\nexamination helps to understand the gaps and potential areas for improvement in\nthe application of LLMs to information retrieval tasks. Our findings highlight\nthe strengths, limitations, and potential of LLMs in the domain of information\nretrieval and literature review automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic literature reviews (SLRs) are a cornerstone of academic research,\nyet they are often labour-intensive and time-consuming due to the detailed\nliterature curation process. The advent of generative AI and large language\nmodels (LLMs) promises to revolutionize this process by assisting researchers\nin several tedious tasks, one of them being the generation of effective Boolean\nqueries that will select the publications to consider including in a review.\nThis paper presents an extensive study of Boolean query generation using LLMs\nfor systematic reviews, reproducing and extending the work of Wang et al. and\nAlaniz et al. Our study investigates the replicability and reliability of\nresults achieved using ChatGPT and compares its performance with open-source\nalternatives like Mistral and Zephyr to provide a more comprehensive analysis\nof LLMs for query generation.\n  Therefore, we implemented a pipeline, which automatically creates a Boolean\nquery for a given review topic by using a previously defined LLM, retrieves all\ndocuments for this query from the PubMed database and then evaluates the\nresults. With this pipeline we first assess whether the results obtained using\nChatGPT for query generation are reproducible and consistent. We then\ngeneralize our results by analyzing and evaluating open-source models and\nevaluating their efficacy in generating Boolean queries.\n  Finally, we conduct a failure analysis to identify and discuss the\nlimitations and shortcomings of using LLMs for Boolean query generation. This\nexamination helps to understand the gaps and potential areas for improvement in\nthe application of LLMs to information retrieval tasks. Our findings highlight\nthe strengths, limitations, and potential of LLMs in the domain of information\nretrieval and literature review automation."
                },
                "authors": [
                    {
                        "name": "Moritz Staudinger"
                    },
                    {
                        "name": "Wojciech Kusa"
                    },
                    {
                        "name": "Florina Piroi"
                    },
                    {
                        "name": "Aldo Lipani"
                    },
                    {
                        "name": "Allan Hanbury"
                    }
                ],
                "author_detail": {
                    "name": "Allan Hanbury"
                },
                "author": "Allan Hanbury",
                "arxiv_doi": "10.1145/3673791.3698432",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3673791.3698432",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.14914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14913v1",
                "updated": "2024-11-22T13:14:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    14,
                    54,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T13:14:54Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    14,
                    54,
                    4,
                    327,
                    0
                ],
                "title": "Enhancing Exploration with Diffusion Policies in Hybrid Off-Policy RL:\n  Application to Non-Prehensile Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Exploration with Diffusion Policies in Hybrid Off-Policy RL:\n  Application to Non-Prehensile Manipulation"
                },
                "summary": "Learning diverse policies for non-prehensile manipulation is essential for\nimproving skill transfer and generalization to out-of-distribution scenarios.\nIn this work, we enhance exploration through a two-fold approach within a\nhybrid framework that tackles both discrete and continuous action spaces.\nFirst, we model the continuous motion parameter policy as a diffusion model,\nand second, we incorporate this into a maximum entropy reinforcement learning\nframework that unifies both the discrete and continuous components. The\ndiscrete action space, such as contact point selection, is optimized through\nQ-value function maximization, while the continuous part is guided by a\ndiffusion-based policy. This hybrid approach leads to a principled objective,\nwhere the maximum entropy term is derived as a lower bound using structured\nvariational inference. We propose the Hybrid Diffusion Policy algorithm (HyDo)\nand evaluate its performance on both simulation and zero-shot sim2real tasks.\nOur results show that HyDo encourages more diverse behavior policies, leading\nto significantly improved success rates across tasks - for example, increasing\nfrom 53% to 72% on a real-world 6D pose alignment task. Project page:\nhttps://leh2rng.github.io/hydo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning diverse policies for non-prehensile manipulation is essential for\nimproving skill transfer and generalization to out-of-distribution scenarios.\nIn this work, we enhance exploration through a two-fold approach within a\nhybrid framework that tackles both discrete and continuous action spaces.\nFirst, we model the continuous motion parameter policy as a diffusion model,\nand second, we incorporate this into a maximum entropy reinforcement learning\nframework that unifies both the discrete and continuous components. The\ndiscrete action space, such as contact point selection, is optimized through\nQ-value function maximization, while the continuous part is guided by a\ndiffusion-based policy. This hybrid approach leads to a principled objective,\nwhere the maximum entropy term is derived as a lower bound using structured\nvariational inference. We propose the Hybrid Diffusion Policy algorithm (HyDo)\nand evaluate its performance on both simulation and zero-shot sim2real tasks.\nOur results show that HyDo encourages more diverse behavior policies, leading\nto significantly improved success rates across tasks - for example, increasing\nfrom 53% to 72% on a real-world 6D pose alignment task. Project page:\nhttps://leh2rng.github.io/hydo"
                },
                "authors": [
                    {
                        "name": "Huy Le"
                    },
                    {
                        "name": "Miroslav Gabriel"
                    },
                    {
                        "name": "Tai Hoang"
                    },
                    {
                        "name": "Gerhard Neumann"
                    },
                    {
                        "name": "Ngo Anh Vien"
                    }
                ],
                "author_detail": {
                    "name": "Ngo Anh Vien"
                },
                "author": "Ngo Anh Vien",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13187v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13187v3",
                "updated": "2024-11-22T13:05:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    5,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-20T10:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    40,
                    8,
                    2,
                    325,
                    0
                ],
                "title": "Engagement-Driven Content Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engagement-Driven Content Generation with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/."
                },
                "authors": [
                    {
                        "name": "Erica Coppolillo"
                    },
                    {
                        "name": "Federico Cinus"
                    },
                    {
                        "name": "Marco Minici"
                    },
                    {
                        "name": "Francesco Bonchi"
                    },
                    {
                        "name": "Giuseppe Manco"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Manco"
                },
                "author": "Giuseppe Manco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13187v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13187v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14905v1",
                "updated": "2024-11-22T13:03:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    3,
                    7,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T13:03:07Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    3,
                    7,
                    4,
                    327,
                    0
                ],
                "title": "Feasibility Study for Supporting Static Malware Analysis Using LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feasibility Study for Supporting Static Malware Analysis Using LLM"
                },
                "summary": "Large language models (LLMs) are becoming more advanced and widespread and\nhave shown their applicability to various domains, including cybersecurity.\nStatic malware analysis is one of the most important tasks in cybersecurity;\nhowever, it is time-consuming and requires a high level of expertise.\nTherefore, we conducted a demonstration experiment focusing on whether an LLM\ncan be used to support static analysis. First, we evaluated the ability of the\nLLM to explain malware functionality. The results showed that the LLM can\ngenerate descriptions that cover functions with an accuracy of up to 90.9\\%. In\naddition, we asked six static analysts to perform a pseudo static analysis task\nusing LLM explanations to verify that the LLM can be used in practice. Through\nsubsequent questionnaires and interviews with the participants, we also\ndemonstrated the practical applicability of LLMs. Lastly, we summarized the\nproblems and required functions when using an LLM as static analysis support,\nas well as recommendations for future research opportunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming more advanced and widespread and\nhave shown their applicability to various domains, including cybersecurity.\nStatic malware analysis is one of the most important tasks in cybersecurity;\nhowever, it is time-consuming and requires a high level of expertise.\nTherefore, we conducted a demonstration experiment focusing on whether an LLM\ncan be used to support static analysis. First, we evaluated the ability of the\nLLM to explain malware functionality. The results showed that the LLM can\ngenerate descriptions that cover functions with an accuracy of up to 90.9\\%. In\naddition, we asked six static analysts to perform a pseudo static analysis task\nusing LLM explanations to verify that the LLM can be used in practice. Through\nsubsequent questionnaires and interviews with the participants, we also\ndemonstrated the practical applicability of LLMs. Lastly, we summarized the\nproblems and required functions when using an LLM as static analysis support,\nas well as recommendations for future research opportunities."
                },
                "authors": [
                    {
                        "name": "Shota Fujii"
                    },
                    {
                        "name": "Rei Yamagishi"
                    }
                ],
                "author_detail": {
                    "name": "Rei Yamagishi"
                },
                "author": "Rei Yamagishi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16485v2",
                "updated": "2024-11-22T12:58:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    12,
                    58,
                    21,
                    4,
                    327,
                    0
                ],
                "published": "2024-07-23T14:00:18Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    14,
                    0,
                    18,
                    1,
                    205,
                    0
                ],
                "title": "Learning General Continuous Constraint from Demonstrations via\n  Positive-Unlabeled Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning General Continuous Constraint from Demonstrations via\n  Positive-Unlabeled Learning"
                },
                "summary": "Planning for a wide range of real-world tasks necessitates to know and write\nall constraints. However, instances exist where these constraints are either\nunknown or challenging to specify accurately. A possible solution is to infer\nthe unknown constraints from expert demonstration. The majority of prior works\nlimit themselves to learning simple linear constraints, or require strong\nknowledge of the true constraint parameterization or environmental model. To\nmitigate these problems, this paper presents a positive-unlabeled (PU) learning\napproach to infer a continuous, arbitrary and possibly nonlinear, constraint\nfrom demonstration. From a PU learning view, We treat all data in\ndemonstrations as positive (feasible) data, and learn a (sub)-optimal policy to\ngenerate high-reward-winning but potentially infeasible trajectories, which\nserve as unlabeled data containing both feasible and infeasible states. Under\nan assumption on data distribution, a feasible-infeasible classifier (i.e.,\nconstraint model) is learned from the two datasets through a postprocessing PU\nlearning technique. The entire method employs an iterative framework\nalternating between updating the policy, which generates and selects\nhigher-reward policies, and updating the constraint model. Additionally, a\nmemory buffer is introduced to record and reuse samples from previous\niterations to prevent forgetting. The effectiveness of the proposed method is\nvalidated in two Mujoco environments, successfully inferring continuous\nnonlinear constraints and outperforming a baseline method in terms of\nconstraint accuracy and policy safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning for a wide range of real-world tasks necessitates to know and write\nall constraints. However, instances exist where these constraints are either\nunknown or challenging to specify accurately. A possible solution is to infer\nthe unknown constraints from expert demonstration. The majority of prior works\nlimit themselves to learning simple linear constraints, or require strong\nknowledge of the true constraint parameterization or environmental model. To\nmitigate these problems, this paper presents a positive-unlabeled (PU) learning\napproach to infer a continuous, arbitrary and possibly nonlinear, constraint\nfrom demonstration. From a PU learning view, We treat all data in\ndemonstrations as positive (feasible) data, and learn a (sub)-optimal policy to\ngenerate high-reward-winning but potentially infeasible trajectories, which\nserve as unlabeled data containing both feasible and infeasible states. Under\nan assumption on data distribution, a feasible-infeasible classifier (i.e.,\nconstraint model) is learned from the two datasets through a postprocessing PU\nlearning technique. The entire method employs an iterative framework\nalternating between updating the policy, which generates and selects\nhigher-reward policies, and updating the constraint model. Additionally, a\nmemory buffer is introduced to record and reuse samples from previous\niterations to prevent forgetting. The effectiveness of the proposed method is\nvalidated in two Mujoco environments, successfully inferring continuous\nnonlinear constraints and outperforming a baseline method in terms of\nconstraint accuracy and policy safety."
                },
                "authors": [
                    {
                        "name": "Baiyu Peng"
                    },
                    {
                        "name": "Aude Billard"
                    }
                ],
                "author_detail": {
                    "name": "Aude Billard"
                },
                "author": "Aude Billard",
                "arxiv_comment": "The paper is hastily uploaded. We prefer to improve it and upload it\n  later, and possibily after it is published",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14901v1",
                "updated": "2024-11-22T12:46:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    12,
                    46,
                    50,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T12:46:50Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    12,
                    46,
                    50,
                    4,
                    327,
                    0
                ],
                "title": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in\n  Hour-Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in\n  Hour-Long Videos"
                },
                "summary": "Large language models (LLMs) excel at retrieving information from lengthy\ntext, but their vision-language counterparts (VLMs) face difficulties with\nhour-long videos, especially for temporal grounding. Specifically, these VLMs\nare constrained by frame limitations, often losing essential temporal details\nneeded for accurate event localization in extended video content. We propose\nReVisionLLM, a recursive vision-language model designed to locate events in\nhour-long videos. Inspired by human search strategies, our model initially\ntargets broad segments of interest, progressively revising its focus to\npinpoint exact temporal boundaries. Our model can seamlessly handle videos of\nvastly different lengths, from minutes to hours. We also introduce a\nhierarchical training strategy that starts with short clips to capture distinct\nevents and progressively extends to longer videos. To our knowledge,\nReVisionLLM is the first VLM capable of temporal grounding in hour-long videos,\noutperforming previous state-of-the-art methods across multiple datasets by a\nsignificant margin (+2.6% R1@0.1 on MAD). The code is available at\nhttps://github.com/Tanveer81/ReVisionLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at retrieving information from lengthy\ntext, but their vision-language counterparts (VLMs) face difficulties with\nhour-long videos, especially for temporal grounding. Specifically, these VLMs\nare constrained by frame limitations, often losing essential temporal details\nneeded for accurate event localization in extended video content. We propose\nReVisionLLM, a recursive vision-language model designed to locate events in\nhour-long videos. Inspired by human search strategies, our model initially\ntargets broad segments of interest, progressively revising its focus to\npinpoint exact temporal boundaries. Our model can seamlessly handle videos of\nvastly different lengths, from minutes to hours. We also introduce a\nhierarchical training strategy that starts with short clips to capture distinct\nevents and progressively extends to longer videos. To our knowledge,\nReVisionLLM is the first VLM capable of temporal grounding in hour-long videos,\noutperforming previous state-of-the-art methods across multiple datasets by a\nsignificant margin (+2.6% R1@0.1 on MAD). The code is available at\nhttps://github.com/Tanveer81/ReVisionLLM."
                },
                "authors": [
                    {
                        "name": "Tanveer Hannan"
                    },
                    {
                        "name": "Md Mohaiminul Islam"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Thomas Seidl"
                    },
                    {
                        "name": "Gedas Bertasius"
                    }
                ],
                "author_detail": {
                    "name": "Gedas Bertasius"
                },
                "author": "Gedas Bertasius",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14896v1",
                "updated": "2024-11-22T12:37:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    12,
                    37,
                    41,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T12:37:41Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    12,
                    37,
                    41,
                    4,
                    327,
                    0
                ],
                "title": "Evaluating LLM Prompts for Data Augmentation in Multi-label\n  Classification of Ecological Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM Prompts for Data Augmentation in Multi-label\n  Classification of Ecological Texts"
                },
                "summary": "Large language models (LLMs) play a crucial role in natural language\nprocessing (NLP) tasks, improving the understanding, generation, and\nmanipulation of human language across domains such as translating, summarizing,\nand classifying text. Previous studies have demonstrated that instruction-based\nLLMs can be effectively utilized for data augmentation to generate diverse and\nrealistic text samples. This study applied prompt-based data augmentation to\ndetect mentions of green practices in Russian social media. Detecting green\npractices in social media aids in understanding their prevalence and helps\nformulate recommendations for scaling eco-friendly actions to mitigate\nenvironmental issues. We evaluated several prompts for augmenting texts in a\nmulti-label classification task, either by rewriting existing datasets using\nLLMs, generating new data, or combining both approaches. Our results revealed\nthat all strategies improved classification performance compared to the models\nfine-tuned only on the original dataset, outperforming baselines in most cases.\nThe best results were obtained with the prompt that paraphrased the original\ntext while clearly indicating the relevant categories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) play a crucial role in natural language\nprocessing (NLP) tasks, improving the understanding, generation, and\nmanipulation of human language across domains such as translating, summarizing,\nand classifying text. Previous studies have demonstrated that instruction-based\nLLMs can be effectively utilized for data augmentation to generate diverse and\nrealistic text samples. This study applied prompt-based data augmentation to\ndetect mentions of green practices in Russian social media. Detecting green\npractices in social media aids in understanding their prevalence and helps\nformulate recommendations for scaling eco-friendly actions to mitigate\nenvironmental issues. We evaluated several prompts for augmenting texts in a\nmulti-label classification task, either by rewriting existing datasets using\nLLMs, generating new data, or combining both approaches. Our results revealed\nthat all strategies improved classification performance compared to the models\nfine-tuned only on the original dataset, outperforming baselines in most cases.\nThe best results were obtained with the prompt that paraphrased the original\ntext while clearly indicating the relevant categories."
                },
                "authors": [
                    {
                        "name": "Anna Glazkova"
                    },
                    {
                        "name": "Olga Zakharova"
                    }
                ],
                "author_detail": {
                    "name": "Olga Zakharova"
                },
                "author": "Olga Zakharova",
                "arxiv_comment": "Ivannikov ISPRAS Open Conference (ISPRAS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4; I.7.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13677v2",
                "updated": "2024-11-22T12:03:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    12,
                    3,
                    34,
                    4,
                    327,
                    0
                ],
                "published": "2024-06-19T16:30:58Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    16,
                    30,
                    58,
                    2,
                    171,
                    0
                ],
                "title": "Leveraging Large Language Models to Measure Gender Representation Bias\n  in Gendered Language Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models to Measure Gender Representation Bias\n  in Gendered Language Corpora"
                },
                "summary": "Gender bias in text corpora that are used for a variety of natural language\nprocessing (NLP) tasks, such as for training large language models (LLMs), can\nlead to the perpetuation and amplification of societal inequalities. This\nphenomenon is particularly pronounced in gendered languages like Spanish or\nFrench, where grammatical structures inherently encode gender, making the bias\nanalysis more challenging. A first step in quantifying gender bias in text\nentails computing biases in gender representation, i.e., differences in the\nprevalence of words referring to males vs. females. Existing methods to measure\ngender representation bias in text corpora have mainly been proposed for\nEnglish and do not generalize to gendered languages due to the intrinsic\nlinguistic differences between English and gendered languages. This paper\nintroduces a novel methodology that leverages the contextual understanding\ncapabilities of LLMs to quantitatively measure gender representation bias in\nSpanish corpora. By utilizing LLMs to identify and classify gendered nouns and\npronouns in relation to their reference to human entities, our approach\nprovides a robust analysis of gender representation bias in gendered languages.\nWe empirically validate our method on four widely-used benchmark datasets,\nuncovering significant gender prevalence disparities with a male-to-female\nratio ranging from 4:1 to 6:1. These findings demonstrate the value of our\nmethodology for bias quantification in gendered language corpora and suggest\nits application in NLP, contributing to the development of more equitable\nlanguage technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gender bias in text corpora that are used for a variety of natural language\nprocessing (NLP) tasks, such as for training large language models (LLMs), can\nlead to the perpetuation and amplification of societal inequalities. This\nphenomenon is particularly pronounced in gendered languages like Spanish or\nFrench, where grammatical structures inherently encode gender, making the bias\nanalysis more challenging. A first step in quantifying gender bias in text\nentails computing biases in gender representation, i.e., differences in the\nprevalence of words referring to males vs. females. Existing methods to measure\ngender representation bias in text corpora have mainly been proposed for\nEnglish and do not generalize to gendered languages due to the intrinsic\nlinguistic differences between English and gendered languages. This paper\nintroduces a novel methodology that leverages the contextual understanding\ncapabilities of LLMs to quantitatively measure gender representation bias in\nSpanish corpora. By utilizing LLMs to identify and classify gendered nouns and\npronouns in relation to their reference to human entities, our approach\nprovides a robust analysis of gender representation bias in gendered languages.\nWe empirically validate our method on four widely-used benchmark datasets,\nuncovering significant gender prevalence disparities with a male-to-female\nratio ranging from 4:1 to 6:1. These findings demonstrate the value of our\nmethodology for bias quantification in gendered language corpora and suggest\nits application in NLP, contributing to the development of more equitable\nlanguage technologies."
                },
                "authors": [
                    {
                        "name": "Erik Derner"
                    },
                    {
                        "name": "Sara Sansalvador de la Fuente"
                    },
                    {
                        "name": "Yoan Gutiérrez"
                    },
                    {
                        "name": "Paloma Moreda"
                    },
                    {
                        "name": "Nuria Oliver"
                    }
                ],
                "author_detail": {
                    "name": "Nuria Oliver"
                },
                "author": "Nuria Oliver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14871v1",
                "updated": "2024-11-22T11:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    11,
                    45,
                    33,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T11:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    11,
                    45,
                    33,
                    4,
                    327,
                    0
                ],
                "title": "Prioritize Denoising Steps on Diffusion Model Preference Alignment via\n  Explicit Denoised Distribution Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prioritize Denoising Steps on Diffusion Model Preference Alignment via\n  Explicit Denoised Distribution Estimation"
                },
                "summary": "Diffusion models have shown remarkable success in text-to-image generation,\nmaking alignment methods for these models increasingly important. A key\nchallenge is the sparsity of preference labels, which are typically available\nonly at the terminal of denoising trajectories. This raises the issue of how to\nassign credit across denoising steps based on these sparse labels. In this\npaper, we propose Denoised Distribution Estimation (DDE), a novel method for\ncredit assignment. Unlike previous approaches that rely on auxiliary models or\nhand-crafted schemes, DDE derives its strategy more explicitly. The proposed\nDDE directly estimates the terminal denoised distribution from the perspective\nof each step. It is equipped with two estimation strategies and capable of\nrepresenting the entire denoising trajectory with a single model inference.\nTheoretically and empirically, we show that DDE prioritizes optimizing the\nmiddle part of the denoising trajectory, resulting in a novel and effective\ncredit assignment scheme. Extensive experiments demonstrate that our approach\nachieves superior performance, both quantitatively and qualitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown remarkable success in text-to-image generation,\nmaking alignment methods for these models increasingly important. A key\nchallenge is the sparsity of preference labels, which are typically available\nonly at the terminal of denoising trajectories. This raises the issue of how to\nassign credit across denoising steps based on these sparse labels. In this\npaper, we propose Denoised Distribution Estimation (DDE), a novel method for\ncredit assignment. Unlike previous approaches that rely on auxiliary models or\nhand-crafted schemes, DDE derives its strategy more explicitly. The proposed\nDDE directly estimates the terminal denoised distribution from the perspective\nof each step. It is equipped with two estimation strategies and capable of\nrepresenting the entire denoising trajectory with a single model inference.\nTheoretically and empirically, we show that DDE prioritizes optimizing the\nmiddle part of the denoising trajectory, resulting in a novel and effective\ncredit assignment scheme. Extensive experiments demonstrate that our approach\nachieves superior performance, both quantitatively and qualitatively."
                },
                "authors": [
                    {
                        "name": "Dingyuan Shi"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Hangyu Li"
                    },
                    {
                        "name": "Xiangxiang Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangxiang Chu"
                },
                "author": "Xiangxiang Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00434v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00434v2",
                "updated": "2024-11-22T11:31:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    11,
                    31,
                    28,
                    4,
                    327,
                    0
                ],
                "published": "2024-10-01T06:33:40Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    33,
                    40,
                    1,
                    275,
                    0
                ],
                "title": "Rapid Integration of LLMs in Healthcare Raises Ethical Concerns: An\n  Investigation into Deceptive Patterns in Social Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Integration of LLMs in Healthcare Raises Ethical Concerns: An\n  Investigation into Deceptive Patterns in Social Robots"
                },
                "summary": "Conversational agents are increasingly used in healthcare, and the\nintegration of Large Language Models (LLMs) has significantly enhanced their\ncapabilities. When integrated into social robots, LLMs offer the potential for\nmore natural interactions. However, while LLMs promise numerous benefits, they\nalso raise critical ethical concerns, particularly around the issue of\nhallucinations and deceptive patterns. In this case study, we observed a\ncritical pattern of deceptive behavior in commercially available LLM-based care\nsoftware integrated into robots. The LLM-equipped robot falsely claimed to have\nmedication reminder functionalities. Not only did these systems assure users of\ntheir ability to manage medication schedules, but they also proactively\nsuggested this capability, despite lacking it. This deceptive behavior poses\nsignificant risks in healthcare environments, where reliability is paramount.\nOur findings highlights the ethical and safety concerns surrounding the\ndeployment of LLM-integrated robots in healthcare, emphasizing the need for\noversight to prevent potentially harmful consequences for vulnerable\npopulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational agents are increasingly used in healthcare, and the\nintegration of Large Language Models (LLMs) has significantly enhanced their\ncapabilities. When integrated into social robots, LLMs offer the potential for\nmore natural interactions. However, while LLMs promise numerous benefits, they\nalso raise critical ethical concerns, particularly around the issue of\nhallucinations and deceptive patterns. In this case study, we observed a\ncritical pattern of deceptive behavior in commercially available LLM-based care\nsoftware integrated into robots. The LLM-equipped robot falsely claimed to have\nmedication reminder functionalities. Not only did these systems assure users of\ntheir ability to manage medication schedules, but they also proactively\nsuggested this capability, despite lacking it. This deceptive behavior poses\nsignificant risks in healthcare environments, where reliability is paramount.\nOur findings highlights the ethical and safety concerns surrounding the\ndeployment of LLM-integrated robots in healthcare, emphasizing the need for\noversight to prevent potentially harmful consequences for vulnerable\npopulations."
                },
                "authors": [
                    {
                        "name": "Robert Ranisch"
                    },
                    {
                        "name": "Joschka Haltaufderheide"
                    }
                ],
                "author_detail": {
                    "name": "Joschka Haltaufderheide"
                },
                "author": "Joschka Haltaufderheide",
                "arxiv_comment": "7 pages, 1table, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00434v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00434v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08713v2",
                "updated": "2024-11-22T11:25:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    11,
                    25,
                    34,
                    4,
                    327,
                    0
                ],
                "published": "2024-07-11T17:50:09Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    17,
                    50,
                    9,
                    3,
                    193,
                    0
                ],
                "title": "GTA: A Benchmark for General Tool Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: A Benchmark for General Tool Agents"
                },
                "summary": "Significant focus has been placed on integrating large language models (LLMs)\nwith various tools in developing general-purpose agents. This poses a challenge\nto LLMs' tool-use capabilities. However, there are evident gaps between\nexisting tool-use evaluations and real-world scenarios. Current evaluations\noften use AI-generated queries, single-step tasks, dummy tools, and text-only\ninteractions, failing to reveal the agents' real-world problem-solving\nabilities effectively. To address this, we propose GTA, a benchmark for General\nTool Agents, featuring three main aspects: (i) Real user queries: human-written\nqueries with simple real-world objectives but implicit tool-use, requiring the\nLLM to reason the suitable tools and plan the solution steps. (ii) Real\ndeployed tools: an evaluation platform equipped with tools across perception,\noperation, logic, and creativity categories to evaluate the agents' actual task\nexecution performance. (iii) Real multimodal inputs: authentic image files,\nsuch as spatial scenes, web page screenshots, tables, code snippets, and\nprinted/handwritten materials, used as the query contexts to align with\nreal-world scenarios closely. We design 229 real-world tasks and executable\ntool chains to evaluate mainstream LLMs. Our findings show that real-world user\nqueries are challenging for existing LLMs, with GPT-4 completing less than 50%\nof the tasks and most LLMs achieving below 25%. This evaluation reveals the\nbottlenecks in the tool-use capabilities of current LLMs in real-world\nscenarios, which provides future direction for advancing general-purpose tool\nagents. The code and dataset are available at\nhttps://github.com/open-compass/GTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant focus has been placed on integrating large language models (LLMs)\nwith various tools in developing general-purpose agents. This poses a challenge\nto LLMs' tool-use capabilities. However, there are evident gaps between\nexisting tool-use evaluations and real-world scenarios. Current evaluations\noften use AI-generated queries, single-step tasks, dummy tools, and text-only\ninteractions, failing to reveal the agents' real-world problem-solving\nabilities effectively. To address this, we propose GTA, a benchmark for General\nTool Agents, featuring three main aspects: (i) Real user queries: human-written\nqueries with simple real-world objectives but implicit tool-use, requiring the\nLLM to reason the suitable tools and plan the solution steps. (ii) Real\ndeployed tools: an evaluation platform equipped with tools across perception,\noperation, logic, and creativity categories to evaluate the agents' actual task\nexecution performance. (iii) Real multimodal inputs: authentic image files,\nsuch as spatial scenes, web page screenshots, tables, code snippets, and\nprinted/handwritten materials, used as the query contexts to align with\nreal-world scenarios closely. We design 229 real-world tasks and executable\ntool chains to evaluate mainstream LLMs. Our findings show that real-world user\nqueries are challenging for existing LLMs, with GPT-4 completing less than 50%\nof the tasks and most LLMs achieving below 25%. This evaluation reveals the\nbottlenecks in the tool-use capabilities of current LLMs in real-world\nscenarios, which provides future direction for advancing general-purpose tool\nagents. The code and dataset are available at\nhttps://github.com/open-compass/GTA."
                },
                "authors": [
                    {
                        "name": "Jize Wang"
                    },
                    {
                        "name": "Zerun Ma"
                    },
                    {
                        "name": "Yining Li"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Cailian Chen"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Xinyi Le"
                    }
                ],
                "author_detail": {
                    "name": "Xinyi Le"
                },
                "author": "Xinyi Le",
                "arxiv_comment": "Github repo: https://github.com/open-compass/GTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14863v1",
                "updated": "2024-11-22T11:24:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    11,
                    24,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T11:24:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    11,
                    24,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "Latent Schrodinger Bridge: Prompting Latent Diffusion for Fast Unpaired\n  Image-to-Image Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Schrodinger Bridge: Prompting Latent Diffusion for Fast Unpaired\n  Image-to-Image Translation"
                },
                "summary": "Diffusion models (DMs), which enable both image generation from noise and\ninversion from data, have inspired powerful unpaired image-to-image (I2I)\ntranslation algorithms. However, they often require a larger number of neural\nfunction evaluations (NFEs), limiting their practical applicability. In this\npaper, we tackle this problem with Schrodinger Bridges (SBs), which are\nstochastic differential equations (SDEs) between distributions with minimal\ntransport cost. We analyze the probability flow ordinary differential equation\n(ODE) formulation of SBs, and observe that we can decompose its vector field\ninto a linear combination of source predictor, target predictor, and noise\npredictor. Inspired by this observation, we propose Latent Schrodinger Bridges\n(LSBs) that approximate the SB ODE via pre-trained Stable Diffusion, and\ndevelop appropriate prompt optimization and change of variables formula to\nmatch the training and inference between distributions. We demonstrate that our\nalgorithm successfully conduct competitive I2I translation in unsupervised\nsetting with only a fraction of computation cost required by previous DM-based\nI2I methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs), which enable both image generation from noise and\ninversion from data, have inspired powerful unpaired image-to-image (I2I)\ntranslation algorithms. However, they often require a larger number of neural\nfunction evaluations (NFEs), limiting their practical applicability. In this\npaper, we tackle this problem with Schrodinger Bridges (SBs), which are\nstochastic differential equations (SDEs) between distributions with minimal\ntransport cost. We analyze the probability flow ordinary differential equation\n(ODE) formulation of SBs, and observe that we can decompose its vector field\ninto a linear combination of source predictor, target predictor, and noise\npredictor. Inspired by this observation, we propose Latent Schrodinger Bridges\n(LSBs) that approximate the SB ODE via pre-trained Stable Diffusion, and\ndevelop appropriate prompt optimization and change of variables formula to\nmatch the training and inference between distributions. We demonstrate that our\nalgorithm successfully conduct competitive I2I translation in unsupervised\nsetting with only a fraction of computation cost required by previous DM-based\nI2I methods."
                },
                "authors": [
                    {
                        "name": "Jeongsol Kim"
                    },
                    {
                        "name": "Beomsu Kim"
                    },
                    {
                        "name": "Jong Chul Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jong Chul Ye"
                },
                "author": "Jong Chul Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12603v2",
                "updated": "2024-11-22T11:21:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    11,
                    21,
                    50,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-19T16:06:32Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    16,
                    6,
                    32,
                    1,
                    324,
                    0
                ],
                "title": "STREAM: A Universal State-Space Model for Sparse Geometric Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STREAM: A Universal State-Space Model for Sparse Geometric Data"
                },
                "summary": "Handling sparse and unstructured geometric data, such as point clouds or\nevent-based vision, is a pressing challenge in the field of machine vision.\nRecently, sequence models such as Transformers and state-space models entered\nthe domain of geometric data. These methods require specialized preprocessing\nto create a sequential view of a set of points. Furthermore, prior works\ninvolving sequence models iterate geometric data with either uniform or learned\nstep sizes, implicitly relying on the model to infer the underlying geometric\nstructure. In this work, we propose to encode geometric structure explicitly\ninto the parameterization of a state-space model. State-space models are based\non linear dynamics governed by a one-dimensional variable such as time or a\nspatial coordinate. We exploit this dynamic variable to inject relative\ndifferences of coordinates into the step size of the state-space model. The\nresulting geometric operation computes interactions between all pairs of N\npoints in O(N) steps. Our model deploys the Mamba selective state-space model\nwith a modified CUDA kernel to efficiently map sparse geometric data to modern\nhardware. The resulting sequence model, which we call STREAM, achieves\ncompetitive results on a range of benchmarks from point-cloud classification to\nevent-based vision and audio classification. STREAM demonstrates a powerful\ninductive bias for sparse geometric data by improving the PointMamba baseline\nwhen trained from scratch on the ModelNet40 and ScanObjectNN point cloud\nanalysis datasets. It further achieves, for the first time, 100% test accuracy\non all 11 classes of the DVS128 Gestures dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling sparse and unstructured geometric data, such as point clouds or\nevent-based vision, is a pressing challenge in the field of machine vision.\nRecently, sequence models such as Transformers and state-space models entered\nthe domain of geometric data. These methods require specialized preprocessing\nto create a sequential view of a set of points. Furthermore, prior works\ninvolving sequence models iterate geometric data with either uniform or learned\nstep sizes, implicitly relying on the model to infer the underlying geometric\nstructure. In this work, we propose to encode geometric structure explicitly\ninto the parameterization of a state-space model. State-space models are based\non linear dynamics governed by a one-dimensional variable such as time or a\nspatial coordinate. We exploit this dynamic variable to inject relative\ndifferences of coordinates into the step size of the state-space model. The\nresulting geometric operation computes interactions between all pairs of N\npoints in O(N) steps. Our model deploys the Mamba selective state-space model\nwith a modified CUDA kernel to efficiently map sparse geometric data to modern\nhardware. The resulting sequence model, which we call STREAM, achieves\ncompetitive results on a range of benchmarks from point-cloud classification to\nevent-based vision and audio classification. STREAM demonstrates a powerful\ninductive bias for sparse geometric data by improving the PointMamba baseline\nwhen trained from scratch on the ModelNet40 and ScanObjectNN point cloud\nanalysis datasets. It further achieves, for the first time, 100% test accuracy\non all 11 classes of the DVS128 Gestures dataset."
                },
                "authors": [
                    {
                        "name": "Mark Schöne"
                    },
                    {
                        "name": "Yash Bhisikar"
                    },
                    {
                        "name": "Karan Bania"
                    },
                    {
                        "name": "Khaleelulla Khan Nazeer"
                    },
                    {
                        "name": "Christian Mayr"
                    },
                    {
                        "name": "Anand Subramoney"
                    },
                    {
                        "name": "David Kappel"
                    }
                ],
                "author_detail": {
                    "name": "David Kappel"
                },
                "author": "David Kappel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15371v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15371v4",
                "updated": "2024-11-22T10:40:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    10,
                    40,
                    35,
                    4,
                    327,
                    0
                ],
                "published": "2024-09-19T10:26:42Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    10,
                    26,
                    42,
                    3,
                    263,
                    0
                ],
                "title": "Bone: Block-Affine Adaptation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bone: Block-Affine Adaptation of Large Language Models"
                },
                "summary": "Low-Rank Adaptation (LoRA) has achieved remarkable training results by\nfreezing the original weights and training only low-rank matrices, establishing\nitself as the predominant fine-tuning method for LLMs. In pursuit of\nperformance closer to full-parameter training, a series of LoRA variants have\nemerged, such as LoRA+, PISSA, Olora, and LoRA-GA. This paper introduces a\nnovel PEFT technique distinct from LoRA, called Block-Affine Adaptation (Bone).\nBy dividing the original weights into multiple subspaces that share a single\nmatrix for weight updates, Bone simplifies the process by requiring the\ntrainable matrix to be initialized to zero, eliminating the need for complex\ninitialization as in some LoRA variants. Compared to LoRA, Bone significantly\nreduces memory usage and achieves faster computation. Evaluation of both NLU\nand NLG tasks demonstrates that Bone substantially outperforms LoRA and its\nvariants. Inspired by Pissa, we further proposed the ``Weight Guide'' theory to\nbetter utilize the information from the original weights. By integrating\n``Weight Guide'' with Bone, we developed a new structure called Block-Affine\nTransformation (Bat), and ablation experiments confirmed the effectiveness of\n``Weight Guide''.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has achieved remarkable training results by\nfreezing the original weights and training only low-rank matrices, establishing\nitself as the predominant fine-tuning method for LLMs. In pursuit of\nperformance closer to full-parameter training, a series of LoRA variants have\nemerged, such as LoRA+, PISSA, Olora, and LoRA-GA. This paper introduces a\nnovel PEFT technique distinct from LoRA, called Block-Affine Adaptation (Bone).\nBy dividing the original weights into multiple subspaces that share a single\nmatrix for weight updates, Bone simplifies the process by requiring the\ntrainable matrix to be initialized to zero, eliminating the need for complex\ninitialization as in some LoRA variants. Compared to LoRA, Bone significantly\nreduces memory usage and achieves faster computation. Evaluation of both NLU\nand NLG tasks demonstrates that Bone substantially outperforms LoRA and its\nvariants. Inspired by Pissa, we further proposed the ``Weight Guide'' theory to\nbetter utilize the information from the original weights. By integrating\n``Weight Guide'' with Bone, we developed a new structure called Block-Affine\nTransformation (Bat), and ablation experiments confirmed the effectiveness of\n``Weight Guide''."
                },
                "authors": [
                    {
                        "name": "Jiale Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jiale Kang"
                },
                "author": "Jiale Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15371v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15371v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00144v2",
                "updated": "2024-11-22T10:39:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    10,
                    39,
                    59,
                    4,
                    327,
                    0
                ],
                "published": "2024-10-31T18:43:48Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    43,
                    48,
                    3,
                    305,
                    0
                ],
                "title": "Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Ensembling Gaussian Splatting for Few-Shot Novel View Synthesis"
                },
                "summary": "3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness for\nnovel view synthesis (NVS). However, the 3DGS model tends to overfit when\ntrained with sparse posed views, limiting its generalization ability to novel\nviews. In this paper, we alleviate the overfitting problem, presenting a\nSelf-Ensembling Gaussian Splatting (SE-GS) approach. Our method encompasses a\n$\\mathbf{\\Sigma}$-model and a $\\mathbf{\\Delta}$-model. The\n$\\mathbf{\\Sigma}$-model serves as an ensemble of 3DGS models that generates\nnovel-view images during inference. We achieve the self-ensembling by\nintroducing an uncertainty-aware perturbation strategy at the training state.\nWe complement the $\\mathbf{\\Sigma}$-model with the $\\mathbf{\\Delta}$-model,\nwhich is dynamically perturbed based on the uncertainties of novel-view\nrenderings across different training steps. The perturbation yields diverse\ntemporal samples in the Gaussian parameter space without additional training\ncosts. The geometry of the $\\mathbf{\\Sigma}$-model is regularized by penalizing\ndiscrepancies between the $\\mathbf{\\Sigma}$-model and these temporal samples.\nTherefore, our SE-GS conducts an effective and efficient regularization across\na large number of 3DGS models, resulting in a robust ensemble, the\n$\\mathbf{\\Sigma}$-model. Our experimental results on the LLFF, Mip-NeRF360,\nDTU, and MVImgNet datasets show that our approach improves NVS quality with\nfew-shot training views, outperforming existing state-of-the-art methods. The\ncode is released at: https://sailor-z.github.io/projects/SEGS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness for\nnovel view synthesis (NVS). However, the 3DGS model tends to overfit when\ntrained with sparse posed views, limiting its generalization ability to novel\nviews. In this paper, we alleviate the overfitting problem, presenting a\nSelf-Ensembling Gaussian Splatting (SE-GS) approach. Our method encompasses a\n$\\mathbf{\\Sigma}$-model and a $\\mathbf{\\Delta}$-model. The\n$\\mathbf{\\Sigma}$-model serves as an ensemble of 3DGS models that generates\nnovel-view images during inference. We achieve the self-ensembling by\nintroducing an uncertainty-aware perturbation strategy at the training state.\nWe complement the $\\mathbf{\\Sigma}$-model with the $\\mathbf{\\Delta}$-model,\nwhich is dynamically perturbed based on the uncertainties of novel-view\nrenderings across different training steps. The perturbation yields diverse\ntemporal samples in the Gaussian parameter space without additional training\ncosts. The geometry of the $\\mathbf{\\Sigma}$-model is regularized by penalizing\ndiscrepancies between the $\\mathbf{\\Sigma}$-model and these temporal samples.\nTherefore, our SE-GS conducts an effective and efficient regularization across\na large number of 3DGS models, resulting in a robust ensemble, the\n$\\mathbf{\\Sigma}$-model. Our experimental results on the LLFF, Mip-NeRF360,\nDTU, and MVImgNet datasets show that our approach improves NVS quality with\nfew-shot training views, outperforming existing state-of-the-art methods. The\ncode is released at: https://sailor-z.github.io/projects/SEGS.html."
                },
                "authors": [
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Saqib Javed"
                    },
                    {
                        "name": "Mathieu Salzmann"
                    }
                ],
                "author_detail": {
                    "name": "Mathieu Salzmann"
                },
                "author": "Mathieu Salzmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14842v1",
                "updated": "2024-11-22T10:30:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    10,
                    30,
                    48,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T10:30:48Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    10,
                    30,
                    48,
                    4,
                    327,
                    0
                ],
                "title": "Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large\n  Language Models"
                },
                "summary": "Adversarial audio attacks pose a significant threat to the growing use of\nlarge language models (LLMs) in voice-based human-machine interactions. While\nexisting research has primarily focused on model-specific adversarial methods,\nreal-world applications demand a more generalizable and universal approach to\naudio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks\n(CAA) benchmark including four distinct types of audio attacks, which aims to\nexplore the the vulnerabilities of LLMs to these audio attacks in\nconversational scenarios. To evaluate the robustness of LLMs, we propose three\nevaluation strategies: Standard Evaluation, utilizing traditional metrics to\nquantify model performance under attacks; GPT-4o-Based Evaluation, which\nsimulates real-world conversational complexities; and Human Evaluation,\noffering insights into user perception and trust. We evaluate six\nstate-of-the-art LLMs with voice interaction capabilities, including\nGemini-1.5-Pro, GPT-4o, and others, using three distinct evaluation methods on\nthe CAA benchmark. Our comprehensive analysis reveals the impact of four types\nof audio attacks on the performance of these models, demonstrating that GPT-4o\nexhibits the highest level of resilience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial audio attacks pose a significant threat to the growing use of\nlarge language models (LLMs) in voice-based human-machine interactions. While\nexisting research has primarily focused on model-specific adversarial methods,\nreal-world applications demand a more generalizable and universal approach to\naudio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks\n(CAA) benchmark including four distinct types of audio attacks, which aims to\nexplore the the vulnerabilities of LLMs to these audio attacks in\nconversational scenarios. To evaluate the robustness of LLMs, we propose three\nevaluation strategies: Standard Evaluation, utilizing traditional metrics to\nquantify model performance under attacks; GPT-4o-Based Evaluation, which\nsimulates real-world conversational complexities; and Human Evaluation,\noffering insights into user perception and trust. We evaluate six\nstate-of-the-art LLMs with voice interaction capabilities, including\nGemini-1.5-Pro, GPT-4o, and others, using three distinct evaluation methods on\nthe CAA benchmark. Our comprehensive analysis reveals the impact of four types\nof audio attacks on the performance of these models, demonstrating that GPT-4o\nexhibits the highest level of resilience."
                },
                "authors": [
                    {
                        "name": "Wanqi Yang"
                    },
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Yunchao Wei"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03817v2",
                "updated": "2024-11-22T10:24:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    10,
                    24,
                    44,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-06T10:35:11Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    35,
                    11,
                    2,
                    311,
                    0
                ],
                "title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning"
                },
                "summary": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Zhirui Deng"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Ruibin Xiong"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14833v1",
                "updated": "2024-11-22T10:16:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    10,
                    16,
                    35,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T10:16:35Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    10,
                    16,
                    35,
                    4,
                    327,
                    0
                ],
                "title": "Cell as Point: One-Stage Framework for Efficient Cell Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell as Point: One-Stage Framework for Efficient Cell Tracking"
                },
                "summary": "Cellular activities are dynamic and intricate, playing a crucial role in\nadvancing diagnostic and therapeutic techniques, yet they often require\nsubstantial resources for accurate tracking. Despite recent progress, the\nconventional multi-stage cell tracking approaches not only heavily rely on\ndetection or segmentation results as a prerequisite for the tracking stage,\ndemanding plenty of refined segmentation masks, but are also deteriorated by\nimbalanced and long sequence data, leading to under-learning in training and\nmissing cells in inference procedures. To alleviate the above issues, this\npaper proposes the novel end-to-end CAP framework, which leverages the idea of\nregarding Cell as Point to achieve efficient and stable cell tracking in one\nstage. CAP abandons detection or segmentation stages and simplifies the process\nby exploiting the correlation among the trajectories of cell points to track\ncells jointly, thus reducing the label demand and complexity of the pipeline.\nWith cell point trajectory and visibility to represent cell locations and\nlineage relationships, CAP leverages the key innovations of adaptive\nevent-guided (AEG) sampling for addressing data imbalance in cell division\nevents and the rolling-as-window (RAW) inference method to ensure continuous\ntracking of new cells in the long term. Eliminating the need for a prerequisite\ndetection or segmentation stage, CAP demonstrates strong cell tracking\nperformance while also being 10 to 55 times more efficient than existing\nmethods. The code and models will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cellular activities are dynamic and intricate, playing a crucial role in\nadvancing diagnostic and therapeutic techniques, yet they often require\nsubstantial resources for accurate tracking. Despite recent progress, the\nconventional multi-stage cell tracking approaches not only heavily rely on\ndetection or segmentation results as a prerequisite for the tracking stage,\ndemanding plenty of refined segmentation masks, but are also deteriorated by\nimbalanced and long sequence data, leading to under-learning in training and\nmissing cells in inference procedures. To alleviate the above issues, this\npaper proposes the novel end-to-end CAP framework, which leverages the idea of\nregarding Cell as Point to achieve efficient and stable cell tracking in one\nstage. CAP abandons detection or segmentation stages and simplifies the process\nby exploiting the correlation among the trajectories of cell points to track\ncells jointly, thus reducing the label demand and complexity of the pipeline.\nWith cell point trajectory and visibility to represent cell locations and\nlineage relationships, CAP leverages the key innovations of adaptive\nevent-guided (AEG) sampling for addressing data imbalance in cell division\nevents and the rolling-as-window (RAW) inference method to ensure continuous\ntracking of new cells in the long term. Eliminating the need for a prerequisite\ndetection or segmentation stage, CAP demonstrates strong cell tracking\nperformance while also being 10 to 55 times more efficient than existing\nmethods. The code and models will be released."
                },
                "authors": [
                    {
                        "name": "Yaxuan Song"
                    },
                    {
                        "name": "Jianan Fan"
                    },
                    {
                        "name": "Heng Huang"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Weidong Cai"
                    }
                ],
                "author_detail": {
                    "name": "Weidong Cai"
                },
                "author": "Weidong Cai",
                "arxiv_comment": "17 pages, 8 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15735v3",
                "updated": "2024-11-22T09:44:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    9,
                    44,
                    1,
                    4,
                    327,
                    0
                ],
                "published": "2024-09-24T04:42:43Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    4,
                    42,
                    43,
                    1,
                    268,
                    0
                ],
                "title": "Boosting Cybersecurity Vulnerability Scanning based on LLM-supported\n  Static Application Security Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Cybersecurity Vulnerability Scanning based on LLM-supported\n  Static Application Security Testing"
                },
                "summary": "The current cybersecurity landscape is increasingly complex, with traditional\nStatic Application Security Testing (SAST) tools struggling to capture complex\nand emerging vulnerabilities due to their reliance on rule-based matching.\nMeanwhile, Large Language Models (LLMs) have demonstrated powerful code\nanalysis capabilities, but their static training data and privacy risks limit\ntheir effectiveness. To overcome the limitations of both approaches, we propose\nLSAST, a novel approach that integrates LLMs with SAST scanners to enhance\nvulnerability detection. LSAST leverages a locally hostable LLM, combined with\na state-of-the-art knowledge retrieval system, to provide up-to-date\nvulnerability insights without compromising data privacy. We set a new\nbenchmark for static vulnerability analysis, offering a robust,\nprivacy-conscious solution that bridges the gap between traditional scanners\nand advanced AI-driven analysis. Our evaluation demonstrates that incorporating\nSAST results into LLM analysis significantly improves detection accuracy,\nidentifying vulnerabilities missed by conventional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current cybersecurity landscape is increasingly complex, with traditional\nStatic Application Security Testing (SAST) tools struggling to capture complex\nand emerging vulnerabilities due to their reliance on rule-based matching.\nMeanwhile, Large Language Models (LLMs) have demonstrated powerful code\nanalysis capabilities, but their static training data and privacy risks limit\ntheir effectiveness. To overcome the limitations of both approaches, we propose\nLSAST, a novel approach that integrates LLMs with SAST scanners to enhance\nvulnerability detection. LSAST leverages a locally hostable LLM, combined with\na state-of-the-art knowledge retrieval system, to provide up-to-date\nvulnerability insights without compromising data privacy. We set a new\nbenchmark for static vulnerability analysis, offering a robust,\nprivacy-conscious solution that bridges the gap between traditional scanners\nand advanced AI-driven analysis. Our evaluation demonstrates that incorporating\nSAST results into LLM analysis significantly improves detection accuracy,\nidentifying vulnerabilities missed by conventional methods."
                },
                "authors": [
                    {
                        "name": "Mete Keltek"
                    },
                    {
                        "name": "Rong Hu"
                    },
                    {
                        "name": "Mohammadreza Fani Sani"
                    },
                    {
                        "name": "Ziyue Li"
                    }
                ],
                "author_detail": {
                    "name": "Ziyue Li"
                },
                "author": "Ziyue Li",
                "arxiv_comment": "Under Review of IEEE SaTML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13257v2",
                "updated": "2024-11-22T09:17:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    9,
                    17,
                    8,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-20T12:14:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    14,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Inference for Multiple and Conditional Observers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Multiple and Conditional Observers"
                },
                "summary": "We consider models for inference which involve observers which may have\nmultiple copies, such as in the Sleeping Beauty problem. We establish a\nframework for describing these problems on a probability space satisfying\nKolmogorov's axioms, and this enables the main competing solutions to be\ncompared precisely.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider models for inference which involve observers which may have\nmultiple copies, such as in the Sleeping Beauty problem. We establish a\nframework for describing these problems on a probability space satisfying\nKolmogorov's axioms, and this enables the main competing solutions to be\ncompared precisely."
                },
                "authors": [
                    {
                        "name": "Martin T. Barlow"
                    }
                ],
                "author_detail": {
                    "name": "Martin T. Barlow"
                },
                "author": "Martin T. Barlow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 62A01 Secondary 60A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18808v2",
                "updated": "2024-11-22T09:00:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    9,
                    0,
                    10,
                    4,
                    327,
                    0
                ],
                "published": "2024-10-24T14:55:09Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    55,
                    9,
                    3,
                    298,
                    0
                ],
                "title": "Delving into the Reversal Curse: How Far Can Large Language Models\n  Generalize?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delving into the Reversal Curse: How Far Can Large Language Models\n  Generalize?"
                },
                "summary": "While large language models (LLMs) showcase unprecedented capabilities, they\nalso exhibit certain inherent limitations when facing seemingly trivial tasks.\nA prime example is the recently debated \"reversal curse\", which surfaces when\nmodels, having been trained on the fact \"A is B\", struggle to generalize this\nknowledge to infer that \"B is A\". In this paper, we examine the manifestation\nof the reversal curse across various tasks and delve into both the\ngeneralization abilities and the problem-solving mechanisms of LLMs. This\ninvestigation leads to a series of significant insights: (1) LLMs are able to\ngeneralize to \"B is A\" when both A and B are presented in the context as in the\ncase of a multiple-choice question. (2) This generalization ability is highly\ncorrelated to the structure of the fact \"A is B\" in the training documents. For\nexample, this generalization only applies to biographies structured in \"[Name]\nis [Description]\" but not to \"[Description] is [Name]\". (3) We propose and\nverify the hypothesis that LLMs possess an inherent bias in fact recalling\nduring knowledge application, which explains and underscores the importance of\nthe document structure to successful learning. (4) The negative impact of this\nbias on the downstream performance of LLMs can hardly be mitigated through\ntraining alone. These findings offer a novel perspective on interpreting LLMs'\ngeneralization through their intrinsic mechanisms and provide insights for\ndeveloping more effective learning methods. Our code and data are available at\nhttps://github.com/alibaba/thinking_bias.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) showcase unprecedented capabilities, they\nalso exhibit certain inherent limitations when facing seemingly trivial tasks.\nA prime example is the recently debated \"reversal curse\", which surfaces when\nmodels, having been trained on the fact \"A is B\", struggle to generalize this\nknowledge to infer that \"B is A\". In this paper, we examine the manifestation\nof the reversal curse across various tasks and delve into both the\ngeneralization abilities and the problem-solving mechanisms of LLMs. This\ninvestigation leads to a series of significant insights: (1) LLMs are able to\ngeneralize to \"B is A\" when both A and B are presented in the context as in the\ncase of a multiple-choice question. (2) This generalization ability is highly\ncorrelated to the structure of the fact \"A is B\" in the training documents. For\nexample, this generalization only applies to biographies structured in \"[Name]\nis [Description]\" but not to \"[Description] is [Name]\". (3) We propose and\nverify the hypothesis that LLMs possess an inherent bias in fact recalling\nduring knowledge application, which explains and underscores the importance of\nthe document structure to successful learning. (4) The negative impact of this\nbias on the downstream performance of LLMs can hardly be mitigated through\ntraining alone. These findings offer a novel perspective on interpreting LLMs'\ngeneralization through their intrinsic mechanisms and provide insights for\ndeveloping more effective learning methods. Our code and data are available at\nhttps://github.com/alibaba/thinking_bias.git."
                },
                "authors": [
                    {
                        "name": "Zhengkai Lin"
                    },
                    {
                        "name": "Zhihang Fu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Binbin Lin"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Accepted at NeurIPS 2024. Our code and data are available at\n  https://github.com/alibaba/thinking_bias.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10083v2",
                "updated": "2024-11-22T08:57:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    57,
                    42,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-15T10:01:52Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    10,
                    1,
                    52,
                    4,
                    320,
                    0
                ],
                "title": "Xmodel-1.5: An 1B-scale Multilingual LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xmodel-1.5: An 1B-scale Multilingual LLM"
                },
                "summary": "We introduce Xmodel-1.5, a 1-billion-parameter multilingual large language\nmodel pretrained on 2 trillion tokens, designed for balanced performance and\nscalability. Unlike most large models that use the BPE tokenizer, Xmodel-1.5\nemploys a custom unigram tokenizer with 65,280 tokens, optimizing both\nefficiency and accuracy. The model delivers competitive results across multiple\nlanguages, including Thai, Arabic, French, Chinese, and English, outperforming\nAlibaba's PolyLM-1.7B on respective evaluation datasets. Xmodel-1.5 excels in\nbenchmarks like mMMLU and PIQA, and achieves state-of-the-art results in Thai.\nTo support low-resource language research, we release Xdata_Thai, a\nThai-specific evaluation dataset featuring unique linguistic challenges such as\ngendered particles and idioms. While the model demonstrates strong performance,\nthere is still room for improvement in handling culturally specific nuances. We\nhope this work contributes to advancements in multilingual AI research. Models\nand code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM-1.5",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Xmodel-1.5, a 1-billion-parameter multilingual large language\nmodel pretrained on 2 trillion tokens, designed for balanced performance and\nscalability. Unlike most large models that use the BPE tokenizer, Xmodel-1.5\nemploys a custom unigram tokenizer with 65,280 tokens, optimizing both\nefficiency and accuracy. The model delivers competitive results across multiple\nlanguages, including Thai, Arabic, French, Chinese, and English, outperforming\nAlibaba's PolyLM-1.7B on respective evaluation datasets. Xmodel-1.5 excels in\nbenchmarks like mMMLU and PIQA, and achieves state-of-the-art results in Thai.\nTo support low-resource language research, we release Xdata_Thai, a\nThai-specific evaluation dataset featuring unique linguistic challenges such as\ngendered particles and idioms. While the model demonstrates strong performance,\nthere is still room for improvement in handling culturally specific nuances. We\nhope this work contributes to advancements in multilingual AI research. Models\nand code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM-1.5"
                },
                "authors": [
                    {
                        "name": "Wang Qun"
                    },
                    {
                        "name": "Liu Yang"
                    },
                    {
                        "name": "Lin Qingquan"
                    },
                    {
                        "name": "Jiang Ling"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Ling"
                },
                "author": "Jiang Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06387v2",
                "updated": "2024-11-22T08:54:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    54,
                    17,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-10T08:11:05Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    11,
                    5,
                    6,
                    315,
                    0
                ],
                "title": "Self-Training Meets Consistency: Improving LLMs' Reasoning With\n  Consistency-Driven Rationale Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Training Meets Consistency: Improving LLMs' Reasoning With\n  Consistency-Driven Rationale Evaluation"
                },
                "summary": "Self-training approach for large language models (LLMs) improves reasoning\nabilities by training the models on their self-generated rationales. Previous\napproaches have labeled rationales that produce correct answers for a given\nquestion as appropriate for training. However, a single measure risks\nmisjudging rationale quality, leading the models to learn flawed reasoning\npatterns. To address this issue, we propose CREST (Consistency-driven Rationale\nEvaluation for Self-Training), a self-training framework that further evaluates\neach rationale through follow-up questions and leverages this evaluation to\nguide its training. Specifically, we introduce two methods: (1) filtering out\nrationales that frequently result in incorrect answers on follow-up questions\nand (2) preference learning based on mixed preferences from rationale\nevaluation results of both original and follow-up questions. Experiments on\nthree question-answering datasets using open LLMs show that CREST not only\nimproves the logical robustness and correctness of rationales but also improves\nreasoning abilities compared to previous self-training approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-training approach for large language models (LLMs) improves reasoning\nabilities by training the models on their self-generated rationales. Previous\napproaches have labeled rationales that produce correct answers for a given\nquestion as appropriate for training. However, a single measure risks\nmisjudging rationale quality, leading the models to learn flawed reasoning\npatterns. To address this issue, we propose CREST (Consistency-driven Rationale\nEvaluation for Self-Training), a self-training framework that further evaluates\neach rationale through follow-up questions and leverages this evaluation to\nguide its training. Specifically, we introduce two methods: (1) filtering out\nrationales that frequently result in incorrect answers on follow-up questions\nand (2) preference learning based on mixed preferences from rationale\nevaluation results of both original and follow-up questions. Experiments on\nthree question-answering datasets using open LLMs show that CREST not only\nimproves the logical robustness and correctness of rationales but also improves\nreasoning abilities compared to previous self-training approaches."
                },
                "authors": [
                    {
                        "name": "Jaehyeok Lee"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "JinYeong Bak"
                    }
                ],
                "author_detail": {
                    "name": "JinYeong Bak"
                },
                "author": "JinYeong Bak",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14798v1",
                "updated": "2024-11-22T08:49:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    49,
                    8,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T08:49:08Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    49,
                    8,
                    4,
                    327,
                    0
                ],
                "title": "Facial Features Matter: a Dynamic Watermark based Proactive Deepfake\n  Detection Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facial Features Matter: a Dynamic Watermark based Proactive Deepfake\n  Detection Approach"
                },
                "summary": "Current passive deepfake face-swapping detection methods encounter\nsignificance bottlenecks in model generalization capabilities. Meanwhile,\nproactive detection methods often use fixed watermarks which lack a close\nrelationship with the content they protect and are vulnerable to security\nrisks. Dynamic watermarks based on facial features offer a promising solution,\nas these features provide unique identifiers. Therefore, this paper proposes a\nFacial Feature-based Proactive deepfake detection method (FaceProtect), which\nutilizes changes in facial characteristics during deepfake manipulation as a\nnovel detection mechanism. We introduce a GAN-based One-way Dynamic Watermark\nGenerating Mechanism (GODWGM) that uses 128-dimensional facial feature vectors\nas inputs. This method creates irreversible mappings from facial features to\nwatermarks, enhancing protection against various reverse inference attacks.\nAdditionally, we propose a Watermark-based Verification Strategy (WVS) that\ncombines steganography with GODWGM, allowing simultaneous transmission of the\nbenchmark watermark representing facial features within the image. Experimental\nresults demonstrate that our proposed method maintains exceptional detection\nperformance and exhibits high practicality on images altered by various\ndeepfake techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current passive deepfake face-swapping detection methods encounter\nsignificance bottlenecks in model generalization capabilities. Meanwhile,\nproactive detection methods often use fixed watermarks which lack a close\nrelationship with the content they protect and are vulnerable to security\nrisks. Dynamic watermarks based on facial features offer a promising solution,\nas these features provide unique identifiers. Therefore, this paper proposes a\nFacial Feature-based Proactive deepfake detection method (FaceProtect), which\nutilizes changes in facial characteristics during deepfake manipulation as a\nnovel detection mechanism. We introduce a GAN-based One-way Dynamic Watermark\nGenerating Mechanism (GODWGM) that uses 128-dimensional facial feature vectors\nas inputs. This method creates irreversible mappings from facial features to\nwatermarks, enhancing protection against various reverse inference attacks.\nAdditionally, we propose a Watermark-based Verification Strategy (WVS) that\ncombines steganography with GODWGM, allowing simultaneous transmission of the\nbenchmark watermark representing facial features within the image. Experimental\nresults demonstrate that our proposed method maintains exceptional detection\nperformance and exhibits high practicality on images altered by various\ndeepfake techniques."
                },
                "authors": [
                    {
                        "name": "Shulin Lan"
                    },
                    {
                        "name": "Kanlin Liu"
                    },
                    {
                        "name": "Yazhou Zhao"
                    },
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Yingchao Wang"
                    },
                    {
                        "name": "Xingshan Yao"
                    },
                    {
                        "name": "Liehuang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liehuang Zhu"
                },
                "author": "Liehuang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05607v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05607v3",
                "updated": "2024-11-22T08:41:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    41,
                    36,
                    4,
                    327,
                    0
                ],
                "published": "2024-06-09T01:27:57Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    1,
                    27,
                    57,
                    6,
                    161,
                    0
                ],
                "title": "HAL-based Plugin Estimation of the Causal Dose-Response Curve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAL-based Plugin Estimation of the Causal Dose-Response Curve"
                },
                "summary": "Estimating and obtaining reliable inference for the marginally adjusted\ncausal dose-response curve for continuous treatments without relying on\nparametric assumptions is a well-known statistical challenge. Parametric models\nrisk introducing significant bias through model misspecification, compromising\nthe accurate representation of the underlying data and dose-response\nrelationship. On the other hand, nonparametric models face difficulties as the\ndose-response curve is not pathwise differentiable, preventing consistent\nestimation at standard rates. The Highly Adaptive Lasso (HAL) maximum\nlikelihood estimator offers a promising approach to this issue. In this paper,\nwe introduce a HAL-based plug-in estimator for the causal dose-response curve\nand assess its empirical performance against other estimators. Through\ncomprehensive simulations, we evaluate the accuracy of the estimation and the\nquality of the inference, particularly in terms of coverage, using robust\nstandard error estimators. Our results demonstrate the finite-sample\neffectiveness of the HAL-based estimator, utilizing an undersmoothed and\nsmoothness-adaptive fit for the conditional outcome model. Additionally, the\nsimulations reveal that the HAL-based estimator consistently outperforms\nexisting methods for estimating the causal dose-response curve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating and obtaining reliable inference for the marginally adjusted\ncausal dose-response curve for continuous treatments without relying on\nparametric assumptions is a well-known statistical challenge. Parametric models\nrisk introducing significant bias through model misspecification, compromising\nthe accurate representation of the underlying data and dose-response\nrelationship. On the other hand, nonparametric models face difficulties as the\ndose-response curve is not pathwise differentiable, preventing consistent\nestimation at standard rates. The Highly Adaptive Lasso (HAL) maximum\nlikelihood estimator offers a promising approach to this issue. In this paper,\nwe introduce a HAL-based plug-in estimator for the causal dose-response curve\nand assess its empirical performance against other estimators. Through\ncomprehensive simulations, we evaluate the accuracy of the estimation and the\nquality of the inference, particularly in terms of coverage, using robust\nstandard error estimators. Our results demonstrate the finite-sample\neffectiveness of the HAL-based estimator, utilizing an undersmoothed and\nsmoothness-adaptive fit for the conditional outcome model. Additionally, the\nsimulations reveal that the HAL-based estimator consistently outperforms\nexisting methods for estimating the causal dose-response curve."
                },
                "authors": [
                    {
                        "name": "Junming Shi"
                    },
                    {
                        "name": "Wenxin Zhang"
                    },
                    {
                        "name": "Alan E. Hubbard"
                    },
                    {
                        "name": "Mark van der Laan"
                    }
                ],
                "author_detail": {
                    "name": "Mark van der Laan"
                },
                "author": "Mark van der Laan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05607v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05607v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14795v1",
                "updated": "2024-11-22T08:35:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    35,
                    35,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T08:35:35Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    35,
                    35,
                    4,
                    327,
                    0
                ],
                "title": "De-biased Multimodal Electrocardiogram Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "De-biased Multimodal Electrocardiogram Analysis"
                },
                "summary": "Multimodal large language models (MLLMs) are increasingly being applied in\nthe medical field, particularly in medical imaging. However, developing MLLMs\nfor ECG signals, which are crucial in clinical settings, has been a significant\nchallenge beyond medical imaging. Previous studies have attempted to address\nthis by converting ECGs into several text tags using an external classifier in\na training-free manner. However, this approach significantly compresses the\ninformation in ECGs and underutilizes the reasoning capabilities of LLMs. In\nthis work, we directly feed the embeddings of ECGs into the LLM through a\nprojection layer, retaining more information about ECGs and better leveraging\nthe reasoning abilities of LLMs. Our method can also effectively handle a\ncommon situation in clinical practice where it is necessary to compare two ECGs\ntaken at different times. Recent studies found that MLLMs may rely solely on\ntext input to provide answers, ignoring inputs from other modalities. We\nanalyzed this phenomenon from a causal perspective in the context of ECG MLLMs\nand discovered that the confounder, severity of illness, introduces a spurious\ncorrelation between the question and answer, leading the model to rely on this\nspurious correlation and ignore the ECG input. Such models do not comprehend\nthe ECG input and perform poorly in adversarial tests where different\nexpressions of the same question are used in the training and testing sets. We\ndesigned a de-biased pre-training method to eliminate the confounder's effect\naccording to the theory of backdoor adjustment. Our model performed well on the\nECG-QA task under adversarial testing and demonstrated zero-shot capabilities.\nAn interesting random ECG test further validated that our model effectively\nunderstands and utilizes the input ECG signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) are increasingly being applied in\nthe medical field, particularly in medical imaging. However, developing MLLMs\nfor ECG signals, which are crucial in clinical settings, has been a significant\nchallenge beyond medical imaging. Previous studies have attempted to address\nthis by converting ECGs into several text tags using an external classifier in\na training-free manner. However, this approach significantly compresses the\ninformation in ECGs and underutilizes the reasoning capabilities of LLMs. In\nthis work, we directly feed the embeddings of ECGs into the LLM through a\nprojection layer, retaining more information about ECGs and better leveraging\nthe reasoning abilities of LLMs. Our method can also effectively handle a\ncommon situation in clinical practice where it is necessary to compare two ECGs\ntaken at different times. Recent studies found that MLLMs may rely solely on\ntext input to provide answers, ignoring inputs from other modalities. We\nanalyzed this phenomenon from a causal perspective in the context of ECG MLLMs\nand discovered that the confounder, severity of illness, introduces a spurious\ncorrelation between the question and answer, leading the model to rely on this\nspurious correlation and ignore the ECG input. Such models do not comprehend\nthe ECG input and perform poorly in adversarial tests where different\nexpressions of the same question are used in the training and testing sets. We\ndesigned a de-biased pre-training method to eliminate the confounder's effect\naccording to the theory of backdoor adjustment. Our model performed well on the\nECG-QA task under adversarial testing and demonstrated zero-shot capabilities.\nAn interesting random ECG test further validated that our model effectively\nunderstands and utilizes the input ECG signal."
                },
                "authors": [
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Ziyu Li"
                    },
                    {
                        "name": "Yiheng Mao"
                    },
                    {
                        "name": "Ziyi Liu"
                    },
                    {
                        "name": "Zhoujian Sun"
                    },
                    {
                        "name": "Zhengxing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Zhengxing Huang"
                },
                "author": "Zhengxing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14790v1",
                "updated": "2024-11-22T08:21:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    21,
                    3,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T08:21:03Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    21,
                    3,
                    4,
                    327,
                    0
                ],
                "title": "KBAda: Efficient Self Adaptation on Specific Knowledge Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KBAda: Efficient Self Adaptation on Specific Knowledge Bases"
                },
                "summary": "Humans can utilize techniques to quickly acquire knowledge from specific\nmaterials in advance, such as creating self-assessment questions, enabling us\nto achieving related tasks more efficiently. In contrast, large language models\n(LLMs) usually relies on retrieval-augmented generation to exploit knowledge\nmaterials in an instant manner, or requires external signals such as human\npreference data and stronger LLM annotations to conduct knowledge adaptation.\nTo unleash the self-learning potential of LLMs, we propose KBAda, an approach\ndesigned for efficient adaptation to downstream tasks involving knowledge\nbases. Our method utilizes iterative training with self-annotated data such as\nQ&A pairs and revision suggestions, enabling the model to grasp the knowledge\ncontent efficiently. Experimental results on multiple datasets demonstrate the\neffectiveness of our approach, significantly boosting model performance in\ndownstream tasks that require specific knowledge at a low cost. Notably, our\napproach achieves over 90% of the performance improvement that can be obtained\nby using GPT-4-turbo annotation, while relying entirely on self-supervision. We\nrelease our experimental data, models, and process analyses to the community\nfor further exploration (https://github.com/thunlp/KBAda).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can utilize techniques to quickly acquire knowledge from specific\nmaterials in advance, such as creating self-assessment questions, enabling us\nto achieving related tasks more efficiently. In contrast, large language models\n(LLMs) usually relies on retrieval-augmented generation to exploit knowledge\nmaterials in an instant manner, or requires external signals such as human\npreference data and stronger LLM annotations to conduct knowledge adaptation.\nTo unleash the self-learning potential of LLMs, we propose KBAda, an approach\ndesigned for efficient adaptation to downstream tasks involving knowledge\nbases. Our method utilizes iterative training with self-annotated data such as\nQ&A pairs and revision suggestions, enabling the model to grasp the knowledge\ncontent efficiently. Experimental results on multiple datasets demonstrate the\neffectiveness of our approach, significantly boosting model performance in\ndownstream tasks that require specific knowledge at a low cost. Notably, our\napproach achieves over 90% of the performance improvement that can be obtained\nby using GPT-4-turbo annotation, while relying entirely on self-supervision. We\nrelease our experimental data, models, and process analyses to the community\nfor further exploration (https://github.com/thunlp/KBAda)."
                },
                "authors": [
                    {
                        "name": "Zheni Zeng"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10499v2",
                "updated": "2024-11-22T08:19:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    19,
                    48,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-15T11:02:23Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    11,
                    2,
                    23,
                    4,
                    320,
                    0
                ],
                "title": "FitDiT: Advancing the Authentic Garment Details for High-fidelity\n  Virtual Try-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FitDiT: Advancing the Authentic Garment Details for High-fidelity\n  Virtual Try-on"
                },
                "summary": "Although image-based virtual try-on has made considerable progress, emerging\napproaches still encounter challenges in producing high-fidelity and robust\nfitting images across diverse scenarios. These methods often struggle with\nissues such as texture-aware maintenance and size-aware fitting, which hinder\ntheir overall effectiveness. To address these limitations, we propose a novel\ngarment perception enhancement technique, termed FitDiT, designed for\nhigh-fidelity virtual try-on using Diffusion Transformers (DiT) allocating more\nparameters and attention to high-resolution features. First, to further improve\ntexture-aware maintenance, we introduce a garment texture extractor that\nincorporates garment priors evolution to fine-tune garment feature,\nfacilitating to better capture rich details such as stripes, patterns, and\ntext. Additionally, we introduce frequency-domain learning by customizing a\nfrequency distance loss to enhance high-frequency garment details. To tackle\nthe size-aware fitting issue, we employ a dilated-relaxed mask strategy that\nadapts to the correct length of garments, preventing the generation of garments\nthat fill the entire mask area during cross-category try-on. Equipped with the\nabove design, FitDiT surpasses all baselines in both qualitative and\nquantitative evaluations. It excels in producing well-fitting garments with\nphotorealistic and intricate details, while also achieving competitive\ninference times of 4.57 seconds for a single 1024x768 image after DiT structure\nslimming, outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although image-based virtual try-on has made considerable progress, emerging\napproaches still encounter challenges in producing high-fidelity and robust\nfitting images across diverse scenarios. These methods often struggle with\nissues such as texture-aware maintenance and size-aware fitting, which hinder\ntheir overall effectiveness. To address these limitations, we propose a novel\ngarment perception enhancement technique, termed FitDiT, designed for\nhigh-fidelity virtual try-on using Diffusion Transformers (DiT) allocating more\nparameters and attention to high-resolution features. First, to further improve\ntexture-aware maintenance, we introduce a garment texture extractor that\nincorporates garment priors evolution to fine-tune garment feature,\nfacilitating to better capture rich details such as stripes, patterns, and\ntext. Additionally, we introduce frequency-domain learning by customizing a\nfrequency distance loss to enhance high-frequency garment details. To tackle\nthe size-aware fitting issue, we employ a dilated-relaxed mask strategy that\nadapts to the correct length of garments, preventing the generation of garments\nthat fill the entire mask area during cross-category try-on. Equipped with the\nabove design, FitDiT surpasses all baselines in both qualitative and\nquantitative evaluations. It excels in producing well-fitting garments with\nphotorealistic and intricate details, while also achieving competitive\ninference times of 4.57 seconds for a single 1024x768 image after DiT structure\nslimming, outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Boyuan Jiang"
                    },
                    {
                        "name": "Xiaobin Hu"
                    },
                    {
                        "name": "Donghao Luo"
                    },
                    {
                        "name": "Qingdong He"
                    },
                    {
                        "name": "Chengming Xu"
                    },
                    {
                        "name": "Jinlong Peng"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Chengjie Wang"
                    },
                    {
                        "name": "Yunsheng Wu"
                    },
                    {
                        "name": "Yanwei Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Fu"
                },
                "author": "Yanwei Fu",
                "arxiv_comment": "Project page: https://byjiang.com/FitDiT/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14789v1",
                "updated": "2024-11-22T08:17:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    17,
                    46,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T08:17:46Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    17,
                    46,
                    4,
                    327,
                    0
                ],
                "title": "Simplifying CLIP: Unleashing the Power of Large-Scale Models on\n  Consumer-level Computers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplifying CLIP: Unleashing the Power of Large-Scale Models on\n  Consumer-level Computers"
                },
                "summary": "Contrastive Language-Image Pre-training (CLIP) has attracted a surge of\nattention for its superior zero-shot performance and excellent transferability\nto downstream tasks. However, training such large-scale models usually requires\nsubstantial computation and storage, which poses barriers for general users\nwith consumer-level computers. Motivated by this observation, in this paper we\ninvestigate how to achieve competitive performance on only one Nvidia RTX3090\nGPU and with one terabyte for storing dataset. On one hand, we simplify the\ntransformer block structure and combine Weight Inheritance with multi-stage\nKnowledge Distillation (WIKD), thereby reducing the parameters and improving\nthe inference speed during training along with deployment. On the other hand,\nconfronted with the convergence challenge posed by small dataset, we generate\nsynthetic captions for each sample as data augmentation, and devise a novel\nPair Matching (PM) loss to fully exploit the distinguishment among positive and\nnegative image-text pairs. Extensive experiments demonstrate that our model can\nachieve a new state-of-the-art datascale-parameter-accuracy tradeoff, which\ncould further popularize the CLIP model in the related research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Language-Image Pre-training (CLIP) has attracted a surge of\nattention for its superior zero-shot performance and excellent transferability\nto downstream tasks. However, training such large-scale models usually requires\nsubstantial computation and storage, which poses barriers for general users\nwith consumer-level computers. Motivated by this observation, in this paper we\ninvestigate how to achieve competitive performance on only one Nvidia RTX3090\nGPU and with one terabyte for storing dataset. On one hand, we simplify the\ntransformer block structure and combine Weight Inheritance with multi-stage\nKnowledge Distillation (WIKD), thereby reducing the parameters and improving\nthe inference speed during training along with deployment. On the other hand,\nconfronted with the convergence challenge posed by small dataset, we generate\nsynthetic captions for each sample as data augmentation, and devise a novel\nPair Matching (PM) loss to fully exploit the distinguishment among positive and\nnegative image-text pairs. Extensive experiments demonstrate that our model can\nachieve a new state-of-the-art datascale-parameter-accuracy tradeoff, which\ncould further popularize the CLIP model in the related research community."
                },
                "authors": [
                    {
                        "name": "Hongbo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hongbo Liu"
                },
                "author": "Hongbo Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14786v1",
                "updated": "2024-11-22T08:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    6,
                    32,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T08:06:32Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    6,
                    32,
                    4,
                    327,
                    0
                ],
                "title": "FastGrasp: Efficient Grasp Synthesis with Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastGrasp: Efficient Grasp Synthesis with Diffusion"
                },
                "summary": "Effectively modeling the interaction between human hands and objects is\nchallenging due to the complex physical constraints and the requirement for\nhigh generation efficiency in applications. Prior approaches often employ\ncomputationally intensive two-stage approaches, which first generate an\nintermediate representation, such as contact maps, followed by an iterative\noptimization procedure that updates hand meshes to capture the hand-object\nrelation. However, due to the high computation complexity during the\noptimization stage, such strategies often suffer from low efficiency in\ninference. To address this limitation, this work introduces a novel\ndiffusion-model-based approach that generates the grasping pose in a one-stage\nmanner. This allows us to significantly improve generation speed and the\ndiversity of generated hand poses. In particular, we develop a Latent Diffusion\nModel with an Adaptation Module for object-conditioned hand pose generation and\na contact-aware loss to enforce the physical constraints between hands and\nobjects. Extensive experiments demonstrate that our method achieves faster\ninference, higher diversity, and superior pose quality than state-of-the-art\napproaches. Code is available at\n\\href{https://github.com/wuxiaofei01/FastGrasp}{https://github.com/wuxiaofei01/FastGrasp.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively modeling the interaction between human hands and objects is\nchallenging due to the complex physical constraints and the requirement for\nhigh generation efficiency in applications. Prior approaches often employ\ncomputationally intensive two-stage approaches, which first generate an\nintermediate representation, such as contact maps, followed by an iterative\noptimization procedure that updates hand meshes to capture the hand-object\nrelation. However, due to the high computation complexity during the\noptimization stage, such strategies often suffer from low efficiency in\ninference. To address this limitation, this work introduces a novel\ndiffusion-model-based approach that generates the grasping pose in a one-stage\nmanner. This allows us to significantly improve generation speed and the\ndiversity of generated hand poses. In particular, we develop a Latent Diffusion\nModel with an Adaptation Module for object-conditioned hand pose generation and\na contact-aware loss to enforce the physical constraints between hands and\nobjects. Extensive experiments demonstrate that our method achieves faster\ninference, higher diversity, and superior pose quality than state-of-the-art\napproaches. Code is available at\n\\href{https://github.com/wuxiaofei01/FastGrasp}{https://github.com/wuxiaofei01/FastGrasp.}"
                },
                "authors": [
                    {
                        "name": "Xiaofei Wu"
                    },
                    {
                        "name": "Tao Liu"
                    },
                    {
                        "name": "Caoji Li"
                    },
                    {
                        "name": "Yuexin Ma"
                    },
                    {
                        "name": "Yujiao Shi"
                    },
                    {
                        "name": "Xuming He"
                    }
                ],
                "author_detail": {
                    "name": "Xuming He"
                },
                "author": "Xuming He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14596v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14596v4",
                "updated": "2024-11-22T07:43:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    7,
                    43,
                    21,
                    4,
                    327,
                    0
                ],
                "published": "2024-06-20T17:45:02Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    45,
                    2,
                    3,
                    172,
                    0
                ],
                "title": "VLM Agents Generate Their Own Memories: Distilling Experience into\n  Embodied Programs of Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM Agents Generate Their Own Memories: Distilling Experience into\n  Embodied Programs of Thought"
                },
                "summary": "Large-scale generative language and vision-language models (LLMs and VLMs)\nexcel in few-shot in-context learning for decision making and instruction\nfollowing. However, they require high-quality exemplar demonstrations in their\ncontext window. In this work, we ask: Can LLMs and VLMs generate their own\nexamples from generic, sub-optimal demonstrations? We propose In-Context\nAbstraction Learning (ICAL), a method that builds a memory of multimodal\nexperience from sub-optimal demonstrations and human feedback. Given a task\ndemonstration that may contain inefficiencies or mistakes, a VLM abstracts the\ntrajectory into a generalized program of thoughts by correcting inefficient\nactions and annotating cognitive abstractions: causal relationships, object\nstate changes, temporal subgoals, and task-relevant visual elements. These\nprograms of thought are iteratively improved through human feedback while the\nagent executes the trajectory in a similar environment. The resulting examples\nsignificantly improve decision-making in retrieval-augmented LLM and VLM\nagents. Moreover, as the agent's library of examples grows, it becomes more\nefficient, relying less on human feedback and requiring fewer environment\ninteractions per demonstration. Our ICAL agent surpasses the SOTA in\ndialogue-based instruction following in TEACh, multimodal web agents in\nVisualWebArena, and action anticipation in Ego4D. In TEACh, we achieve a 12.6%\nimprovement in goal-condition success. In VisualWebArena, our task success rate\nimproves over few-shot GPT4V. In Ego4D action forecasting, we improve over\nfew-shot GPT-4V and remain competitive with supervised models. We show\nfinetuning our retrieval-augmented in-context agent yields additional\nimprovements. Our approach significantly reduces reliance on manual prompt\nengineering and consistently outperforms in-context learning from action plans\nthat lack such programs of thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale generative language and vision-language models (LLMs and VLMs)\nexcel in few-shot in-context learning for decision making and instruction\nfollowing. However, they require high-quality exemplar demonstrations in their\ncontext window. In this work, we ask: Can LLMs and VLMs generate their own\nexamples from generic, sub-optimal demonstrations? We propose In-Context\nAbstraction Learning (ICAL), a method that builds a memory of multimodal\nexperience from sub-optimal demonstrations and human feedback. Given a task\ndemonstration that may contain inefficiencies or mistakes, a VLM abstracts the\ntrajectory into a generalized program of thoughts by correcting inefficient\nactions and annotating cognitive abstractions: causal relationships, object\nstate changes, temporal subgoals, and task-relevant visual elements. These\nprograms of thought are iteratively improved through human feedback while the\nagent executes the trajectory in a similar environment. The resulting examples\nsignificantly improve decision-making in retrieval-augmented LLM and VLM\nagents. Moreover, as the agent's library of examples grows, it becomes more\nefficient, relying less on human feedback and requiring fewer environment\ninteractions per demonstration. Our ICAL agent surpasses the SOTA in\ndialogue-based instruction following in TEACh, multimodal web agents in\nVisualWebArena, and action anticipation in Ego4D. In TEACh, we achieve a 12.6%\nimprovement in goal-condition success. In VisualWebArena, our task success rate\nimproves over few-shot GPT4V. In Ego4D action forecasting, we improve over\nfew-shot GPT-4V and remain competitive with supervised models. We show\nfinetuning our retrieval-augmented in-context agent yields additional\nimprovements. Our approach significantly reduces reliance on manual prompt\nengineering and consistently outperforms in-context learning from action plans\nthat lack such programs of thought."
                },
                "authors": [
                    {
                        "name": "Gabriel Sarch"
                    },
                    {
                        "name": "Lawrence Jang"
                    },
                    {
                        "name": "Michael J. Tarr"
                    },
                    {
                        "name": "William W. Cohen"
                    },
                    {
                        "name": "Kenneth Marino"
                    },
                    {
                        "name": "Katerina Fragkiadaki"
                    }
                ],
                "author_detail": {
                    "name": "Katerina Fragkiadaki"
                },
                "author": "Katerina Fragkiadaki",
                "arxiv_comment": "Project website: http://ical-learning.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14596v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14596v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.03134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.03134v3",
                "updated": "2024-11-22T07:16:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    7,
                    16,
                    55,
                    4,
                    327,
                    0
                ],
                "published": "2023-05-04T20:29:48Z",
                "published_parsed": [
                    2023,
                    5,
                    4,
                    20,
                    29,
                    48,
                    3,
                    124,
                    0
                ],
                "title": "Debiased Inference for Dynamic Nonlinear Panels with Multi-dimensional\n  Heterogeneities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debiased Inference for Dynamic Nonlinear Panels with Multi-dimensional\n  Heterogeneities"
                },
                "summary": "We introduce a generic class of dynamic nonlinear heterogeneous parameter\nmodels that incorporate individual and time effects in both the intercept and\nslope. To address the incidental parameter problem inherent in this class of\nmodels, we develop an analytical bias correction procedure to construct a\nbias-corrected likelihood. The resulting maximum likelihood estimators are\nautomatically bias-corrected. Moreover, likelihood-based tests statistics --\nincluding likelihood-ratio, Lagrange-multiplier, and Wald tests -- follow the\nlimiting chi-square distribution under the null hypothesis. Simulations\ndemonstrate the effectiveness of the proposed correction method, and an\nempirical application on the labor force participation of single mothers\nunderscores its practical importance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a generic class of dynamic nonlinear heterogeneous parameter\nmodels that incorporate individual and time effects in both the intercept and\nslope. To address the incidental parameter problem inherent in this class of\nmodels, we develop an analytical bias correction procedure to construct a\nbias-corrected likelihood. The resulting maximum likelihood estimators are\nautomatically bias-corrected. Moreover, likelihood-based tests statistics --\nincluding likelihood-ratio, Lagrange-multiplier, and Wald tests -- follow the\nlimiting chi-square distribution under the null hypothesis. Simulations\ndemonstrate the effectiveness of the proposed correction method, and an\nempirical application on the labor force participation of single mothers\nunderscores its practical importance."
                },
                "authors": [
                    {
                        "name": "Xuan Leng"
                    },
                    {
                        "name": "Jiaming Mao"
                    },
                    {
                        "name": "Yutao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Sun"
                },
                "author": "Yutao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.03134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.03134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13909v2",
                "updated": "2024-11-22T07:03:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    7,
                    3,
                    11,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-21T07:47:27Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    47,
                    27,
                    3,
                    326,
                    0
                ],
                "title": "Panther: Illuminate the Sight of Multimodal LLMs with Instruction-Guided\n  Visual Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panther: Illuminate the Sight of Multimodal LLMs with Instruction-Guided\n  Visual Prompts"
                },
                "summary": "Multimodal large language models (MLLMs) are closing the gap to human visual\nperception capability rapidly, while, still lag behind on attending to subtle\nimages details or locating small objects precisely, etc. Common schemes to\ntackle these issues include deploying multiple vision encoders or operating on\noriginal high-resolution images. Few studies have concentrated on taking the\ntextual instruction into improving visual representation, resulting in losing\nfocus in some vision-centric tasks, a phenomenon we herein termed as Amblyopia.\nIn this work, we introduce Panther, a MLLM that closely adheres to user\ninstruction and locates targets of interests precisely, with the finesse of a\nblack panther. Specifically, Panther comprises three integral components:\nPanther-VE, Panther-Bridge, and Panther-Decoder. Panther-VE integrates user\ninstruction information at the early stages of the vision encoder, thereby\nextracting the most relevant and useful visual representations. The\nPanther-Bridge module, equipped with powerful filtering capabilities,\nsignificantly reduces redundant visual information, leading to a substantial\nsavings in training costs. The Panther-Decoder is versatile and can be employed\nwith any decoder-only architecture of LLMs without discrimination. Experimental\nresults, particularly on vision-centric benchmarks, have demonstrated the\neffectiveness of Panther.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) are closing the gap to human visual\nperception capability rapidly, while, still lag behind on attending to subtle\nimages details or locating small objects precisely, etc. Common schemes to\ntackle these issues include deploying multiple vision encoders or operating on\noriginal high-resolution images. Few studies have concentrated on taking the\ntextual instruction into improving visual representation, resulting in losing\nfocus in some vision-centric tasks, a phenomenon we herein termed as Amblyopia.\nIn this work, we introduce Panther, a MLLM that closely adheres to user\ninstruction and locates targets of interests precisely, with the finesse of a\nblack panther. Specifically, Panther comprises three integral components:\nPanther-VE, Panther-Bridge, and Panther-Decoder. Panther-VE integrates user\ninstruction information at the early stages of the vision encoder, thereby\nextracting the most relevant and useful visual representations. The\nPanther-Bridge module, equipped with powerful filtering capabilities,\nsignificantly reduces redundant visual information, leading to a substantial\nsavings in training costs. The Panther-Decoder is versatile and can be employed\nwith any decoder-only architecture of LLMs without discrimination. Experimental\nresults, particularly on vision-centric benchmarks, have demonstrated the\neffectiveness of Panther."
                },
                "authors": [
                    {
                        "name": "Honglin Li"
                    },
                    {
                        "name": "Yuting Gao"
                    },
                    {
                        "name": "Chenglu Zhu"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Ming Yang"
                    },
                    {
                        "name": "Lin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yang"
                },
                "author": "Lin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14763v1",
                "updated": "2024-11-22T06:57:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    6,
                    57,
                    39,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T06:57:39Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    6,
                    57,
                    39,
                    4,
                    327,
                    0
                ],
                "title": "From Replications to Revelations: Heteroskedasticity-Robust Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Replications to Revelations: Heteroskedasticity-Robust Inference"
                },
                "summary": "We compare heteroskedasticity-robust inference methods with a large-scale\nMonte Carlo study based on regressions from 155 reproduction packages of\nleading economic journals. The results confirm established wisdom and uncover\nnew insights. Among well established methods HC2 standard errors with the\ndegree of freedom specification proposed by Bell and McCaffrey (2002) perform\nbest. To further improve the accuracy of t-tests, we propose a novel\ndegree-of-freedom specification based on partial leverages. We also show how\nHC2 to HC4 standard errors can be refined by more effectively addressing the\n15.6% of cases where at least one observation exhibits a leverage of one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compare heteroskedasticity-robust inference methods with a large-scale\nMonte Carlo study based on regressions from 155 reproduction packages of\nleading economic journals. The results confirm established wisdom and uncover\nnew insights. Among well established methods HC2 standard errors with the\ndegree of freedom specification proposed by Bell and McCaffrey (2002) perform\nbest. To further improve the accuracy of t-tests, we propose a novel\ndegree-of-freedom specification based on partial leverages. We also show how\nHC2 to HC4 standard errors can be refined by more effectively addressing the\n15.6% of cases where at least one observation exhibits a leverage of one."
                },
                "authors": [
                    {
                        "name": "Sebastian Kranz"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Kranz"
                },
                "author": "Sebastian Kranz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09944v2",
                "updated": "2024-11-22T06:44:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    6,
                    44,
                    22,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-15T04:44:34Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    44,
                    34,
                    4,
                    320,
                    0
                ],
                "title": "SlimLM: An Efficient Small Language Model for On-Device Document\n  Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimLM: An Efficient Small Language Model for On-Device Document\n  Assistance"
                },
                "summary": "While small language models (SLMs) show promises for mobile deployment, their\nreal-world performance and applications on smartphones remains underexplored.\nWe present SlimLM, a series of SLMs optimized for document assistance tasks on\nmobile devices. Through extensive experiments on a Samsung Galaxy S24, we\nidentify the optimal trade-offs between model size (ranging from 125M to 7B\nparameters), context length, and inference time for efficient on-device\nprocessing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on\nDocAssist, our constructed dataset for summarization, question answering and\nsuggestion tasks. Our smallest model demonstrates efficient performance on S24,\nwhile larger variants offer enhanced capabilities within mobile constraints. We\nevaluate SlimLM against existing SLMs, showing comparable or superior\nperformance and offering a benchmark for future research in on-device language\nmodels. We also provide an Android application, offering practical insights\ninto SLM deployment. Our findings provide valuable insights and illuminate the\ncapabilities of running advanced language models on high-end smartphones,\npotentially reducing server costs and enhancing privacy through on-device\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While small language models (SLMs) show promises for mobile deployment, their\nreal-world performance and applications on smartphones remains underexplored.\nWe present SlimLM, a series of SLMs optimized for document assistance tasks on\nmobile devices. Through extensive experiments on a Samsung Galaxy S24, we\nidentify the optimal trade-offs between model size (ranging from 125M to 7B\nparameters), context length, and inference time for efficient on-device\nprocessing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on\nDocAssist, our constructed dataset for summarization, question answering and\nsuggestion tasks. Our smallest model demonstrates efficient performance on S24,\nwhile larger variants offer enhanced capabilities within mobile constraints. We\nevaluate SlimLM against existing SLMs, showing comparable or superior\nperformance and offering a benchmark for future research in on-device language\nmodels. We also provide an Android application, offering practical insights\ninto SLM deployment. Our findings provide valuable insights and illuminate the\ncapabilities of running advanced language models on high-end smartphones,\npotentially reducing server costs and enhancing privacy through on-device\nprocessing."
                },
                "authors": [
                    {
                        "name": "Thang M. Pham"
                    },
                    {
                        "name": "Phat T. Nguyen"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Trung Bui"
                    }
                ],
                "author_detail": {
                    "name": "Trung Bui"
                },
                "author": "Trung Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15549v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15549v3",
                "updated": "2024-11-22T06:33:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    6,
                    33,
                    23,
                    4,
                    327,
                    0
                ],
                "published": "2024-05-24T13:35:56Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    13,
                    35,
                    56,
                    4,
                    145,
                    0
                ],
                "title": "SEP: Self-Enhanced Prompt Tuning for Visual-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEP: Self-Enhanced Prompt Tuning for Visual-Language Model"
                },
                "summary": "Prompt tuning based on Context Optimization (CoOp) effectively adapts\nvisual-language models (VLMs) to downstream tasks by inferring additional\nlearnable prompt tokens. However, these tokens are less discriminative as they\nare independent of the pre-trained tokens and fail to capture input-specific\nknowledge, such as class-aware textual or instance-aware visual knowledge.\nLeveraging the discriminative and generalization capabilities inherent in\npre-trained tokens, we introduce a novel approach named Self-Enhanced Prompt\nTuning (SEP). The core principle of SEP involves adapting the learnable prompt\ntokens at each encoder layer from the corresponding self-pretrained tokens,\nthereby explicitly incorporating discriminative prior knowledge to enhance both\ntextual-level and visual-level embeddings. Furthermore, SEP's self-enhanced\ntokens not only boost discrimination but also mitigate domain shifts in unseen\ndomains, enhancing generalization. In practice, SEP selects several\nrepresentative tokens from all pre-trained tokens for each input data at every\nlayer of the text/visual encoders. Subsequently, a Token Fusion Module (TFM) is\nintroduced to generate a self-enhanced token by merging these representative\ntokens with the learnable tokens using a cross-attention mechanism. This\nself-enhanced token is then concatenated with all pre-trained tokens, serving\nas input for subsequent encoder layers to produce the relevant embeddings.\nComprehensive evaluations across various benchmarks and tasks confirm SEP's\nefficacy in prompt tuning. Code: \\href{Code}{https://github.com/htyao89/SEP}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt tuning based on Context Optimization (CoOp) effectively adapts\nvisual-language models (VLMs) to downstream tasks by inferring additional\nlearnable prompt tokens. However, these tokens are less discriminative as they\nare independent of the pre-trained tokens and fail to capture input-specific\nknowledge, such as class-aware textual or instance-aware visual knowledge.\nLeveraging the discriminative and generalization capabilities inherent in\npre-trained tokens, we introduce a novel approach named Self-Enhanced Prompt\nTuning (SEP). The core principle of SEP involves adapting the learnable prompt\ntokens at each encoder layer from the corresponding self-pretrained tokens,\nthereby explicitly incorporating discriminative prior knowledge to enhance both\ntextual-level and visual-level embeddings. Furthermore, SEP's self-enhanced\ntokens not only boost discrimination but also mitigate domain shifts in unseen\ndomains, enhancing generalization. In practice, SEP selects several\nrepresentative tokens from all pre-trained tokens for each input data at every\nlayer of the text/visual encoders. Subsequently, a Token Fusion Module (TFM) is\nintroduced to generate a self-enhanced token by merging these representative\ntokens with the learnable tokens using a cross-attention mechanism. This\nself-enhanced token is then concatenated with all pre-trained tokens, serving\nas input for subsequent encoder layers to produce the relevant embeddings.\nComprehensive evaluations across various benchmarks and tasks confirm SEP's\nefficacy in prompt tuning. Code: \\href{Code}{https://github.com/htyao89/SEP}."
                },
                "authors": [
                    {
                        "name": "Hantao Yao"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Lu Yu"
                    },
                    {
                        "name": "Yongdong Zhang"
                    },
                    {
                        "name": "Changsheng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Changsheng Xu"
                },
                "author": "Changsheng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15549v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15549v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14758v1",
                "updated": "2024-11-22T06:32:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    6,
                    32,
                    50,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T06:32:50Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    6,
                    32,
                    50,
                    4,
                    327,
                    0
                ],
                "title": "Anisotropy in the cosmic acceleration inferred from supernovae",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anisotropy in the cosmic acceleration inferred from supernovae"
                },
                "summary": "Under the assumption that they are standard(isable) candles, the lightcurves\nof Type Ia supernovae have been analyzed in the framework of the standard\nFriedmann-Lema\\^itre-Robertson-Walker cosmology to conclude that the expansion\nrate of the Universe is accelerating due to dark energy. While the original\nclaims in the late 1990s were made using overlapping samples of less than 100\nsupernovae in total, catalogues of nearly 2000 supernovae are now available. In\nlight of recent developments such as the cosmic dipole anomaly and the larger\nthan expected bulk flow in the local Universe (which does not converge to the\nCosmic Rest Frame), we analyze the newer datasets using a Maximum Likelihood\nEstimator and find that the acceleration of the expansion rate of the Universe\nis unequivocally anisotropic. The associated debate in the literature\nhighlights the artifices of using supernovae as standardisable candles, while\nalso providing deeper insights into a consistent relativistic view of peculiar\nmotions as departures from the Hubble expansion of the Universe. The effects of\nour being `tilted observers' embedded in a deep bulk flow may have been\nmistaken for cosmic acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Under the assumption that they are standard(isable) candles, the lightcurves\nof Type Ia supernovae have been analyzed in the framework of the standard\nFriedmann-Lema\\^itre-Robertson-Walker cosmology to conclude that the expansion\nrate of the Universe is accelerating due to dark energy. While the original\nclaims in the late 1990s were made using overlapping samples of less than 100\nsupernovae in total, catalogues of nearly 2000 supernovae are now available. In\nlight of recent developments such as the cosmic dipole anomaly and the larger\nthan expected bulk flow in the local Universe (which does not converge to the\nCosmic Rest Frame), we analyze the newer datasets using a Maximum Likelihood\nEstimator and find that the acceleration of the expansion rate of the Universe\nis unequivocally anisotropic. The associated debate in the literature\nhighlights the artifices of using supernovae as standardisable candles, while\nalso providing deeper insights into a consistent relativistic view of peculiar\nmotions as departures from the Hubble expansion of the Universe. The effects of\nour being `tilted observers' embedded in a deep bulk flow may have been\nmistaken for cosmic acceleration."
                },
                "authors": [
                    {
                        "name": "Mohamed Rameez"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Rameez"
                },
                "author": "Mohamed Rameez",
                "arxiv_comment": "13 pages, 3 figures, 1 table. Invited proceedings for the Royal\n  Society discussion meeting \"Challenging the Standard Cosmological Model\"\n  [https://royalsociety.org/science-events-and-lectures/2024/04/cosmological-model/].\n  Submitted to Philosophical Transactions A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15166v2",
                "updated": "2024-11-22T06:25:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    6,
                    25,
                    28,
                    4,
                    327,
                    0
                ],
                "published": "2024-10-19T17:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    17,
                    35,
                    12,
                    5,
                    293,
                    0
                ],
                "title": "Joint Probability Estimation of Many Binary Outcomes via Localized\n  Adversarial Lasso",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Probability Estimation of Many Binary Outcomes via Localized\n  Adversarial Lasso"
                },
                "summary": "In this work we consider estimating the probability of many (possibly\ndependent) binary outcomes which is at the core of many applications, e.g.,\nmulti-level treatments in causal inference, demands for bundle of products,\netc. Without further conditions, the probability distribution of an M\ndimensional binary vector is characterized by exponentially in M coefficients\nwhich can lead to a high-dimensional problem even without the presence of\ncovariates. Understanding the (in)dependence structure allows us to\nsubstantially improve the estimation as it allows for an effective\nfactorization of the probability distribution. In order to estimate the\nprobability distribution of a M dimensional binary vector, we leverage a\nBahadur representation that connects the sparsity of its coefficients with\nindependence across the components. We propose to use regularized and\nadversarial regularized estimators to obtain an adaptive estimator with respect\nto the dependence structure which allows for rates of convergence to depend on\nthis intrinsic (lower) dimension. These estimators are needed to handle several\nchallenges within this setting, including estimating nuisance parameters,\nestimating covariates, and nonseparable moment conditions. Our main results\nconsider the presence of (low dimensional) covariates for which we propose a\nlocally penalized estimator. We provide pointwise rates of convergence\naddressing several issues in the theoretical analyses as we strive for making a\ncomputationally tractable formulation. We apply our results in the estimation\nof causal effects with multiple binary treatments and show how our estimators\ncan improve the finite sample performance when compared with non-adaptive\nestimators that try to estimate all the probabilities directly. We also provide\nsimulations that are consistent with our theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we consider estimating the probability of many (possibly\ndependent) binary outcomes which is at the core of many applications, e.g.,\nmulti-level treatments in causal inference, demands for bundle of products,\netc. Without further conditions, the probability distribution of an M\ndimensional binary vector is characterized by exponentially in M coefficients\nwhich can lead to a high-dimensional problem even without the presence of\ncovariates. Understanding the (in)dependence structure allows us to\nsubstantially improve the estimation as it allows for an effective\nfactorization of the probability distribution. In order to estimate the\nprobability distribution of a M dimensional binary vector, we leverage a\nBahadur representation that connects the sparsity of its coefficients with\nindependence across the components. We propose to use regularized and\nadversarial regularized estimators to obtain an adaptive estimator with respect\nto the dependence structure which allows for rates of convergence to depend on\nthis intrinsic (lower) dimension. These estimators are needed to handle several\nchallenges within this setting, including estimating nuisance parameters,\nestimating covariates, and nonseparable moment conditions. Our main results\nconsider the presence of (low dimensional) covariates for which we propose a\nlocally penalized estimator. We provide pointwise rates of convergence\naddressing several issues in the theoretical analyses as we strive for making a\ncomputationally tractable formulation. We apply our results in the estimation\nof causal effects with multiple binary treatments and show how our estimators\ncan improve the finite sample performance when compared with non-adaptive\nestimators that try to estimate all the probabilities directly. We also provide\nsimulations that are consistent with our theoretical findings."
                },
                "authors": [
                    {
                        "name": "Alexandre Belloni"
                    },
                    {
                        "name": "Yan Chen"
                    },
                    {
                        "name": "Matthew Harding"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Harding"
                },
                "author": "Matthew Harding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00627v2",
                "updated": "2024-11-22T06:19:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    6,
                    19,
                    35,
                    4,
                    327,
                    0
                ],
                "published": "2024-06-02T06:09:56Z",
                "published_parsed": [
                    2024,
                    6,
                    2,
                    6,
                    9,
                    56,
                    6,
                    154,
                    0
                ],
                "title": "Prompt Framework for Role-playing: Generation and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Framework for Role-playing: Generation and Evaluation"
                },
                "summary": "Large language models (LLMs) exhibit impressive proficiency in natural\nlanguage generation, understanding user instructions, and emulating human-like\nlanguage use, which has led to significant interest in their application to\nrole-playing scenarios. However, the manual collection of role-specific script\ndata and the evaluation of model performance are resource-intensive processes.\nThis project introduces a prompt-based framework designed to leverage GPT's\ncapabilities for the generation of role-playing dialogue datasets and the\nevaluation of role-playing performance. To validate the effectiveness of the\nGPT-based generation and evaluation, we further incorporate the recall-oriented\nRouge-L metric, providing an additional quantitative measure of performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit impressive proficiency in natural\nlanguage generation, understanding user instructions, and emulating human-like\nlanguage use, which has led to significant interest in their application to\nrole-playing scenarios. However, the manual collection of role-specific script\ndata and the evaluation of model performance are resource-intensive processes.\nThis project introduces a prompt-based framework designed to leverage GPT's\ncapabilities for the generation of role-playing dialogue datasets and the\nevaluation of role-playing performance. To validate the effectiveness of the\nGPT-based generation and evaluation, we further incorporate the recall-oriented\nRouge-L metric, providing an additional quantitative measure of performance."
                },
                "authors": [
                    {
                        "name": "Xun Liu"
                    },
                    {
                        "name": "Zhengwei Ni"
                    }
                ],
                "author_detail": {
                    "name": "Zhengwei Ni"
                },
                "author": "Zhengwei Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10026v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10026v3",
                "updated": "2024-11-22T05:55:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    55,
                    59,
                    4,
                    327,
                    0
                ],
                "published": "2024-05-16T12:10:43Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    12,
                    10,
                    43,
                    3,
                    137,
                    0
                ],
                "title": "Ideal trials, target trials and actual randomized trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ideal trials, target trials and actual randomized trials"
                },
                "summary": "Causal inference is the goal of randomized controlled trials and many\nobservational studies. The first step in a formal approach to causal inference\nis to define the estimand of interest, and in both types of study this can be\nintuitively defined as the effect in an ideal trial: a hypothetical perfect\nrandomized experiment (with representative sample, perfect adherence, etc.).\nThe target trial framework is an increasingly popular approach to causal\ninference in observational studies, but clarity is lacking in how a target\ntrial should be specified and, crucially, how it relates to the ideal trial. In\nthis paper, we consider these questions and use an example from respiratory\nepidemiology to highlight challenges with an approach that is commonly seen in\napplications: to specify a target trial in a way that is closely aligned to the\nobservational study (e.g. uses the same eligibility criteria, outcome measure,\netc.). The main issue is that such a target trial generally deviates from the\nideal trial. Thus, even if the target trial can be emulated perfectly apart\nfrom randomization, biases beyond baseline confounding are likely to remain,\nrelative to the estimand of interest. Without consideration of the ideal trial,\nthese biases may go unnoticed, mirroring the often-overlooked biases of actual\ntrials. Therefore, we suggest that, in both actual trials and observational\nstudies, specifying the ideal trial and how the target or actual trial differs\nfrom it is necessary to systematically assess all potential sources of biases,\nand therefore appropriately design analyses and interpret findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference is the goal of randomized controlled trials and many\nobservational studies. The first step in a formal approach to causal inference\nis to define the estimand of interest, and in both types of study this can be\nintuitively defined as the effect in an ideal trial: a hypothetical perfect\nrandomized experiment (with representative sample, perfect adherence, etc.).\nThe target trial framework is an increasingly popular approach to causal\ninference in observational studies, but clarity is lacking in how a target\ntrial should be specified and, crucially, how it relates to the ideal trial. In\nthis paper, we consider these questions and use an example from respiratory\nepidemiology to highlight challenges with an approach that is commonly seen in\napplications: to specify a target trial in a way that is closely aligned to the\nobservational study (e.g. uses the same eligibility criteria, outcome measure,\netc.). The main issue is that such a target trial generally deviates from the\nideal trial. Thus, even if the target trial can be emulated perfectly apart\nfrom randomization, biases beyond baseline confounding are likely to remain,\nrelative to the estimand of interest. Without consideration of the ideal trial,\nthese biases may go unnoticed, mirroring the often-overlooked biases of actual\ntrials. Therefore, we suggest that, in both actual trials and observational\nstudies, specifying the ideal trial and how the target or actual trial differs\nfrom it is necessary to systematically assess all potential sources of biases,\nand therefore appropriately design analyses and interpret findings."
                },
                "authors": [
                    {
                        "name": "Margarita Moreno-Betancur"
                    },
                    {
                        "name": "Rushani Wijesuriya"
                    },
                    {
                        "name": "John B. Carlin"
                    }
                ],
                "author_detail": {
                    "name": "John B. Carlin"
                },
                "author": "John B. Carlin",
                "arxiv_comment": "Major revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10026v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10026v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14748v1",
                "updated": "2024-11-22T05:53:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    53,
                    46,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T05:53:46Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    53,
                    46,
                    4,
                    327,
                    0
                ],
                "title": "Cosmological Analysis with Calibrated Neural Quantile Estimation and\n  Approximate Simulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmological Analysis with Calibrated Neural Quantile Estimation and\n  Approximate Simulators"
                },
                "summary": "A major challenge in extracting information from current and upcoming surveys\nof cosmological Large-Scale Structure (LSS) is the limited availability of\ncomputationally expensive high-fidelity simulations. We introduce Neural\nQuantile Estimation (NQE), a new Simulation-Based Inference (SBI) method that\nleverages a large number of approximate simulations for training and a small\nnumber of high-fidelity simulations for calibration. This approach guarantees\nan unbiased posterior and achieves near-optimal constraining power when the\napproximate simulations are reasonably accurate. As a proof of concept, we\ndemonstrate that cosmological parameters can be inferred at field level from\nprojected 2-dim dark matter density maps up to $k_{\\rm max}\\sim1.5\\,h$/Mpc at\n$z=0$ by training on $\\sim10^4$ Particle-Mesh (PM) simulations with transfer\nfunction correction and calibrating with $\\sim10^2$ Particle-Particle (PP)\nsimulations. The calibrated posteriors closely match those obtained by directly\ntraining on $\\sim10^4$ expensive PP simulations, but at a fraction of the\ncomputational cost. Our method offers a practical and scalable framework for\nSBI of cosmological LSS, enabling precise inference across vast volumes and\ndown to small scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major challenge in extracting information from current and upcoming surveys\nof cosmological Large-Scale Structure (LSS) is the limited availability of\ncomputationally expensive high-fidelity simulations. We introduce Neural\nQuantile Estimation (NQE), a new Simulation-Based Inference (SBI) method that\nleverages a large number of approximate simulations for training and a small\nnumber of high-fidelity simulations for calibration. This approach guarantees\nan unbiased posterior and achieves near-optimal constraining power when the\napproximate simulations are reasonably accurate. As a proof of concept, we\ndemonstrate that cosmological parameters can be inferred at field level from\nprojected 2-dim dark matter density maps up to $k_{\\rm max}\\sim1.5\\,h$/Mpc at\n$z=0$ by training on $\\sim10^4$ Particle-Mesh (PM) simulations with transfer\nfunction correction and calibrating with $\\sim10^2$ Particle-Particle (PP)\nsimulations. The calibrated posteriors closely match those obtained by directly\ntraining on $\\sim10^4$ expensive PP simulations, but at a fraction of the\ncomputational cost. Our method offers a practical and scalable framework for\nSBI of cosmological LSS, enabling precise inference across vast volumes and\ndown to small scales."
                },
                "authors": [
                    {
                        "name": "He Jia"
                    }
                ],
                "author_detail": {
                    "name": "He Jia"
                },
                "author": "He Jia",
                "arxiv_comment": "5+4 pages, 5+3 figures, to be submitted, comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15094v2",
                "updated": "2024-11-22T05:41:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    41,
                    58,
                    4,
                    327,
                    0
                ],
                "published": "2024-08-27T14:25:42Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    14,
                    25,
                    42,
                    1,
                    240,
                    0
                ],
                "title": "Constrained Diffusion Models via Dual Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained Diffusion Models via Dual Training"
                },
                "summary": "Diffusion models have attained prominence for their ability to synthesize a\nprobability distribution for a given dataset via a diffusion process, enabling\nthe generation of new data points with high fidelity. However, diffusion\nprocesses are prone to generating samples that reflect biases in a training\ndataset. To address this issue, we develop constrained diffusion models by\nimposing diffusion constraints based on desired distributions that are informed\nby requirements. Specifically, we cast the training of diffusion models under\nrequirements as a constrained distribution optimization problem that aims to\nreduce the distribution difference between original and generated data while\nobeying constraints on the distribution of generated data. We show that our\nconstrained diffusion models generate new data from a mixture data distribution\nthat achieves the optimal trade-off among objective and constraints. To train\nconstrained diffusion models, we develop a dual training algorithm and\ncharacterize the optimality of the trained constrained diffusion model. We\nempirically demonstrate the effectiveness of our constrained models in two\nconstrained generation tasks: (i) we consider a dataset with one or more\nunderrepresented classes where we train the model with constraints to ensure\nfairly sampling from all classes during inference; (ii) we fine-tune a\npre-trained diffusion model to sample from a new dataset while avoiding\noverfitting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have attained prominence for their ability to synthesize a\nprobability distribution for a given dataset via a diffusion process, enabling\nthe generation of new data points with high fidelity. However, diffusion\nprocesses are prone to generating samples that reflect biases in a training\ndataset. To address this issue, we develop constrained diffusion models by\nimposing diffusion constraints based on desired distributions that are informed\nby requirements. Specifically, we cast the training of diffusion models under\nrequirements as a constrained distribution optimization problem that aims to\nreduce the distribution difference between original and generated data while\nobeying constraints on the distribution of generated data. We show that our\nconstrained diffusion models generate new data from a mixture data distribution\nthat achieves the optimal trade-off among objective and constraints. To train\nconstrained diffusion models, we develop a dual training algorithm and\ncharacterize the optimality of the trained constrained diffusion model. We\nempirically demonstrate the effectiveness of our constrained models in two\nconstrained generation tasks: (i) we consider a dataset with one or more\nunderrepresented classes where we train the model with constraints to ensure\nfairly sampling from all classes during inference; (ii) we fine-tune a\npre-trained diffusion model to sample from a new dataset while avoiding\noverfitting."
                },
                "authors": [
                    {
                        "name": "Shervin Khalafi"
                    },
                    {
                        "name": "Dongsheng Ding"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Ribeiro"
                },
                "author": "Alejandro Ribeiro",
                "arxiv_comment": "31 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09699v2",
                "updated": "2024-11-22T05:31:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    31,
                    51,
                    4,
                    327,
                    0
                ],
                "published": "2024-04-15T11:59:45Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    11,
                    59,
                    45,
                    0,
                    106,
                    0
                ],
                "title": "Generative AI for Game Theory-based Mobile Networking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI for Game Theory-based Mobile Networking"
                },
                "summary": "With the continuous advancement of network technology, various emerging\ncomplex networking optimization problems have created a wide range of\napplications utilizing game theory. However, since game theory is a\nmathematical framework, game theory-based solutions often rely heavily on the\nexperience and knowledge of human experts. Recently, the remarkable advantages\nexhibited by generative artificial intelligence (GAI) have gained widespread\nattention. In this work, we propose a novel GAI-enabled game theory solution\nthat combines the powerful reasoning and generation capabilities of GAI to the\ndesign and optimization of mobile networking. Specifically, we first outline\nthe game theory and key technologies of GAI, and explore the advantages of\ncombining GAI with game theory. Then, we review the contributions and\nlimitations of existing research and demonstrate the potential application\nvalues of GAI applied to game theory in mobile networking. Subsequently, we\ndevelop a large language model (LLM)-enabled game theory framework to realize\nthis combination, and demonstrate the effectiveness of the proposed framework\nthrough a case study in secured UAV networks. Finally, we provide several\ndirections for future extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous advancement of network technology, various emerging\ncomplex networking optimization problems have created a wide range of\napplications utilizing game theory. However, since game theory is a\nmathematical framework, game theory-based solutions often rely heavily on the\nexperience and knowledge of human experts. Recently, the remarkable advantages\nexhibited by generative artificial intelligence (GAI) have gained widespread\nattention. In this work, we propose a novel GAI-enabled game theory solution\nthat combines the powerful reasoning and generation capabilities of GAI to the\ndesign and optimization of mobile networking. Specifically, we first outline\nthe game theory and key technologies of GAI, and explore the advantages of\ncombining GAI with game theory. Then, we review the contributions and\nlimitations of existing research and demonstrate the potential application\nvalues of GAI applied to game theory in mobile networking. Subsequently, we\ndevelop a large language model (LLM)-enabled game theory framework to realize\nthis combination, and demonstrate the effectiveness of the proposed framework\nthrough a case study in secured UAV networks. Finally, we provide several\ndirections for future extensions."
                },
                "authors": [
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Fang Mei"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Mérouane Debbah"
                    },
                    {
                        "name": "Zhu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Han"
                },
                "author": "Zhu Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14741v1",
                "updated": "2024-11-22T05:31:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    31,
                    36,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T05:31:36Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    31,
                    36,
                    4,
                    327,
                    0
                ],
                "title": "SecONN: An Optical Neural Network Framework with Concurrent Detection of\n  Thermal Fault Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecONN: An Optical Neural Network Framework with Concurrent Detection of\n  Thermal Fault Injection Attacks"
                },
                "summary": "Silicon Photonics-based AI Accelerators (SPAAs) have been considered as\npromising AI accelerators achieving high energy efficiency and low latency.\nWhile many researchers focus on improving SPAAs' energy efficiency and latency,\ntheir physical security has not been sufficiently studied. This paper first\nproposes a threat of thermal fault injection attacks on SPAAs based on\nVector-Matrix Multipliers (VMMs) utilizing Mach-Zhender Interferometers. This\npaper then proposes SecONN, an optical neural network framework that is capable\nof not only inferences but also concurrent detection of the attacks. In\naddition, this paper introduces a concept of Wavelength Division Perturbation\n(WDP) where wavelength dependent VMM results are utilized to increase detection\naccuracy. Simulation results show that the proposed method achieves 88.7%\nattack-caused average misprediction recall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Silicon Photonics-based AI Accelerators (SPAAs) have been considered as\npromising AI accelerators achieving high energy efficiency and low latency.\nWhile many researchers focus on improving SPAAs' energy efficiency and latency,\ntheir physical security has not been sufficiently studied. This paper first\nproposes a threat of thermal fault injection attacks on SPAAs based on\nVector-Matrix Multipliers (VMMs) utilizing Mach-Zhender Interferometers. This\npaper then proposes SecONN, an optical neural network framework that is capable\nof not only inferences but also concurrent detection of the attacks. In\naddition, this paper introduces a concept of Wavelength Division Perturbation\n(WDP) where wavelength dependent VMM results are utilized to increase detection\naccuracy. Simulation results show that the proposed method achieves 88.7%\nattack-caused average misprediction recall."
                },
                "authors": [
                    {
                        "name": "Kota Nishida"
                    },
                    {
                        "name": "Yoshihiro Midoh"
                    },
                    {
                        "name": "Noriyuki Miura"
                    },
                    {
                        "name": "Satoshi Kawakami"
                    },
                    {
                        "name": "Jun Shiomi"
                    }
                ],
                "author_detail": {
                    "name": "Jun Shiomi"
                },
                "author": "Jun Shiomi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14739v1",
                "updated": "2024-11-22T05:18:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    18,
                    35,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T05:18:35Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    18,
                    35,
                    4,
                    327,
                    0
                ],
                "title": "IRLab@iKAT24: Learned Sparse Retrieval with Multi-aspect LLM Query\n  Generation for Conversational Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRLab@iKAT24: Learned Sparse Retrieval with Multi-aspect LLM Query\n  Generation for Conversational Search"
                },
                "summary": "The Interactive Knowledge Assistant Track (iKAT) 2024 focuses on advancing\nconversational assistants, able to adapt their interaction and responses from\npersonalized user knowledge. The track incorporates a Personal Textual\nKnowledge Base (PTKB) alongside Conversational AI tasks, such as passage\nranking and response generation. Query Rewrite being an effective approach for\nresolving conversational context, we explore Large Language Models (LLMs), as\nquery rewriters. Specifically, our submitted runs explore multi-aspect query\ngeneration using the MQ4CS framework, which we further enhance with Learned\nSparse Retrieval via the SPLADE architecture, coupled with robust cross-encoder\nmodels. We also propose an alternative to the previous interleaving strategy,\naggregating multiple aspects during the reranking phase. Our findings indicate\nthat multi-aspect query generation is effective in enhancing performance when\nintegrated with advanced retrieval and reranking models. Our results also lead\nthe way for better personalization in Conversational Search, relying on LLMs to\nintegrate personalization within query rewrite, and outperforming human rewrite\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Interactive Knowledge Assistant Track (iKAT) 2024 focuses on advancing\nconversational assistants, able to adapt their interaction and responses from\npersonalized user knowledge. The track incorporates a Personal Textual\nKnowledge Base (PTKB) alongside Conversational AI tasks, such as passage\nranking and response generation. Query Rewrite being an effective approach for\nresolving conversational context, we explore Large Language Models (LLMs), as\nquery rewriters. Specifically, our submitted runs explore multi-aspect query\ngeneration using the MQ4CS framework, which we further enhance with Learned\nSparse Retrieval via the SPLADE architecture, coupled with robust cross-encoder\nmodels. We also propose an alternative to the previous interleaving strategy,\naggregating multiple aspects during the reranking phase. Our findings indicate\nthat multi-aspect query generation is effective in enhancing performance when\nintegrated with advanced retrieval and reranking models. Our results also lead\nthe way for better personalization in Conversational Search, relying on LLMs to\nintegrate personalization within query rewrite, and outperforming human rewrite\nperformance."
                },
                "authors": [
                    {
                        "name": "Simon Lupart"
                    },
                    {
                        "name": "Zahra Abbasiantaeb"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Aliannejadi"
                },
                "author": "Mohammad Aliannejadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14738v1",
                "updated": "2024-11-22T05:17:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    17,
                    18,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T05:17:18Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    17,
                    18,
                    4,
                    327,
                    0
                ],
                "title": "Universal and Context-Independent Triggers for Precise Control of LLM\n  Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal and Context-Independent Triggers for Precise Control of LLM\n  Outputs"
                },
                "summary": "Large language models (LLMs) have been widely adopted in applications such as\nautomated content generation and even critical decision-making systems.\nHowever, the risk of prompt injection allows for potential manipulation of LLM\noutputs. While numerous attack methods have been documented, achieving full\ncontrol over these outputs remains challenging, often requiring experienced\nattackers to make multiple attempts and depending heavily on the prompt\ncontext. Recent advancements in gradient-based white-box attack techniques have\nshown promise in tasks like jailbreaks and system prompt leaks. Our research\ngeneralizes gradient-based attacks to find a trigger that is (1) Universal:\neffective irrespective of the target output; (2) Context-Independent: robust\nacross diverse prompt contexts; and (3) Precise Output: capable of manipulating\nLLM inputs to yield any specified output with high accuracy. We propose a novel\nmethod to efficiently discover such triggers and assess the effectiveness of\nthe proposed attack. Furthermore, we discuss the substantial threats posed by\nsuch attacks to LLM-based applications, highlighting the potential for\nadversaries to taking over the decisions and actions made by AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted in applications such as\nautomated content generation and even critical decision-making systems.\nHowever, the risk of prompt injection allows for potential manipulation of LLM\noutputs. While numerous attack methods have been documented, achieving full\ncontrol over these outputs remains challenging, often requiring experienced\nattackers to make multiple attempts and depending heavily on the prompt\ncontext. Recent advancements in gradient-based white-box attack techniques have\nshown promise in tasks like jailbreaks and system prompt leaks. Our research\ngeneralizes gradient-based attacks to find a trigger that is (1) Universal:\neffective irrespective of the target output; (2) Context-Independent: robust\nacross diverse prompt contexts; and (3) Precise Output: capable of manipulating\nLLM inputs to yield any specified output with high accuracy. We propose a novel\nmethod to efficiently discover such triggers and assess the effectiveness of\nthe proposed attack. Furthermore, we discuss the substantial threats posed by\nsuch attacks to LLM-based applications, highlighting the potential for\nadversaries to taking over the decisions and actions made by AI agents."
                },
                "authors": [
                    {
                        "name": "Jiashuo Liang"
                    },
                    {
                        "name": "Guancheng Li"
                    },
                    {
                        "name": "Yang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Yu"
                },
                "author": "Yang Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14735v1",
                "updated": "2024-11-22T05:05:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    5,
                    44,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T05:05:44Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    5,
                    44,
                    4,
                    327,
                    0
                ],
                "title": "Automatic Inference of Relational Object Invariants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Inference of Relational Object Invariants"
                },
                "summary": "Relational object invariants (or representation invariants) are relational\nproperties held by the fields of a (memory) object throughout its lifetime. For\nexample, the length of a buffer never exceeds its capacity. Automatic inference\nof these invariants is particularly challenging because they are often broken\ntemporarily during field updates. In this paper, we present an Abstract\nInterpretation-based solution to infer object invariants. Our key insight is a\nnew object abstraction for memory objects, where memory is divided into\nmultiple memory banks, each containing several objects. Within each bank, the\nobjects are further abstracted by separating the most recently used (MRU)\nobject, represented precisely with strong updates, while the rest are\nsummarized. For an effective implementation of this approach, we introduce a\nnew composite abstract domain, which forms a reduced product of numerical and\nequality sub-domains. This design efficiently expresses relationships between a\nsmall number of variables (e.g., fields of the same abstract object). We\nimplement the new domain in the CRAB abstract interpreter and evaluate it on\nseveral benchmarks for memory safety. We show that our approach is\nsignificantly more scalable for relational properties than the existing\nimplementation of CRAB. For evaluating precision, we have integrated our\nanalysis as a pre-processing step to SEABMC bounded model checker, and show\nthat it is effective at both discharging assertions during pre-processing, and\nsignificantly improving the run-time of SEABMC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational object invariants (or representation invariants) are relational\nproperties held by the fields of a (memory) object throughout its lifetime. For\nexample, the length of a buffer never exceeds its capacity. Automatic inference\nof these invariants is particularly challenging because they are often broken\ntemporarily during field updates. In this paper, we present an Abstract\nInterpretation-based solution to infer object invariants. Our key insight is a\nnew object abstraction for memory objects, where memory is divided into\nmultiple memory banks, each containing several objects. Within each bank, the\nobjects are further abstracted by separating the most recently used (MRU)\nobject, represented precisely with strong updates, while the rest are\nsummarized. For an effective implementation of this approach, we introduce a\nnew composite abstract domain, which forms a reduced product of numerical and\nequality sub-domains. This design efficiently expresses relationships between a\nsmall number of variables (e.g., fields of the same abstract object). We\nimplement the new domain in the CRAB abstract interpreter and evaluate it on\nseveral benchmarks for memory safety. We show that our approach is\nsignificantly more scalable for relational properties than the existing\nimplementation of CRAB. For evaluating precision, we have integrated our\nanalysis as a pre-processing step to SEABMC bounded model checker, and show\nthat it is effective at both discharging assertions during pre-processing, and\nsignificantly improving the run-time of SEABMC."
                },
                "authors": [
                    {
                        "name": "Yusen Su"
                    },
                    {
                        "name": "Jorge A. Navas"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    },
                    {
                        "name": "Isabel Garcia-Contreras"
                    }
                ],
                "author_detail": {
                    "name": "Isabel Garcia-Contreras"
                },
                "author": "Isabel Garcia-Contreras",
                "arxiv_comment": "This is an extended version of the VMCAI 2025 paper, consisting of 26\n  pages. The artifact is available at https://doi.org/10.5281/zenodo.13849174",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.11728v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.11728v4",
                "updated": "2024-11-22T04:56:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    56,
                    49,
                    4,
                    327,
                    0
                ],
                "published": "2023-10-18T05:56:53Z",
                "published_parsed": [
                    2023,
                    10,
                    18,
                    5,
                    56,
                    53,
                    2,
                    291,
                    0
                ],
                "title": "EchoScan: Scanning Complex Room Geometries via Acoustic Echoes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoScan: Scanning Complex Room Geometries via Acoustic Echoes"
                },
                "summary": "Accurate estimation of indoor space geometries is vital for constructing\nprecise digital twins, whose broad industrial applications include navigation\nin unfamiliar environments and efficient evacuation planning, particularly in\nlow-light conditions. This study introduces EchoScan, a deep neural network\nmodel that utilizes acoustic echoes to perform room geometry inference.\nConventional sound-based techniques rely on estimating geometry-related room\nparameters such as wall position and room size, thereby limiting the diversity\nof inferable room geometries. Contrarily, EchoScan overcomes this limitation by\ndirectly inferring room floorplan maps and height maps, thereby enabling it to\nhandle rooms with complex shapes, including curved walls. The segmentation task\nfor predicting floorplan and height maps enables the model to leverage both\nlow- and high-order reflections. The use of high-order reflections further\nallows EchoScan to infer complex room shapes when some walls of the room are\nunobservable from the position of an audio device. Herein, EchoScan was trained\nand evaluated using RIRs synthesized from complex environments, including the\nManhattan and Atlanta layouts, employing a practical audio device configuration\ncompatible with commercial, off-the-shelf devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate estimation of indoor space geometries is vital for constructing\nprecise digital twins, whose broad industrial applications include navigation\nin unfamiliar environments and efficient evacuation planning, particularly in\nlow-light conditions. This study introduces EchoScan, a deep neural network\nmodel that utilizes acoustic echoes to perform room geometry inference.\nConventional sound-based techniques rely on estimating geometry-related room\nparameters such as wall position and room size, thereby limiting the diversity\nof inferable room geometries. Contrarily, EchoScan overcomes this limitation by\ndirectly inferring room floorplan maps and height maps, thereby enabling it to\nhandle rooms with complex shapes, including curved walls. The segmentation task\nfor predicting floorplan and height maps enables the model to leverage both\nlow- and high-order reflections. The use of high-order reflections further\nallows EchoScan to infer complex room shapes when some walls of the room are\nunobservable from the position of an audio device. Herein, EchoScan was trained\nand evaluated using RIRs synthesized from complex environments, including the\nManhattan and Atlanta layouts, employing a practical audio device configuration\ncompatible with commercial, off-the-shelf devices."
                },
                "authors": [
                    {
                        "name": "Inmo Yeon"
                    },
                    {
                        "name": "Iljoo Jeong"
                    },
                    {
                        "name": "Seungchul Lee"
                    },
                    {
                        "name": "Jung-Woo Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jung-Woo Choi"
                },
                "author": "Jung-Woo Choi",
                "arxiv_doi": "10.1109/TASLP.2024.3485516",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TASLP.2024.3485516",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.11728v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.11728v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 15 figures, 2 tables",
                "arxiv_journal_ref": "in IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing, vol. 32, pp. 4768-4782, 2024",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.00270v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.00270v5",
                "updated": "2024-11-22T04:51:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    51,
                    38,
                    4,
                    327,
                    0
                ],
                "published": "2023-08-01T04:05:33Z",
                "published_parsed": [
                    2023,
                    8,
                    1,
                    4,
                    5,
                    33,
                    1,
                    213,
                    0
                ],
                "title": "An Approximate Kerr-Newman-like Metric Endowed with a Magnetic Dipole\n  and Mass Quadrupole",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Approximate Kerr-Newman-like Metric Endowed with a Magnetic Dipole\n  and Mass Quadrupole"
                },
                "summary": "Approximate all-terrain spacetimes for astrophysical applications are\npresented. The metrics possess five relativistic multipole moments, namely\nmass, rotation, mass quadrupole, charge, and magnetic dipole moment. All these\nspacetimes approximately satisfy the Einstein-Maxwell field equations. The\nfirst metric is generated by means of the Hoenselaers-Perj\\'es method from\ngiven relativistic multipoles. The second metric is a perturbation of the\nKerr-Newman metric, which makes it a relevant approximation for astrophysical\ncalculations. The last metric is an extension of the Hartle-Thorne metric that\nis important for obtaining internal models of compact objects perturbatively.\nThe electromagnetic field is calculated using Cartan forms for locally\nnonrotating observers. These spacetimes are relevant to infer properties of\ncompact objects from astrophysical observations. Furthermore, the numerical\nimplementations of these metrics are straightforward, making them versatile for\nsimulating the potential astrophysical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate all-terrain spacetimes for astrophysical applications are\npresented. The metrics possess five relativistic multipole moments, namely\nmass, rotation, mass quadrupole, charge, and magnetic dipole moment. All these\nspacetimes approximately satisfy the Einstein-Maxwell field equations. The\nfirst metric is generated by means of the Hoenselaers-Perj\\'es method from\ngiven relativistic multipoles. The second metric is a perturbation of the\nKerr-Newman metric, which makes it a relevant approximation for astrophysical\ncalculations. The last metric is an extension of the Hartle-Thorne metric that\nis important for obtaining internal models of compact objects perturbatively.\nThe electromagnetic field is calculated using Cartan forms for locally\nnonrotating observers. These spacetimes are relevant to infer properties of\ncompact objects from astrophysical observations. Furthermore, the numerical\nimplementations of these metrics are straightforward, making them versatile for\nsimulating the potential astrophysical applications."
                },
                "authors": [
                    {
                        "name": "Francisco Frutos-Alfaro"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Frutos-Alfaro"
                },
                "author": "Francisco Frutos-Alfaro",
                "arxiv_doi": "10.1088/1572-9494/ad4cde",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1572-9494/ad4cde",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.00270v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.00270v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Communications in Theoretical Physics, 76, 085404, 2024",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14723v1",
                "updated": "2024-11-22T04:36:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    36,
                    12,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T04:36:12Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    36,
                    12,
                    4,
                    327,
                    0
                ],
                "title": "Effective SAM Combination for Open-Vocabulary Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective SAM Combination for Open-Vocabulary Semantic Segmentation"
                },
                "summary": "Open-vocabulary semantic segmentation aims to assign pixel-level labels to\nimages across an unlimited range of classes. Traditional methods address this\nby sequentially connecting a powerful mask proposal generator, such as the\nSegment Anything Model (SAM), with a pre-trained vision-language model like\nCLIP. But these two-stage approaches often suffer from high computational\ncosts, memory inefficiencies. In this paper, we propose ESC-Net, a novel\none-stage open-vocabulary segmentation model that leverages the SAM decoder\nblocks for class-agnostic segmentation within an efficient inference framework.\nBy embedding pseudo prompts generated from image-text correlations into SAM's\npromptable segmentation framework, ESC-Net achieves refined spatial aggregation\nfor accurate mask predictions. ESC-Net achieves superior performance on\nstandard benchmarks, including ADE20K, PASCAL-VOC, and PASCAL-Context,\noutperforming prior methods in both efficiency and accuracy. Comprehensive\nablation studies further demonstrate its robustness across challenging\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-vocabulary semantic segmentation aims to assign pixel-level labels to\nimages across an unlimited range of classes. Traditional methods address this\nby sequentially connecting a powerful mask proposal generator, such as the\nSegment Anything Model (SAM), with a pre-trained vision-language model like\nCLIP. But these two-stage approaches often suffer from high computational\ncosts, memory inefficiencies. In this paper, we propose ESC-Net, a novel\none-stage open-vocabulary segmentation model that leverages the SAM decoder\nblocks for class-agnostic segmentation within an efficient inference framework.\nBy embedding pseudo prompts generated from image-text correlations into SAM's\npromptable segmentation framework, ESC-Net achieves refined spatial aggregation\nfor accurate mask predictions. ESC-Net achieves superior performance on\nstandard benchmarks, including ADE20K, PASCAL-VOC, and PASCAL-Context,\noutperforming prior methods in both efficiency and accuracy. Comprehensive\nablation studies further demonstrate its robustness across challenging\nconditions."
                },
                "authors": [
                    {
                        "name": "Minhyeok Lee"
                    },
                    {
                        "name": "Suhwan Cho"
                    },
                    {
                        "name": "Jungho Lee"
                    },
                    {
                        "name": "Sunghun Yang"
                    },
                    {
                        "name": "Heeseung Choi"
                    },
                    {
                        "name": "Ig-Jae Kim"
                    },
                    {
                        "name": "Sangyoun Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sangyoun Lee"
                },
                "author": "Sangyoun Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01544v2",
                "updated": "2024-11-22T04:32:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    32,
                    55,
                    4,
                    327,
                    0
                ],
                "published": "2024-10-02T13:30:32Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    30,
                    32,
                    2,
                    276,
                    0
                ],
                "title": "Boosting Weakly-Supervised Referring Image Segmentation via Progressive\n  Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Weakly-Supervised Referring Image Segmentation via Progressive\n  Comprehension"
                },
                "summary": "This paper explores the weakly-supervised referring image segmentation (WRIS)\nproblem, and focuses on a challenging setup where target localization is\nlearned directly from image-text pairs. We note that the input text description\ntypically already contains detailed information on how to localize the target\nobject, and we also observe that humans often follow a step-by-step\ncomprehension process (\\ie, progressively utilizing target-related attributes\nand relations as cues) to identify the target object. Hence, we propose a novel\nProgressive Comprehension Network (PCNet) to leverage target-related textual\ncues from the input description for progressively localizing the target object.\nSpecifically, we first use a Large Language Model (LLM) to decompose the input\ntext description into short phrases. These short phrases are taken as\ntarget-related cues and fed into a Conditional Referring Module (CRM) in\nmultiple stages, to allow updating the referring text embedding and enhance the\nresponse map for target localization in a multi-stage manner. Based on the CRM,\nwe then propose a Region-aware Shrinking (RaS) loss to constrain the visual\nlocalization to be conducted progressively in a coarse-to-fine manner across\ndifferent stages. Finally, we introduce an Instance-aware Disambiguation (IaD)\nloss to suppress instance localization ambiguity by differentiating overlapping\nresponse maps generated by different referring texts on the same image.\nExtensive experiments show that our method outperforms SOTA methods on three\ncommon benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the weakly-supervised referring image segmentation (WRIS)\nproblem, and focuses on a challenging setup where target localization is\nlearned directly from image-text pairs. We note that the input text description\ntypically already contains detailed information on how to localize the target\nobject, and we also observe that humans often follow a step-by-step\ncomprehension process (\\ie, progressively utilizing target-related attributes\nand relations as cues) to identify the target object. Hence, we propose a novel\nProgressive Comprehension Network (PCNet) to leverage target-related textual\ncues from the input description for progressively localizing the target object.\nSpecifically, we first use a Large Language Model (LLM) to decompose the input\ntext description into short phrases. These short phrases are taken as\ntarget-related cues and fed into a Conditional Referring Module (CRM) in\nmultiple stages, to allow updating the referring text embedding and enhance the\nresponse map for target localization in a multi-stage manner. Based on the CRM,\nwe then propose a Region-aware Shrinking (RaS) loss to constrain the visual\nlocalization to be conducted progressively in a coarse-to-fine manner across\ndifferent stages. Finally, we introduce an Instance-aware Disambiguation (IaD)\nloss to suppress instance localization ambiguity by differentiating overlapping\nresponse maps generated by different referring texts on the same image.\nExtensive experiments show that our method outperforms SOTA methods on three\ncommon benchmarks."
                },
                "authors": [
                    {
                        "name": "Zaiquan Yang"
                    },
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Jiaying Lin"
                    },
                    {
                        "name": "Gerhard Hancke"
                    },
                    {
                        "name": "Rynson W. H. Lau"
                    }
                ],
                "author_detail": {
                    "name": "Rynson W. H. Lau"
                },
                "author": "Rynson W. H. Lau",
                "arxiv_comment": "Accepted by NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14721v1",
                "updated": "2024-11-22T04:28:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    28,
                    56,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T04:28:56Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    28,
                    56,
                    4,
                    327,
                    0
                ],
                "title": "MolReFlect: Towards In-Context Fine-grained Alignments between Molecules\n  and Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MolReFlect: Towards In-Context Fine-grained Alignments between Molecules\n  and Texts"
                },
                "summary": "Molecule discovery is a pivotal research field, impacting everything from the\nmedicines we take to the materials we use. Recently, Large Language Models\n(LLMs) have been widely adopted in molecule understanding and generation, yet\nthe alignments between molecules and their corresponding captions remain a\nsignificant challenge. Previous endeavours often treat the molecule as a\ngeneral SMILES string or molecular graph, neglecting the fine-grained\nalignments between the molecular sub-structures and the descriptive textual\nphrases, which are crucial for accurate and explainable predictions. In this\ncase, we introduce MolReFlect, a novel teacher-student framework designed to\ncontextually perform the molecule-caption alignments in a fine-grained way. Our\napproach initially leverages a larger teacher LLM to label the detailed\nalignments by directly extracting critical phrases from molecule captions or\nSMILES strings and implying them to corresponding sub-structures or\ncharacteristics. To refine these alignments, we propose In-Context Selective\nReflection, which retrieves previous extraction results as context examples for\nteacher LLM to reflect and lets a smaller student LLM select from in-context\nreflection and previous extraction results. Finally, we enhance the learning\nprocess of the student LLM through Chain-of-Thought In-Context Molecule Tuning,\nintegrating the fine-grained alignments and the reasoning processes within the\nChain-of-Thought format. Our experimental results demonstrate that MolReFlect\nenables LLMs like Mistral-7B to significantly outperform the previous\nbaselines, achieving SOTA performance on the ChEBI-20 dataset. This advancement\nnot only enhances the generative capabilities of LLMs in the molecule-caption\ntranslation task, but also contributes to a more explainable framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecule discovery is a pivotal research field, impacting everything from the\nmedicines we take to the materials we use. Recently, Large Language Models\n(LLMs) have been widely adopted in molecule understanding and generation, yet\nthe alignments between molecules and their corresponding captions remain a\nsignificant challenge. Previous endeavours often treat the molecule as a\ngeneral SMILES string or molecular graph, neglecting the fine-grained\nalignments between the molecular sub-structures and the descriptive textual\nphrases, which are crucial for accurate and explainable predictions. In this\ncase, we introduce MolReFlect, a novel teacher-student framework designed to\ncontextually perform the molecule-caption alignments in a fine-grained way. Our\napproach initially leverages a larger teacher LLM to label the detailed\nalignments by directly extracting critical phrases from molecule captions or\nSMILES strings and implying them to corresponding sub-structures or\ncharacteristics. To refine these alignments, we propose In-Context Selective\nReflection, which retrieves previous extraction results as context examples for\nteacher LLM to reflect and lets a smaller student LLM select from in-context\nreflection and previous extraction results. Finally, we enhance the learning\nprocess of the student LLM through Chain-of-Thought In-Context Molecule Tuning,\nintegrating the fine-grained alignments and the reasoning processes within the\nChain-of-Thought format. Our experimental results demonstrate that MolReFlect\nenables LLMs like Mistral-7B to significantly outperform the previous\nbaselines, achieving SOTA performance on the ChEBI-20 dataset. This advancement\nnot only enhances the generative capabilities of LLMs in the molecule-caption\ntranslation task, but also contributes to a more explainable framework."
                },
                "authors": [
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Yunqing Liu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jingdi Le"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "22 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14720v1",
                "updated": "2024-11-22T04:19:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    19,
                    32,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T04:19:32Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    19,
                    32,
                    4,
                    327,
                    0
                ],
                "title": "Optimizing Social Media Annotation of HPV Vaccine Skepticism and\n  Misinformation Using Large Language Models: An Experimental Evaluation of\n  In-Context Learning and Fine-Tuning Stance Detection Across Multiple Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Social Media Annotation of HPV Vaccine Skepticism and\n  Misinformation Using Large Language Models: An Experimental Evaluation of\n  In-Context Learning and Fine-Tuning Stance Detection Across Multiple Models"
                },
                "summary": "This paper leverages large-language models (LLMs) to experimentally determine\noptimal strategies for scaling up social media content annotation for stance\ndetection on HPV vaccine-related tweets. We examine both conventional\nfine-tuning and emergent in-context learning methods, systematically varying\nstrategies of prompt engineering across widely used LLMs and their variants\n(e.g., GPT4, Mistral, and Llama3, etc.). Specifically, we varied prompt\ntemplate design, shot sampling methods, and shot quantity to detect stance on\nHPV vaccination. Our findings reveal that 1) in general, in-context learning\noutperforms fine-tuning in stance detection for HPV vaccine social media\ncontent; 2) increasing shot quantity does not necessarily enhance performance\nacross models; and 3) different LLMs and their variants present differing\nsensitivity to in-context learning conditions. We uncovered that the optimal\nin-context learning configuration for stance detection on HPV vaccine tweets\ninvolves six stratified shots paired with detailed contextual prompts. This\nstudy highlights the potential and provides an applicable approach for applying\nLLMs to research on social media stance and skepticism detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper leverages large-language models (LLMs) to experimentally determine\noptimal strategies for scaling up social media content annotation for stance\ndetection on HPV vaccine-related tweets. We examine both conventional\nfine-tuning and emergent in-context learning methods, systematically varying\nstrategies of prompt engineering across widely used LLMs and their variants\n(e.g., GPT4, Mistral, and Llama3, etc.). Specifically, we varied prompt\ntemplate design, shot sampling methods, and shot quantity to detect stance on\nHPV vaccination. Our findings reveal that 1) in general, in-context learning\noutperforms fine-tuning in stance detection for HPV vaccine social media\ncontent; 2) increasing shot quantity does not necessarily enhance performance\nacross models; and 3) different LLMs and their variants present differing\nsensitivity to in-context learning conditions. We uncovered that the optimal\nin-context learning configuration for stance detection on HPV vaccine tweets\ninvolves six stratified shots paired with detailed contextual prompts. This\nstudy highlights the potential and provides an applicable approach for applying\nLLMs to research on social media stance and skepticism detection."
                },
                "authors": [
                    {
                        "name": "Luhang Sun"
                    },
                    {
                        "name": "Varsha Pendyala"
                    },
                    {
                        "name": "Yun-Shiuan Chuang"
                    },
                    {
                        "name": "Shanglin Yang"
                    },
                    {
                        "name": "Jonathan Feldman"
                    },
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    },
                    {
                        "name": "Sijia Yang"
                    },
                    {
                        "name": "Dhavan Shah"
                    }
                ],
                "author_detail": {
                    "name": "Dhavan Shah"
                },
                "author": "Dhavan Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14718v1",
                "updated": "2024-11-22T04:10:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    10,
                    49,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T04:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    10,
                    49,
                    4,
                    327,
                    0
                ],
                "title": "GraphTheft: Quantifying Privacy Risks in Graph Prompt Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphTheft: Quantifying Privacy Risks in Graph Prompt Learning"
                },
                "summary": "Graph Prompt Learning (GPL) represents an innovative approach in graph\nrepresentation learning, enabling task-specific adaptations by fine-tuning\nprompts without altering the underlying pre-trained model. Despite its growing\nprominence, the privacy risks inherent in GPL remain unexplored. In this study,\nwe provide the first evaluation of privacy leakage in GPL across three attacker\ncapabilities: black-box attacks when GPL as a service, and scenarios where node\nembeddings and prompt representations are accessible to third parties. We\nassess GPL's privacy vulnerabilities through Attribute Inference Attacks (AIAs)\nand Link Inference Attacks (LIAs), finding that under any capability, attackers\ncan effectively infer the properties and relationships of sensitive nodes, and\nthe success rate of inference on some data sets is as high as 98%. Importantly,\nwhile targeted inference attacks on specific prompts (e.g., GPF-plus) maintain\nhigh success rates, our analysis suggests that the prompt-tuning in GPL does\nnot significantly elevate privacy risks compared to traditional GNNs. To\nmitigate these risks, we explored defense mechanisms, identifying that\nLaplacian noise perturbation can substantially reduce inference success, though\nbalancing privacy protection with model performance remains challenging. This\nwork highlights critical privacy risks in GPL, offering new insights and\nfoundational directions for future privacy-preserving strategies in graph\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Prompt Learning (GPL) represents an innovative approach in graph\nrepresentation learning, enabling task-specific adaptations by fine-tuning\nprompts without altering the underlying pre-trained model. Despite its growing\nprominence, the privacy risks inherent in GPL remain unexplored. In this study,\nwe provide the first evaluation of privacy leakage in GPL across three attacker\ncapabilities: black-box attacks when GPL as a service, and scenarios where node\nembeddings and prompt representations are accessible to third parties. We\nassess GPL's privacy vulnerabilities through Attribute Inference Attacks (AIAs)\nand Link Inference Attacks (LIAs), finding that under any capability, attackers\ncan effectively infer the properties and relationships of sensitive nodes, and\nthe success rate of inference on some data sets is as high as 98%. Importantly,\nwhile targeted inference attacks on specific prompts (e.g., GPF-plus) maintain\nhigh success rates, our analysis suggests that the prompt-tuning in GPL does\nnot significantly elevate privacy risks compared to traditional GNNs. To\nmitigate these risks, we explored defense mechanisms, identifying that\nLaplacian noise perturbation can substantially reduce inference success, though\nbalancing privacy protection with model performance remains challenging. This\nwork highlights critical privacy risks in GPL, offering new insights and\nfoundational directions for future privacy-preserving strategies in graph\nlearning."
                },
                "authors": [
                    {
                        "name": "Jiani Zhu"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Yuxin Qi"
                    },
                    {
                        "name": "Qinghua Mao"
                    }
                ],
                "author_detail": {
                    "name": "Qinghua Mao"
                },
                "author": "Qinghua Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14713v1",
                "updated": "2024-11-22T03:43:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    3,
                    43,
                    41,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T03:43:41Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    3,
                    43,
                    41,
                    4,
                    327,
                    0
                ],
                "title": "LIBER: Lifelong User Behavior Modeling Based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIBER: Lifelong User Behavior Modeling Based on Large Language Models"
                },
                "summary": "CTR prediction plays a vital role in recommender systems. Recently, large\nlanguage models (LLMs) have been applied in recommender systems due to their\nemergence abilities. While leveraging semantic information from LLMs has shown\nsome improvements in the performance of recommender systems, two notable\nlimitations persist in these studies. First, LLM-enhanced recommender systems\nencounter challenges in extracting valuable information from lifelong user\nbehavior sequences within textual contexts for recommendation tasks. Second,\nthe inherent variability in human behaviors leads to a constant stream of new\nbehaviors and irregularly fluctuating user interests. This characteristic\nimposes two significant challenges on existing models. On the one hand, it\npresents difficulties for LLMs in effectively capturing the dynamic shifts in\nuser interests within these sequences, and on the other hand, there exists the\nissue of substantial computational overhead if the LLMs necessitate recurrent\ncalls upon each update to the user sequences. In this work, we propose Lifelong\nUser Behavior Modeling (LIBER) based on large language models, which includes\nthree modules: (1) User Behavior Streaming Partition (UBSP), (2) User Interest\nLearning (UIL), and (3) User Interest Fusion (UIF). Initially, UBSP is employed\nto condense lengthy user behavior sequences into shorter partitions in an\nincremental paradigm, facilitating more efficient processing. Subsequently, UIL\nleverages LLMs in a cascading way to infer insights from these partitions.\nFinally, UIF integrates the textual outputs generated by the aforementioned\nprocesses to construct a comprehensive representation, which can be\nincorporated by any recommendation model to enhance performance. LIBER has been\ndeployed on Huawei's music recommendation service and achieved substantial\nimprovements in users' play count and play time by 3.01% and 7.69%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTR prediction plays a vital role in recommender systems. Recently, large\nlanguage models (LLMs) have been applied in recommender systems due to their\nemergence abilities. While leveraging semantic information from LLMs has shown\nsome improvements in the performance of recommender systems, two notable\nlimitations persist in these studies. First, LLM-enhanced recommender systems\nencounter challenges in extracting valuable information from lifelong user\nbehavior sequences within textual contexts for recommendation tasks. Second,\nthe inherent variability in human behaviors leads to a constant stream of new\nbehaviors and irregularly fluctuating user interests. This characteristic\nimposes two significant challenges on existing models. On the one hand, it\npresents difficulties for LLMs in effectively capturing the dynamic shifts in\nuser interests within these sequences, and on the other hand, there exists the\nissue of substantial computational overhead if the LLMs necessitate recurrent\ncalls upon each update to the user sequences. In this work, we propose Lifelong\nUser Behavior Modeling (LIBER) based on large language models, which includes\nthree modules: (1) User Behavior Streaming Partition (UBSP), (2) User Interest\nLearning (UIL), and (3) User Interest Fusion (UIF). Initially, UBSP is employed\nto condense lengthy user behavior sequences into shorter partitions in an\nincremental paradigm, facilitating more efficient processing. Subsequently, UIL\nleverages LLMs in a cascading way to infer insights from these partitions.\nFinally, UIF integrates the textual outputs generated by the aforementioned\nprocesses to construct a comprehensive representation, which can be\nincorporated by any recommendation model to enhance performance. LIBER has been\ndeployed on Huawei's music recommendation service and achieved substantial\nimprovements in users' play count and play time by 3.01% and 7.69%."
                },
                "authors": [
                    {
                        "name": "Chenxu Zhu"
                    },
                    {
                        "name": "Shigang Quan"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Xiaoling Cai"
                    },
                    {
                        "name": "Hong Zhu"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Yunjia Xi"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14708v1",
                "updated": "2024-11-22T03:33:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    3,
                    33,
                    51,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T03:33:51Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    3,
                    33,
                    51,
                    4,
                    327,
                    0
                ],
                "title": "Understanding LLM Embeddings for Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding LLM Embeddings for Regression"
                },
                "summary": "With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance."
                },
                "authors": [
                    {
                        "name": "Eric Tang"
                    },
                    {
                        "name": "Bangding Yang"
                    },
                    {
                        "name": "Xingyou Song"
                    }
                ],
                "author_detail": {
                    "name": "Xingyou Song"
                },
                "author": "Xingyou Song",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.15131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15131v1",
                "updated": "2024-11-22T18:56:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    56,
                    56,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:56:56Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    56,
                    56,
                    4,
                    327,
                    0
                ],
                "title": "WildLMa: Long Horizon Loco-Manipulation in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildLMa: Long Horizon Loco-Manipulation in the Wild"
                },
                "summary": "`In-the-wild' mobile manipulation aims to deploy robots in diverse real-world\nenvironments, which requires the robot to (1) have skills that generalize\nacross object configurations; (2) be capable of long-horizon task execution in\ndiverse environments; and (3) perform complex manipulation beyond\npick-and-place. Quadruped robots with manipulators hold promise for extending\nthe workspace and enabling robust locomotion, but existing results do not\ninvestigate such a capability. This paper proposes WildLMa with three\ncomponents to address these issues: (1) adaptation of learned low-level\ncontroller for VR-enabled whole-body teleoperation and traversability; (2)\nWildLMa-Skill -- a library of generalizable visuomotor skills acquired via\nimitation learning or heuristics and (3) WildLMa-Planner -- an interface of\nlearned skills that allow LLM planners to coordinate skills for long-horizon\ntasks. We demonstrate the importance of high-quality training data by achieving\nhigher grasping success rate over existing RL baselines using only tens of\ndemonstrations. WildLMa exploits CLIP for language-conditioned imitation\nlearning that empirically generalizes to objects unseen in training\ndemonstrations. Besides extensive quantitative evaluation, we qualitatively\ndemonstrate practical robot applications, such as cleaning up trash in\nuniversity hallways or outdoor terrains, operating articulated objects, and\nrearranging items on a bookshelf.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "`In-the-wild' mobile manipulation aims to deploy robots in diverse real-world\nenvironments, which requires the robot to (1) have skills that generalize\nacross object configurations; (2) be capable of long-horizon task execution in\ndiverse environments; and (3) perform complex manipulation beyond\npick-and-place. Quadruped robots with manipulators hold promise for extending\nthe workspace and enabling robust locomotion, but existing results do not\ninvestigate such a capability. This paper proposes WildLMa with three\ncomponents to address these issues: (1) adaptation of learned low-level\ncontroller for VR-enabled whole-body teleoperation and traversability; (2)\nWildLMa-Skill -- a library of generalizable visuomotor skills acquired via\nimitation learning or heuristics and (3) WildLMa-Planner -- an interface of\nlearned skills that allow LLM planners to coordinate skills for long-horizon\ntasks. We demonstrate the importance of high-quality training data by achieving\nhigher grasping success rate over existing RL baselines using only tens of\ndemonstrations. WildLMa exploits CLIP for language-conditioned imitation\nlearning that empirically generalizes to objects unseen in training\ndemonstrations. Besides extensive quantitative evaluation, we qualitatively\ndemonstrate practical robot applications, such as cleaning up trash in\nuniversity hallways or outdoor terrains, operating articulated objects, and\nrearranging items on a bookshelf."
                },
                "authors": [
                    {
                        "name": "Ri-Zhao Qiu"
                    },
                    {
                        "name": "Yuchen Song"
                    },
                    {
                        "name": "Xuanbin Peng"
                    },
                    {
                        "name": "Sai Aneesh Suryadevara"
                    },
                    {
                        "name": "Ge Yang"
                    },
                    {
                        "name": "Minghuan Liu"
                    },
                    {
                        "name": "Mazeyu Ji"
                    },
                    {
                        "name": "Chengzhe Jia"
                    },
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "arxiv_comment": "Website: https://wildlma.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15129v1",
                "updated": "2024-11-22T18:55:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    55,
                    21,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:55:21Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    55,
                    21,
                    4,
                    327,
                    0
                ],
                "title": "Measuring Bullshit in the Language Games played by ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Bullshit in the Language Games played by ChatGPT"
                },
                "summary": "Generative large language models (LLMs), which create text without direct\ncorrespondence to truth value, are widely understood to resemble the uses of\nlanguage described in Frankfurt's popular monograph On Bullshit. In this paper,\nwe offer a rigorous investigation of this topic, identifying how the phenomenon\nhas arisen, and how it might be analysed. In this paper, we elaborate on this\nargument to propose that LLM-based chatbots play the 'language game of\nbullshit'. We use statistical text analysis to investigate the features of this\nWittgensteinian language game, based on a dataset constructed to contrast the\nlanguage of 1,000 scientific publications with typical pseudo-scientific text\ngenerated by ChatGPT. We then explore whether the same language features can be\ndetected in two well-known contexts of social dysfunction: George Orwell's\ncritique of politics and language, and David Graeber's characterisation of\nbullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a\nstatistical model of the language of bullshit can reliably relate the\nFrankfurtian artificial bullshit of ChatGPT to the political and workplace\nfunctions of bullshit as observed in natural human language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs), which create text without direct\ncorrespondence to truth value, are widely understood to resemble the uses of\nlanguage described in Frankfurt's popular monograph On Bullshit. In this paper,\nwe offer a rigorous investigation of this topic, identifying how the phenomenon\nhas arisen, and how it might be analysed. In this paper, we elaborate on this\nargument to propose that LLM-based chatbots play the 'language game of\nbullshit'. We use statistical text analysis to investigate the features of this\nWittgensteinian language game, based on a dataset constructed to contrast the\nlanguage of 1,000 scientific publications with typical pseudo-scientific text\ngenerated by ChatGPT. We then explore whether the same language features can be\ndetected in two well-known contexts of social dysfunction: George Orwell's\ncritique of politics and language, and David Graeber's characterisation of\nbullshit jobs. Using simple hypothesis-testing methods, we demonstrate that a\nstatistical model of the language of bullshit can reliably relate the\nFrankfurtian artificial bullshit of ChatGPT to the political and workplace\nfunctions of bullshit as observed in natural human language."
                },
                "authors": [
                    {
                        "name": "Alessandro Trevisan"
                    },
                    {
                        "name": "Harry Giddens"
                    },
                    {
                        "name": "Sarah Dillon"
                    },
                    {
                        "name": "Alan F. Blackwell"
                    }
                ],
                "author_detail": {
                    "name": "Alan F. Blackwell"
                },
                "author": "Alan F. Blackwell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15100v1",
                "updated": "2024-11-22T18:01:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    1,
                    37,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:01:37Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    1,
                    37,
                    4,
                    327,
                    0
                ],
                "title": "XGrammar: Flexible and Efficient Structured Generation Engine for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XGrammar: Flexible and Efficient Structured Generation Engine for Large\n  Language Models"
                },
                "summary": "The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving."
                },
                "authors": [
                    {
                        "name": "Yixin Dong"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yaxing Cai"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15091v1",
                "updated": "2024-11-22T17:40:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    17,
                    40,
                    16,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T17:40:16Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    17,
                    40,
                    16,
                    4,
                    327,
                    0
                ],
                "title": "Somesite I Used To Crawl: Awareness, Agency and Efficacy in Protecting\n  Content Creators From AI Crawlers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Somesite I Used To Crawl: Awareness, Agency and Efficacy in Protecting\n  Content Creators From AI Crawlers"
                },
                "summary": "The success of generative AI relies heavily on training on data scraped\nthrough extensive crawling of the Internet, a practice that has raised\nsignificant copyright, privacy, and ethical concerns. While few measures are\ndesigned to resist a resource-rich adversary determined to scrape a site,\ncrawlers can be impacted by a range of existing tools such as robots.txt, NoAI\nmeta tags, and active crawler blocking by reverse proxies.\n  In this work, we seek to understand the ability and efficacy of today's\nnetworking tools to protect content creators against AI-related crawling. For\ntargeted populations like human artists, do they have the technical knowledge\nand agency to utilize crawler-blocking tools such as robots.txt, and can such\ntools be effective? Using large scale measurements and a targeted user study of\n182 professional artists, we find strong demand for tools like robots.txt, but\nsignificantly constrained by significant hurdles in technical awareness, agency\nin deploying them, and limited efficacy against unresponsive crawlers. We\nfurther test and evaluate network level crawler blockers by reverse-proxies,\nand find that despite very limited deployment today, their reliable and\ncomprehensive blocking of AI-crawlers make them the strongest protection for\nartists moving forward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of generative AI relies heavily on training on data scraped\nthrough extensive crawling of the Internet, a practice that has raised\nsignificant copyright, privacy, and ethical concerns. While few measures are\ndesigned to resist a resource-rich adversary determined to scrape a site,\ncrawlers can be impacted by a range of existing tools such as robots.txt, NoAI\nmeta tags, and active crawler blocking by reverse proxies.\n  In this work, we seek to understand the ability and efficacy of today's\nnetworking tools to protect content creators against AI-related crawling. For\ntargeted populations like human artists, do they have the technical knowledge\nand agency to utilize crawler-blocking tools such as robots.txt, and can such\ntools be effective? Using large scale measurements and a targeted user study of\n182 professional artists, we find strong demand for tools like robots.txt, but\nsignificantly constrained by significant hurdles in technical awareness, agency\nin deploying them, and limited efficacy against unresponsive crawlers. We\nfurther test and evaluate network level crawler blockers by reverse-proxies,\nand find that despite very limited deployment today, their reliable and\ncomprehensive blocking of AI-crawlers make them the strongest protection for\nartists moving forward."
                },
                "authors": [
                    {
                        "name": "Enze Liu"
                    },
                    {
                        "name": "Elisa Luo"
                    },
                    {
                        "name": "Shawn Shan"
                    },
                    {
                        "name": "Geoffrey M. Voelker"
                    },
                    {
                        "name": "Ben Y. Zhao"
                    },
                    {
                        "name": "Stefan Savage"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Savage"
                },
                "author": "Stefan Savage",
                "arxiv_comment": "Under Submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.03720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.03720v2",
                "updated": "2024-11-22T16:34:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    34,
                    12,
                    4,
                    327,
                    0
                ],
                "published": "2023-11-26T08:44:58Z",
                "published_parsed": [
                    2023,
                    11,
                    26,
                    8,
                    44,
                    58,
                    6,
                    330,
                    0
                ],
                "title": "Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Negotiating with LLMS: Prompt Hacks, Skill Gaps, and Reasoning Deficits"
                },
                "summary": "Large language models LLMs like ChatGPT have reached the 100 Mio user barrier\nin record time and might increasingly enter all areas of our life leading to a\ndiverse set of interactions between those Artificial Intelligence models and\nhumans. While many studies have discussed governance and regulations\ndeductively from first-order principles, few studies provide an inductive,\ndata-driven lens based on observing dialogues between humans and LLMs\nespecially when it comes to non-collaborative, competitive situations that have\nthe potential to pose a serious threat to people. In this work, we conduct a\nuser study engaging over 40 individuals across all age groups in price\nnegotiations with an LLM. We explore how people interact with an LLM,\ninvestigating differences in negotiation outcomes and strategies. Furthermore,\nwe highlight shortcomings of LLMs with respect to their reasoning capabilities\nand, in turn, susceptiveness to prompt hacking, which intends to manipulate the\nLLM to make agreements that are against its instructions or beyond any\nrationality. We also show that the negotiated prices humans manage to achieve\nspan a broad range, which points to a literacy gap in effectively interacting\nwith LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models LLMs like ChatGPT have reached the 100 Mio user barrier\nin record time and might increasingly enter all areas of our life leading to a\ndiverse set of interactions between those Artificial Intelligence models and\nhumans. While many studies have discussed governance and regulations\ndeductively from first-order principles, few studies provide an inductive,\ndata-driven lens based on observing dialogues between humans and LLMs\nespecially when it comes to non-collaborative, competitive situations that have\nthe potential to pose a serious threat to people. In this work, we conduct a\nuser study engaging over 40 individuals across all age groups in price\nnegotiations with an LLM. We explore how people interact with an LLM,\ninvestigating differences in negotiation outcomes and strategies. Furthermore,\nwe highlight shortcomings of LLMs with respect to their reasoning capabilities\nand, in turn, susceptiveness to prompt hacking, which intends to manipulate the\nLLM to make agreements that are against its instructions or beyond any\nrationality. We also show that the negotiated prices humans manage to achieve\nspan a broad range, which points to a literacy gap in effectively interacting\nwith LLMs."
                },
                "authors": [
                    {
                        "name": "Johannes Schneider"
                    },
                    {
                        "name": "Steffi Haag"
                    },
                    {
                        "name": "Leona Chandra Kruse"
                    }
                ],
                "author_detail": {
                    "name": "Leona Chandra Kruse"
                },
                "author": "Leona Chandra Kruse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.03720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.03720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00903v2",
                "updated": "2024-11-22T16:31:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    31,
                    25,
                    4,
                    327,
                    0
                ],
                "published": "2024-10-01T17:46:21Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    17,
                    46,
                    21,
                    1,
                    275,
                    0
                ],
                "title": "Causal Representation Learning with Generative Artificial Intelligence:\n  Application to Texts as Treatments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Representation Learning with Generative Artificial Intelligence:\n  Application to Texts as Treatments"
                },
                "summary": "In this paper, we demonstrate how to enhance the validity of causal inference\nwith unstructured high-dimensional treatments like texts, by leveraging the\npower of generative Artificial Intelligence. Specifically, we propose to use a\ndeep generative model such as large language models (LLMs) to efficiently\ngenerate treatments and use their internal representation for subsequent causal\neffect estimation. We show that the knowledge of this true internal\nrepresentation helps disentangle the treatment features of interest, such as\nspecific sentiments and certain topics, from other possibly unknown confounding\nfeatures. Unlike the existing methods, our proposed approach eliminates the\nneed to learn causal representation from the data and hence produces more\naccurate and efficient estimates. We formally establish the conditions required\nfor the nonparametric identification of the average treatment effect, propose\nan estimation strategy that avoids the violation of the overlap assumption, and\nderive the asymptotic properties of the proposed estimator through the\napplication of double machine learning. Finally, using an instrumental\nvariables approach, we extend the proposed methodology to the settings, in\nwhich the treatment feature is based on human perception rather than is assumed\nto be fixed given the treatment object. The proposed methodology is also\napplicable to text reuse where an LLM is used to regenerate the existing texts.\nWe conduct simulation and empirical studies, using the generated text data from\nan open-source LLM, Llama 3, to illustrate the advantages of our estimator over\nthe state-of-the-art causal representation learning algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we demonstrate how to enhance the validity of causal inference\nwith unstructured high-dimensional treatments like texts, by leveraging the\npower of generative Artificial Intelligence. Specifically, we propose to use a\ndeep generative model such as large language models (LLMs) to efficiently\ngenerate treatments and use their internal representation for subsequent causal\neffect estimation. We show that the knowledge of this true internal\nrepresentation helps disentangle the treatment features of interest, such as\nspecific sentiments and certain topics, from other possibly unknown confounding\nfeatures. Unlike the existing methods, our proposed approach eliminates the\nneed to learn causal representation from the data and hence produces more\naccurate and efficient estimates. We formally establish the conditions required\nfor the nonparametric identification of the average treatment effect, propose\nan estimation strategy that avoids the violation of the overlap assumption, and\nderive the asymptotic properties of the proposed estimator through the\napplication of double machine learning. Finally, using an instrumental\nvariables approach, we extend the proposed methodology to the settings, in\nwhich the treatment feature is based on human perception rather than is assumed\nto be fixed given the treatment object. The proposed methodology is also\napplicable to text reuse where an LLM is used to regenerate the existing texts.\nWe conduct simulation and empirical studies, using the generated text data from\nan open-source LLM, Llama 3, to illustrate the advantages of our estimator over\nthe state-of-the-art causal representation learning algorithms."
                },
                "authors": [
                    {
                        "name": "Kosuke Imai"
                    },
                    {
                        "name": "Kentaro Nakamura"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Nakamura"
                },
                "author": "Kentaro Nakamura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08977v2",
                "updated": "2024-11-22T16:22:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    22,
                    35,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-13T19:08:23Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    19,
                    8,
                    23,
                    2,
                    318,
                    0
                ],
                "title": "Robustness and Confounders in the Demographic Alignment of LLMs with\n  Human Perceptions of Offensiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness and Confounders in the Demographic Alignment of LLMs with\n  Human Perceptions of Offensiveness"
                },
                "summary": "Large language models (LLMs) are known to exhibit demographic biases, yet few\nstudies systematically evaluate these biases across multiple datasets or\naccount for confounding factors. In this work, we examine LLM alignment with\nhuman annotations in five offensive language datasets, comprising approximately\n220K annotations. Our findings reveal that while demographic traits,\nparticularly race, influence alignment, these effects are inconsistent across\ndatasets and often entangled with other factors. Confounders -- such as\ndocument difficulty, annotator sensitivity, and within-group agreement --\naccount for more variation in alignment patterns than demographic traits alone.\nSpecifically, alignment increases with higher annotator sensitivity and group\nagreement, while greater document difficulty corresponds to reduced alignment.\nOur results underscore the importance of multi-dataset analyses and\nconfounder-aware methodologies in developing robust measures of demographic\nbias in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to exhibit demographic biases, yet few\nstudies systematically evaluate these biases across multiple datasets or\naccount for confounding factors. In this work, we examine LLM alignment with\nhuman annotations in five offensive language datasets, comprising approximately\n220K annotations. Our findings reveal that while demographic traits,\nparticularly race, influence alignment, these effects are inconsistent across\ndatasets and often entangled with other factors. Confounders -- such as\ndocument difficulty, annotator sensitivity, and within-group agreement --\naccount for more variation in alignment patterns than demographic traits alone.\nSpecifically, alignment increases with higher annotator sensitivity and group\nagreement, while greater document difficulty corresponds to reduced alignment.\nOur results underscore the importance of multi-dataset analyses and\nconfounder-aware methodologies in developing robust measures of demographic\nbias in LLMs."
                },
                "authors": [
                    {
                        "name": "Shayan Alipour"
                    },
                    {
                        "name": "Indira Sen"
                    },
                    {
                        "name": "Mattia Samory"
                    },
                    {
                        "name": "Tanushree Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tanushree Mitra"
                },
                "author": "Tanushree Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15036v1",
                "updated": "2024-11-22T16:08:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    8,
                    42,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T16:08:42Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    8,
                    42,
                    4,
                    327,
                    0
                ],
                "title": "Safe Multi-Agent Reinforcement Learning with Convergence to Generalized\n  Nash Equilibrium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe Multi-Agent Reinforcement Learning with Convergence to Generalized\n  Nash Equilibrium"
                },
                "summary": "Multi-agent reinforcement learning (MARL) has achieved notable success in\ncooperative tasks, demonstrating impressive performance and scalability.\nHowever, deploying MARL agents in real-world applications presents critical\nsafety challenges. Current safe MARL algorithms are largely based on the\nconstrained Markov decision process (CMDP) framework, which enforces\nconstraints only on discounted cumulative costs and lacks an all-time safety\nassurance. Moreover, these methods often overlook the feasibility issue (the\nsystem will inevitably violate state constraints within certain regions of the\nconstraint set), resulting in either suboptimal performance or increased\nconstraint violations. To address these challenges, we propose a novel\ntheoretical framework for safe MARL with $\\textit{state-wise}$ constraints,\nwhere safety requirements are enforced at every state the agents visit. To\nresolve the feasibility issue, we leverage a control-theoretic notion of the\nfeasible region, the controlled invariant set (CIS), characterized by the\nsafety value function. We develop a multi-agent method for identifying CISs,\nensuring convergence to a Nash equilibrium on the safety value function. By\nincorporating CIS identification into the learning process, we introduce a\nmulti-agent dual policy iteration algorithm that guarantees convergence to a\ngeneralized Nash equilibrium in state-wise constrained cooperative Markov\ngames, achieving an optimal balance between feasibility and performance.\nFurthermore, for practical deployment in complex high-dimensional systems, we\npropose $\\textit{Multi-Agent Dual Actor-Critic}$ (MADAC), a safe MARL algorithm\nthat approximates the proposed iteration scheme within the deep RL paradigm.\nEmpirical evaluations on safe MARL benchmarks demonstrate that MADAC\nconsistently outperforms existing methods, delivering much higher rewards while\nreducing constraint violations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent reinforcement learning (MARL) has achieved notable success in\ncooperative tasks, demonstrating impressive performance and scalability.\nHowever, deploying MARL agents in real-world applications presents critical\nsafety challenges. Current safe MARL algorithms are largely based on the\nconstrained Markov decision process (CMDP) framework, which enforces\nconstraints only on discounted cumulative costs and lacks an all-time safety\nassurance. Moreover, these methods often overlook the feasibility issue (the\nsystem will inevitably violate state constraints within certain regions of the\nconstraint set), resulting in either suboptimal performance or increased\nconstraint violations. To address these challenges, we propose a novel\ntheoretical framework for safe MARL with $\\textit{state-wise}$ constraints,\nwhere safety requirements are enforced at every state the agents visit. To\nresolve the feasibility issue, we leverage a control-theoretic notion of the\nfeasible region, the controlled invariant set (CIS), characterized by the\nsafety value function. We develop a multi-agent method for identifying CISs,\nensuring convergence to a Nash equilibrium on the safety value function. By\nincorporating CIS identification into the learning process, we introduce a\nmulti-agent dual policy iteration algorithm that guarantees convergence to a\ngeneralized Nash equilibrium in state-wise constrained cooperative Markov\ngames, achieving an optimal balance between feasibility and performance.\nFurthermore, for practical deployment in complex high-dimensional systems, we\npropose $\\textit{Multi-Agent Dual Actor-Critic}$ (MADAC), a safe MARL algorithm\nthat approximates the proposed iteration scheme within the deep RL paradigm.\nEmpirical evaluations on safe MARL benchmarks demonstrate that MADAC\nconsistently outperforms existing methods, delivering much higher rewards while\nreducing constraint violations."
                },
                "authors": [
                    {
                        "name": "Zeyang Li"
                    },
                    {
                        "name": "Navid Azizan"
                    }
                ],
                "author_detail": {
                    "name": "Navid Azizan"
                },
                "author": "Navid Azizan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15033v1",
                "updated": "2024-11-22T16:05:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    5,
                    54,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T16:05:54Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    5,
                    54,
                    4,
                    327,
                    0
                ],
                "title": "One to rule them all: natural language to bind communication, perception\n  and action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One to rule them all: natural language to bind communication, perception\n  and action"
                },
                "summary": "In recent years, research in the area of human-robot interaction has focused\non developing robots capable of understanding complex human instructions and\nperforming tasks in dynamic and diverse environments. These systems have a wide\nrange of applications, from personal assistance to industrial robotics,\nemphasizing the importance of robots interacting flexibly, naturally and safely\nwith humans. This paper presents an advanced architecture for robotic action\nplanning that integrates communication, perception, and planning with Large\nLanguage Models (LLMs). Our system is designed to translate commands expressed\nin natural language into executable robot actions, incorporating environmental\ninformation and dynamically updating plans based on real-time feedback. The\nPlanner Module is the core of the system where LLMs embedded in a modified\nReAct framework are employed to interpret and carry out user commands. By\nleveraging their extensive pre-trained knowledge, LLMs can effectively process\nuser requests without the need to introduce new knowledge on the changing\nenvironment. The modified ReAct framework further enhances the execution space\nby providing real-time environmental perception and the outcomes of physical\nactions. By combining robust and dynamic semantic map representations as graphs\nwith control components and failure explanations, this architecture enhances a\nrobot adaptability, task execution, and seamless collaboration with human users\nin shared and dynamic environments. Through the integration of continuous\nfeedback loops with the environment the system can dynamically adjusts the plan\nto accommodate unexpected changes, optimizing the robot ability to perform\ntasks. Using a dataset of previous experience is possible to provide detailed\nfeedback about the failure. Updating the LLMs context of the next iteration\nwith suggestion on how to overcame the issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, research in the area of human-robot interaction has focused\non developing robots capable of understanding complex human instructions and\nperforming tasks in dynamic and diverse environments. These systems have a wide\nrange of applications, from personal assistance to industrial robotics,\nemphasizing the importance of robots interacting flexibly, naturally and safely\nwith humans. This paper presents an advanced architecture for robotic action\nplanning that integrates communication, perception, and planning with Large\nLanguage Models (LLMs). Our system is designed to translate commands expressed\nin natural language into executable robot actions, incorporating environmental\ninformation and dynamically updating plans based on real-time feedback. The\nPlanner Module is the core of the system where LLMs embedded in a modified\nReAct framework are employed to interpret and carry out user commands. By\nleveraging their extensive pre-trained knowledge, LLMs can effectively process\nuser requests without the need to introduce new knowledge on the changing\nenvironment. The modified ReAct framework further enhances the execution space\nby providing real-time environmental perception and the outcomes of physical\nactions. By combining robust and dynamic semantic map representations as graphs\nwith control components and failure explanations, this architecture enhances a\nrobot adaptability, task execution, and seamless collaboration with human users\nin shared and dynamic environments. Through the integration of continuous\nfeedback loops with the environment the system can dynamically adjusts the plan\nto accommodate unexpected changes, optimizing the robot ability to perform\ntasks. Using a dataset of previous experience is possible to provide detailed\nfeedback about the failure. Updating the LLMs context of the next iteration\nwith suggestion on how to overcame the issue."
                },
                "authors": [
                    {
                        "name": "Simone Colombani"
                    },
                    {
                        "name": "Dimitri Ognibene"
                    },
                    {
                        "name": "Giuseppe Boccignone"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Boccignone"
                },
                "author": "Giuseppe Boccignone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23054v2",
                "updated": "2024-11-22T16:04:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    16,
                    4,
                    44,
                    4,
                    327,
                    0
                ],
                "published": "2024-10-30T14:21:33Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    21,
                    33,
                    2,
                    304,
                    0
                ],
                "title": "Controlling Language and Diffusion Models by Transporting Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Language and Diffusion Models by Transporting Activations"
                },
                "summary": "The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing capabilities of large generative models and their ever more\nwidespread deployment have raised concerns about their reliability, safety, and\npotential misuse. To address these issues, recent works have proposed to\ncontrol model generation by steering model activations in order to effectively\ninduce or prevent the emergence of concepts or behaviors in the generated\noutput. In this paper we introduce Activation Transport (AcT), a general\nframework to steer activations guided by optimal transport theory that\ngeneralizes many previous activation-steering works. AcT is modality-agnostic\nand provides fine-grained control over the model behavior with negligible\ncomputational overhead, while minimally impacting model abilities. We\nexperimentally show the effectiveness and versatility of our approach by\naddressing key challenges in large language models (LLMs) and text-to-image\ndiffusion models (T2Is). For LLMs, we show that AcT can effectively mitigate\ntoxicity, induce arbitrary concepts, and increase their truthfulness. In T2Is,\nwe show how AcT enables fine-grained style control and concept negation."
                },
                "authors": [
                    {
                        "name": "Pau Rodriguez"
                    },
                    {
                        "name": "Arno Blaas"
                    },
                    {
                        "name": "Michal Klein"
                    },
                    {
                        "name": "Luca Zappella"
                    },
                    {
                        "name": "Nicholas Apostoloff"
                    },
                    {
                        "name": "Marco Cuturi"
                    },
                    {
                        "name": "Xavier Suau"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Suau"
                },
                "author": "Xavier Suau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 49Q22",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.7; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15027v1",
                "updated": "2024-11-22T15:58:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    58,
                    26,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:58:26Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    58,
                    26,
                    4,
                    327,
                    0
                ],
                "title": "Time is on my sight: scene graph filtering for dynamic environment\n  perception in an LLM-driven robot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time is on my sight: scene graph filtering for dynamic environment\n  perception in an LLM-driven robot"
                },
                "summary": "Robots are increasingly being used in dynamic environments like workplaces,\nhospitals, and homes. As a result, interactions with robots must be simple and\nintuitive, with robots perception adapting efficiently to human-induced\nchanges. This paper presents a robot control architecture that addresses key\nchallenges in human-robot interaction, with a particular focus on the dynamic\ncreation and continuous update of the robot state representation. The\narchitecture uses Large Language Models to integrate diverse information\nsources, including natural language commands, robotic skills representation,\nreal-time dynamic semantic mapping of the perceived scene. This enables\nflexible and adaptive robotic behavior in complex, dynamic environments.\nTraditional robotic systems often rely on static, pre-programmed instructions\nand settings, limiting their adaptability to dynamic environments and real-time\ncollaboration. In contrast, this architecture uses LLMs to interpret complex,\nhigh-level instructions and generate actionable plans that enhance human-robot\ncollaboration. At its core, the system Perception Module generates and\ncontinuously updates a semantic scene graph using RGB-D sensor data, providing\na detailed and structured representation of the environment. A particle filter\nis employed to ensure accurate object localization in dynamic, real-world\nsettings. The Planner Module leverages this up-to-date semantic map to break\ndown high-level tasks into sub-tasks and link them to robotic skills such as\nnavigation, object manipulation (e.g., PICK and PLACE), and movement (e.g.,\nGOTO). By combining real-time perception, state tracking, and LLM-driven\ncommunication and task planning, the architecture enhances adaptability, task\nefficiency, and human-robot collaboration in dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robots are increasingly being used in dynamic environments like workplaces,\nhospitals, and homes. As a result, interactions with robots must be simple and\nintuitive, with robots perception adapting efficiently to human-induced\nchanges. This paper presents a robot control architecture that addresses key\nchallenges in human-robot interaction, with a particular focus on the dynamic\ncreation and continuous update of the robot state representation. The\narchitecture uses Large Language Models to integrate diverse information\nsources, including natural language commands, robotic skills representation,\nreal-time dynamic semantic mapping of the perceived scene. This enables\nflexible and adaptive robotic behavior in complex, dynamic environments.\nTraditional robotic systems often rely on static, pre-programmed instructions\nand settings, limiting their adaptability to dynamic environments and real-time\ncollaboration. In contrast, this architecture uses LLMs to interpret complex,\nhigh-level instructions and generate actionable plans that enhance human-robot\ncollaboration. At its core, the system Perception Module generates and\ncontinuously updates a semantic scene graph using RGB-D sensor data, providing\na detailed and structured representation of the environment. A particle filter\nis employed to ensure accurate object localization in dynamic, real-world\nsettings. The Planner Module leverages this up-to-date semantic map to break\ndown high-level tasks into sub-tasks and link them to robotic skills such as\nnavigation, object manipulation (e.g., PICK and PLACE), and movement (e.g.,\nGOTO). By combining real-time perception, state tracking, and LLM-driven\ncommunication and task planning, the architecture enhances adaptability, task\nefficiency, and human-robot collaboration in dynamic environments."
                },
                "authors": [
                    {
                        "name": "Simone Colombani"
                    },
                    {
                        "name": "Luca Brini"
                    },
                    {
                        "name": "Dimitri Ognibene"
                    },
                    {
                        "name": "Giuseppe Boccignone"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Boccignone"
                },
                "author": "Giuseppe Boccignone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15020v1",
                "updated": "2024-11-22T15:49:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    49,
                    27,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:49:27Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    49,
                    27,
                    4,
                    327,
                    0
                ],
                "title": "ZT-SDN: An ML-powered Zero-Trust Architecture for Software-Defined\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZT-SDN: An ML-powered Zero-Trust Architecture for Software-Defined\n  Networks"
                },
                "summary": "Zero Trust (ZT) is a security paradigm aiming to curtail an attacker's\nlateral movements within a network by implementing least-privilege and\nper-request access control policies. However, its widespread adoption is\nhindered by the difficulty of generating proper rules due to the lack of\ndetailed knowledge of communication requirements and the characteristic\nbehaviors of communicating entities under benign conditions. Consequently,\nmanual rule generation becomes cumbersome and error-prone. To address these\nproblems, we propose ZT-SDN, an automated framework for learning and enforcing\nnetwork access control in Software-Defined Networks. ZT-SDN collects data from\nthe underlying network and models the network \"transactions\" performed by\ncommunicating entities as graphs. The nodes represent entities, while the\ndirected edges represent transactions identified by different protocol stacks\nobserved. It uses novel unsupervised learning approaches to extract transaction\npatterns directly from the network data, such as the allowed protocol stacks\nand port numbers and data transmission behavior. Finally, ZT-SDN uses an\ninnovative approach to generate correct access control rules and infer strong\nassociations between them, allowing proactive rule deployment in forwarding\ndevices. We show the framework's efficacy in detecting abnormal network\naccesses and abuses of permitted flows in changing network conditions with real\nnetwork datasets. Additionally, we showcase ZT-SDN's scalability and the\nnetwork's performance when applied in an SDN environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero Trust (ZT) is a security paradigm aiming to curtail an attacker's\nlateral movements within a network by implementing least-privilege and\nper-request access control policies. However, its widespread adoption is\nhindered by the difficulty of generating proper rules due to the lack of\ndetailed knowledge of communication requirements and the characteristic\nbehaviors of communicating entities under benign conditions. Consequently,\nmanual rule generation becomes cumbersome and error-prone. To address these\nproblems, we propose ZT-SDN, an automated framework for learning and enforcing\nnetwork access control in Software-Defined Networks. ZT-SDN collects data from\nthe underlying network and models the network \"transactions\" performed by\ncommunicating entities as graphs. The nodes represent entities, while the\ndirected edges represent transactions identified by different protocol stacks\nobserved. It uses novel unsupervised learning approaches to extract transaction\npatterns directly from the network data, such as the allowed protocol stacks\nand port numbers and data transmission behavior. Finally, ZT-SDN uses an\ninnovative approach to generate correct access control rules and infer strong\nassociations between them, allowing proactive rule deployment in forwarding\ndevices. We show the framework's efficacy in detecting abnormal network\naccesses and abuses of permitted flows in changing network conditions with real\nnetwork datasets. Additionally, we showcase ZT-SDN's scalability and the\nnetwork's performance when applied in an SDN environment."
                },
                "authors": [
                    {
                        "name": "Charalampos Katsis"
                    },
                    {
                        "name": "Elisa Bertino"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Bertino"
                },
                "author": "Elisa Bertino",
                "arxiv_comment": "32 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05966v3",
                "updated": "2024-11-22T15:36:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    36,
                    32,
                    4,
                    327,
                    0
                ],
                "published": "2024-05-09T17:59:32Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    17,
                    59,
                    32,
                    3,
                    130,
                    0
                ],
                "title": "Natural Language Processing RELIES on Linguistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Processing RELIES on Linguistics"
                },
                "summary": "Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become capable of generating highly fluent\ntext in certain languages, without modules specially designed to capture\ngrammar or semantic coherence. What does this mean for the future of linguistic\nexpertise in NLP? We highlight several aspects in which NLP (still) relies on\nlinguistics, or where linguistic thinking can illuminate new directions. We\nargue our case around the acronym RELIES that encapsulates six major facets\nwhere linguistics contributes to NLP: Resources, Evaluation, Low-resource\nsettings, Interpretability, Explanation, and the Study of language. This list\nis not exhaustive, nor is linguistics the main point of reference for every\neffort under these themes; but at a macro level, these facets highlight the\nenduring importance of studying machine systems vis-\\`a-vis systems of human\nlanguage."
                },
                "authors": [
                    {
                        "name": "Juri Opitz"
                    },
                    {
                        "name": "Shira Wein"
                    },
                    {
                        "name": "Nathan Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Schneider"
                },
                "author": "Nathan Schneider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15007v1",
                "updated": "2024-11-22T15:31:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    31,
                    20,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:31:20Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    31,
                    20,
                    4,
                    327,
                    0
                ],
                "title": "FTA generation using GenAI with an Autonomy sensor Usecase",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FTA generation using GenAI with an Autonomy sensor Usecase"
                },
                "summary": "Functional safety forms an important aspect in the design of systems. Its\nemphasis on the automotive industry has evolved significantly over the years.\nTill date many methods have been developed to get appropriate FTA(Fault Tree\nanalysis) for various scenarios and features pertaining to Autonomous Driving.\nThis paper is an attempt to explore the scope of using Generative Artificial\nIntelligence(GenAI) in order to develop Fault Tree Analysis(FTA) with the use\ncase of malfunction for the Lidar sensor in mind. We explore various available\nopen source Large Language Models(LLM) models and then dive deep into one of\nthem to study its responses and provide our analysis. This paper successfully\nshows the possibility to train existing Large Language models through Prompt\nEngineering for fault tree analysis for any Autonomy usecase aided with\nPlantUML tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional safety forms an important aspect in the design of systems. Its\nemphasis on the automotive industry has evolved significantly over the years.\nTill date many methods have been developed to get appropriate FTA(Fault Tree\nanalysis) for various scenarios and features pertaining to Autonomous Driving.\nThis paper is an attempt to explore the scope of using Generative Artificial\nIntelligence(GenAI) in order to develop Fault Tree Analysis(FTA) with the use\ncase of malfunction for the Lidar sensor in mind. We explore various available\nopen source Large Language Models(LLM) models and then dive deep into one of\nthem to study its responses and provide our analysis. This paper successfully\nshows the possibility to train existing Large Language models through Prompt\nEngineering for fault tree analysis for any Autonomy usecase aided with\nPlantUML tool."
                },
                "authors": [
                    {
                        "name": "Sneha Sudhir Shetiya"
                    },
                    {
                        "name": "Divya Garikapati"
                    },
                    {
                        "name": "Veeraja Sohoni"
                    }
                ],
                "author_detail": {
                    "name": "Veeraja Sohoni"
                },
                "author": "Veeraja Sohoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15004v1",
                "updated": "2024-11-22T15:26:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    26,
                    23,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:26:23Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    26,
                    23,
                    4,
                    327,
                    0
                ],
                "title": "ScribeAgent: Towards Specialized Web Agents Using Production-Scale\n  Workflow Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScribeAgent: Towards Specialized Web Agents Using Production-Scale\n  Workflow Data"
                },
                "summary": "Large Language Model (LLM) agents are rapidly improving to handle\nincreasingly complex web-based tasks. Most of these agents rely on\ngeneral-purpose, proprietary models like GPT-4 and focus on designing better\nprompts to improve their planning abilities. However, general-purpose LLMs are\nnot specifically trained to understand specialized web contexts such as HTML,\nand they often struggle with long-horizon planning. We explore an alternative\napproach that fine-tunes open-source LLMs using production-scale workflow data\ncollected from over 250 domains corresponding to 6 billion tokens. This simple\nyet effective approach shows substantial gains over prompting-based agents on\nexisting benchmarks -- ScribeAgent achieves state-of-the-art direct generation\nperformance on Mind2Web and improves the task success rate by 14.1% over the\nprevious best text-only web agents on WebArena. We further perform detailed\nablation studies on various fine-tuning design choices and provide insights\ninto LLM selection, training recipes, context window optimization, and effect\nof dataset sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents are rapidly improving to handle\nincreasingly complex web-based tasks. Most of these agents rely on\ngeneral-purpose, proprietary models like GPT-4 and focus on designing better\nprompts to improve their planning abilities. However, general-purpose LLMs are\nnot specifically trained to understand specialized web contexts such as HTML,\nand they often struggle with long-horizon planning. We explore an alternative\napproach that fine-tunes open-source LLMs using production-scale workflow data\ncollected from over 250 domains corresponding to 6 billion tokens. This simple\nyet effective approach shows substantial gains over prompting-based agents on\nexisting benchmarks -- ScribeAgent achieves state-of-the-art direct generation\nperformance on Mind2Web and improves the task success rate by 14.1% over the\nprevious best text-only web agents on WebArena. We further perform detailed\nablation studies on various fine-tuning design choices and provide insights\ninto LLM selection, training recipes, context window optimization, and effect\nof dataset sizes."
                },
                "authors": [
                    {
                        "name": "Junhong Shen"
                    },
                    {
                        "name": "Atishay Jain"
                    },
                    {
                        "name": "Zedian Xiao"
                    },
                    {
                        "name": "Ishan Amlekar"
                    },
                    {
                        "name": "Mouad Hadji"
                    },
                    {
                        "name": "Aaron Podolny"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    }
                ],
                "author_detail": {
                    "name": "Ameet Talwalkar"
                },
                "author": "Ameet Talwalkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13485v2",
                "updated": "2024-11-22T15:24:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    24,
                    7,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-20T17:35:21Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    35,
                    21,
                    2,
                    325,
                    0
                ],
                "title": "Utilizing Large Language Models to Synthesize Product Desirability\n  Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models to Synthesize Product Desirability\n  Datasets"
                },
                "summary": "This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production."
                },
                "authors": [
                    {
                        "name": "John D. Hastings"
                    },
                    {
                        "name": "Sherri Weitl-Harms"
                    },
                    {
                        "name": "Joseph Doty"
                    },
                    {
                        "name": "Zachary J. Myers"
                    },
                    {
                        "name": "Warren Thompson"
                    }
                ],
                "author_detail": {
                    "name": "Warren Thompson"
                },
                "author": "Warren Thompson",
                "arxiv_comment": "9 pages, 2 figures, 6 tables, updated author list",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.2.6; H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08127v2",
                "updated": "2024-11-22T14:58:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    58,
                    31,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-12T19:09:45Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    19,
                    9,
                    45,
                    1,
                    317,
                    0
                ],
                "title": "TIPO: Text to Image with Text Presampling for Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIPO: Text to Image with Text Presampling for Prompt Optimization"
                },
                "summary": "TIPO (Text to Image with text pre-sampling for Prompt Optimization) is an\ninnovative framework designed to enhance text-to-image (T2I) generation by\nlanguage model (LM) for automatic prompt engineering. By refining and extending\nuser-provided prompts, TIPO bridges the gap between simple inputs and the\ndetailed prompts required for high-quality image generation. Unlike previous\napproaches that rely on Large Language Models (LLMs) or reinforcement learning\n(RL), TIPO adjusts user input prompts with the distribution of a trained prompt\ndataset, eliminating the need for complex runtime cost via lightweight model.\nThis pre-sampling approach enables efficient and scalable prompt optimization,\ngrounded in the model's training distribution. Experimental results demonstrate\nTIPO's effectiveness in improving aesthetic scores, reducing image corruption,\nand better aligning generated images with dataset distributions. These findings\nhighlight the critical role of prompt engineering in T2I systems and open\navenues for broader applications of automatic prompt refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIPO (Text to Image with text pre-sampling for Prompt Optimization) is an\ninnovative framework designed to enhance text-to-image (T2I) generation by\nlanguage model (LM) for automatic prompt engineering. By refining and extending\nuser-provided prompts, TIPO bridges the gap between simple inputs and the\ndetailed prompts required for high-quality image generation. Unlike previous\napproaches that rely on Large Language Models (LLMs) or reinforcement learning\n(RL), TIPO adjusts user input prompts with the distribution of a trained prompt\ndataset, eliminating the need for complex runtime cost via lightweight model.\nThis pre-sampling approach enables efficient and scalable prompt optimization,\ngrounded in the model's training distribution. Experimental results demonstrate\nTIPO's effectiveness in improving aesthetic scores, reducing image corruption,\nand better aligning generated images with dataset distributions. These findings\nhighlight the critical role of prompt engineering in T2I systems and open\navenues for broader applications of automatic prompt refinement."
                },
                "authors": [
                    {
                        "name": "Shih-Ying Yeh"
                    },
                    {
                        "name": "Sang-Hyun Park"
                    },
                    {
                        "name": "Giyeong Oh"
                    },
                    {
                        "name": "Min Song"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "26 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14986v1",
                "updated": "2024-11-22T14:47:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    47,
                    0,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T14:47:00Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    47,
                    0,
                    4,
                    327,
                    0
                ],
                "title": "Generative AI may backfire for counterspeech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI may backfire for counterspeech"
                },
                "summary": "Online hate speech poses a serious threat to individual well-being and\nsocietal cohesion. A promising solution to curb online hate speech is\ncounterspeech. Counterspeech is aimed at encouraging users to reconsider\nhateful posts by direct replies. However, current methods lack scalability due\nto the need for human intervention or fail to adapt to the specific context of\nthe post. A potential remedy is the use of generative AI, specifically large\nlanguage models (LLMs), to write tailored counterspeech messages. In this\npaper, we analyze whether contextualized counterspeech generated by\nstate-of-the-art LLMs is effective in curbing online hate speech. To do so, we\nconducted a large-scale, pre-registered field experiment (N=2,664) on the\nsocial media platform Twitter/X. Our experiment followed a 2x2 between-subjects\ndesign and, additionally, a control condition with no counterspeech. On the one\nhand, users posting hateful content on Twitter/X were randomly assigned to\nreceive either (a) contextualized counterspeech or (b) non-contextualized\ncounterspeech. Here, the former is generated through LLMs, while the latter\nrelies on predefined, generic messages. On the other hand, we tested two\ncounterspeech strategies: (a) promoting empathy and (b) warning about the\nconsequences of online misbehavior. We then measured whether users deleted\ntheir initial hateful posts and whether their behavior changed after the\ncounterspeech intervention (e.g., whether users adopted a less toxic language).\nWe find that non-contextualized counterspeech employing a\nwarning-of-consequence strategy significantly reduces online hate speech.\nHowever, contextualized counterspeech generated by LLMs proves ineffective and\nmay even backfire.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online hate speech poses a serious threat to individual well-being and\nsocietal cohesion. A promising solution to curb online hate speech is\ncounterspeech. Counterspeech is aimed at encouraging users to reconsider\nhateful posts by direct replies. However, current methods lack scalability due\nto the need for human intervention or fail to adapt to the specific context of\nthe post. A potential remedy is the use of generative AI, specifically large\nlanguage models (LLMs), to write tailored counterspeech messages. In this\npaper, we analyze whether contextualized counterspeech generated by\nstate-of-the-art LLMs is effective in curbing online hate speech. To do so, we\nconducted a large-scale, pre-registered field experiment (N=2,664) on the\nsocial media platform Twitter/X. Our experiment followed a 2x2 between-subjects\ndesign and, additionally, a control condition with no counterspeech. On the one\nhand, users posting hateful content on Twitter/X were randomly assigned to\nreceive either (a) contextualized counterspeech or (b) non-contextualized\ncounterspeech. Here, the former is generated through LLMs, while the latter\nrelies on predefined, generic messages. On the other hand, we tested two\ncounterspeech strategies: (a) promoting empathy and (b) warning about the\nconsequences of online misbehavior. We then measured whether users deleted\ntheir initial hateful posts and whether their behavior changed after the\ncounterspeech intervention (e.g., whether users adopted a less toxic language).\nWe find that non-contextualized counterspeech employing a\nwarning-of-consequence strategy significantly reduces online hate speech.\nHowever, contextualized counterspeech generated by LLMs proves ineffective and\nmay even backfire."
                },
                "authors": [
                    {
                        "name": "Dominik Bär"
                    },
                    {
                        "name": "Abdurahman Maarouf"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Feuerriegel"
                },
                "author": "Stefan Feuerriegel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02079v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02079v3",
                "updated": "2024-11-22T14:30:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    30,
                    28,
                    4,
                    327,
                    0
                ],
                "published": "2024-07-02T09:16:43Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    16,
                    43,
                    1,
                    184,
                    0
                ],
                "title": "Theseus: Exploring Efficient Wafer-Scale Chip Design for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theseus: Exploring Efficient Wafer-Scale Chip Design for Large Language\n  Models"
                },
                "summary": "The emergence of the large language model~(LLM) poses an exponential growth\nof demand for computation throughput, memory capacity, and communication\nbandwidth. Such a demand growth has significantly surpassed the improvement of\ncorresponding chip designs. With the advancement of fabrication and integration\ntechnologies, designers have been developing Wafer-Scale Chips~(WSCs) to scale\nup and exploit the limits of computation density, memory capacity, and\ncommunication bandwidth at the level of a single chip. Existing solutions have\ndemonstrated the significant advantages of WSCs over traditional designs,\nshowing potential to effectively support LLM workloads.\n  Despite the benefits, exploring the early-stage design space of WSCs for LLMs\nis a crucial yet challenging task due to the enormous and complicated design\nspace, time-consuming evaluation methods, and inefficient exploration\nstrategies. To address these challenges, we propose Theseus, an efficient WSC\ndesign space exploration framework for LLMs. We construct the design space of\nWSCs with various constraints considering the unique characteristics of WSCs.\nWe propose efficient evaluation methodologies for large-scale NoC-based WSCs\nand introduce multi-fidelity Bayesian optimization to efficiently explore the\ndesign space. Evaluation results demonstrate the efficiency of Theseus that the\nsearched Pareto optimal results outperform GPU cluster and existing WSC designs\nby up to 62.8\\%/73.7\\% in performance and 38.6\\%/42.4\\% in power consumption\nfor LLM training, while improving up to 23.2$\\times$ and 15.7$\\times$ for the\nperformance and power of inference tasks. Furthermore, we conduct case studies\nto address the design tradeoffs in WSCs and provide insights to facilitate WSC\ndesigns for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of the large language model~(LLM) poses an exponential growth\nof demand for computation throughput, memory capacity, and communication\nbandwidth. Such a demand growth has significantly surpassed the improvement of\ncorresponding chip designs. With the advancement of fabrication and integration\ntechnologies, designers have been developing Wafer-Scale Chips~(WSCs) to scale\nup and exploit the limits of computation density, memory capacity, and\ncommunication bandwidth at the level of a single chip. Existing solutions have\ndemonstrated the significant advantages of WSCs over traditional designs,\nshowing potential to effectively support LLM workloads.\n  Despite the benefits, exploring the early-stage design space of WSCs for LLMs\nis a crucial yet challenging task due to the enormous and complicated design\nspace, time-consuming evaluation methods, and inefficient exploration\nstrategies. To address these challenges, we propose Theseus, an efficient WSC\ndesign space exploration framework for LLMs. We construct the design space of\nWSCs with various constraints considering the unique characteristics of WSCs.\nWe propose efficient evaluation methodologies for large-scale NoC-based WSCs\nand introduce multi-fidelity Bayesian optimization to efficiently explore the\ndesign space. Evaluation results demonstrate the efficiency of Theseus that the\nsearched Pareto optimal results outperform GPU cluster and existing WSC designs\nby up to 62.8\\%/73.7\\% in performance and 38.6\\%/42.4\\% in power consumption\nfor LLM training, while improving up to 23.2$\\times$ and 15.7$\\times$ for the\nperformance and power of inference tasks. Furthermore, we conduct case studies\nto address the design tradeoffs in WSCs and provide insights to facilitate WSC\ndesigns for LLMs."
                },
                "authors": [
                    {
                        "name": "Jingchen Zhu"
                    },
                    {
                        "name": "Chenhao Xue"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yu Shen"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Zekang Cheng"
                    },
                    {
                        "name": "Yu Jiang"
                    },
                    {
                        "name": "Tianqi Wang"
                    },
                    {
                        "name": "Yibo Lin"
                    },
                    {
                        "name": "Wei Hu"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02079v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02079v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14971v1",
                "updated": "2024-11-22T14:27:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    27,
                    27,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T14:27:27Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    27,
                    27,
                    4,
                    327,
                    0
                ],
                "title": "Leveraging LLMs for Legacy Code Modernization: Challenges and\n  Opportunities for LLM-Generated Documentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Legacy Code Modernization: Challenges and\n  Opportunities for LLM-Generated Documentation"
                },
                "summary": "Legacy software systems, written in outdated languages like MUMPS and\nmainframe assembly, pose challenges in efficiency, maintenance, staffing, and\nsecurity. While LLMs offer promise for modernizing these systems, their ability\nto understand legacy languages is largely unknown. This paper investigates the\nutilization of LLMs to generate documentation for legacy code using two\ndatasets: an electronic health records (EHR) system in MUMPS and open-source\napplications in IBM mainframe Assembly Language Code (ALC). We propose a\nprompting strategy for generating line-wise code comments and a rubric to\nevaluate their completeness, readability, usefulness, and hallucination. Our\nstudy assesses the correlation between human evaluations and automated metrics,\nsuch as code complexity and reference-based metrics. We find that LLM-generated\ncomments for MUMPS and ALC are generally hallucination-free, complete,\nreadable, and useful compared to ground-truth comments, though ALC poses\nchallenges. However, no automated metrics strongly correlate with comment\nquality to predict or measure LLM performance. Our findings highlight the\nlimitations of current automated measures and the need for better evaluation\nmetrics for LLM-generated documentation in legacy systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legacy software systems, written in outdated languages like MUMPS and\nmainframe assembly, pose challenges in efficiency, maintenance, staffing, and\nsecurity. While LLMs offer promise for modernizing these systems, their ability\nto understand legacy languages is largely unknown. This paper investigates the\nutilization of LLMs to generate documentation for legacy code using two\ndatasets: an electronic health records (EHR) system in MUMPS and open-source\napplications in IBM mainframe Assembly Language Code (ALC). We propose a\nprompting strategy for generating line-wise code comments and a rubric to\nevaluate their completeness, readability, usefulness, and hallucination. Our\nstudy assesses the correlation between human evaluations and automated metrics,\nsuch as code complexity and reference-based metrics. We find that LLM-generated\ncomments for MUMPS and ALC are generally hallucination-free, complete,\nreadable, and useful compared to ground-truth comments, though ALC poses\nchallenges. However, no automated metrics strongly correlate with comment\nquality to predict or measure LLM performance. Our findings highlight the\nlimitations of current automated measures and the need for better evaluation\nmetrics for LLM-generated documentation in legacy systems."
                },
                "authors": [
                    {
                        "name": "Colin Diggs"
                    },
                    {
                        "name": "Michael Doyle"
                    },
                    {
                        "name": "Amit Madan"
                    },
                    {
                        "name": "Siggy Scott"
                    },
                    {
                        "name": "Emily Escamilla"
                    },
                    {
                        "name": "Jacob Zimmer"
                    },
                    {
                        "name": "Naveed Nekoo"
                    },
                    {
                        "name": "Paul Ursino"
                    },
                    {
                        "name": "Michael Bartholf"
                    },
                    {
                        "name": "Zachary Robin"
                    },
                    {
                        "name": "Anand Patel"
                    },
                    {
                        "name": "Chris Glasz"
                    },
                    {
                        "name": "William Macke"
                    },
                    {
                        "name": "Paul Kirk"
                    },
                    {
                        "name": "Jasper Phillips"
                    },
                    {
                        "name": "Arun Sridharan"
                    },
                    {
                        "name": "Doug Wendt"
                    },
                    {
                        "name": "Scott Rosen"
                    },
                    {
                        "name": "Nitin Naik"
                    },
                    {
                        "name": "Justin F. Brunelle"
                    },
                    {
                        "name": "Samruddhi Thaker"
                    }
                ],
                "author_detail": {
                    "name": "Samruddhi Thaker"
                },
                "author": "Samruddhi Thaker",
                "arxiv_comment": "Abbreviated version submitted to LLM4Code 2025 (a workshop co-located\n  with ICSE 2025), 13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14967v1",
                "updated": "2024-11-22T14:23:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    23,
                    7,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T14:23:07Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    23,
                    7,
                    4,
                    327,
                    0
                ],
                "title": "SwissADT: An Audio Description Translation System for Swiss Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwissADT: An Audio Description Translation System for Swiss Languages"
                },
                "summary": "Audio description (AD) is a crucial accessibility service provided to blind\npersons and persons with visual impairment, designed to convey visual\ninformation in acoustic form. Despite recent advancements in multilingual\nmachine translation research, the lack of well-crafted and time-synchronized AD\ndata impedes the development of audio description translation (ADT) systems\nthat address the needs of multilingual countries such as Switzerland.\nFurthermore, since the majority of ADT systems rely solely on text, uncertainty\nexists as to whether incorporating visual information from the corresponding\nvideo clips can enhance the quality of ADT outputs. In this work, we present\nSwissADT, the first ADT system implemented for three main Swiss languages and\nEnglish. By collecting well-crafted AD data augmented with video clips in\nGerman, French, Italian, and English, and leveraging the power of Large\nLanguage Models (LLMs), we aim to enhance information accessibility for diverse\nlanguage populations in Switzerland by automatically translating AD scripts to\nthe desired Swiss language. Our extensive experimental ADT results, composed of\nboth automatic and human evaluations of ADT quality, demonstrate the promising\ncapability of SwissADT for the ADT task. We believe that combining human\nexpertise with the generation power of LLMs can further enhance the performance\nof ADT systems, ultimately benefiting a larger multilingual target population.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio description (AD) is a crucial accessibility service provided to blind\npersons and persons with visual impairment, designed to convey visual\ninformation in acoustic form. Despite recent advancements in multilingual\nmachine translation research, the lack of well-crafted and time-synchronized AD\ndata impedes the development of audio description translation (ADT) systems\nthat address the needs of multilingual countries such as Switzerland.\nFurthermore, since the majority of ADT systems rely solely on text, uncertainty\nexists as to whether incorporating visual information from the corresponding\nvideo clips can enhance the quality of ADT outputs. In this work, we present\nSwissADT, the first ADT system implemented for three main Swiss languages and\nEnglish. By collecting well-crafted AD data augmented with video clips in\nGerman, French, Italian, and English, and leveraging the power of Large\nLanguage Models (LLMs), we aim to enhance information accessibility for diverse\nlanguage populations in Switzerland by automatically translating AD scripts to\nthe desired Swiss language. Our extensive experimental ADT results, composed of\nboth automatic and human evaluations of ADT quality, demonstrate the promising\ncapability of SwissADT for the ADT task. We believe that combining human\nexpertise with the generation power of LLMs can further enhance the performance\nof ADT systems, ultimately benefiting a larger multilingual target population."
                },
                "authors": [
                    {
                        "name": "Lukas Fischer"
                    },
                    {
                        "name": "Yingqiang Gao"
                    },
                    {
                        "name": "Alexa Lintner"
                    },
                    {
                        "name": "Sarah Ebling"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Ebling"
                },
                "author": "Sarah Ebling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14962v1",
                "updated": "2024-11-22T14:21:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    21,
                    18,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T14:21:18Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    21,
                    18,
                    4,
                    327,
                    0
                ],
                "title": "LLM for Barcodes: Generating Diverse Synthetic Data for Identity\n  Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM for Barcodes: Generating Diverse Synthetic Data for Identity\n  Documents"
                },
                "summary": "Accurate barcode detection and decoding in Identity documents is crucial for\napplications like security, healthcare, and education, where reliable data\nextraction and verification are essential. However, building robust detection\nmodels is challenging due to the lack of diverse, realistic datasets an issue\noften tied to privacy concerns and the wide variety of document formats.\nTraditional tools like Faker rely on predefined templates, making them less\neffective for capturing the complexity of real-world identity documents. In\nthis paper, we introduce a new approach to synthetic data generation that uses\nLLMs to create contextually rich and realistic data without relying on\npredefined field. Using the vast knowledge LLMs have about different documents\nand content, our method creates data that reflects the variety found in real\nidentity documents. This data is then encoded into barcode and overlayed on\ntemplates for documents such as Driver's licenses, Insurance cards, Student\nIDs. Our approach simplifies the process of dataset creation, eliminating the\nneed for extensive domain knowledge or predefined fields. Compared to\ntraditional methods like Faker, data generated by LLM demonstrates greater\ndiversity and contextual relevance, leading to improved performance in barcode\ndetection models. This scalable, privacy-first solution is a big step forward\nin advancing machine learning for automated document processing and identity\nverification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate barcode detection and decoding in Identity documents is crucial for\napplications like security, healthcare, and education, where reliable data\nextraction and verification are essential. However, building robust detection\nmodels is challenging due to the lack of diverse, realistic datasets an issue\noften tied to privacy concerns and the wide variety of document formats.\nTraditional tools like Faker rely on predefined templates, making them less\neffective for capturing the complexity of real-world identity documents. In\nthis paper, we introduce a new approach to synthetic data generation that uses\nLLMs to create contextually rich and realistic data without relying on\npredefined field. Using the vast knowledge LLMs have about different documents\nand content, our method creates data that reflects the variety found in real\nidentity documents. This data is then encoded into barcode and overlayed on\ntemplates for documents such as Driver's licenses, Insurance cards, Student\nIDs. Our approach simplifies the process of dataset creation, eliminating the\nneed for extensive domain knowledge or predefined fields. Compared to\ntraditional methods like Faker, data generated by LLM demonstrates greater\ndiversity and contextual relevance, leading to improved performance in barcode\ndetection models. This scalable, privacy-first solution is a big step forward\nin advancing machine learning for automated document processing and identity\nverification."
                },
                "authors": [
                    {
                        "name": "Hitesh Laxmichand Patel"
                    },
                    {
                        "name": "Amit Agarwal"
                    },
                    {
                        "name": "Bhargava Kumar"
                    },
                    {
                        "name": "Karan Gupta"
                    },
                    {
                        "name": "Priyaranjan Pattnayak"
                    }
                ],
                "author_detail": {
                    "name": "Priyaranjan Pattnayak"
                },
                "author": "Priyaranjan Pattnayak",
                "arxiv_comment": "5 pages, 1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14959v1",
                "updated": "2024-11-22T14:17:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    17,
                    46,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T14:17:46Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    17,
                    46,
                    4,
                    327,
                    0
                ],
                "title": "Design-o-meter: Towards Evaluating and Refining Graphic Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design-o-meter: Towards Evaluating and Refining Graphic Designs"
                },
                "summary": "Graphic designs are an effective medium for visual communication. They range\nfrom greeting cards to corporate flyers and beyond. Off-late, machine learning\ntechniques are able to generate such designs, which accelerates the rate of\ncontent production. An automated way of evaluating their quality becomes\ncritical. Towards this end, we introduce Design-o-meter, a data-driven\nmethodology to quantify the goodness of graphic designs. Further, our approach\ncan suggest modifications to these designs to improve its visual appeal. To the\nbest of our knowledge, Design-o-meter is the first approach that scores and\nrefines designs in a unified framework despite the inherent subjectivity and\nambiguity of the setting. Our exhaustive quantitative and qualitative analysis\nof our approach against baselines adapted for the task (including recent\nMultimodal LLM-based approaches) brings out the efficacy of our methodology. We\nhope our work will usher more interest in this important and pragmatic problem\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphic designs are an effective medium for visual communication. They range\nfrom greeting cards to corporate flyers and beyond. Off-late, machine learning\ntechniques are able to generate such designs, which accelerates the rate of\ncontent production. An automated way of evaluating their quality becomes\ncritical. Towards this end, we introduce Design-o-meter, a data-driven\nmethodology to quantify the goodness of graphic designs. Further, our approach\ncan suggest modifications to these designs to improve its visual appeal. To the\nbest of our knowledge, Design-o-meter is the first approach that scores and\nrefines designs in a unified framework despite the inherent subjectivity and\nambiguity of the setting. Our exhaustive quantitative and qualitative analysis\nof our approach against baselines adapted for the task (including recent\nMultimodal LLM-based approaches) brings out the efficacy of our methodology. We\nhope our work will usher more interest in this important and pragmatic problem\nsetting."
                },
                "authors": [
                    {
                        "name": "Sahil Goyal"
                    },
                    {
                        "name": "Abhinav Mahajan"
                    },
                    {
                        "name": "Swasti Mishra"
                    },
                    {
                        "name": "Prateksha Udhayanan"
                    },
                    {
                        "name": "Tripti Shukla"
                    },
                    {
                        "name": "K J Joseph"
                    },
                    {
                        "name": "Balaji Vasan Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Balaji Vasan Srinivasan"
                },
                "author": "Balaji Vasan Srinivasan",
                "arxiv_comment": "Accepted to WACV 2025. Project page:\n  https://sahilg06.github.io/Design-o-meter/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02451v2",
                "updated": "2024-11-22T14:11:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    14,
                    11,
                    13,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-03T10:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    10,
                    6,
                    14,
                    6,
                    308,
                    0
                ],
                "title": "High-performance automated abstract screening with large language model\n  ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-performance automated abstract screening with large language model\n  ensembles"
                },
                "summary": "Large language models (LLMs) excel in tasks requiring processing and\ninterpretation of input text. Abstract screening is a labour-intensive\ncomponent of systematic review involving repetitive application of inclusion\nand exclusion criteria on a large volume of studies identified by a literature\nsearch. Here, LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Llama 3 70B, Gemini 1.5\nPro, and Claude Sonnet 3.5) were trialled on systematic reviews in a full issue\nof the Cochrane Library to evaluate their accuracy in zero-shot binary\nclassification for abstract screening. Trials over a subset of 800 records\nidentified optimal prompting strategies and demonstrated superior performance\nof LLMs to human researchers in terms of sensitivity (LLM-max = 1.000,\nhuman-max = 0.775), precision (LLM-max = 0.927, human-max = 0.911), and\nbalanced accuracy (LLM-max = 0.904, human-max = 0.865). The best performing\nLLM-prompt combinations were trialled across every replicated search result (n\n= 119,691), and exhibited consistent sensitivity (range 0.756-1.000) but\ndiminished precision (range 0.004-0.096). 66 LLM-human and LLM-LLM ensembles\nexhibited perfect sensitivity with a maximal precision of 0.458, with less\nobserved performance drop in larger trials. Significant variation in\nperformance was observed between reviews, highlighting the importance of\ndomain-specific validation before deployment. LLMs may reduce the human labour\ncost of systematic review with maintained or improved accuracy and sensitivity.\nSystematic review is the foundation of evidence synthesis across academic\ndisciplines, including evidence-based medicine, and LLMs may increase the\nefficiency and quality of this mode of research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel in tasks requiring processing and\ninterpretation of input text. Abstract screening is a labour-intensive\ncomponent of systematic review involving repetitive application of inclusion\nand exclusion criteria on a large volume of studies identified by a literature\nsearch. Here, LLMs (GPT-3.5 Turbo, GPT-4 Turbo, GPT-4o, Llama 3 70B, Gemini 1.5\nPro, and Claude Sonnet 3.5) were trialled on systematic reviews in a full issue\nof the Cochrane Library to evaluate their accuracy in zero-shot binary\nclassification for abstract screening. Trials over a subset of 800 records\nidentified optimal prompting strategies and demonstrated superior performance\nof LLMs to human researchers in terms of sensitivity (LLM-max = 1.000,\nhuman-max = 0.775), precision (LLM-max = 0.927, human-max = 0.911), and\nbalanced accuracy (LLM-max = 0.904, human-max = 0.865). The best performing\nLLM-prompt combinations were trialled across every replicated search result (n\n= 119,691), and exhibited consistent sensitivity (range 0.756-1.000) but\ndiminished precision (range 0.004-0.096). 66 LLM-human and LLM-LLM ensembles\nexhibited perfect sensitivity with a maximal precision of 0.458, with less\nobserved performance drop in larger trials. Significant variation in\nperformance was observed between reviews, highlighting the importance of\ndomain-specific validation before deployment. LLMs may reduce the human labour\ncost of systematic review with maintained or improved accuracy and sensitivity.\nSystematic review is the foundation of evidence synthesis across academic\ndisciplines, including evidence-based medicine, and LLMs may increase the\nefficiency and quality of this mode of research."
                },
                "authors": [
                    {
                        "name": "Rohan Sanghera"
                    },
                    {
                        "name": "Arun James Thirunavukarasu"
                    },
                    {
                        "name": "Marc El Khoury"
                    },
                    {
                        "name": "Jessica O'Logbon"
                    },
                    {
                        "name": "Yuqing Chen"
                    },
                    {
                        "name": "Archie Watt"
                    },
                    {
                        "name": "Mustafa Mahmood"
                    },
                    {
                        "name": "Hamid Butt"
                    },
                    {
                        "name": "George Nishimura"
                    },
                    {
                        "name": "Andrew Soltan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Soltan"
                },
                "author": "Andrew Soltan",
                "arxiv_comment": "RS and AJT are joint-first authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14922v1",
                "updated": "2024-11-22T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    24,
                    1,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    24,
                    1,
                    4,
                    327,
                    0
                ],
                "title": "GOT4Rec: Graph of Thoughts for Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GOT4Rec: Graph of Thoughts for Sequential Recommendation"
                },
                "summary": "With the advancement of large language models (LLMs), researchers have\nexplored various methods to optimally leverage their comprehension and\ngeneration capabilities in sequential recommendation scenarios. However,\nseveral challenges persist in this endeavor. Firstly, most existing approaches\nrely on the input-output prompting paradigm, which can result in irrelevant or\ninaccurate responses. Secondly, while there have been attempts to enhance LLMs\nusing prompting strategies such as chain-of-thought (CoT), these efforts have\nnot fully harnessed the reasoning abilities of LLMs or effectively captured the\nmultifaceted information contained within user sequences. To address these\nlimitations, we propose GOT4Rec, a sequential recommendation method that\nutilizes the graph of thoughts (GoT) prompting strategy. Specifically, we\nidentify and utilize three key types of information within user history\nsequences: short-term interests, long-term interests and collaborative\ninformation from other users. Our approach enables LLMs to independently reason\nand generate recommendations based on these distinct types of information,\nsubsequently aggregating the results within the GoT framework to derive the\nfinal recommended items. This method allows LLMs, with enhanced reasoning\ncapabilities, to more effectively consider the diverse information within user\nsequences, resulting in more accurate recommendations and more comprehensive\nexplanations. Extensive experiments on real-world datasets demonstrate the\neffectiveness of GOT4Rec, indicating that it outperforms existing\nstate-of-the-art baselines. Our code is available at\nhttps://anonymous.4open.science/r/GOT4Rec-ED99.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of large language models (LLMs), researchers have\nexplored various methods to optimally leverage their comprehension and\ngeneration capabilities in sequential recommendation scenarios. However,\nseveral challenges persist in this endeavor. Firstly, most existing approaches\nrely on the input-output prompting paradigm, which can result in irrelevant or\ninaccurate responses. Secondly, while there have been attempts to enhance LLMs\nusing prompting strategies such as chain-of-thought (CoT), these efforts have\nnot fully harnessed the reasoning abilities of LLMs or effectively captured the\nmultifaceted information contained within user sequences. To address these\nlimitations, we propose GOT4Rec, a sequential recommendation method that\nutilizes the graph of thoughts (GoT) prompting strategy. Specifically, we\nidentify and utilize three key types of information within user history\nsequences: short-term interests, long-term interests and collaborative\ninformation from other users. Our approach enables LLMs to independently reason\nand generate recommendations based on these distinct types of information,\nsubsequently aggregating the results within the GoT framework to derive the\nfinal recommended items. This method allows LLMs, with enhanced reasoning\ncapabilities, to more effectively consider the diverse information within user\nsequences, resulting in more accurate recommendations and more comprehensive\nexplanations. Extensive experiments on real-world datasets demonstrate the\neffectiveness of GOT4Rec, indicating that it outperforms existing\nstate-of-the-art baselines. Our code is available at\nhttps://anonymous.4open.science/r/GOT4Rec-ED99."
                },
                "authors": [
                    {
                        "name": "Zewen Long"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14917v1",
                "updated": "2024-11-22T13:18:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    18,
                    41,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T13:18:41Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    18,
                    41,
                    4,
                    327,
                    0
                ],
                "title": "Task-Aware Robotic Grasping by evaluating Quality Diversity Solutions\n  through Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware Robotic Grasping by evaluating Quality Diversity Solutions\n  through Foundation Models"
                },
                "summary": "Task-aware robotic grasping is a challenging problem that requires the\nintegration of semantic understanding and geometric reasoning. Traditional\ngrasp planning approaches focus on stable or feasible grasps, often\ndisregarding the specific tasks the robot needs to accomplish. This paper\nproposes a novel framework that leverages Large Language Models (LLMs) and\nQuality Diversity (QD) algorithms to enable zero-shot task-conditioned grasp\nselection. The framework segments objects into meaningful subparts and labels\neach subpart semantically, creating structured representations that can be used\nto prompt an LLM. By coupling semantic and geometric representations of an\nobject's structure, the LLM's knowledge about tasks and which parts to grasp\ncan be applied in the physical world. The QD-generated grasp archive provides a\ndiverse set of grasps, allowing us to select the most suitable grasp based on\nthe task. We evaluate the proposed method on a subset of the YCB dataset, where\na Franka Emika robot is assigned to perform various actions based on\nobject-specific task requirements. We created a ground truth by conducting a\nsurvey with six participants to determine the best grasp region for each\ntask-object combination according to human intuition. The model was evaluated\non 12 different objects across 4--7 object-specific tasks, achieving a weighted\nintersection over union (IoU) of 76.4% when compared to the survey data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-aware robotic grasping is a challenging problem that requires the\nintegration of semantic understanding and geometric reasoning. Traditional\ngrasp planning approaches focus on stable or feasible grasps, often\ndisregarding the specific tasks the robot needs to accomplish. This paper\nproposes a novel framework that leverages Large Language Models (LLMs) and\nQuality Diversity (QD) algorithms to enable zero-shot task-conditioned grasp\nselection. The framework segments objects into meaningful subparts and labels\neach subpart semantically, creating structured representations that can be used\nto prompt an LLM. By coupling semantic and geometric representations of an\nobject's structure, the LLM's knowledge about tasks and which parts to grasp\ncan be applied in the physical world. The QD-generated grasp archive provides a\ndiverse set of grasps, allowing us to select the most suitable grasp based on\nthe task. We evaluate the proposed method on a subset of the YCB dataset, where\na Franka Emika robot is assigned to perform various actions based on\nobject-specific task requirements. We created a ground truth by conducting a\nsurvey with six participants to determine the best grasp region for each\ntask-object combination according to human intuition. The model was evaluated\non 12 different objects across 4--7 object-specific tasks, achieving a weighted\nintersection over union (IoU) of 76.4% when compared to the survey data."
                },
                "authors": [
                    {
                        "name": "Aurel X. Appius"
                    },
                    {
                        "name": "Emiland Garrabe"
                    },
                    {
                        "name": "Francois Helenon"
                    },
                    {
                        "name": "Mahdi Khoramshahi"
                    },
                    {
                        "name": "Stephane Doncieux"
                    }
                ],
                "author_detail": {
                    "name": "Stephane Doncieux"
                },
                "author": "Stephane Doncieux",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14914v1",
                "updated": "2024-11-22T13:15:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    15,
                    3,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T13:15:03Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    15,
                    3,
                    4,
                    327,
                    0
                ],
                "title": "A Reproducibility and Generalizability Study of Large Language Models\n  for Query Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducibility and Generalizability Study of Large Language Models\n  for Query Generation"
                },
                "summary": "Systematic literature reviews (SLRs) are a cornerstone of academic research,\nyet they are often labour-intensive and time-consuming due to the detailed\nliterature curation process. The advent of generative AI and large language\nmodels (LLMs) promises to revolutionize this process by assisting researchers\nin several tedious tasks, one of them being the generation of effective Boolean\nqueries that will select the publications to consider including in a review.\nThis paper presents an extensive study of Boolean query generation using LLMs\nfor systematic reviews, reproducing and extending the work of Wang et al. and\nAlaniz et al. Our study investigates the replicability and reliability of\nresults achieved using ChatGPT and compares its performance with open-source\nalternatives like Mistral and Zephyr to provide a more comprehensive analysis\nof LLMs for query generation.\n  Therefore, we implemented a pipeline, which automatically creates a Boolean\nquery for a given review topic by using a previously defined LLM, retrieves all\ndocuments for this query from the PubMed database and then evaluates the\nresults. With this pipeline we first assess whether the results obtained using\nChatGPT for query generation are reproducible and consistent. We then\ngeneralize our results by analyzing and evaluating open-source models and\nevaluating their efficacy in generating Boolean queries.\n  Finally, we conduct a failure analysis to identify and discuss the\nlimitations and shortcomings of using LLMs for Boolean query generation. This\nexamination helps to understand the gaps and potential areas for improvement in\nthe application of LLMs to information retrieval tasks. Our findings highlight\nthe strengths, limitations, and potential of LLMs in the domain of information\nretrieval and literature review automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic literature reviews (SLRs) are a cornerstone of academic research,\nyet they are often labour-intensive and time-consuming due to the detailed\nliterature curation process. The advent of generative AI and large language\nmodels (LLMs) promises to revolutionize this process by assisting researchers\nin several tedious tasks, one of them being the generation of effective Boolean\nqueries that will select the publications to consider including in a review.\nThis paper presents an extensive study of Boolean query generation using LLMs\nfor systematic reviews, reproducing and extending the work of Wang et al. and\nAlaniz et al. Our study investigates the replicability and reliability of\nresults achieved using ChatGPT and compares its performance with open-source\nalternatives like Mistral and Zephyr to provide a more comprehensive analysis\nof LLMs for query generation.\n  Therefore, we implemented a pipeline, which automatically creates a Boolean\nquery for a given review topic by using a previously defined LLM, retrieves all\ndocuments for this query from the PubMed database and then evaluates the\nresults. With this pipeline we first assess whether the results obtained using\nChatGPT for query generation are reproducible and consistent. We then\ngeneralize our results by analyzing and evaluating open-source models and\nevaluating their efficacy in generating Boolean queries.\n  Finally, we conduct a failure analysis to identify and discuss the\nlimitations and shortcomings of using LLMs for Boolean query generation. This\nexamination helps to understand the gaps and potential areas for improvement in\nthe application of LLMs to information retrieval tasks. Our findings highlight\nthe strengths, limitations, and potential of LLMs in the domain of information\nretrieval and literature review automation."
                },
                "authors": [
                    {
                        "name": "Moritz Staudinger"
                    },
                    {
                        "name": "Wojciech Kusa"
                    },
                    {
                        "name": "Florina Piroi"
                    },
                    {
                        "name": "Aldo Lipani"
                    },
                    {
                        "name": "Allan Hanbury"
                    }
                ],
                "author_detail": {
                    "name": "Allan Hanbury"
                },
                "author": "Allan Hanbury",
                "arxiv_doi": "10.1145/3673791.3698432",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3673791.3698432",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.14914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13187v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13187v3",
                "updated": "2024-11-22T13:05:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    5,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-20T10:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    40,
                    8,
                    2,
                    325,
                    0
                ],
                "title": "Engagement-Driven Content Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engagement-Driven Content Generation with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/."
                },
                "authors": [
                    {
                        "name": "Erica Coppolillo"
                    },
                    {
                        "name": "Federico Cinus"
                    },
                    {
                        "name": "Marco Minici"
                    },
                    {
                        "name": "Francesco Bonchi"
                    },
                    {
                        "name": "Giuseppe Manco"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Manco"
                },
                "author": "Giuseppe Manco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13187v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13187v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14905v1",
                "updated": "2024-11-22T13:03:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    3,
                    7,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T13:03:07Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    3,
                    7,
                    4,
                    327,
                    0
                ],
                "title": "Feasibility Study for Supporting Static Malware Analysis Using LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feasibility Study for Supporting Static Malware Analysis Using LLM"
                },
                "summary": "Large language models (LLMs) are becoming more advanced and widespread and\nhave shown their applicability to various domains, including cybersecurity.\nStatic malware analysis is one of the most important tasks in cybersecurity;\nhowever, it is time-consuming and requires a high level of expertise.\nTherefore, we conducted a demonstration experiment focusing on whether an LLM\ncan be used to support static analysis. First, we evaluated the ability of the\nLLM to explain malware functionality. The results showed that the LLM can\ngenerate descriptions that cover functions with an accuracy of up to 90.9\\%. In\naddition, we asked six static analysts to perform a pseudo static analysis task\nusing LLM explanations to verify that the LLM can be used in practice. Through\nsubsequent questionnaires and interviews with the participants, we also\ndemonstrated the practical applicability of LLMs. Lastly, we summarized the\nproblems and required functions when using an LLM as static analysis support,\nas well as recommendations for future research opportunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming more advanced and widespread and\nhave shown their applicability to various domains, including cybersecurity.\nStatic malware analysis is one of the most important tasks in cybersecurity;\nhowever, it is time-consuming and requires a high level of expertise.\nTherefore, we conducted a demonstration experiment focusing on whether an LLM\ncan be used to support static analysis. First, we evaluated the ability of the\nLLM to explain malware functionality. The results showed that the LLM can\ngenerate descriptions that cover functions with an accuracy of up to 90.9\\%. In\naddition, we asked six static analysts to perform a pseudo static analysis task\nusing LLM explanations to verify that the LLM can be used in practice. Through\nsubsequent questionnaires and interviews with the participants, we also\ndemonstrated the practical applicability of LLMs. Lastly, we summarized the\nproblems and required functions when using an LLM as static analysis support,\nas well as recommendations for future research opportunities."
                },
                "authors": [
                    {
                        "name": "Shota Fujii"
                    },
                    {
                        "name": "Rei Yamagishi"
                    }
                ],
                "author_detail": {
                    "name": "Rei Yamagishi"
                },
                "author": "Rei Yamagishi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14901v1",
                "updated": "2024-11-22T12:46:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    12,
                    46,
                    50,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T12:46:50Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    12,
                    46,
                    50,
                    4,
                    327,
                    0
                ],
                "title": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in\n  Hour-Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in\n  Hour-Long Videos"
                },
                "summary": "Large language models (LLMs) excel at retrieving information from lengthy\ntext, but their vision-language counterparts (VLMs) face difficulties with\nhour-long videos, especially for temporal grounding. Specifically, these VLMs\nare constrained by frame limitations, often losing essential temporal details\nneeded for accurate event localization in extended video content. We propose\nReVisionLLM, a recursive vision-language model designed to locate events in\nhour-long videos. Inspired by human search strategies, our model initially\ntargets broad segments of interest, progressively revising its focus to\npinpoint exact temporal boundaries. Our model can seamlessly handle videos of\nvastly different lengths, from minutes to hours. We also introduce a\nhierarchical training strategy that starts with short clips to capture distinct\nevents and progressively extends to longer videos. To our knowledge,\nReVisionLLM is the first VLM capable of temporal grounding in hour-long videos,\noutperforming previous state-of-the-art methods across multiple datasets by a\nsignificant margin (+2.6% R1@0.1 on MAD). The code is available at\nhttps://github.com/Tanveer81/ReVisionLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at retrieving information from lengthy\ntext, but their vision-language counterparts (VLMs) face difficulties with\nhour-long videos, especially for temporal grounding. Specifically, these VLMs\nare constrained by frame limitations, often losing essential temporal details\nneeded for accurate event localization in extended video content. We propose\nReVisionLLM, a recursive vision-language model designed to locate events in\nhour-long videos. Inspired by human search strategies, our model initially\ntargets broad segments of interest, progressively revising its focus to\npinpoint exact temporal boundaries. Our model can seamlessly handle videos of\nvastly different lengths, from minutes to hours. We also introduce a\nhierarchical training strategy that starts with short clips to capture distinct\nevents and progressively extends to longer videos. To our knowledge,\nReVisionLLM is the first VLM capable of temporal grounding in hour-long videos,\noutperforming previous state-of-the-art methods across multiple datasets by a\nsignificant margin (+2.6% R1@0.1 on MAD). The code is available at\nhttps://github.com/Tanveer81/ReVisionLLM."
                },
                "authors": [
                    {
                        "name": "Tanveer Hannan"
                    },
                    {
                        "name": "Md Mohaiminul Islam"
                    },
                    {
                        "name": "Jindong Gu"
                    },
                    {
                        "name": "Thomas Seidl"
                    },
                    {
                        "name": "Gedas Bertasius"
                    }
                ],
                "author_detail": {
                    "name": "Gedas Bertasius"
                },
                "author": "Gedas Bertasius",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14896v1",
                "updated": "2024-11-22T12:37:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    12,
                    37,
                    41,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T12:37:41Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    12,
                    37,
                    41,
                    4,
                    327,
                    0
                ],
                "title": "Evaluating LLM Prompts for Data Augmentation in Multi-label\n  Classification of Ecological Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLM Prompts for Data Augmentation in Multi-label\n  Classification of Ecological Texts"
                },
                "summary": "Large language models (LLMs) play a crucial role in natural language\nprocessing (NLP) tasks, improving the understanding, generation, and\nmanipulation of human language across domains such as translating, summarizing,\nand classifying text. Previous studies have demonstrated that instruction-based\nLLMs can be effectively utilized for data augmentation to generate diverse and\nrealistic text samples. This study applied prompt-based data augmentation to\ndetect mentions of green practices in Russian social media. Detecting green\npractices in social media aids in understanding their prevalence and helps\nformulate recommendations for scaling eco-friendly actions to mitigate\nenvironmental issues. We evaluated several prompts for augmenting texts in a\nmulti-label classification task, either by rewriting existing datasets using\nLLMs, generating new data, or combining both approaches. Our results revealed\nthat all strategies improved classification performance compared to the models\nfine-tuned only on the original dataset, outperforming baselines in most cases.\nThe best results were obtained with the prompt that paraphrased the original\ntext while clearly indicating the relevant categories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) play a crucial role in natural language\nprocessing (NLP) tasks, improving the understanding, generation, and\nmanipulation of human language across domains such as translating, summarizing,\nand classifying text. Previous studies have demonstrated that instruction-based\nLLMs can be effectively utilized for data augmentation to generate diverse and\nrealistic text samples. This study applied prompt-based data augmentation to\ndetect mentions of green practices in Russian social media. Detecting green\npractices in social media aids in understanding their prevalence and helps\nformulate recommendations for scaling eco-friendly actions to mitigate\nenvironmental issues. We evaluated several prompts for augmenting texts in a\nmulti-label classification task, either by rewriting existing datasets using\nLLMs, generating new data, or combining both approaches. Our results revealed\nthat all strategies improved classification performance compared to the models\nfine-tuned only on the original dataset, outperforming baselines in most cases.\nThe best results were obtained with the prompt that paraphrased the original\ntext while clearly indicating the relevant categories."
                },
                "authors": [
                    {
                        "name": "Anna Glazkova"
                    },
                    {
                        "name": "Olga Zakharova"
                    }
                ],
                "author_detail": {
                    "name": "Olga Zakharova"
                },
                "author": "Olga Zakharova",
                "arxiv_comment": "Ivannikov ISPRAS Open Conference (ISPRAS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4; I.7.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13677v2",
                "updated": "2024-11-22T12:03:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    12,
                    3,
                    34,
                    4,
                    327,
                    0
                ],
                "published": "2024-06-19T16:30:58Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    16,
                    30,
                    58,
                    2,
                    171,
                    0
                ],
                "title": "Leveraging Large Language Models to Measure Gender Representation Bias\n  in Gendered Language Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models to Measure Gender Representation Bias\n  in Gendered Language Corpora"
                },
                "summary": "Gender bias in text corpora that are used for a variety of natural language\nprocessing (NLP) tasks, such as for training large language models (LLMs), can\nlead to the perpetuation and amplification of societal inequalities. This\nphenomenon is particularly pronounced in gendered languages like Spanish or\nFrench, where grammatical structures inherently encode gender, making the bias\nanalysis more challenging. A first step in quantifying gender bias in text\nentails computing biases in gender representation, i.e., differences in the\nprevalence of words referring to males vs. females. Existing methods to measure\ngender representation bias in text corpora have mainly been proposed for\nEnglish and do not generalize to gendered languages due to the intrinsic\nlinguistic differences between English and gendered languages. This paper\nintroduces a novel methodology that leverages the contextual understanding\ncapabilities of LLMs to quantitatively measure gender representation bias in\nSpanish corpora. By utilizing LLMs to identify and classify gendered nouns and\npronouns in relation to their reference to human entities, our approach\nprovides a robust analysis of gender representation bias in gendered languages.\nWe empirically validate our method on four widely-used benchmark datasets,\nuncovering significant gender prevalence disparities with a male-to-female\nratio ranging from 4:1 to 6:1. These findings demonstrate the value of our\nmethodology for bias quantification in gendered language corpora and suggest\nits application in NLP, contributing to the development of more equitable\nlanguage technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gender bias in text corpora that are used for a variety of natural language\nprocessing (NLP) tasks, such as for training large language models (LLMs), can\nlead to the perpetuation and amplification of societal inequalities. This\nphenomenon is particularly pronounced in gendered languages like Spanish or\nFrench, where grammatical structures inherently encode gender, making the bias\nanalysis more challenging. A first step in quantifying gender bias in text\nentails computing biases in gender representation, i.e., differences in the\nprevalence of words referring to males vs. females. Existing methods to measure\ngender representation bias in text corpora have mainly been proposed for\nEnglish and do not generalize to gendered languages due to the intrinsic\nlinguistic differences between English and gendered languages. This paper\nintroduces a novel methodology that leverages the contextual understanding\ncapabilities of LLMs to quantitatively measure gender representation bias in\nSpanish corpora. By utilizing LLMs to identify and classify gendered nouns and\npronouns in relation to their reference to human entities, our approach\nprovides a robust analysis of gender representation bias in gendered languages.\nWe empirically validate our method on four widely-used benchmark datasets,\nuncovering significant gender prevalence disparities with a male-to-female\nratio ranging from 4:1 to 6:1. These findings demonstrate the value of our\nmethodology for bias quantification in gendered language corpora and suggest\nits application in NLP, contributing to the development of more equitable\nlanguage technologies."
                },
                "authors": [
                    {
                        "name": "Erik Derner"
                    },
                    {
                        "name": "Sara Sansalvador de la Fuente"
                    },
                    {
                        "name": "Yoan Gutiérrez"
                    },
                    {
                        "name": "Paloma Moreda"
                    },
                    {
                        "name": "Nuria Oliver"
                    }
                ],
                "author_detail": {
                    "name": "Nuria Oliver"
                },
                "author": "Nuria Oliver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00434v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00434v2",
                "updated": "2024-11-22T11:31:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    11,
                    31,
                    28,
                    4,
                    327,
                    0
                ],
                "published": "2024-10-01T06:33:40Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    33,
                    40,
                    1,
                    275,
                    0
                ],
                "title": "Rapid Integration of LLMs in Healthcare Raises Ethical Concerns: An\n  Investigation into Deceptive Patterns in Social Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid Integration of LLMs in Healthcare Raises Ethical Concerns: An\n  Investigation into Deceptive Patterns in Social Robots"
                },
                "summary": "Conversational agents are increasingly used in healthcare, and the\nintegration of Large Language Models (LLMs) has significantly enhanced their\ncapabilities. When integrated into social robots, LLMs offer the potential for\nmore natural interactions. However, while LLMs promise numerous benefits, they\nalso raise critical ethical concerns, particularly around the issue of\nhallucinations and deceptive patterns. In this case study, we observed a\ncritical pattern of deceptive behavior in commercially available LLM-based care\nsoftware integrated into robots. The LLM-equipped robot falsely claimed to have\nmedication reminder functionalities. Not only did these systems assure users of\ntheir ability to manage medication schedules, but they also proactively\nsuggested this capability, despite lacking it. This deceptive behavior poses\nsignificant risks in healthcare environments, where reliability is paramount.\nOur findings highlights the ethical and safety concerns surrounding the\ndeployment of LLM-integrated robots in healthcare, emphasizing the need for\noversight to prevent potentially harmful consequences for vulnerable\npopulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational agents are increasingly used in healthcare, and the\nintegration of Large Language Models (LLMs) has significantly enhanced their\ncapabilities. When integrated into social robots, LLMs offer the potential for\nmore natural interactions. However, while LLMs promise numerous benefits, they\nalso raise critical ethical concerns, particularly around the issue of\nhallucinations and deceptive patterns. In this case study, we observed a\ncritical pattern of deceptive behavior in commercially available LLM-based care\nsoftware integrated into robots. The LLM-equipped robot falsely claimed to have\nmedication reminder functionalities. Not only did these systems assure users of\ntheir ability to manage medication schedules, but they also proactively\nsuggested this capability, despite lacking it. This deceptive behavior poses\nsignificant risks in healthcare environments, where reliability is paramount.\nOur findings highlights the ethical and safety concerns surrounding the\ndeployment of LLM-integrated robots in healthcare, emphasizing the need for\noversight to prevent potentially harmful consequences for vulnerable\npopulations."
                },
                "authors": [
                    {
                        "name": "Robert Ranisch"
                    },
                    {
                        "name": "Joschka Haltaufderheide"
                    }
                ],
                "author_detail": {
                    "name": "Joschka Haltaufderheide"
                },
                "author": "Joschka Haltaufderheide",
                "arxiv_comment": "7 pages, 1table, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00434v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00434v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08713v2",
                "updated": "2024-11-22T11:25:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    11,
                    25,
                    34,
                    4,
                    327,
                    0
                ],
                "published": "2024-07-11T17:50:09Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    17,
                    50,
                    9,
                    3,
                    193,
                    0
                ],
                "title": "GTA: A Benchmark for General Tool Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: A Benchmark for General Tool Agents"
                },
                "summary": "Significant focus has been placed on integrating large language models (LLMs)\nwith various tools in developing general-purpose agents. This poses a challenge\nto LLMs' tool-use capabilities. However, there are evident gaps between\nexisting tool-use evaluations and real-world scenarios. Current evaluations\noften use AI-generated queries, single-step tasks, dummy tools, and text-only\ninteractions, failing to reveal the agents' real-world problem-solving\nabilities effectively. To address this, we propose GTA, a benchmark for General\nTool Agents, featuring three main aspects: (i) Real user queries: human-written\nqueries with simple real-world objectives but implicit tool-use, requiring the\nLLM to reason the suitable tools and plan the solution steps. (ii) Real\ndeployed tools: an evaluation platform equipped with tools across perception,\noperation, logic, and creativity categories to evaluate the agents' actual task\nexecution performance. (iii) Real multimodal inputs: authentic image files,\nsuch as spatial scenes, web page screenshots, tables, code snippets, and\nprinted/handwritten materials, used as the query contexts to align with\nreal-world scenarios closely. We design 229 real-world tasks and executable\ntool chains to evaluate mainstream LLMs. Our findings show that real-world user\nqueries are challenging for existing LLMs, with GPT-4 completing less than 50%\nof the tasks and most LLMs achieving below 25%. This evaluation reveals the\nbottlenecks in the tool-use capabilities of current LLMs in real-world\nscenarios, which provides future direction for advancing general-purpose tool\nagents. The code and dataset are available at\nhttps://github.com/open-compass/GTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant focus has been placed on integrating large language models (LLMs)\nwith various tools in developing general-purpose agents. This poses a challenge\nto LLMs' tool-use capabilities. However, there are evident gaps between\nexisting tool-use evaluations and real-world scenarios. Current evaluations\noften use AI-generated queries, single-step tasks, dummy tools, and text-only\ninteractions, failing to reveal the agents' real-world problem-solving\nabilities effectively. To address this, we propose GTA, a benchmark for General\nTool Agents, featuring three main aspects: (i) Real user queries: human-written\nqueries with simple real-world objectives but implicit tool-use, requiring the\nLLM to reason the suitable tools and plan the solution steps. (ii) Real\ndeployed tools: an evaluation platform equipped with tools across perception,\noperation, logic, and creativity categories to evaluate the agents' actual task\nexecution performance. (iii) Real multimodal inputs: authentic image files,\nsuch as spatial scenes, web page screenshots, tables, code snippets, and\nprinted/handwritten materials, used as the query contexts to align with\nreal-world scenarios closely. We design 229 real-world tasks and executable\ntool chains to evaluate mainstream LLMs. Our findings show that real-world user\nqueries are challenging for existing LLMs, with GPT-4 completing less than 50%\nof the tasks and most LLMs achieving below 25%. This evaluation reveals the\nbottlenecks in the tool-use capabilities of current LLMs in real-world\nscenarios, which provides future direction for advancing general-purpose tool\nagents. The code and dataset are available at\nhttps://github.com/open-compass/GTA."
                },
                "authors": [
                    {
                        "name": "Jize Wang"
                    },
                    {
                        "name": "Zerun Ma"
                    },
                    {
                        "name": "Yining Li"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Cailian Chen"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Xinyi Le"
                    }
                ],
                "author_detail": {
                    "name": "Xinyi Le"
                },
                "author": "Xinyi Le",
                "arxiv_comment": "Github repo: https://github.com/open-compass/GTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15371v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15371v4",
                "updated": "2024-11-22T10:40:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    10,
                    40,
                    35,
                    4,
                    327,
                    0
                ],
                "published": "2024-09-19T10:26:42Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    10,
                    26,
                    42,
                    3,
                    263,
                    0
                ],
                "title": "Bone: Block-Affine Adaptation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bone: Block-Affine Adaptation of Large Language Models"
                },
                "summary": "Low-Rank Adaptation (LoRA) has achieved remarkable training results by\nfreezing the original weights and training only low-rank matrices, establishing\nitself as the predominant fine-tuning method for LLMs. In pursuit of\nperformance closer to full-parameter training, a series of LoRA variants have\nemerged, such as LoRA+, PISSA, Olora, and LoRA-GA. This paper introduces a\nnovel PEFT technique distinct from LoRA, called Block-Affine Adaptation (Bone).\nBy dividing the original weights into multiple subspaces that share a single\nmatrix for weight updates, Bone simplifies the process by requiring the\ntrainable matrix to be initialized to zero, eliminating the need for complex\ninitialization as in some LoRA variants. Compared to LoRA, Bone significantly\nreduces memory usage and achieves faster computation. Evaluation of both NLU\nand NLG tasks demonstrates that Bone substantially outperforms LoRA and its\nvariants. Inspired by Pissa, we further proposed the ``Weight Guide'' theory to\nbetter utilize the information from the original weights. By integrating\n``Weight Guide'' with Bone, we developed a new structure called Block-Affine\nTransformation (Bat), and ablation experiments confirmed the effectiveness of\n``Weight Guide''.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has achieved remarkable training results by\nfreezing the original weights and training only low-rank matrices, establishing\nitself as the predominant fine-tuning method for LLMs. In pursuit of\nperformance closer to full-parameter training, a series of LoRA variants have\nemerged, such as LoRA+, PISSA, Olora, and LoRA-GA. This paper introduces a\nnovel PEFT technique distinct from LoRA, called Block-Affine Adaptation (Bone).\nBy dividing the original weights into multiple subspaces that share a single\nmatrix for weight updates, Bone simplifies the process by requiring the\ntrainable matrix to be initialized to zero, eliminating the need for complex\ninitialization as in some LoRA variants. Compared to LoRA, Bone significantly\nreduces memory usage and achieves faster computation. Evaluation of both NLU\nand NLG tasks demonstrates that Bone substantially outperforms LoRA and its\nvariants. Inspired by Pissa, we further proposed the ``Weight Guide'' theory to\nbetter utilize the information from the original weights. By integrating\n``Weight Guide'' with Bone, we developed a new structure called Block-Affine\nTransformation (Bat), and ablation experiments confirmed the effectiveness of\n``Weight Guide''."
                },
                "authors": [
                    {
                        "name": "Jiale Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jiale Kang"
                },
                "author": "Jiale Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15371v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15371v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14842v1",
                "updated": "2024-11-22T10:30:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    10,
                    30,
                    48,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T10:30:48Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    10,
                    30,
                    48,
                    4,
                    327,
                    0
                ],
                "title": "Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large\n  Language Models"
                },
                "summary": "Adversarial audio attacks pose a significant threat to the growing use of\nlarge language models (LLMs) in voice-based human-machine interactions. While\nexisting research has primarily focused on model-specific adversarial methods,\nreal-world applications demand a more generalizable and universal approach to\naudio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks\n(CAA) benchmark including four distinct types of audio attacks, which aims to\nexplore the the vulnerabilities of LLMs to these audio attacks in\nconversational scenarios. To evaluate the robustness of LLMs, we propose three\nevaluation strategies: Standard Evaluation, utilizing traditional metrics to\nquantify model performance under attacks; GPT-4o-Based Evaluation, which\nsimulates real-world conversational complexities; and Human Evaluation,\noffering insights into user perception and trust. We evaluate six\nstate-of-the-art LLMs with voice interaction capabilities, including\nGemini-1.5-Pro, GPT-4o, and others, using three distinct evaluation methods on\nthe CAA benchmark. Our comprehensive analysis reveals the impact of four types\nof audio attacks on the performance of these models, demonstrating that GPT-4o\nexhibits the highest level of resilience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial audio attacks pose a significant threat to the growing use of\nlarge language models (LLMs) in voice-based human-machine interactions. While\nexisting research has primarily focused on model-specific adversarial methods,\nreal-world applications demand a more generalizable and universal approach to\naudio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks\n(CAA) benchmark including four distinct types of audio attacks, which aims to\nexplore the the vulnerabilities of LLMs to these audio attacks in\nconversational scenarios. To evaluate the robustness of LLMs, we propose three\nevaluation strategies: Standard Evaluation, utilizing traditional metrics to\nquantify model performance under attacks; GPT-4o-Based Evaluation, which\nsimulates real-world conversational complexities; and Human Evaluation,\noffering insights into user perception and trust. We evaluate six\nstate-of-the-art LLMs with voice interaction capabilities, including\nGemini-1.5-Pro, GPT-4o, and others, using three distinct evaluation methods on\nthe CAA benchmark. Our comprehensive analysis reveals the impact of four types\nof audio attacks on the performance of these models, demonstrating that GPT-4o\nexhibits the highest level of resilience."
                },
                "authors": [
                    {
                        "name": "Wanqi Yang"
                    },
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Yunchao Wei"
                    },
                    {
                        "name": "Tianyi Zhou"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03817v2",
                "updated": "2024-11-22T10:24:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    10,
                    24,
                    44,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-06T10:35:11Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    10,
                    35,
                    11,
                    2,
                    311,
                    0
                ],
                "title": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Novice to Expert: LLM Agent Policy Optimization via Step-wise\n  Reinforcement Learning"
                },
                "summary": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The outstanding capabilities of large language models (LLMs) render them a\ncrucial component in various autonomous agent systems. While traditional\nmethods depend on the inherent knowledge of LLMs without fine-tuning, more\nrecent approaches have shifted toward the reinforcement learning strategy to\nfurther enhance agents' ability to solve complex interactive tasks with\nenvironments and tools. However, previous approaches are constrained by the\nsparse reward issue, where existing datasets solely provide a final scalar\nreward for each multi-step reasoning chain, potentially leading to\nineffectiveness and inefficiency in policy learning. In this paper, we\nintroduce StepAgent, which utilizes step-wise reward to optimize the agent's\nreinforcement learning process. Inheriting the spirit of novice-to-expert\ntheory, we first compare the actions of the expert and the agent to\nautomatically generate intermediate rewards for fine-grained optimization.\nAdditionally, we propose implicit-reward and inverse reinforcement learning\ntechniques to facilitate agent reflection and policy adjustment. Further\ntheoretical analysis demonstrates that the action distribution of the agent can\nconverge toward the expert action distribution over multiple training cycles.\nExperimental results across various datasets indicate that StepAgent\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Zhirui Deng"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Ruibin Xiong"
                    },
                    {
                        "name": "Mang Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14823v1",
                "updated": "2024-11-22T09:44:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    9,
                    44,
                    13,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T09:44:13Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    9,
                    44,
                    13,
                    4,
                    327,
                    0
                ],
                "title": "Omni-IML: Towards Unified Image Manipulation Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omni-IML: Towards Unified Image Manipulation Localization"
                },
                "summary": "Image manipulation can lead to misinterpretation of visual content, posing\nsignificant risks to information security. Image Manipulation Localization\n(IML) has thus received increasing attention. However, existing IML methods\nrely heavily on task-specific designs, making them perform well only on one\ntarget image type but are mostly random guessing on other image types, and even\njoint training on multiple image types causes significant performance\ndegradation. This hinders the deployment for real applications as it notably\nincreases maintenance costs and the misclassification of image types leads to\nserious error accumulation. To this end, we propose Omni-IML, the first\ngeneralist model to unify diverse IML tasks. Specifically, Omni-IML achieves\ngeneralism by adopting the Modal Gate Encoder and the Dynamic Weight Decoder to\nadaptively determine the optimal encoding modality and the optimal decoder\nfilters for each sample. We additionally propose an Anomaly Enhancement module\nthat enhances the features of tampered regions with box supervision and helps\nthe generalist model to extract common features across different IML tasks. We\nvalidate our approach on IML tasks across three major scenarios: natural\nimages, document images, and face images. Without bells and whistles, our\nOmni-IML achieves state-of-the-art performance on all three tasks with a single\nunified model, providing valuable strategies and insights for real-world\napplication and future research in generalist image forensics. Our code will be\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image manipulation can lead to misinterpretation of visual content, posing\nsignificant risks to information security. Image Manipulation Localization\n(IML) has thus received increasing attention. However, existing IML methods\nrely heavily on task-specific designs, making them perform well only on one\ntarget image type but are mostly random guessing on other image types, and even\njoint training on multiple image types causes significant performance\ndegradation. This hinders the deployment for real applications as it notably\nincreases maintenance costs and the misclassification of image types leads to\nserious error accumulation. To this end, we propose Omni-IML, the first\ngeneralist model to unify diverse IML tasks. Specifically, Omni-IML achieves\ngeneralism by adopting the Modal Gate Encoder and the Dynamic Weight Decoder to\nadaptively determine the optimal encoding modality and the optimal decoder\nfilters for each sample. We additionally propose an Anomaly Enhancement module\nthat enhances the features of tampered regions with box supervision and helps\nthe generalist model to extract common features across different IML tasks. We\nvalidate our approach on IML tasks across three major scenarios: natural\nimages, document images, and face images. Without bells and whistles, our\nOmni-IML achieves state-of-the-art performance on all three tasks with a single\nunified model, providing valuable strategies and insights for real-world\napplication and future research in generalist image forensics. Our code will be\npublicly available."
                },
                "authors": [
                    {
                        "name": "Chenfan Qu"
                    },
                    {
                        "name": "Yiwu Zhong"
                    },
                    {
                        "name": "Fengjun Guo"
                    },
                    {
                        "name": "Lianwen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Lianwen Jin"
                },
                "author": "Lianwen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15735v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15735v3",
                "updated": "2024-11-22T09:44:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    9,
                    44,
                    1,
                    4,
                    327,
                    0
                ],
                "published": "2024-09-24T04:42:43Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    4,
                    42,
                    43,
                    1,
                    268,
                    0
                ],
                "title": "Boosting Cybersecurity Vulnerability Scanning based on LLM-supported\n  Static Application Security Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Cybersecurity Vulnerability Scanning based on LLM-supported\n  Static Application Security Testing"
                },
                "summary": "The current cybersecurity landscape is increasingly complex, with traditional\nStatic Application Security Testing (SAST) tools struggling to capture complex\nand emerging vulnerabilities due to their reliance on rule-based matching.\nMeanwhile, Large Language Models (LLMs) have demonstrated powerful code\nanalysis capabilities, but their static training data and privacy risks limit\ntheir effectiveness. To overcome the limitations of both approaches, we propose\nLSAST, a novel approach that integrates LLMs with SAST scanners to enhance\nvulnerability detection. LSAST leverages a locally hostable LLM, combined with\na state-of-the-art knowledge retrieval system, to provide up-to-date\nvulnerability insights without compromising data privacy. We set a new\nbenchmark for static vulnerability analysis, offering a robust,\nprivacy-conscious solution that bridges the gap between traditional scanners\nand advanced AI-driven analysis. Our evaluation demonstrates that incorporating\nSAST results into LLM analysis significantly improves detection accuracy,\nidentifying vulnerabilities missed by conventional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current cybersecurity landscape is increasingly complex, with traditional\nStatic Application Security Testing (SAST) tools struggling to capture complex\nand emerging vulnerabilities due to their reliance on rule-based matching.\nMeanwhile, Large Language Models (LLMs) have demonstrated powerful code\nanalysis capabilities, but their static training data and privacy risks limit\ntheir effectiveness. To overcome the limitations of both approaches, we propose\nLSAST, a novel approach that integrates LLMs with SAST scanners to enhance\nvulnerability detection. LSAST leverages a locally hostable LLM, combined with\na state-of-the-art knowledge retrieval system, to provide up-to-date\nvulnerability insights without compromising data privacy. We set a new\nbenchmark for static vulnerability analysis, offering a robust,\nprivacy-conscious solution that bridges the gap between traditional scanners\nand advanced AI-driven analysis. Our evaluation demonstrates that incorporating\nSAST results into LLM analysis significantly improves detection accuracy,\nidentifying vulnerabilities missed by conventional methods."
                },
                "authors": [
                    {
                        "name": "Mete Keltek"
                    },
                    {
                        "name": "Rong Hu"
                    },
                    {
                        "name": "Mohammadreza Fani Sani"
                    },
                    {
                        "name": "Ziyue Li"
                    }
                ],
                "author_detail": {
                    "name": "Ziyue Li"
                },
                "author": "Ziyue Li",
                "arxiv_comment": "Under Review of IEEE SaTML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15735v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15735v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14817v1",
                "updated": "2024-11-22T09:26:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    9,
                    26,
                    53,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T09:26:53Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    9,
                    26,
                    53,
                    4,
                    327,
                    0
                ],
                "title": "Continuous-Variable Source-Independent Quantum Random Number Generator\n  with a Single Phase-Insensitive Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous-Variable Source-Independent Quantum Random Number Generator\n  with a Single Phase-Insensitive Detector"
                },
                "summary": "Quantum random number generators (QRNGs) harness quantum mechanical\nunpredictability to produce true randomness, which is crucial for cryptography\nand secure communications. Among various QRNGs, source-independent QRNGs\n(SI-QRNGs) relax the trust on the quantum source, allowing for flexible use of\nadvanced detectors to achieve high randomness generation rates.\nContinuous-variable (CV) SI-QRNGs, in particular, hold promise for practical\ndeployment due to their simplicity and randomness generation rates comparable\nto trusted-device QRNGs. In this work, we propose a novel CV-SI-QRNG scheme\nwith a single phase-insensitive detector, and provide security proof based on\nsemi-definite programming (SDP). We introduce a dimension reduction technique,\nwhich rigorously reduces an infinite-dimensional SDP problem to a\nfinite-dimensional one, enabling efficient computation while maintaining valid\nrandomness lower bound. We further validate our method through simulations.\nThese results demonstrate the feasibility of our framework, paving the way for\npractical and simple SI-QRNG implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum random number generators (QRNGs) harness quantum mechanical\nunpredictability to produce true randomness, which is crucial for cryptography\nand secure communications. Among various QRNGs, source-independent QRNGs\n(SI-QRNGs) relax the trust on the quantum source, allowing for flexible use of\nadvanced detectors to achieve high randomness generation rates.\nContinuous-variable (CV) SI-QRNGs, in particular, hold promise for practical\ndeployment due to their simplicity and randomness generation rates comparable\nto trusted-device QRNGs. In this work, we propose a novel CV-SI-QRNG scheme\nwith a single phase-insensitive detector, and provide security proof based on\nsemi-definite programming (SDP). We introduce a dimension reduction technique,\nwhich rigorously reduces an infinite-dimensional SDP problem to a\nfinite-dimensional one, enabling efficient computation while maintaining valid\nrandomness lower bound. We further validate our method through simulations.\nThese results demonstrate the feasibility of our framework, paving the way for\npractical and simple SI-QRNG implementations."
                },
                "authors": [
                    {
                        "name": "Hongyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Hongyi Zhou"
                },
                "author": "Hongyi Zhou",
                "arxiv_comment": "6 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14810v1",
                "updated": "2024-11-22T09:09:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    9,
                    9,
                    20,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T09:09:20Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    9,
                    9,
                    20,
                    4,
                    327,
                    0
                ],
                "title": "Scalable Wavelength Arbitration for Microring-based DWDM Transceivers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Wavelength Arbitration for Microring-based DWDM Transceivers"
                },
                "summary": "This paper introduces the concept of autonomous microring arbitration, or\n\\textit{wavelength arbitration}, to address the challenge of multi-microring\ninitialization in microring-based Dense-Wavelength-Division-Multiplexed (DWDM)\ntransceivers. This arbitration is inherently policy-driven, defining critical\nsystem characteristics such as the spectral ordering of microrings.\nFurthermore, to facilitate large-scale deployment, the arbitration algorithms\nmust operate independently of specific wavelength information and be resilient\nto system variability. Addressing these complexities requires a holistic\napproach that encompasses the entire system, from device-level variabilities to\nthe transceiver interface - this system-wide perspective is the focus of this\npaper. To support efficient analysis, we develop a hierarchical framework\nincorporating an ideal, wavelength-aware arbitration model to examine\narbitration failures at both the policy and algorithmic levels. The\neffectiveness of this approach is demonstrated in two ways: by analyzing the\nrobustness of each policy in relation to device variabilities, and by\ndeveloping an algorithm that achieves near-perfect alignment with the ideal\nmodel, offering superior robustness compared to the traditional sequential\ntuning method. The simulator code used in this paper is available at\n\\url{https://github.com/wdmsim/wdm-simulator}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the concept of autonomous microring arbitration, or\n\\textit{wavelength arbitration}, to address the challenge of multi-microring\ninitialization in microring-based Dense-Wavelength-Division-Multiplexed (DWDM)\ntransceivers. This arbitration is inherently policy-driven, defining critical\nsystem characteristics such as the spectral ordering of microrings.\nFurthermore, to facilitate large-scale deployment, the arbitration algorithms\nmust operate independently of specific wavelength information and be resilient\nto system variability. Addressing these complexities requires a holistic\napproach that encompasses the entire system, from device-level variabilities to\nthe transceiver interface - this system-wide perspective is the focus of this\npaper. To support efficient analysis, we develop a hierarchical framework\nincorporating an ideal, wavelength-aware arbitration model to examine\narbitration failures at both the policy and algorithmic levels. The\neffectiveness of this approach is demonstrated in two ways: by analyzing the\nrobustness of each policy in relation to device variabilities, and by\ndeveloping an algorithm that achieves near-perfect alignment with the ideal\nmodel, offering superior robustness compared to the traditional sequential\ntuning method. The simulator code used in this paper is available at\n\\url{https://github.com/wdmsim/wdm-simulator}."
                },
                "authors": [
                    {
                        "name": "Sunjin Choi"
                    },
                    {
                        "name": "Vladimir Stojanović"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Stojanović"
                },
                "author": "Vladimir Stojanović",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18808v2",
                "updated": "2024-11-22T09:00:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    9,
                    0,
                    10,
                    4,
                    327,
                    0
                ],
                "published": "2024-10-24T14:55:09Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    55,
                    9,
                    3,
                    298,
                    0
                ],
                "title": "Delving into the Reversal Curse: How Far Can Large Language Models\n  Generalize?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delving into the Reversal Curse: How Far Can Large Language Models\n  Generalize?"
                },
                "summary": "While large language models (LLMs) showcase unprecedented capabilities, they\nalso exhibit certain inherent limitations when facing seemingly trivial tasks.\nA prime example is the recently debated \"reversal curse\", which surfaces when\nmodels, having been trained on the fact \"A is B\", struggle to generalize this\nknowledge to infer that \"B is A\". In this paper, we examine the manifestation\nof the reversal curse across various tasks and delve into both the\ngeneralization abilities and the problem-solving mechanisms of LLMs. This\ninvestigation leads to a series of significant insights: (1) LLMs are able to\ngeneralize to \"B is A\" when both A and B are presented in the context as in the\ncase of a multiple-choice question. (2) This generalization ability is highly\ncorrelated to the structure of the fact \"A is B\" in the training documents. For\nexample, this generalization only applies to biographies structured in \"[Name]\nis [Description]\" but not to \"[Description] is [Name]\". (3) We propose and\nverify the hypothesis that LLMs possess an inherent bias in fact recalling\nduring knowledge application, which explains and underscores the importance of\nthe document structure to successful learning. (4) The negative impact of this\nbias on the downstream performance of LLMs can hardly be mitigated through\ntraining alone. These findings offer a novel perspective on interpreting LLMs'\ngeneralization through their intrinsic mechanisms and provide insights for\ndeveloping more effective learning methods. Our code and data are available at\nhttps://github.com/alibaba/thinking_bias.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) showcase unprecedented capabilities, they\nalso exhibit certain inherent limitations when facing seemingly trivial tasks.\nA prime example is the recently debated \"reversal curse\", which surfaces when\nmodels, having been trained on the fact \"A is B\", struggle to generalize this\nknowledge to infer that \"B is A\". In this paper, we examine the manifestation\nof the reversal curse across various tasks and delve into both the\ngeneralization abilities and the problem-solving mechanisms of LLMs. This\ninvestigation leads to a series of significant insights: (1) LLMs are able to\ngeneralize to \"B is A\" when both A and B are presented in the context as in the\ncase of a multiple-choice question. (2) This generalization ability is highly\ncorrelated to the structure of the fact \"A is B\" in the training documents. For\nexample, this generalization only applies to biographies structured in \"[Name]\nis [Description]\" but not to \"[Description] is [Name]\". (3) We propose and\nverify the hypothesis that LLMs possess an inherent bias in fact recalling\nduring knowledge application, which explains and underscores the importance of\nthe document structure to successful learning. (4) The negative impact of this\nbias on the downstream performance of LLMs can hardly be mitigated through\ntraining alone. These findings offer a novel perspective on interpreting LLMs'\ngeneralization through their intrinsic mechanisms and provide insights for\ndeveloping more effective learning methods. Our code and data are available at\nhttps://github.com/alibaba/thinking_bias.git."
                },
                "authors": [
                    {
                        "name": "Zhengkai Lin"
                    },
                    {
                        "name": "Zhihang Fu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Binbin Lin"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Accepted at NeurIPS 2024. Our code and data are available at\n  https://github.com/alibaba/thinking_bias.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10083v2",
                "updated": "2024-11-22T08:57:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    57,
                    42,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-15T10:01:52Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    10,
                    1,
                    52,
                    4,
                    320,
                    0
                ],
                "title": "Xmodel-1.5: An 1B-scale Multilingual LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Xmodel-1.5: An 1B-scale Multilingual LLM"
                },
                "summary": "We introduce Xmodel-1.5, a 1-billion-parameter multilingual large language\nmodel pretrained on 2 trillion tokens, designed for balanced performance and\nscalability. Unlike most large models that use the BPE tokenizer, Xmodel-1.5\nemploys a custom unigram tokenizer with 65,280 tokens, optimizing both\nefficiency and accuracy. The model delivers competitive results across multiple\nlanguages, including Thai, Arabic, French, Chinese, and English, outperforming\nAlibaba's PolyLM-1.7B on respective evaluation datasets. Xmodel-1.5 excels in\nbenchmarks like mMMLU and PIQA, and achieves state-of-the-art results in Thai.\nTo support low-resource language research, we release Xdata_Thai, a\nThai-specific evaluation dataset featuring unique linguistic challenges such as\ngendered particles and idioms. While the model demonstrates strong performance,\nthere is still room for improvement in handling culturally specific nuances. We\nhope this work contributes to advancements in multilingual AI research. Models\nand code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM-1.5",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Xmodel-1.5, a 1-billion-parameter multilingual large language\nmodel pretrained on 2 trillion tokens, designed for balanced performance and\nscalability. Unlike most large models that use the BPE tokenizer, Xmodel-1.5\nemploys a custom unigram tokenizer with 65,280 tokens, optimizing both\nefficiency and accuracy. The model delivers competitive results across multiple\nlanguages, including Thai, Arabic, French, Chinese, and English, outperforming\nAlibaba's PolyLM-1.7B on respective evaluation datasets. Xmodel-1.5 excels in\nbenchmarks like mMMLU and PIQA, and achieves state-of-the-art results in Thai.\nTo support low-resource language research, we release Xdata_Thai, a\nThai-specific evaluation dataset featuring unique linguistic challenges such as\ngendered particles and idioms. While the model demonstrates strong performance,\nthere is still room for improvement in handling culturally specific nuances. We\nhope this work contributes to advancements in multilingual AI research. Models\nand code are publicly available on GitHub at\nhttps://github.com/XiaoduoAILab/XmodelLM-1.5"
                },
                "authors": [
                    {
                        "name": "Wang Qun"
                    },
                    {
                        "name": "Liu Yang"
                    },
                    {
                        "name": "Lin Qingquan"
                    },
                    {
                        "name": "Jiang Ling"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Ling"
                },
                "author": "Jiang Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06387v2",
                "updated": "2024-11-22T08:54:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    54,
                    17,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-10T08:11:05Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    11,
                    5,
                    6,
                    315,
                    0
                ],
                "title": "Self-Training Meets Consistency: Improving LLMs' Reasoning With\n  Consistency-Driven Rationale Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Training Meets Consistency: Improving LLMs' Reasoning With\n  Consistency-Driven Rationale Evaluation"
                },
                "summary": "Self-training approach for large language models (LLMs) improves reasoning\nabilities by training the models on their self-generated rationales. Previous\napproaches have labeled rationales that produce correct answers for a given\nquestion as appropriate for training. However, a single measure risks\nmisjudging rationale quality, leading the models to learn flawed reasoning\npatterns. To address this issue, we propose CREST (Consistency-driven Rationale\nEvaluation for Self-Training), a self-training framework that further evaluates\neach rationale through follow-up questions and leverages this evaluation to\nguide its training. Specifically, we introduce two methods: (1) filtering out\nrationales that frequently result in incorrect answers on follow-up questions\nand (2) preference learning based on mixed preferences from rationale\nevaluation results of both original and follow-up questions. Experiments on\nthree question-answering datasets using open LLMs show that CREST not only\nimproves the logical robustness and correctness of rationales but also improves\nreasoning abilities compared to previous self-training approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-training approach for large language models (LLMs) improves reasoning\nabilities by training the models on their self-generated rationales. Previous\napproaches have labeled rationales that produce correct answers for a given\nquestion as appropriate for training. However, a single measure risks\nmisjudging rationale quality, leading the models to learn flawed reasoning\npatterns. To address this issue, we propose CREST (Consistency-driven Rationale\nEvaluation for Self-Training), a self-training framework that further evaluates\neach rationale through follow-up questions and leverages this evaluation to\nguide its training. Specifically, we introduce two methods: (1) filtering out\nrationales that frequently result in incorrect answers on follow-up questions\nand (2) preference learning based on mixed preferences from rationale\nevaluation results of both original and follow-up questions. Experiments on\nthree question-answering datasets using open LLMs show that CREST not only\nimproves the logical robustness and correctness of rationales but also improves\nreasoning abilities compared to previous self-training approaches."
                },
                "authors": [
                    {
                        "name": "Jaehyeok Lee"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "JinYeong Bak"
                    }
                ],
                "author_detail": {
                    "name": "JinYeong Bak"
                },
                "author": "JinYeong Bak",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14795v1",
                "updated": "2024-11-22T08:35:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    35,
                    35,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T08:35:35Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    35,
                    35,
                    4,
                    327,
                    0
                ],
                "title": "De-biased Multimodal Electrocardiogram Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "De-biased Multimodal Electrocardiogram Analysis"
                },
                "summary": "Multimodal large language models (MLLMs) are increasingly being applied in\nthe medical field, particularly in medical imaging. However, developing MLLMs\nfor ECG signals, which are crucial in clinical settings, has been a significant\nchallenge beyond medical imaging. Previous studies have attempted to address\nthis by converting ECGs into several text tags using an external classifier in\na training-free manner. However, this approach significantly compresses the\ninformation in ECGs and underutilizes the reasoning capabilities of LLMs. In\nthis work, we directly feed the embeddings of ECGs into the LLM through a\nprojection layer, retaining more information about ECGs and better leveraging\nthe reasoning abilities of LLMs. Our method can also effectively handle a\ncommon situation in clinical practice where it is necessary to compare two ECGs\ntaken at different times. Recent studies found that MLLMs may rely solely on\ntext input to provide answers, ignoring inputs from other modalities. We\nanalyzed this phenomenon from a causal perspective in the context of ECG MLLMs\nand discovered that the confounder, severity of illness, introduces a spurious\ncorrelation between the question and answer, leading the model to rely on this\nspurious correlation and ignore the ECG input. Such models do not comprehend\nthe ECG input and perform poorly in adversarial tests where different\nexpressions of the same question are used in the training and testing sets. We\ndesigned a de-biased pre-training method to eliminate the confounder's effect\naccording to the theory of backdoor adjustment. Our model performed well on the\nECG-QA task under adversarial testing and demonstrated zero-shot capabilities.\nAn interesting random ECG test further validated that our model effectively\nunderstands and utilizes the input ECG signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) are increasingly being applied in\nthe medical field, particularly in medical imaging. However, developing MLLMs\nfor ECG signals, which are crucial in clinical settings, has been a significant\nchallenge beyond medical imaging. Previous studies have attempted to address\nthis by converting ECGs into several text tags using an external classifier in\na training-free manner. However, this approach significantly compresses the\ninformation in ECGs and underutilizes the reasoning capabilities of LLMs. In\nthis work, we directly feed the embeddings of ECGs into the LLM through a\nprojection layer, retaining more information about ECGs and better leveraging\nthe reasoning abilities of LLMs. Our method can also effectively handle a\ncommon situation in clinical practice where it is necessary to compare two ECGs\ntaken at different times. Recent studies found that MLLMs may rely solely on\ntext input to provide answers, ignoring inputs from other modalities. We\nanalyzed this phenomenon from a causal perspective in the context of ECG MLLMs\nand discovered that the confounder, severity of illness, introduces a spurious\ncorrelation between the question and answer, leading the model to rely on this\nspurious correlation and ignore the ECG input. Such models do not comprehend\nthe ECG input and perform poorly in adversarial tests where different\nexpressions of the same question are used in the training and testing sets. We\ndesigned a de-biased pre-training method to eliminate the confounder's effect\naccording to the theory of backdoor adjustment. Our model performed well on the\nECG-QA task under adversarial testing and demonstrated zero-shot capabilities.\nAn interesting random ECG test further validated that our model effectively\nunderstands and utilizes the input ECG signal."
                },
                "authors": [
                    {
                        "name": "Haitao Li"
                    },
                    {
                        "name": "Ziyu Li"
                    },
                    {
                        "name": "Yiheng Mao"
                    },
                    {
                        "name": "Ziyi Liu"
                    },
                    {
                        "name": "Zhoujian Sun"
                    },
                    {
                        "name": "Zhengxing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Zhengxing Huang"
                },
                "author": "Zhengxing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14790v1",
                "updated": "2024-11-22T08:21:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    21,
                    3,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T08:21:03Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    21,
                    3,
                    4,
                    327,
                    0
                ],
                "title": "KBAda: Efficient Self Adaptation on Specific Knowledge Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KBAda: Efficient Self Adaptation on Specific Knowledge Bases"
                },
                "summary": "Humans can utilize techniques to quickly acquire knowledge from specific\nmaterials in advance, such as creating self-assessment questions, enabling us\nto achieving related tasks more efficiently. In contrast, large language models\n(LLMs) usually relies on retrieval-augmented generation to exploit knowledge\nmaterials in an instant manner, or requires external signals such as human\npreference data and stronger LLM annotations to conduct knowledge adaptation.\nTo unleash the self-learning potential of LLMs, we propose KBAda, an approach\ndesigned for efficient adaptation to downstream tasks involving knowledge\nbases. Our method utilizes iterative training with self-annotated data such as\nQ&A pairs and revision suggestions, enabling the model to grasp the knowledge\ncontent efficiently. Experimental results on multiple datasets demonstrate the\neffectiveness of our approach, significantly boosting model performance in\ndownstream tasks that require specific knowledge at a low cost. Notably, our\napproach achieves over 90% of the performance improvement that can be obtained\nby using GPT-4-turbo annotation, while relying entirely on self-supervision. We\nrelease our experimental data, models, and process analyses to the community\nfor further exploration (https://github.com/thunlp/KBAda).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can utilize techniques to quickly acquire knowledge from specific\nmaterials in advance, such as creating self-assessment questions, enabling us\nto achieving related tasks more efficiently. In contrast, large language models\n(LLMs) usually relies on retrieval-augmented generation to exploit knowledge\nmaterials in an instant manner, or requires external signals such as human\npreference data and stronger LLM annotations to conduct knowledge adaptation.\nTo unleash the self-learning potential of LLMs, we propose KBAda, an approach\ndesigned for efficient adaptation to downstream tasks involving knowledge\nbases. Our method utilizes iterative training with self-annotated data such as\nQ&A pairs and revision suggestions, enabling the model to grasp the knowledge\ncontent efficiently. Experimental results on multiple datasets demonstrate the\neffectiveness of our approach, significantly boosting model performance in\ndownstream tasks that require specific knowledge at a low cost. Notably, our\napproach achieves over 90% of the performance improvement that can be obtained\nby using GPT-4-turbo annotation, while relying entirely on self-supervision. We\nrelease our experimental data, models, and process analyses to the community\nfor further exploration (https://github.com/thunlp/KBAda)."
                },
                "authors": [
                    {
                        "name": "Zheni Zeng"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Shi Yu"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14789v1",
                "updated": "2024-11-22T08:17:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    17,
                    46,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T08:17:46Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    17,
                    46,
                    4,
                    327,
                    0
                ],
                "title": "Simplifying CLIP: Unleashing the Power of Large-Scale Models on\n  Consumer-level Computers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplifying CLIP: Unleashing the Power of Large-Scale Models on\n  Consumer-level Computers"
                },
                "summary": "Contrastive Language-Image Pre-training (CLIP) has attracted a surge of\nattention for its superior zero-shot performance and excellent transferability\nto downstream tasks. However, training such large-scale models usually requires\nsubstantial computation and storage, which poses barriers for general users\nwith consumer-level computers. Motivated by this observation, in this paper we\ninvestigate how to achieve competitive performance on only one Nvidia RTX3090\nGPU and with one terabyte for storing dataset. On one hand, we simplify the\ntransformer block structure and combine Weight Inheritance with multi-stage\nKnowledge Distillation (WIKD), thereby reducing the parameters and improving\nthe inference speed during training along with deployment. On the other hand,\nconfronted with the convergence challenge posed by small dataset, we generate\nsynthetic captions for each sample as data augmentation, and devise a novel\nPair Matching (PM) loss to fully exploit the distinguishment among positive and\nnegative image-text pairs. Extensive experiments demonstrate that our model can\nachieve a new state-of-the-art datascale-parameter-accuracy tradeoff, which\ncould further popularize the CLIP model in the related research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Language-Image Pre-training (CLIP) has attracted a surge of\nattention for its superior zero-shot performance and excellent transferability\nto downstream tasks. However, training such large-scale models usually requires\nsubstantial computation and storage, which poses barriers for general users\nwith consumer-level computers. Motivated by this observation, in this paper we\ninvestigate how to achieve competitive performance on only one Nvidia RTX3090\nGPU and with one terabyte for storing dataset. On one hand, we simplify the\ntransformer block structure and combine Weight Inheritance with multi-stage\nKnowledge Distillation (WIKD), thereby reducing the parameters and improving\nthe inference speed during training along with deployment. On the other hand,\nconfronted with the convergence challenge posed by small dataset, we generate\nsynthetic captions for each sample as data augmentation, and devise a novel\nPair Matching (PM) loss to fully exploit the distinguishment among positive and\nnegative image-text pairs. Extensive experiments demonstrate that our model can\nachieve a new state-of-the-art datascale-parameter-accuracy tradeoff, which\ncould further popularize the CLIP model in the related research community."
                },
                "authors": [
                    {
                        "name": "Hongbo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hongbo Liu"
                },
                "author": "Hongbo Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14596v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14596v4",
                "updated": "2024-11-22T07:43:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    7,
                    43,
                    21,
                    4,
                    327,
                    0
                ],
                "published": "2024-06-20T17:45:02Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    45,
                    2,
                    3,
                    172,
                    0
                ],
                "title": "VLM Agents Generate Their Own Memories: Distilling Experience into\n  Embodied Programs of Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM Agents Generate Their Own Memories: Distilling Experience into\n  Embodied Programs of Thought"
                },
                "summary": "Large-scale generative language and vision-language models (LLMs and VLMs)\nexcel in few-shot in-context learning for decision making and instruction\nfollowing. However, they require high-quality exemplar demonstrations in their\ncontext window. In this work, we ask: Can LLMs and VLMs generate their own\nexamples from generic, sub-optimal demonstrations? We propose In-Context\nAbstraction Learning (ICAL), a method that builds a memory of multimodal\nexperience from sub-optimal demonstrations and human feedback. Given a task\ndemonstration that may contain inefficiencies or mistakes, a VLM abstracts the\ntrajectory into a generalized program of thoughts by correcting inefficient\nactions and annotating cognitive abstractions: causal relationships, object\nstate changes, temporal subgoals, and task-relevant visual elements. These\nprograms of thought are iteratively improved through human feedback while the\nagent executes the trajectory in a similar environment. The resulting examples\nsignificantly improve decision-making in retrieval-augmented LLM and VLM\nagents. Moreover, as the agent's library of examples grows, it becomes more\nefficient, relying less on human feedback and requiring fewer environment\ninteractions per demonstration. Our ICAL agent surpasses the SOTA in\ndialogue-based instruction following in TEACh, multimodal web agents in\nVisualWebArena, and action anticipation in Ego4D. In TEACh, we achieve a 12.6%\nimprovement in goal-condition success. In VisualWebArena, our task success rate\nimproves over few-shot GPT4V. In Ego4D action forecasting, we improve over\nfew-shot GPT-4V and remain competitive with supervised models. We show\nfinetuning our retrieval-augmented in-context agent yields additional\nimprovements. Our approach significantly reduces reliance on manual prompt\nengineering and consistently outperforms in-context learning from action plans\nthat lack such programs of thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale generative language and vision-language models (LLMs and VLMs)\nexcel in few-shot in-context learning for decision making and instruction\nfollowing. However, they require high-quality exemplar demonstrations in their\ncontext window. In this work, we ask: Can LLMs and VLMs generate their own\nexamples from generic, sub-optimal demonstrations? We propose In-Context\nAbstraction Learning (ICAL), a method that builds a memory of multimodal\nexperience from sub-optimal demonstrations and human feedback. Given a task\ndemonstration that may contain inefficiencies or mistakes, a VLM abstracts the\ntrajectory into a generalized program of thoughts by correcting inefficient\nactions and annotating cognitive abstractions: causal relationships, object\nstate changes, temporal subgoals, and task-relevant visual elements. These\nprograms of thought are iteratively improved through human feedback while the\nagent executes the trajectory in a similar environment. The resulting examples\nsignificantly improve decision-making in retrieval-augmented LLM and VLM\nagents. Moreover, as the agent's library of examples grows, it becomes more\nefficient, relying less on human feedback and requiring fewer environment\ninteractions per demonstration. Our ICAL agent surpasses the SOTA in\ndialogue-based instruction following in TEACh, multimodal web agents in\nVisualWebArena, and action anticipation in Ego4D. In TEACh, we achieve a 12.6%\nimprovement in goal-condition success. In VisualWebArena, our task success rate\nimproves over few-shot GPT4V. In Ego4D action forecasting, we improve over\nfew-shot GPT-4V and remain competitive with supervised models. We show\nfinetuning our retrieval-augmented in-context agent yields additional\nimprovements. Our approach significantly reduces reliance on manual prompt\nengineering and consistently outperforms in-context learning from action plans\nthat lack such programs of thought."
                },
                "authors": [
                    {
                        "name": "Gabriel Sarch"
                    },
                    {
                        "name": "Lawrence Jang"
                    },
                    {
                        "name": "Michael J. Tarr"
                    },
                    {
                        "name": "William W. Cohen"
                    },
                    {
                        "name": "Kenneth Marino"
                    },
                    {
                        "name": "Katerina Fragkiadaki"
                    }
                ],
                "author_detail": {
                    "name": "Katerina Fragkiadaki"
                },
                "author": "Katerina Fragkiadaki",
                "arxiv_comment": "Project website: http://ical-learning.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14596v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14596v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13909v2",
                "updated": "2024-11-22T07:03:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    7,
                    3,
                    11,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-21T07:47:27Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    7,
                    47,
                    27,
                    3,
                    326,
                    0
                ],
                "title": "Panther: Illuminate the Sight of Multimodal LLMs with Instruction-Guided\n  Visual Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panther: Illuminate the Sight of Multimodal LLMs with Instruction-Guided\n  Visual Prompts"
                },
                "summary": "Multimodal large language models (MLLMs) are closing the gap to human visual\nperception capability rapidly, while, still lag behind on attending to subtle\nimages details or locating small objects precisely, etc. Common schemes to\ntackle these issues include deploying multiple vision encoders or operating on\noriginal high-resolution images. Few studies have concentrated on taking the\ntextual instruction into improving visual representation, resulting in losing\nfocus in some vision-centric tasks, a phenomenon we herein termed as Amblyopia.\nIn this work, we introduce Panther, a MLLM that closely adheres to user\ninstruction and locates targets of interests precisely, with the finesse of a\nblack panther. Specifically, Panther comprises three integral components:\nPanther-VE, Panther-Bridge, and Panther-Decoder. Panther-VE integrates user\ninstruction information at the early stages of the vision encoder, thereby\nextracting the most relevant and useful visual representations. The\nPanther-Bridge module, equipped with powerful filtering capabilities,\nsignificantly reduces redundant visual information, leading to a substantial\nsavings in training costs. The Panther-Decoder is versatile and can be employed\nwith any decoder-only architecture of LLMs without discrimination. Experimental\nresults, particularly on vision-centric benchmarks, have demonstrated the\neffectiveness of Panther.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) are closing the gap to human visual\nperception capability rapidly, while, still lag behind on attending to subtle\nimages details or locating small objects precisely, etc. Common schemes to\ntackle these issues include deploying multiple vision encoders or operating on\noriginal high-resolution images. Few studies have concentrated on taking the\ntextual instruction into improving visual representation, resulting in losing\nfocus in some vision-centric tasks, a phenomenon we herein termed as Amblyopia.\nIn this work, we introduce Panther, a MLLM that closely adheres to user\ninstruction and locates targets of interests precisely, with the finesse of a\nblack panther. Specifically, Panther comprises three integral components:\nPanther-VE, Panther-Bridge, and Panther-Decoder. Panther-VE integrates user\ninstruction information at the early stages of the vision encoder, thereby\nextracting the most relevant and useful visual representations. The\nPanther-Bridge module, equipped with powerful filtering capabilities,\nsignificantly reduces redundant visual information, leading to a substantial\nsavings in training costs. The Panther-Decoder is versatile and can be employed\nwith any decoder-only architecture of LLMs without discrimination. Experimental\nresults, particularly on vision-centric benchmarks, have demonstrated the\neffectiveness of Panther."
                },
                "authors": [
                    {
                        "name": "Honglin Li"
                    },
                    {
                        "name": "Yuting Gao"
                    },
                    {
                        "name": "Chenglu Zhu"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Ming Yang"
                    },
                    {
                        "name": "Lin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yang"
                },
                "author": "Lin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09944v2",
                "updated": "2024-11-22T06:44:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    6,
                    44,
                    22,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-15T04:44:34Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    4,
                    44,
                    34,
                    4,
                    320,
                    0
                ],
                "title": "SlimLM: An Efficient Small Language Model for On-Device Document\n  Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimLM: An Efficient Small Language Model for On-Device Document\n  Assistance"
                },
                "summary": "While small language models (SLMs) show promises for mobile deployment, their\nreal-world performance and applications on smartphones remains underexplored.\nWe present SlimLM, a series of SLMs optimized for document assistance tasks on\nmobile devices. Through extensive experiments on a Samsung Galaxy S24, we\nidentify the optimal trade-offs between model size (ranging from 125M to 7B\nparameters), context length, and inference time for efficient on-device\nprocessing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on\nDocAssist, our constructed dataset for summarization, question answering and\nsuggestion tasks. Our smallest model demonstrates efficient performance on S24,\nwhile larger variants offer enhanced capabilities within mobile constraints. We\nevaluate SlimLM against existing SLMs, showing comparable or superior\nperformance and offering a benchmark for future research in on-device language\nmodels. We also provide an Android application, offering practical insights\ninto SLM deployment. Our findings provide valuable insights and illuminate the\ncapabilities of running advanced language models on high-end smartphones,\npotentially reducing server costs and enhancing privacy through on-device\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While small language models (SLMs) show promises for mobile deployment, their\nreal-world performance and applications on smartphones remains underexplored.\nWe present SlimLM, a series of SLMs optimized for document assistance tasks on\nmobile devices. Through extensive experiments on a Samsung Galaxy S24, we\nidentify the optimal trade-offs between model size (ranging from 125M to 7B\nparameters), context length, and inference time for efficient on-device\nprocessing. SlimLM is pre-trained on SlimPajama-627B and fine-tuned on\nDocAssist, our constructed dataset for summarization, question answering and\nsuggestion tasks. Our smallest model demonstrates efficient performance on S24,\nwhile larger variants offer enhanced capabilities within mobile constraints. We\nevaluate SlimLM against existing SLMs, showing comparable or superior\nperformance and offering a benchmark for future research in on-device language\nmodels. We also provide an Android application, offering practical insights\ninto SLM deployment. Our findings provide valuable insights and illuminate the\ncapabilities of running advanced language models on high-end smartphones,\npotentially reducing server costs and enhancing privacy through on-device\nprocessing."
                },
                "authors": [
                    {
                        "name": "Thang M. Pham"
                    },
                    {
                        "name": "Phat T. Nguyen"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Trung Bui"
                    }
                ],
                "author_detail": {
                    "name": "Trung Bui"
                },
                "author": "Trung Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03609v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03609v2",
                "updated": "2024-11-22T06:39:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    6,
                    39,
                    26,
                    4,
                    327,
                    0
                ],
                "published": "2024-07-04T03:39:59Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    3,
                    39,
                    59,
                    3,
                    186,
                    0
                ],
                "title": "Continuous-variable quantum digital signatures that can withstand\n  coherent attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous-variable quantum digital signatures that can withstand\n  coherent attacks"
                },
                "summary": "Quantum digital signatures (QDSs), which utilize correlated bit strings among\nsender and recipients, guarantee the authenticity, integrity, and\nnonrepudiation of classical messages based on quantum laws. Continuous-variable\n(CV) quantum protocol with heterodyne and homodyne measurement has obvious\nadvantages of low-cost implementation and easy wavelength division\nmultiplexing. However, security analyses in previous researches are limited to\nthe proof against collective attacks in finite-size scenarios. Moreover,\nexisting multibit CV QDS schemes have primarily focused on adapting single-bit\nprotocols for simplicity of security proof, often sacrificing signature\nefficiency. Here, we introduce a CV QDS protocol designed to withstand general\ncoherent attacks through the use of a cutting-edge fidelity test function,\nwhile achieving high signature efficiency by employing a refined one-time\nuniversal hashing signing technique. Our protocol is proved to be robust\nagainst finite-size effects and excess noise in quantum channels. In\nsimulation, results demonstrate a significant reduction of eight orders of\nmagnitude in signature length for a megabit message signing task compared with\nexisting CV QDS protocols and this advantage expands as the message size grows.\nOur work offers a solution with enhanced security and efficiency, paving the\nway for large-scale deployment of CV QDSs in future quantum networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum digital signatures (QDSs), which utilize correlated bit strings among\nsender and recipients, guarantee the authenticity, integrity, and\nnonrepudiation of classical messages based on quantum laws. Continuous-variable\n(CV) quantum protocol with heterodyne and homodyne measurement has obvious\nadvantages of low-cost implementation and easy wavelength division\nmultiplexing. However, security analyses in previous researches are limited to\nthe proof against collective attacks in finite-size scenarios. Moreover,\nexisting multibit CV QDS schemes have primarily focused on adapting single-bit\nprotocols for simplicity of security proof, often sacrificing signature\nefficiency. Here, we introduce a CV QDS protocol designed to withstand general\ncoherent attacks through the use of a cutting-edge fidelity test function,\nwhile achieving high signature efficiency by employing a refined one-time\nuniversal hashing signing technique. Our protocol is proved to be robust\nagainst finite-size effects and excess noise in quantum channels. In\nsimulation, results demonstrate a significant reduction of eight orders of\nmagnitude in signature length for a megabit message signing task compared with\nexisting CV QDS protocols and this advantage expands as the message size grows.\nOur work offers a solution with enhanced security and efficiency, paving the\nway for large-scale deployment of CV QDSs in future quantum networks."
                },
                "authors": [
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Wen-Bo Liu"
                    },
                    {
                        "name": "Bing-Hong Li"
                    },
                    {
                        "name": "Hua-Lei Yin"
                    },
                    {
                        "name": "Zeng-Bing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zeng-Bing Chen"
                },
                "author": "Zeng-Bing Chen",
                "arxiv_doi": "10.1103/PhysRevA.110.052613",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevA.110.052613",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.03609v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03609v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "19 pages, 8 figures",
                "arxiv_journal_ref": "Phys. Rev. A 110, 052613 (2024)",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00627v2",
                "updated": "2024-11-22T06:19:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    6,
                    19,
                    35,
                    4,
                    327,
                    0
                ],
                "published": "2024-06-02T06:09:56Z",
                "published_parsed": [
                    2024,
                    6,
                    2,
                    6,
                    9,
                    56,
                    6,
                    154,
                    0
                ],
                "title": "Prompt Framework for Role-playing: Generation and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Framework for Role-playing: Generation and Evaluation"
                },
                "summary": "Large language models (LLMs) exhibit impressive proficiency in natural\nlanguage generation, understanding user instructions, and emulating human-like\nlanguage use, which has led to significant interest in their application to\nrole-playing scenarios. However, the manual collection of role-specific script\ndata and the evaluation of model performance are resource-intensive processes.\nThis project introduces a prompt-based framework designed to leverage GPT's\ncapabilities for the generation of role-playing dialogue datasets and the\nevaluation of role-playing performance. To validate the effectiveness of the\nGPT-based generation and evaluation, we further incorporate the recall-oriented\nRouge-L metric, providing an additional quantitative measure of performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit impressive proficiency in natural\nlanguage generation, understanding user instructions, and emulating human-like\nlanguage use, which has led to significant interest in their application to\nrole-playing scenarios. However, the manual collection of role-specific script\ndata and the evaluation of model performance are resource-intensive processes.\nThis project introduces a prompt-based framework designed to leverage GPT's\ncapabilities for the generation of role-playing dialogue datasets and the\nevaluation of role-playing performance. To validate the effectiveness of the\nGPT-based generation and evaluation, we further incorporate the recall-oriented\nRouge-L metric, providing an additional quantitative measure of performance."
                },
                "authors": [
                    {
                        "name": "Xun Liu"
                    },
                    {
                        "name": "Zhengwei Ni"
                    }
                ],
                "author_detail": {
                    "name": "Zhengwei Ni"
                },
                "author": "Zhengwei Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09699v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09699v2",
                "updated": "2024-11-22T05:31:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    31,
                    51,
                    4,
                    327,
                    0
                ],
                "published": "2024-04-15T11:59:45Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    11,
                    59,
                    45,
                    0,
                    106,
                    0
                ],
                "title": "Generative AI for Game Theory-based Mobile Networking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI for Game Theory-based Mobile Networking"
                },
                "summary": "With the continuous advancement of network technology, various emerging\ncomplex networking optimization problems have created a wide range of\napplications utilizing game theory. However, since game theory is a\nmathematical framework, game theory-based solutions often rely heavily on the\nexperience and knowledge of human experts. Recently, the remarkable advantages\nexhibited by generative artificial intelligence (GAI) have gained widespread\nattention. In this work, we propose a novel GAI-enabled game theory solution\nthat combines the powerful reasoning and generation capabilities of GAI to the\ndesign and optimization of mobile networking. Specifically, we first outline\nthe game theory and key technologies of GAI, and explore the advantages of\ncombining GAI with game theory. Then, we review the contributions and\nlimitations of existing research and demonstrate the potential application\nvalues of GAI applied to game theory in mobile networking. Subsequently, we\ndevelop a large language model (LLM)-enabled game theory framework to realize\nthis combination, and demonstrate the effectiveness of the proposed framework\nthrough a case study in secured UAV networks. Finally, we provide several\ndirections for future extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous advancement of network technology, various emerging\ncomplex networking optimization problems have created a wide range of\napplications utilizing game theory. However, since game theory is a\nmathematical framework, game theory-based solutions often rely heavily on the\nexperience and knowledge of human experts. Recently, the remarkable advantages\nexhibited by generative artificial intelligence (GAI) have gained widespread\nattention. In this work, we propose a novel GAI-enabled game theory solution\nthat combines the powerful reasoning and generation capabilities of GAI to the\ndesign and optimization of mobile networking. Specifically, we first outline\nthe game theory and key technologies of GAI, and explore the advantages of\ncombining GAI with game theory. Then, we review the contributions and\nlimitations of existing research and demonstrate the potential application\nvalues of GAI applied to game theory in mobile networking. Subsequently, we\ndevelop a large language model (LLM)-enabled game theory framework to realize\nthis combination, and demonstrate the effectiveness of the proposed framework\nthrough a case study in secured UAV networks. Finally, we provide several\ndirections for future extensions."
                },
                "authors": [
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Fang Mei"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Mérouane Debbah"
                    },
                    {
                        "name": "Zhu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Han"
                },
                "author": "Zhu Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09699v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09699v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14739v1",
                "updated": "2024-11-22T05:18:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    18,
                    35,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T05:18:35Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    18,
                    35,
                    4,
                    327,
                    0
                ],
                "title": "IRLab@iKAT24: Learned Sparse Retrieval with Multi-aspect LLM Query\n  Generation for Conversational Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRLab@iKAT24: Learned Sparse Retrieval with Multi-aspect LLM Query\n  Generation for Conversational Search"
                },
                "summary": "The Interactive Knowledge Assistant Track (iKAT) 2024 focuses on advancing\nconversational assistants, able to adapt their interaction and responses from\npersonalized user knowledge. The track incorporates a Personal Textual\nKnowledge Base (PTKB) alongside Conversational AI tasks, such as passage\nranking and response generation. Query Rewrite being an effective approach for\nresolving conversational context, we explore Large Language Models (LLMs), as\nquery rewriters. Specifically, our submitted runs explore multi-aspect query\ngeneration using the MQ4CS framework, which we further enhance with Learned\nSparse Retrieval via the SPLADE architecture, coupled with robust cross-encoder\nmodels. We also propose an alternative to the previous interleaving strategy,\naggregating multiple aspects during the reranking phase. Our findings indicate\nthat multi-aspect query generation is effective in enhancing performance when\nintegrated with advanced retrieval and reranking models. Our results also lead\nthe way for better personalization in Conversational Search, relying on LLMs to\nintegrate personalization within query rewrite, and outperforming human rewrite\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Interactive Knowledge Assistant Track (iKAT) 2024 focuses on advancing\nconversational assistants, able to adapt their interaction and responses from\npersonalized user knowledge. The track incorporates a Personal Textual\nKnowledge Base (PTKB) alongside Conversational AI tasks, such as passage\nranking and response generation. Query Rewrite being an effective approach for\nresolving conversational context, we explore Large Language Models (LLMs), as\nquery rewriters. Specifically, our submitted runs explore multi-aspect query\ngeneration using the MQ4CS framework, which we further enhance with Learned\nSparse Retrieval via the SPLADE architecture, coupled with robust cross-encoder\nmodels. We also propose an alternative to the previous interleaving strategy,\naggregating multiple aspects during the reranking phase. Our findings indicate\nthat multi-aspect query generation is effective in enhancing performance when\nintegrated with advanced retrieval and reranking models. Our results also lead\nthe way for better personalization in Conversational Search, relying on LLMs to\nintegrate personalization within query rewrite, and outperforming human rewrite\nperformance."
                },
                "authors": [
                    {
                        "name": "Simon Lupart"
                    },
                    {
                        "name": "Zahra Abbasiantaeb"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Aliannejadi"
                },
                "author": "Mohammad Aliannejadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14738v1",
                "updated": "2024-11-22T05:17:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    17,
                    18,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T05:17:18Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    17,
                    18,
                    4,
                    327,
                    0
                ],
                "title": "Universal and Context-Independent Triggers for Precise Control of LLM\n  Outputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal and Context-Independent Triggers for Precise Control of LLM\n  Outputs"
                },
                "summary": "Large language models (LLMs) have been widely adopted in applications such as\nautomated content generation and even critical decision-making systems.\nHowever, the risk of prompt injection allows for potential manipulation of LLM\noutputs. While numerous attack methods have been documented, achieving full\ncontrol over these outputs remains challenging, often requiring experienced\nattackers to make multiple attempts and depending heavily on the prompt\ncontext. Recent advancements in gradient-based white-box attack techniques have\nshown promise in tasks like jailbreaks and system prompt leaks. Our research\ngeneralizes gradient-based attacks to find a trigger that is (1) Universal:\neffective irrespective of the target output; (2) Context-Independent: robust\nacross diverse prompt contexts; and (3) Precise Output: capable of manipulating\nLLM inputs to yield any specified output with high accuracy. We propose a novel\nmethod to efficiently discover such triggers and assess the effectiveness of\nthe proposed attack. Furthermore, we discuss the substantial threats posed by\nsuch attacks to LLM-based applications, highlighting the potential for\nadversaries to taking over the decisions and actions made by AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted in applications such as\nautomated content generation and even critical decision-making systems.\nHowever, the risk of prompt injection allows for potential manipulation of LLM\noutputs. While numerous attack methods have been documented, achieving full\ncontrol over these outputs remains challenging, often requiring experienced\nattackers to make multiple attempts and depending heavily on the prompt\ncontext. Recent advancements in gradient-based white-box attack techniques have\nshown promise in tasks like jailbreaks and system prompt leaks. Our research\ngeneralizes gradient-based attacks to find a trigger that is (1) Universal:\neffective irrespective of the target output; (2) Context-Independent: robust\nacross diverse prompt contexts; and (3) Precise Output: capable of manipulating\nLLM inputs to yield any specified output with high accuracy. We propose a novel\nmethod to efficiently discover such triggers and assess the effectiveness of\nthe proposed attack. Furthermore, we discuss the substantial threats posed by\nsuch attacks to LLM-based applications, highlighting the potential for\nadversaries to taking over the decisions and actions made by AI agents."
                },
                "authors": [
                    {
                        "name": "Jiashuo Liang"
                    },
                    {
                        "name": "Guancheng Li"
                    },
                    {
                        "name": "Yang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Yu"
                },
                "author": "Yang Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14733v1",
                "updated": "2024-11-22T05:01:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    1,
                    35,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T05:01:35Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    5,
                    1,
                    35,
                    4,
                    327,
                    0
                ],
                "title": "FLARE: FP-Less PTQ and Low-ENOB ADC Based AMS-PiM for Error-Resilient,\n  Fast, and Efficient Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLARE: FP-Less PTQ and Low-ENOB ADC Based AMS-PiM for Error-Resilient,\n  Fast, and Efficient Transformer Acceleration"
                },
                "summary": "Encoder-based transformers, powered by self-attention layers, have\nrevolutionized machine learning with their context-aware representations.\nHowever, their quadratic growth in computational and memory demands presents\nsignificant bottlenecks. Analog-Mixed-Signal Process-in-Memory (AMS-PiM)\narchitectures address these challenges by enabling efficient on-chip\nprocessing. Traditionally, AMS-PiM relies on Quantization-Aware Training (QAT),\nwhich is hardware-efficient but requires extensive retraining to adapt models\nto AMS-PiMs, making it increasingly impractical for transformer models.\nPost-Training Quantization (PTQ) mitigates this training overhead but\nintroduces significant hardware inefficiencies. PTQ relies on\ndequantization-quantization (DQ-Q) processes, floating-point units (FPUs), and\nhigh-ENOB (Effective Number of Bits) analog-to-digital converters (ADCs).\nParticularly, High-ENOB ADCs scale exponentially in area and energy\n($2^{ENOB}$), reduce sensing margins, and increase susceptibility to process,\nvoltage, and temperature (PVT) variations, further compounding PTQ's challenges\nin AMS-PiM systems. To overcome these limitations, we propose RAP, an AMS-PiM\narchitecture that eliminates DQ-Q processes, introduces FPU- and division-free\nnonlinear processing, and employs a low-ENOB-ADC-based sparse Matrix Vector\nmultiplication technique. Using the proposed techniques, RAP improves error\nresiliency, area/energy efficiency, and computational speed while preserving\nnumerical stability. Experimental results demonstrate that RAP outperforms\nstate-of-the-art GPUs and conventional PiM architectures in energy efficiency,\nlatency, and accuracy, making it a scalable solution for the efficient\ndeployment of transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encoder-based transformers, powered by self-attention layers, have\nrevolutionized machine learning with their context-aware representations.\nHowever, their quadratic growth in computational and memory demands presents\nsignificant bottlenecks. Analog-Mixed-Signal Process-in-Memory (AMS-PiM)\narchitectures address these challenges by enabling efficient on-chip\nprocessing. Traditionally, AMS-PiM relies on Quantization-Aware Training (QAT),\nwhich is hardware-efficient but requires extensive retraining to adapt models\nto AMS-PiMs, making it increasingly impractical for transformer models.\nPost-Training Quantization (PTQ) mitigates this training overhead but\nintroduces significant hardware inefficiencies. PTQ relies on\ndequantization-quantization (DQ-Q) processes, floating-point units (FPUs), and\nhigh-ENOB (Effective Number of Bits) analog-to-digital converters (ADCs).\nParticularly, High-ENOB ADCs scale exponentially in area and energy\n($2^{ENOB}$), reduce sensing margins, and increase susceptibility to process,\nvoltage, and temperature (PVT) variations, further compounding PTQ's challenges\nin AMS-PiM systems. To overcome these limitations, we propose RAP, an AMS-PiM\narchitecture that eliminates DQ-Q processes, introduces FPU- and division-free\nnonlinear processing, and employs a low-ENOB-ADC-based sparse Matrix Vector\nmultiplication technique. Using the proposed techniques, RAP improves error\nresiliency, area/energy efficiency, and computational speed while preserving\nnumerical stability. Experimental results demonstrate that RAP outperforms\nstate-of-the-art GPUs and conventional PiM architectures in energy efficiency,\nlatency, and accuracy, making it a scalable solution for the efficient\ndeployment of transformers."
                },
                "authors": [
                    {
                        "name": "Donghyeon Yi"
                    },
                    {
                        "name": "Seoyoung Lee"
                    },
                    {
                        "name": "Jongho Kim"
                    },
                    {
                        "name": "Junyoung Kim"
                    },
                    {
                        "name": "Sohmyung Ha"
                    },
                    {
                        "name": "Ik Joon Chang"
                    },
                    {
                        "name": "Minkyu Je"
                    }
                ],
                "author_detail": {
                    "name": "Minkyu Je"
                },
                "author": "Minkyu Je",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14729v1",
                "updated": "2024-11-22T04:52:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    52,
                    13,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T04:52:13Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    52,
                    13,
                    4,
                    327,
                    0
                ],
                "title": "A Lightweight Edge-CNN-Transformer Model for Detecting Coordinated Cyber\n  and Digital Twin Attacks in Cooperative Smart Farming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Lightweight Edge-CNN-Transformer Model for Detecting Coordinated Cyber\n  and Digital Twin Attacks in Cooperative Smart Farming"
                },
                "summary": "The agriculture sector is increasingly adopting innovative technologies to\nmeet the growing food demands of the global population. To optimize resource\nutilization and minimize crop losses, farmers are joining cooperatives to share\ntheir data and resources among member farms. However, while farmers benefit\nfrom this data sharing and interconnection, it exposes them to cybersecurity\nthreats and privacy concerns. A cyberattack on one farm can have widespread\nconsequences, affecting the targeted farm as well as all member farms within a\ncooperative. In this research, we address existing gaps by proposing a novel\nand secure architecture for Cooperative Smart Farming (CSF). First, we\nhighlight the role of edge-based DTs in enhancing the efficiency and resilience\nof agricultural operations. To validate this, we develop a test environment for\nCSF, implementing various cyberattacks on both the DTs and their physical\ncounterparts using different attack vectors. We collect two smart farming\nnetwork datasets to identify potential threats. After identifying these\nthreats, we focus on preventing the transmission of malicious data from\ncompromised farms to the central cloud server. To achieve this, we propose a\nCNN-Transformer-based network anomaly detection model, specifically designed\nfor deployment at the edge. As a proof of concept, we implement this model and\nevaluate its performance by varying the number of encoder layers. Additionally,\nwe apply Post-Quantization to compress the model and demonstrate the impact of\ncompression on its performance in edge environments. Finally, we compare the\nmodel's performance with traditional machine learning approaches to assess its\noverall effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The agriculture sector is increasingly adopting innovative technologies to\nmeet the growing food demands of the global population. To optimize resource\nutilization and minimize crop losses, farmers are joining cooperatives to share\ntheir data and resources among member farms. However, while farmers benefit\nfrom this data sharing and interconnection, it exposes them to cybersecurity\nthreats and privacy concerns. A cyberattack on one farm can have widespread\nconsequences, affecting the targeted farm as well as all member farms within a\ncooperative. In this research, we address existing gaps by proposing a novel\nand secure architecture for Cooperative Smart Farming (CSF). First, we\nhighlight the role of edge-based DTs in enhancing the efficiency and resilience\nof agricultural operations. To validate this, we develop a test environment for\nCSF, implementing various cyberattacks on both the DTs and their physical\ncounterparts using different attack vectors. We collect two smart farming\nnetwork datasets to identify potential threats. After identifying these\nthreats, we focus on preventing the transmission of malicious data from\ncompromised farms to the central cloud server. To achieve this, we propose a\nCNN-Transformer-based network anomaly detection model, specifically designed\nfor deployment at the edge. As a proof of concept, we implement this model and\nevaluate its performance by varying the number of encoder layers. Additionally,\nwe apply Post-Quantization to compress the model and demonstrate the impact of\ncompression on its performance in edge environments. Finally, we compare the\nmodel's performance with traditional machine learning approaches to assess its\noverall effectiveness."
                },
                "authors": [
                    {
                        "name": "Lopamudra Praharaj"
                    },
                    {
                        "name": "Deepti Gupta"
                    },
                    {
                        "name": "Maanak Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Maanak Gupta"
                },
                "author": "Maanak Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01544v2",
                "updated": "2024-11-22T04:32:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    32,
                    55,
                    4,
                    327,
                    0
                ],
                "published": "2024-10-02T13:30:32Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    30,
                    32,
                    2,
                    276,
                    0
                ],
                "title": "Boosting Weakly-Supervised Referring Image Segmentation via Progressive\n  Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Weakly-Supervised Referring Image Segmentation via Progressive\n  Comprehension"
                },
                "summary": "This paper explores the weakly-supervised referring image segmentation (WRIS)\nproblem, and focuses on a challenging setup where target localization is\nlearned directly from image-text pairs. We note that the input text description\ntypically already contains detailed information on how to localize the target\nobject, and we also observe that humans often follow a step-by-step\ncomprehension process (\\ie, progressively utilizing target-related attributes\nand relations as cues) to identify the target object. Hence, we propose a novel\nProgressive Comprehension Network (PCNet) to leverage target-related textual\ncues from the input description for progressively localizing the target object.\nSpecifically, we first use a Large Language Model (LLM) to decompose the input\ntext description into short phrases. These short phrases are taken as\ntarget-related cues and fed into a Conditional Referring Module (CRM) in\nmultiple stages, to allow updating the referring text embedding and enhance the\nresponse map for target localization in a multi-stage manner. Based on the CRM,\nwe then propose a Region-aware Shrinking (RaS) loss to constrain the visual\nlocalization to be conducted progressively in a coarse-to-fine manner across\ndifferent stages. Finally, we introduce an Instance-aware Disambiguation (IaD)\nloss to suppress instance localization ambiguity by differentiating overlapping\nresponse maps generated by different referring texts on the same image.\nExtensive experiments show that our method outperforms SOTA methods on three\ncommon benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the weakly-supervised referring image segmentation (WRIS)\nproblem, and focuses on a challenging setup where target localization is\nlearned directly from image-text pairs. We note that the input text description\ntypically already contains detailed information on how to localize the target\nobject, and we also observe that humans often follow a step-by-step\ncomprehension process (\\ie, progressively utilizing target-related attributes\nand relations as cues) to identify the target object. Hence, we propose a novel\nProgressive Comprehension Network (PCNet) to leverage target-related textual\ncues from the input description for progressively localizing the target object.\nSpecifically, we first use a Large Language Model (LLM) to decompose the input\ntext description into short phrases. These short phrases are taken as\ntarget-related cues and fed into a Conditional Referring Module (CRM) in\nmultiple stages, to allow updating the referring text embedding and enhance the\nresponse map for target localization in a multi-stage manner. Based on the CRM,\nwe then propose a Region-aware Shrinking (RaS) loss to constrain the visual\nlocalization to be conducted progressively in a coarse-to-fine manner across\ndifferent stages. Finally, we introduce an Instance-aware Disambiguation (IaD)\nloss to suppress instance localization ambiguity by differentiating overlapping\nresponse maps generated by different referring texts on the same image.\nExtensive experiments show that our method outperforms SOTA methods on three\ncommon benchmarks."
                },
                "authors": [
                    {
                        "name": "Zaiquan Yang"
                    },
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Jiaying Lin"
                    },
                    {
                        "name": "Gerhard Hancke"
                    },
                    {
                        "name": "Rynson W. H. Lau"
                    }
                ],
                "author_detail": {
                    "name": "Rynson W. H. Lau"
                },
                "author": "Rynson W. H. Lau",
                "arxiv_comment": "Accepted by NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14721v1",
                "updated": "2024-11-22T04:28:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    28,
                    56,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T04:28:56Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    28,
                    56,
                    4,
                    327,
                    0
                ],
                "title": "MolReFlect: Towards In-Context Fine-grained Alignments between Molecules\n  and Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MolReFlect: Towards In-Context Fine-grained Alignments between Molecules\n  and Texts"
                },
                "summary": "Molecule discovery is a pivotal research field, impacting everything from the\nmedicines we take to the materials we use. Recently, Large Language Models\n(LLMs) have been widely adopted in molecule understanding and generation, yet\nthe alignments between molecules and their corresponding captions remain a\nsignificant challenge. Previous endeavours often treat the molecule as a\ngeneral SMILES string or molecular graph, neglecting the fine-grained\nalignments between the molecular sub-structures and the descriptive textual\nphrases, which are crucial for accurate and explainable predictions. In this\ncase, we introduce MolReFlect, a novel teacher-student framework designed to\ncontextually perform the molecule-caption alignments in a fine-grained way. Our\napproach initially leverages a larger teacher LLM to label the detailed\nalignments by directly extracting critical phrases from molecule captions or\nSMILES strings and implying them to corresponding sub-structures or\ncharacteristics. To refine these alignments, we propose In-Context Selective\nReflection, which retrieves previous extraction results as context examples for\nteacher LLM to reflect and lets a smaller student LLM select from in-context\nreflection and previous extraction results. Finally, we enhance the learning\nprocess of the student LLM through Chain-of-Thought In-Context Molecule Tuning,\nintegrating the fine-grained alignments and the reasoning processes within the\nChain-of-Thought format. Our experimental results demonstrate that MolReFlect\nenables LLMs like Mistral-7B to significantly outperform the previous\nbaselines, achieving SOTA performance on the ChEBI-20 dataset. This advancement\nnot only enhances the generative capabilities of LLMs in the molecule-caption\ntranslation task, but also contributes to a more explainable framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecule discovery is a pivotal research field, impacting everything from the\nmedicines we take to the materials we use. Recently, Large Language Models\n(LLMs) have been widely adopted in molecule understanding and generation, yet\nthe alignments between molecules and their corresponding captions remain a\nsignificant challenge. Previous endeavours often treat the molecule as a\ngeneral SMILES string or molecular graph, neglecting the fine-grained\nalignments between the molecular sub-structures and the descriptive textual\nphrases, which are crucial for accurate and explainable predictions. In this\ncase, we introduce MolReFlect, a novel teacher-student framework designed to\ncontextually perform the molecule-caption alignments in a fine-grained way. Our\napproach initially leverages a larger teacher LLM to label the detailed\nalignments by directly extracting critical phrases from molecule captions or\nSMILES strings and implying them to corresponding sub-structures or\ncharacteristics. To refine these alignments, we propose In-Context Selective\nReflection, which retrieves previous extraction results as context examples for\nteacher LLM to reflect and lets a smaller student LLM select from in-context\nreflection and previous extraction results. Finally, we enhance the learning\nprocess of the student LLM through Chain-of-Thought In-Context Molecule Tuning,\nintegrating the fine-grained alignments and the reasoning processes within the\nChain-of-Thought format. Our experimental results demonstrate that MolReFlect\nenables LLMs like Mistral-7B to significantly outperform the previous\nbaselines, achieving SOTA performance on the ChEBI-20 dataset. This advancement\nnot only enhances the generative capabilities of LLMs in the molecule-caption\ntranslation task, but also contributes to a more explainable framework."
                },
                "authors": [
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Yunqing Liu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jingdi Le"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "22 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05933v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05933v2",
                "updated": "2024-11-22T04:27:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    27,
                    10,
                    4,
                    327,
                    0
                ],
                "published": "2023-05-10T07:05:43Z",
                "published_parsed": [
                    2023,
                    5,
                    10,
                    7,
                    5,
                    43,
                    2,
                    130,
                    0
                ],
                "title": "Spectrum Breathing: Protecting Over-the-Air Federated Learning Against\n  Interference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectrum Breathing: Protecting Over-the-Air Federated Learning Against\n  Interference"
                },
                "summary": "Federated Learning (FL) is a widely embraced paradigm for distilling\nartificial intelligence from distributed mobile data. However, the deployment\nof FL in mobile networks can be compromised by exposure to interference from\nneighboring cells or jammers. Existing interference mitigation techniques\nrequire multi-cell cooperation or at least interference channel state\ninformation, which is expensive in practice. On the other hand, power control\nthat treats interference as noise may not be effective due to limited power\nbudgets, and also that this mechanism can trigger countermeasures by\ninterference sources. As a practical approach for protecting FL against\ninterference, we propose Spectrum Breathing, which cascades stochastic-gradient\npruning and spread spectrum to suppress interference without bandwidth\nexpansion. The cost is higher learning latency by exploiting the graceful\ndegradation of learning speed due to pruning. We synchronize the two operations\nsuch that their levels are controlled by the same parameter, Breathing Depth.\nTo optimally control the parameter, we develop a martingale-based approach to\nconvergence analysis of Over-the-Air FL with spectrum breathing, termed\nAirBreathing FL. We show a performance tradeoff between gradient-pruning and\ninterference-induced error as regulated by the breathing depth. Given receive\nSIR and model size, the optimization of the tradeoff yields two schemes for\ncontrolling the breathing depth that can be either fixed or adaptive to\nchannels and the learning process. As shown by experiments, in scenarios where\ntraditional Over-the-Air FL fails to converge in the presence of strong\ninterference, AirBreahing FL with either fixed or adaptive breathing depth can\nensure convergence where the adaptive scheme achieves close-to-ideal\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a widely embraced paradigm for distilling\nartificial intelligence from distributed mobile data. However, the deployment\nof FL in mobile networks can be compromised by exposure to interference from\nneighboring cells or jammers. Existing interference mitigation techniques\nrequire multi-cell cooperation or at least interference channel state\ninformation, which is expensive in practice. On the other hand, power control\nthat treats interference as noise may not be effective due to limited power\nbudgets, and also that this mechanism can trigger countermeasures by\ninterference sources. As a practical approach for protecting FL against\ninterference, we propose Spectrum Breathing, which cascades stochastic-gradient\npruning and spread spectrum to suppress interference without bandwidth\nexpansion. The cost is higher learning latency by exploiting the graceful\ndegradation of learning speed due to pruning. We synchronize the two operations\nsuch that their levels are controlled by the same parameter, Breathing Depth.\nTo optimally control the parameter, we develop a martingale-based approach to\nconvergence analysis of Over-the-Air FL with spectrum breathing, termed\nAirBreathing FL. We show a performance tradeoff between gradient-pruning and\ninterference-induced error as regulated by the breathing depth. Given receive\nSIR and model size, the optimization of the tradeoff yields two schemes for\ncontrolling the breathing depth that can be either fixed or adaptive to\nchannels and the learning process. As shown by experiments, in scenarios where\ntraditional Over-the-Air FL fails to converge in the presence of strong\ninterference, AirBreahing FL with either fixed or adaptive breathing depth can\nensure convergence where the adaptive scheme achieves close-to-ideal\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhanwei Wang"
                    },
                    {
                        "name": "Kaibin Huang"
                    },
                    {
                        "name": "Yonina C. Eldar"
                    }
                ],
                "author_detail": {
                    "name": "Yonina C. Eldar"
                },
                "author": "Yonina C. Eldar",
                "arxiv_doi": "10.1109/TWC.2024.3368197",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TWC.2024.3368197",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.05933v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05933v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14720v1",
                "updated": "2024-11-22T04:19:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    19,
                    32,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T04:19:32Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    4,
                    19,
                    32,
                    4,
                    327,
                    0
                ],
                "title": "Optimizing Social Media Annotation of HPV Vaccine Skepticism and\n  Misinformation Using Large Language Models: An Experimental Evaluation of\n  In-Context Learning and Fine-Tuning Stance Detection Across Multiple Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Social Media Annotation of HPV Vaccine Skepticism and\n  Misinformation Using Large Language Models: An Experimental Evaluation of\n  In-Context Learning and Fine-Tuning Stance Detection Across Multiple Models"
                },
                "summary": "This paper leverages large-language models (LLMs) to experimentally determine\noptimal strategies for scaling up social media content annotation for stance\ndetection on HPV vaccine-related tweets. We examine both conventional\nfine-tuning and emergent in-context learning methods, systematically varying\nstrategies of prompt engineering across widely used LLMs and their variants\n(e.g., GPT4, Mistral, and Llama3, etc.). Specifically, we varied prompt\ntemplate design, shot sampling methods, and shot quantity to detect stance on\nHPV vaccination. Our findings reveal that 1) in general, in-context learning\noutperforms fine-tuning in stance detection for HPV vaccine social media\ncontent; 2) increasing shot quantity does not necessarily enhance performance\nacross models; and 3) different LLMs and their variants present differing\nsensitivity to in-context learning conditions. We uncovered that the optimal\nin-context learning configuration for stance detection on HPV vaccine tweets\ninvolves six stratified shots paired with detailed contextual prompts. This\nstudy highlights the potential and provides an applicable approach for applying\nLLMs to research on social media stance and skepticism detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper leverages large-language models (LLMs) to experimentally determine\noptimal strategies for scaling up social media content annotation for stance\ndetection on HPV vaccine-related tweets. We examine both conventional\nfine-tuning and emergent in-context learning methods, systematically varying\nstrategies of prompt engineering across widely used LLMs and their variants\n(e.g., GPT4, Mistral, and Llama3, etc.). Specifically, we varied prompt\ntemplate design, shot sampling methods, and shot quantity to detect stance on\nHPV vaccination. Our findings reveal that 1) in general, in-context learning\noutperforms fine-tuning in stance detection for HPV vaccine social media\ncontent; 2) increasing shot quantity does not necessarily enhance performance\nacross models; and 3) different LLMs and their variants present differing\nsensitivity to in-context learning conditions. We uncovered that the optimal\nin-context learning configuration for stance detection on HPV vaccine tweets\ninvolves six stratified shots paired with detailed contextual prompts. This\nstudy highlights the potential and provides an applicable approach for applying\nLLMs to research on social media stance and skepticism detection."
                },
                "authors": [
                    {
                        "name": "Luhang Sun"
                    },
                    {
                        "name": "Varsha Pendyala"
                    },
                    {
                        "name": "Yun-Shiuan Chuang"
                    },
                    {
                        "name": "Shanglin Yang"
                    },
                    {
                        "name": "Jonathan Feldman"
                    },
                    {
                        "name": "Andrew Zhao"
                    },
                    {
                        "name": "Munmun De Choudhury"
                    },
                    {
                        "name": "Sijia Yang"
                    },
                    {
                        "name": "Dhavan Shah"
                    }
                ],
                "author_detail": {
                    "name": "Dhavan Shah"
                },
                "author": "Dhavan Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14713v1",
                "updated": "2024-11-22T03:43:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    3,
                    43,
                    41,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T03:43:41Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    3,
                    43,
                    41,
                    4,
                    327,
                    0
                ],
                "title": "LIBER: Lifelong User Behavior Modeling Based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIBER: Lifelong User Behavior Modeling Based on Large Language Models"
                },
                "summary": "CTR prediction plays a vital role in recommender systems. Recently, large\nlanguage models (LLMs) have been applied in recommender systems due to their\nemergence abilities. While leveraging semantic information from LLMs has shown\nsome improvements in the performance of recommender systems, two notable\nlimitations persist in these studies. First, LLM-enhanced recommender systems\nencounter challenges in extracting valuable information from lifelong user\nbehavior sequences within textual contexts for recommendation tasks. Second,\nthe inherent variability in human behaviors leads to a constant stream of new\nbehaviors and irregularly fluctuating user interests. This characteristic\nimposes two significant challenges on existing models. On the one hand, it\npresents difficulties for LLMs in effectively capturing the dynamic shifts in\nuser interests within these sequences, and on the other hand, there exists the\nissue of substantial computational overhead if the LLMs necessitate recurrent\ncalls upon each update to the user sequences. In this work, we propose Lifelong\nUser Behavior Modeling (LIBER) based on large language models, which includes\nthree modules: (1) User Behavior Streaming Partition (UBSP), (2) User Interest\nLearning (UIL), and (3) User Interest Fusion (UIF). Initially, UBSP is employed\nto condense lengthy user behavior sequences into shorter partitions in an\nincremental paradigm, facilitating more efficient processing. Subsequently, UIL\nleverages LLMs in a cascading way to infer insights from these partitions.\nFinally, UIF integrates the textual outputs generated by the aforementioned\nprocesses to construct a comprehensive representation, which can be\nincorporated by any recommendation model to enhance performance. LIBER has been\ndeployed on Huawei's music recommendation service and achieved substantial\nimprovements in users' play count and play time by 3.01% and 7.69%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTR prediction plays a vital role in recommender systems. Recently, large\nlanguage models (LLMs) have been applied in recommender systems due to their\nemergence abilities. While leveraging semantic information from LLMs has shown\nsome improvements in the performance of recommender systems, two notable\nlimitations persist in these studies. First, LLM-enhanced recommender systems\nencounter challenges in extracting valuable information from lifelong user\nbehavior sequences within textual contexts for recommendation tasks. Second,\nthe inherent variability in human behaviors leads to a constant stream of new\nbehaviors and irregularly fluctuating user interests. This characteristic\nimposes two significant challenges on existing models. On the one hand, it\npresents difficulties for LLMs in effectively capturing the dynamic shifts in\nuser interests within these sequences, and on the other hand, there exists the\nissue of substantial computational overhead if the LLMs necessitate recurrent\ncalls upon each update to the user sequences. In this work, we propose Lifelong\nUser Behavior Modeling (LIBER) based on large language models, which includes\nthree modules: (1) User Behavior Streaming Partition (UBSP), (2) User Interest\nLearning (UIL), and (3) User Interest Fusion (UIF). Initially, UBSP is employed\nto condense lengthy user behavior sequences into shorter partitions in an\nincremental paradigm, facilitating more efficient processing. Subsequently, UIL\nleverages LLMs in a cascading way to infer insights from these partitions.\nFinally, UIF integrates the textual outputs generated by the aforementioned\nprocesses to construct a comprehensive representation, which can be\nincorporated by any recommendation model to enhance performance. LIBER has been\ndeployed on Huawei's music recommendation service and achieved substantial\nimprovements in users' play count and play time by 3.01% and 7.69%."
                },
                "authors": [
                    {
                        "name": "Chenxu Zhu"
                    },
                    {
                        "name": "Shigang Quan"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Xiaoling Cai"
                    },
                    {
                        "name": "Hong Zhu"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Yunjia Xi"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14708v1",
                "updated": "2024-11-22T03:33:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    3,
                    33,
                    51,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T03:33:51Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    3,
                    33,
                    51,
                    4,
                    327,
                    0
                ],
                "title": "Understanding LLM Embeddings for Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding LLM Embeddings for Regression"
                },
                "summary": "With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of large language models (LLMs) for flexibly processing\ninformation as strings, a natural application is regression, specifically by\npreprocessing string representations into LLM embeddings as downstream features\nfor metric prediction. In this paper, we provide one of the first comprehensive\ninvestigations into embedding-based regression and demonstrate that LLM\nembeddings as features can be better for high-dimensional regression tasks than\nusing traditional feature engineering. This regression performance can be\nexplained in part due to LLM embeddings over numeric data inherently preserving\nLipschitz continuity over the feature space. Furthermore, we quantify the\ncontribution of different model effects, most notably model size and language\nunderstanding, which we find surprisingly do not always improve regression\nperformance."
                },
                "authors": [
                    {
                        "name": "Eric Tang"
                    },
                    {
                        "name": "Bangding Yang"
                    },
                    {
                        "name": "Xingyou Song"
                    }
                ],
                "author_detail": {
                    "name": "Xingyou Song"
                },
                "author": "Xingyou Song",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13587v2",
                "updated": "2024-11-22T03:16:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    3,
                    16,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-18T01:52:20Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    1,
                    52,
                    20,
                    0,
                    323,
                    0
                ],
                "title": "Exploring the Adversarial Vulnerabilities of Vision-Language-Action\n  Models in Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Adversarial Vulnerabilities of Vision-Language-Action\n  Models in Robotics"
                },
                "summary": "Recently in robotics, Vision-Language-Action (VLA) models have emerged as a\ntransformative approach, enabling robots to execute complex tasks by\nintegrating visual and linguistic inputs within an end-to-end learning\nframework. While VLA models offer significant capabilities, they also introduce\nnew attack surfaces, making them vulnerable to adversarial attacks. With these\nvulnerabilities largely unexplored, this paper systematically quantifies the\nrobustness of VLA-based robotic systems. Recognizing the unique demands of\nrobotic execution, our attack objectives target the inherent spatial and\nfunctional characteristics of robotic systems. In particular, we introduce an\nuntargeted position-aware attack objective that leverages spatial foundations\nto destabilize robotic actions, and a targeted attack objective that\nmanipulates the robotic trajectory. Additionally, we design an adversarial\npatch generation approach that places a small, colorful patch within the\ncamera's view, effectively executing the attack in both digital and physical\nenvironments. Our evaluation reveals a marked degradation in task success\nrates, with up to a 100\\% reduction across a suite of simulated robotic tasks,\nhighlighting critical security gaps in current VLA architectures. By unveiling\nthese vulnerabilities and proposing actionable evaluation metrics, this work\nadvances both the understanding and enhancement of safety for VLA-based robotic\nsystems, underscoring the necessity for developing robust defense strategies\nprior to physical-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently in robotics, Vision-Language-Action (VLA) models have emerged as a\ntransformative approach, enabling robots to execute complex tasks by\nintegrating visual and linguistic inputs within an end-to-end learning\nframework. While VLA models offer significant capabilities, they also introduce\nnew attack surfaces, making them vulnerable to adversarial attacks. With these\nvulnerabilities largely unexplored, this paper systematically quantifies the\nrobustness of VLA-based robotic systems. Recognizing the unique demands of\nrobotic execution, our attack objectives target the inherent spatial and\nfunctional characteristics of robotic systems. In particular, we introduce an\nuntargeted position-aware attack objective that leverages spatial foundations\nto destabilize robotic actions, and a targeted attack objective that\nmanipulates the robotic trajectory. Additionally, we design an adversarial\npatch generation approach that places a small, colorful patch within the\ncamera's view, effectively executing the attack in both digital and physical\nenvironments. Our evaluation reveals a marked degradation in task success\nrates, with up to a 100\\% reduction across a suite of simulated robotic tasks,\nhighlighting critical security gaps in current VLA architectures. By unveiling\nthese vulnerabilities and proposing actionable evaluation metrics, this work\nadvances both the understanding and enhancement of safety for VLA-based robotic\nsystems, underscoring the necessity for developing robust defense strategies\nprior to physical-world deployments."
                },
                "authors": [
                    {
                        "name": "Taowen Wang"
                    },
                    {
                        "name": "Dongfang Liu"
                    },
                    {
                        "name": "James Chenhao Liang"
                    },
                    {
                        "name": "Wenhao Yang"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Cheng Han"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Ruixiang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruixiang Tang"
                },
                "author": "Ruixiang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14698v1",
                "updated": "2024-11-22T03:12:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    3,
                    12,
                    39,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T03:12:39Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    3,
                    12,
                    39,
                    4,
                    327,
                    0
                ],
                "title": "Improving Mathematical Reasoning Capabilities of Small Language Models\n  via Feedback-Driven Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Mathematical Reasoning Capabilities of Small Language Models\n  via Feedback-Driven Distillation"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional reasoning capabilities,\noften achieving state-of-the-art performance in various tasks. However, their\nsubstantial computational and memory demands, due to billions of parameters,\nhinder deployment in resource-constrained environments. A promising solution is\nknowledge distillation, where LLMs transfer reasoning capabilities to Small\nLanguage Models (SLMs, $\\le$ 1B parameters), enabling wider deployment on\nlow-resource devices. Existing methods primarily focus on generating\nhigh-quality reasoning rationales for distillation datasets but often neglect\nthe critical role of data quantity and quality. To address these challenges, we\npropose a Feedback-Driven Distillation (FDD) framework to enhance SLMs'\nmathematical reasoning capabilities. In the initialization stage, a\ndistillation dataset is constructed by prompting LLMs to pair mathematical\nproblems with corresponding reasoning rationales. We classify problems into\neasy and hard categories based on SLM performance. For easy problems, LLMs\ngenerate more complex variations, while for hard problems, new questions of\nsimilar complexity are synthesized. In addition, we propose a multi-round\ndistillation paradigm to iteratively enrich the distillation datasets, thereby\nprogressively improving the mathematical reasoning abilities of SLMs.\nExperimental results demonstrate that our method can make SLMs achieve SOTA\nmathematical reasoning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional reasoning capabilities,\noften achieving state-of-the-art performance in various tasks. However, their\nsubstantial computational and memory demands, due to billions of parameters,\nhinder deployment in resource-constrained environments. A promising solution is\nknowledge distillation, where LLMs transfer reasoning capabilities to Small\nLanguage Models (SLMs, $\\le$ 1B parameters), enabling wider deployment on\nlow-resource devices. Existing methods primarily focus on generating\nhigh-quality reasoning rationales for distillation datasets but often neglect\nthe critical role of data quantity and quality. To address these challenges, we\npropose a Feedback-Driven Distillation (FDD) framework to enhance SLMs'\nmathematical reasoning capabilities. In the initialization stage, a\ndistillation dataset is constructed by prompting LLMs to pair mathematical\nproblems with corresponding reasoning rationales. We classify problems into\neasy and hard categories based on SLM performance. For easy problems, LLMs\ngenerate more complex variations, while for hard problems, new questions of\nsimilar complexity are synthesized. In addition, we propose a multi-round\ndistillation paradigm to iteratively enrich the distillation datasets, thereby\nprogressively improving the mathematical reasoning abilities of SLMs.\nExperimental results demonstrate that our method can make SLMs achieve SOTA\nmathematical reasoning performance."
                },
                "authors": [
                    {
                        "name": "Xunyu Zhu"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Can Ma"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13802v2",
                "updated": "2024-11-22T02:32:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    2,
                    32,
                    9,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-21T03:05:38Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    5,
                    38,
                    3,
                    326,
                    0
                ],
                "title": "SemiKong: Curating, Training, and Evaluating A Semiconductor\n  Industry-Specific Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemiKong: Curating, Training, and Evaluating A Semiconductor\n  Industry-Specific Large Language Model"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the potential to address some\nissues within the semiconductor industry. However, they are often\ngeneral-purpose models that lack the specialized knowledge needed to tackle the\nunique challenges of this sector, such as the intricate physics and chemistry\nof semiconductor devices and processes. SemiKong, the first industry-specific\nLLM for the semiconductor domain, provides a foundation that can be used to\ndevelop tailored proprietary models. With SemiKong 1.0, we aim to develop a\nfoundational model capable of understanding etching problems at an expert\nlevel. Our key contributions include (a) curating a comprehensive corpus of\nsemiconductor-related texts, (b) creating a foundational model with in-depth\nsemiconductor knowledge, and (c) introducing a framework for integrating expert\nknowledge, thereby advancing the evaluation process of domain-specific AI\nmodels. Through fine-tuning a pre-trained LLM using our curated dataset, we\nhave shown that SemiKong outperforms larger, general-purpose LLMs in various\nsemiconductor manufacturing and design tasks. Our extensive experiments\nunderscore the importance of developing domain-specific LLMs as a foundation\nfor company- or tool-specific proprietary models, paving the way for further\nresearch and applications in the semiconductor domain. Code and dataset will be\navailable at https://github.com/aitomatic/semikong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the potential to address some\nissues within the semiconductor industry. However, they are often\ngeneral-purpose models that lack the specialized knowledge needed to tackle the\nunique challenges of this sector, such as the intricate physics and chemistry\nof semiconductor devices and processes. SemiKong, the first industry-specific\nLLM for the semiconductor domain, provides a foundation that can be used to\ndevelop tailored proprietary models. With SemiKong 1.0, we aim to develop a\nfoundational model capable of understanding etching problems at an expert\nlevel. Our key contributions include (a) curating a comprehensive corpus of\nsemiconductor-related texts, (b) creating a foundational model with in-depth\nsemiconductor knowledge, and (c) introducing a framework for integrating expert\nknowledge, thereby advancing the evaluation process of domain-specific AI\nmodels. Through fine-tuning a pre-trained LLM using our curated dataset, we\nhave shown that SemiKong outperforms larger, general-purpose LLMs in various\nsemiconductor manufacturing and design tasks. Our extensive experiments\nunderscore the importance of developing domain-specific LLMs as a foundation\nfor company- or tool-specific proprietary models, paving the way for further\nresearch and applications in the semiconductor domain. Code and dataset will be\navailable at https://github.com/aitomatic/semikong"
                },
                "authors": [
                    {
                        "name": "Christopher Nguyen"
                    },
                    {
                        "name": "William Nguyen"
                    },
                    {
                        "name": "Atsushi Suzuki"
                    },
                    {
                        "name": "Daisuke Oku"
                    },
                    {
                        "name": "Hong An Phan"
                    },
                    {
                        "name": "Sang Dinh"
                    },
                    {
                        "name": "Zooey Nguyen"
                    },
                    {
                        "name": "Anh Ha"
                    },
                    {
                        "name": "Shruti Raghavan"
                    },
                    {
                        "name": "Huy Vo"
                    },
                    {
                        "name": "Thang Nguyen"
                    },
                    {
                        "name": "Lan Nguyen"
                    },
                    {
                        "name": "Yoshikuni Hirayama"
                    }
                ],
                "author_detail": {
                    "name": "Yoshikuni Hirayama"
                },
                "author": "Yoshikuni Hirayama",
                "arxiv_comment": "On-going work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12448v2",
                "updated": "2024-11-22T02:31:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    2,
                    31,
                    13,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-19T12:15:40Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    15,
                    40,
                    1,
                    324,
                    0
                ],
                "title": "Large Language Models for Lossless Image Compression: Next-Pixel\n  Prediction in Language Space is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Lossless Image Compression: Next-Pixel\n  Prediction in Language Space is All You Need"
                },
                "summary": "We have recently witnessed that ``Intelligence\" and `` Compression\" are the\ntwo sides of the same coin, where the language large model (LLM) with\nunprecedented intelligence is a general-purpose lossless compressor for various\ndata modalities. This attribute particularly appeals to the lossless image\ncompression community, given the increasing need to compress high-resolution\nimages in the current streaming media era. Consequently, a spontaneous envision\nemerges: Can the compression performance of the LLM elevate lossless image\ncompression to new heights? However, our findings indicate that the naive\napplication of LLM-based lossless image compressors suffers from a considerable\nperformance gap compared with existing state-of-the-art (SOTA) codecs on common\nbenchmark datasets. In light of this, we are dedicated to fulfilling the\nunprecedented intelligence (compression) capacity of the LLM for lossless image\ncompression tasks, thereby bridging the gap between theoretical and practical\ncompression performance. Specifically, we propose P$^{2}$-LLM, a next-pixel\nprediction-based LLM, which integrates various elaborated insights and\nmethodologies, \\textit{e.g.,} pixel-level priors, the in-context ability of\nLLM, and a pixel-level semantic preservation strategy, to enhance the\nunderstanding capacity of pixel sequences for better next-pixel predictions.\nExtensive experiments on benchmark datasets demonstrate that P$^{2}$-LLM can\nbeat SOTA classical and learned codecs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have recently witnessed that ``Intelligence\" and `` Compression\" are the\ntwo sides of the same coin, where the language large model (LLM) with\nunprecedented intelligence is a general-purpose lossless compressor for various\ndata modalities. This attribute particularly appeals to the lossless image\ncompression community, given the increasing need to compress high-resolution\nimages in the current streaming media era. Consequently, a spontaneous envision\nemerges: Can the compression performance of the LLM elevate lossless image\ncompression to new heights? However, our findings indicate that the naive\napplication of LLM-based lossless image compressors suffers from a considerable\nperformance gap compared with existing state-of-the-art (SOTA) codecs on common\nbenchmark datasets. In light of this, we are dedicated to fulfilling the\nunprecedented intelligence (compression) capacity of the LLM for lossless image\ncompression tasks, thereby bridging the gap between theoretical and practical\ncompression performance. Specifically, we propose P$^{2}$-LLM, a next-pixel\nprediction-based LLM, which integrates various elaborated insights and\nmethodologies, \\textit{e.g.,} pixel-level priors, the in-context ability of\nLLM, and a pixel-level semantic preservation strategy, to enhance the\nunderstanding capacity of pixel sequences for better next-pixel predictions.\nExtensive experiments on benchmark datasets demonstrate that P$^{2}$-LLM can\nbeat SOTA classical and learned codecs."
                },
                "authors": [
                    {
                        "name": "Kecheng Chen"
                    },
                    {
                        "name": "Pingping Zhang"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Yibing Liu"
                    },
                    {
                        "name": "Jiaxin Huang"
                    },
                    {
                        "name": "Shiqi Wang"
                    },
                    {
                        "name": "Hong Yan"
                    },
                    {
                        "name": "Haoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Haoliang Li"
                },
                "author": "Haoliang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14672v1",
                "updated": "2024-11-22T02:11:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    2,
                    11,
                    37,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T02:11:37Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    2,
                    11,
                    37,
                    4,
                    327,
                    0
                ],
                "title": "Multiverse of Greatness: Generating Story Branches with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiverse of Greatness: Generating Story Branches with LLMs"
                },
                "summary": "This paper presents Dynamic Context Prompting/Programming (DCP/P), a novel\nframework for interacting with LLMs to generate graph-based content with a\ndynamic context window history. While there is an existing study utilizing LLMs\nto generate a visual novel game, the previous study involved a manual process\nof output extraction and did not provide flexibility in generating a longer,\ncoherent story. We evaluate DCP/P against our baseline, which does not provide\ncontext history to an LLM and only relies on the initial story data. Through\nobjective evaluation, we show that simply providing the LLM with a summary\nleads to a subpar story compared to additionally providing the LLM with the\nproper context of the story. We also provide an extensive qualitative analysis\nand discussion. We qualitatively examine the quality of the objectively\nbest-performing generated game from each approach. In addition, we examine\nbiases in word choices and word sentiment of the generated content. We find a\nconsistent observation with previous studies that LLMs are biased towards\ncertain words, even with a different LLM family. Finally, we provide a\ncomprehensive discussion on opportunities for future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Dynamic Context Prompting/Programming (DCP/P), a novel\nframework for interacting with LLMs to generate graph-based content with a\ndynamic context window history. While there is an existing study utilizing LLMs\nto generate a visual novel game, the previous study involved a manual process\nof output extraction and did not provide flexibility in generating a longer,\ncoherent story. We evaluate DCP/P against our baseline, which does not provide\ncontext history to an LLM and only relies on the initial story data. Through\nobjective evaluation, we show that simply providing the LLM with a summary\nleads to a subpar story compared to additionally providing the LLM with the\nproper context of the story. We also provide an extensive qualitative analysis\nand discussion. We qualitatively examine the quality of the objectively\nbest-performing generated game from each approach. In addition, we examine\nbiases in word choices and word sentiment of the generated content. We find a\nconsistent observation with previous studies that LLMs are biased towards\ncertain words, even with a different LLM family. Finally, we provide a\ncomprehensive discussion on opportunities for future studies."
                },
                "authors": [
                    {
                        "name": "Pittawat Taveekitworachai"
                    },
                    {
                        "name": "Chollakorn Nimpattanavong"
                    },
                    {
                        "name": "Mustafa Can Gursesli"
                    },
                    {
                        "name": "Antonio Lanata"
                    },
                    {
                        "name": "Andrea Guazzini"
                    },
                    {
                        "name": "Ruck Thawonmas"
                    }
                ],
                "author_detail": {
                    "name": "Ruck Thawonmas"
                },
                "author": "Ruck Thawonmas",
                "arxiv_comment": "12 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03205v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03205v4",
                "updated": "2024-11-22T02:00:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    2,
                    0,
                    21,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-05T15:53:59Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    53,
                    59,
                    1,
                    310,
                    0
                ],
                "title": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis"
                },
                "summary": "Recent advancements in Generative AI offer promising capabilities for spatial\nanalysis. Despite their potential, the integration of generative AI with\nestablished GIS platforms remains underexplored. In this study, we propose a\nframework for integrating LLMs directly into existing GIS platforms, using QGIS\nas an example. Our approach leverages the reasoning and programming\ncapabilities of LLMs to autonomously generate spatial analysis workflows and\ncode through an informed agent that has comprehensive documentation of key GIS\ntools and parameters. The implementation of this framework resulted in the\ndevelopment of a \"GIS Copilot\" that allows GIS users to interact with QGIS\nusing natural language commands for spatial analysis. The GIS Copilot was\nevaluated with over 100 spatial analysis tasks with three complexity levels:\nbasic tasks that require one GIS tool and typically involve one data layer to\nperform simple operations; intermediate tasks involving multi-step processes\nwith multiple tools, guided by user instructions; and advanced tasks which\ninvolve multi-step processes that require multiple tools but not guided by user\ninstructions, necessitating the agent to independently decide on and executes\nthe necessary steps. The evaluation reveals that the GIS Copilot demonstrates\nstrong potential in automating foundational GIS operations, with a high success\nrate in tool selection and code generation for basic and intermediate tasks,\nwhile challenges remain in achieving full autonomy for more complex tasks. This\nstudy contributes to the emerging vision of Autonomous GIS, providing a pathway\nfor non-experts to engage with geospatial analysis with minimal prior\nexpertise. While full autonomy is yet to be achieved, the GIS Copilot\ndemonstrates significant potential for simplifying GIS workflows and enhancing\ndecision-making processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Generative AI offer promising capabilities for spatial\nanalysis. Despite their potential, the integration of generative AI with\nestablished GIS platforms remains underexplored. In this study, we propose a\nframework for integrating LLMs directly into existing GIS platforms, using QGIS\nas an example. Our approach leverages the reasoning and programming\ncapabilities of LLMs to autonomously generate spatial analysis workflows and\ncode through an informed agent that has comprehensive documentation of key GIS\ntools and parameters. The implementation of this framework resulted in the\ndevelopment of a \"GIS Copilot\" that allows GIS users to interact with QGIS\nusing natural language commands for spatial analysis. The GIS Copilot was\nevaluated with over 100 spatial analysis tasks with three complexity levels:\nbasic tasks that require one GIS tool and typically involve one data layer to\nperform simple operations; intermediate tasks involving multi-step processes\nwith multiple tools, guided by user instructions; and advanced tasks which\ninvolve multi-step processes that require multiple tools but not guided by user\ninstructions, necessitating the agent to independently decide on and executes\nthe necessary steps. The evaluation reveals that the GIS Copilot demonstrates\nstrong potential in automating foundational GIS operations, with a high success\nrate in tool selection and code generation for basic and intermediate tasks,\nwhile challenges remain in achieving full autonomy for more complex tasks. This\nstudy contributes to the emerging vision of Autonomous GIS, providing a pathway\nfor non-experts to engage with geospatial analysis with minimal prior\nexpertise. While full autonomy is yet to be achieved, the GIS Copilot\ndemonstrates significant potential for simplifying GIS workflows and enhancing\ndecision-making processes."
                },
                "authors": [
                    {
                        "name": "Temitope Akinboyewa"
                    },
                    {
                        "name": "Zhenlong Li"
                    },
                    {
                        "name": "Huan Ning"
                    },
                    {
                        "name": "M. Naser Lessani"
                    }
                ],
                "author_detail": {
                    "name": "M. Naser Lessani"
                },
                "author": "M. Naser Lessani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03205v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03205v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04873v2",
                "updated": "2024-11-22T01:13:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    1,
                    13,
                    13,
                    4,
                    327,
                    0
                ],
                "published": "2024-07-05T21:44:11Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    21,
                    44,
                    11,
                    4,
                    187,
                    0
                ],
                "title": "Evaluating Language Models for Generating and Judging Programming\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Language Models for Generating and Judging Programming\n  Feedback"
                },
                "summary": "The emergence of large language models (LLMs) has transformed research and\npractice across a wide range of domains. Within the computing education\nresearch (CER) domain, LLMs have garnered significant attention, particularly\nin the context of learning programming. Much of the work on LLMs in CER,\nhowever, has focused on applying and evaluating proprietary models. In this\narticle, we evaluate the efficiency of open-source LLMs in generating\nhigh-quality feedback for programming assignments and judging the quality of\nprogramming feedback, contrasting the results with proprietary models. Our\nevaluations on a dataset of students' submissions to introductory Python\nprogramming exercises suggest that state-of-the-art open-source LLMs are nearly\non par with proprietary models in both generating and assessing programming\nfeedback. Additionally, we demonstrate the efficiency of smaller LLMs in these\ntasks and highlight the wide range of LLMs accessible, even for free, to\neducators and practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has transformed research and\npractice across a wide range of domains. Within the computing education\nresearch (CER) domain, LLMs have garnered significant attention, particularly\nin the context of learning programming. Much of the work on LLMs in CER,\nhowever, has focused on applying and evaluating proprietary models. In this\narticle, we evaluate the efficiency of open-source LLMs in generating\nhigh-quality feedback for programming assignments and judging the quality of\nprogramming feedback, contrasting the results with proprietary models. Our\nevaluations on a dataset of students' submissions to introductory Python\nprogramming exercises suggest that state-of-the-art open-source LLMs are nearly\non par with proprietary models in both generating and assessing programming\nfeedback. Additionally, we demonstrate the efficiency of smaller LLMs in these\ntasks and highlight the wide range of LLMs accessible, even for free, to\neducators and practitioners."
                },
                "authors": [
                    {
                        "name": "Charles Koutcheme"
                    },
                    {
                        "name": "Nicola Dainese"
                    },
                    {
                        "name": "Arto Hellas"
                    },
                    {
                        "name": "Sami Sarsa"
                    },
                    {
                        "name": "Juho Leinonen"
                    },
                    {
                        "name": "Syed Ashraf"
                    },
                    {
                        "name": "Paul Denny"
                    }
                ],
                "author_detail": {
                    "name": "Paul Denny"
                },
                "author": "Paul Denny",
                "arxiv_comment": "2 tables. Accepted for SIGCSE TS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14654v1",
                "updated": "2024-11-22T00:59:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    0,
                    59,
                    25,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T00:59:25Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    0,
                    59,
                    25,
                    4,
                    327,
                    0
                ],
                "title": "Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP) by delivering state-of-the-art performance across a variety of tasks.\nAmong these, Transformer-based models like BERT and GPT rely on pooling layers\nto aggregate token-level embeddings into sentence-level representations. Common\npooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in\nthis aggregation process. Despite their widespread use, the comparative\nperformance of these strategies on different LLM architectures remains\nunderexplored. To address this gap, this paper investigates the effects of\nthese pooling mechanisms on two prominent LLM families -- BERT and GPT, in the\ncontext of sentence-level sentiment analysis. Comprehensive experiments reveal\nthat each pooling mechanism exhibits unique strengths and weaknesses depending\non the task's specific requirements. Our findings underline the importance of\nselecting pooling methods tailored to the demands of particular applications,\nprompting a re-evaluation of common assumptions regarding pooling operations.\nBy offering actionable insights, this study contributes to the optimization of\nLLM-based models for downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP) by delivering state-of-the-art performance across a variety of tasks.\nAmong these, Transformer-based models like BERT and GPT rely on pooling layers\nto aggregate token-level embeddings into sentence-level representations. Common\npooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in\nthis aggregation process. Despite their widespread use, the comparative\nperformance of these strategies on different LLM architectures remains\nunderexplored. To address this gap, this paper investigates the effects of\nthese pooling mechanisms on two prominent LLM families -- BERT and GPT, in the\ncontext of sentence-level sentiment analysis. Comprehensive experiments reveal\nthat each pooling mechanism exhibits unique strengths and weaknesses depending\non the task's specific requirements. Our findings underline the importance of\nselecting pooling methods tailored to the demands of particular applications,\nprompting a re-evaluation of common assumptions regarding pooling operations.\nBy offering actionable insights, this study contributes to the optimization of\nLLM-based models for downstream tasks."
                },
                "authors": [
                    {
                        "name": "Jinming Xing"
                    },
                    {
                        "name": "Ruilin Xing"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14652v1",
                "updated": "2024-11-22T00:55:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    0,
                    55,
                    7,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T00:55:07Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    0,
                    55,
                    7,
                    4,
                    327,
                    0
                ],
                "title": "Social Media Algorithms Can Shape Affective Polarization via Exposure to\n  Antidemocratic Attitudes and Partisan Animosity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Media Algorithms Can Shape Affective Polarization via Exposure to\n  Antidemocratic Attitudes and Partisan Animosity"
                },
                "summary": "There is widespread concern about the negative impacts of social media feed\nranking algorithms on political polarization. Leveraging advancements in large\nlanguage models (LLMs), we develop an approach to re-rank feeds in real-time to\ntest the effects of content that is likely to polarize: expressions of\nantidemocratic attitudes and partisan animosity (AAPA). In a preregistered\n10-day field experiment on X/Twitter with 1,256 consented participants, we\nincrease or decrease participants' exposure to AAPA in their algorithmically\ncurated feeds. We observe more positive outparty feelings when AAPA exposure is\ndecreased and more negative outparty feelings when AAPA exposure is increased.\nExposure to AAPA content also results in an immediate increase in negative\nemotions, such as sadness and anger. The interventions do not significantly\nimpact traditional engagement metrics such as re-post and favorite rates. These\nfindings highlight a potential pathway for developing feed algorithms that\nmitigate affective polarization by addressing content that undermines the\nshared values required for a healthy democracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is widespread concern about the negative impacts of social media feed\nranking algorithms on political polarization. Leveraging advancements in large\nlanguage models (LLMs), we develop an approach to re-rank feeds in real-time to\ntest the effects of content that is likely to polarize: expressions of\nantidemocratic attitudes and partisan animosity (AAPA). In a preregistered\n10-day field experiment on X/Twitter with 1,256 consented participants, we\nincrease or decrease participants' exposure to AAPA in their algorithmically\ncurated feeds. We observe more positive outparty feelings when AAPA exposure is\ndecreased and more negative outparty feelings when AAPA exposure is increased.\nExposure to AAPA content also results in an immediate increase in negative\nemotions, such as sadness and anger. The interventions do not significantly\nimpact traditional engagement metrics such as re-post and favorite rates. These\nfindings highlight a potential pathway for developing feed algorithms that\nmitigate affective polarization by addressing content that undermines the\nshared values required for a healthy democracy."
                },
                "authors": [
                    {
                        "name": "Tiziano Piccardi"
                    },
                    {
                        "name": "Martin Saveski"
                    },
                    {
                        "name": "Chenyan Jia"
                    },
                    {
                        "name": "Jeffrey T. Hancock"
                    },
                    {
                        "name": "Jeanne L. Tsai"
                    },
                    {
                        "name": "Michael Bernstein"
                    }
                ],
                "author_detail": {
                    "name": "Michael Bernstein"
                },
                "author": "Michael Bernstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04683v2",
                "updated": "2024-11-22T00:52:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    0,
                    52,
                    8,
                    4,
                    327,
                    0
                ],
                "published": "2024-10-07T01:34:42Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    1,
                    34,
                    42,
                    0,
                    281,
                    0
                ],
                "title": "Towards Measuring Goal-Directedness in AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Measuring Goal-Directedness in AI Systems"
                },
                "summary": "Recent advances in deep learning have brought attention to the possibility of\ncreating advanced, general AI systems that outperform humans across many tasks.\nHowever, if these systems pursue unintended goals, there could be catastrophic\nconsequences. A key prerequisite for AI systems pursuing unintended goals is\nwhether they will behave in a coherent and goal-directed manner in the first\nplace, optimizing for some unknown goal; there exists significant research\ntrying to evaluate systems for said behaviors. However, the most rigorous\ndefinitions of goal-directedness we currently have are difficult to compute in\nreal-world settings. Drawing upon this previous literature, we explore policy\ngoal-directedness within reinforcement learning (RL) environments. In our\nfindings, we propose a different family of definitions of the goal-directedness\nof a policy that analyze whether it is well-modeled as near-optimal for many\n(sparse) reward functions. We operationalize this preliminary definition of\ngoal-directedness and test it in toy Markov decision process (MDP)\nenvironments. Furthermore, we explore how goal-directedness could be measured\nin frontier large-language models (LLMs). Our contribution is a definition of\ngoal-directedness that is simpler and more easily computable in order to\napproach the question of whether AI systems could pursue dangerous goals. We\nrecommend further exploration of measuring coherence and goal-directedness,\nbased on our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning have brought attention to the possibility of\ncreating advanced, general AI systems that outperform humans across many tasks.\nHowever, if these systems pursue unintended goals, there could be catastrophic\nconsequences. A key prerequisite for AI systems pursuing unintended goals is\nwhether they will behave in a coherent and goal-directed manner in the first\nplace, optimizing for some unknown goal; there exists significant research\ntrying to evaluate systems for said behaviors. However, the most rigorous\ndefinitions of goal-directedness we currently have are difficult to compute in\nreal-world settings. Drawing upon this previous literature, we explore policy\ngoal-directedness within reinforcement learning (RL) environments. In our\nfindings, we propose a different family of definitions of the goal-directedness\nof a policy that analyze whether it is well-modeled as near-optimal for many\n(sparse) reward functions. We operationalize this preliminary definition of\ngoal-directedness and test it in toy Markov decision process (MDP)\nenvironments. Furthermore, we explore how goal-directedness could be measured\nin frontier large-language models (LLMs). Our contribution is a definition of\ngoal-directedness that is simpler and more easily computable in order to\napproach the question of whether AI systems could pursue dangerous goals. We\nrecommend further exploration of measuring coherence and goal-directedness,\nbased on our findings."
                },
                "authors": [
                    {
                        "name": "Dylan Xu"
                    },
                    {
                        "name": "Juan-Pablo Rivera"
                    }
                ],
                "author_detail": {
                    "name": "Juan-Pablo Rivera"
                },
                "author": "Juan-Pablo Rivera",
                "arxiv_comment": "Updated acknowledgements",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14637v1",
                "updated": "2024-11-22T00:07:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    0,
                    7,
                    36,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T00:07:36Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    0,
                    7,
                    36,
                    4,
                    327,
                    0
                ],
                "title": "Enhancing Clinical Trial Patient Matching through Knowledge Augmentation\n  with Multi-Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Clinical Trial Patient Matching through Knowledge Augmentation\n  with Multi-Agents"
                },
                "summary": "Matching patients effectively and efficiently for clinical trials is a\nsignificant challenge due to the complexity and variability of patient profiles\nand trial criteria. This paper presents a novel framework, Multi-Agents for\nKnowledge Augmentation (MAKA), designed to enhance patient-trial matching by\ndynamically supplementing matching prompts with external, domain-specific\nknowledge. The MAKA architecture consists of five key components: a knowledge\nprobing agent that detects gaps in domain knowledge, a navigation agent that\nmanages interactions among multiple specialized knowledge augmentation agents,\na knowledge augmentation agent that incorporates relevant information into\npatient-trial matching prompts, a supervision agent aligning the outputs from\nother agents with the instructions and a matching agent making the final\nselection decision. This approach enhances the accuracy and contextual richness\nof patient matching, addresses inherent knowledge gaps in both trail criteria\nand large language models (LLMs), and improves the alignment between patient\ncharacteristics and the criteria.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matching patients effectively and efficiently for clinical trials is a\nsignificant challenge due to the complexity and variability of patient profiles\nand trial criteria. This paper presents a novel framework, Multi-Agents for\nKnowledge Augmentation (MAKA), designed to enhance patient-trial matching by\ndynamically supplementing matching prompts with external, domain-specific\nknowledge. The MAKA architecture consists of five key components: a knowledge\nprobing agent that detects gaps in domain knowledge, a navigation agent that\nmanages interactions among multiple specialized knowledge augmentation agents,\na knowledge augmentation agent that incorporates relevant information into\npatient-trial matching prompts, a supervision agent aligning the outputs from\nother agents with the instructions and a matching agent making the final\nselection decision. This approach enhances the accuracy and contextual richness\nof patient matching, addresses inherent knowledge gaps in both trail criteria\nand large language models (LLMs), and improves the alignment between patient\ncharacteristics and the criteria."
                },
                "authors": [
                    {
                        "name": "Hanwen Shi"
                    },
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Kunpeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kunpeng Zhang"
                },
                "author": "Kunpeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01769v2",
                "updated": "2024-11-21T23:39:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    23,
                    39,
                    12,
                    3,
                    326,
                    0
                ],
                "published": "2024-05-02T22:43:02Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    22,
                    43,
                    2,
                    3,
                    123,
                    0
                ],
                "title": "A Survey on Large Language Models for Critical Societal Domains:\n  Finance, Healthcare, and Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Models for Critical Societal Domains:\n  Finance, Healthcare, and Law"
                },
                "summary": "In the fast-evolving domain of artificial intelligence, large language models\n(LLMs) such as GPT-3 and GPT-4 are revolutionizing the landscapes of finance,\nhealthcare, and law: domains characterized by their reliance on professional\nexpertise, challenging data acquisition, high-stakes, and stringent regulatory\ncompliance. This survey offers a detailed exploration of the methodologies,\napplications, challenges, and forward-looking opportunities of LLMs within\nthese high-stakes sectors. We highlight the instrumental role of LLMs in\nenhancing diagnostic and treatment methodologies in healthcare, innovating\nfinancial analytics, and refining legal interpretation and compliance\nstrategies. Moreover, we critically examine the ethics for LLM applications in\nthese fields, pointing out the existing ethical concerns and the need for\ntransparent, fair, and robust AI systems that respect regulatory norms. By\npresenting a thorough review of current literature and practical applications,\nwe showcase the transformative impact of LLMs, and outline the imperative for\ninterdisciplinary cooperation, methodological advancements, and ethical\nvigilance. Through this lens, we aim to spark dialogue and inspire future\nresearch dedicated to maximizing the benefits of LLMs while mitigating their\nrisks in these precision-dependent sectors. To facilitate future research on\nLLMs in these critical societal domains, we also initiate a reading list that\ntracks the latest advancements under this topic, which will be continually\nupdated: \\url{https://github.com/czyssrs/LLM_X_papers}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the fast-evolving domain of artificial intelligence, large language models\n(LLMs) such as GPT-3 and GPT-4 are revolutionizing the landscapes of finance,\nhealthcare, and law: domains characterized by their reliance on professional\nexpertise, challenging data acquisition, high-stakes, and stringent regulatory\ncompliance. This survey offers a detailed exploration of the methodologies,\napplications, challenges, and forward-looking opportunities of LLMs within\nthese high-stakes sectors. We highlight the instrumental role of LLMs in\nenhancing diagnostic and treatment methodologies in healthcare, innovating\nfinancial analytics, and refining legal interpretation and compliance\nstrategies. Moreover, we critically examine the ethics for LLM applications in\nthese fields, pointing out the existing ethical concerns and the need for\ntransparent, fair, and robust AI systems that respect regulatory norms. By\npresenting a thorough review of current literature and practical applications,\nwe showcase the transformative impact of LLMs, and outline the imperative for\ninterdisciplinary cooperation, methodological advancements, and ethical\nvigilance. Through this lens, we aim to spark dialogue and inspire future\nresearch dedicated to maximizing the benefits of LLMs while mitigating their\nrisks in these precision-dependent sectors. To facilitate future research on\nLLMs in these critical societal domains, we also initiate a reading list that\ntracks the latest advancements under this topic, which will be continually\nupdated: \\url{https://github.com/czyssrs/LLM_X_papers}."
                },
                "authors": [
                    {
                        "name": "Zhiyu Zoey Chen"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Xinlu Zhang"
                    },
                    {
                        "name": "Nan Hao"
                    },
                    {
                        "name": "An Yan"
                    },
                    {
                        "name": "Armineh Nourbakhsh"
                    },
                    {
                        "name": "Xianjun Yang"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Linda Petzold"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "TMLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.05480v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.05480v4",
                "updated": "2024-11-21T22:08:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    22,
                    8,
                    3,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-08T18:04:13Z",
                "published_parsed": [
                    2023,
                    6,
                    8,
                    18,
                    4,
                    13,
                    3,
                    159,
                    0
                ],
                "title": "Artificial General Intelligence for Medical Imaging Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial General Intelligence for Medical Imaging Analysis"
                },
                "summary": "Large-scale Artificial General Intelligence (AGI) models, including Large\nLanguage Models (LLMs) such as ChatGPT/GPT-4, have achieved unprecedented\nsuccess in a variety of general domain tasks. Yet, when applied directly to\nspecialized domains like medical imaging, which require in-depth expertise,\nthese models face notable challenges arising from the medical field's inherent\ncomplexities and unique characteristics. In this review, we delve into the\npotential applications of AGI models in medical imaging and healthcare, with a\nprimary focus on LLMs, Large Vision Models, and Large Multimodal Models. We\nprovide a thorough overview of the key features and enabling techniques of LLMs\nand AGI, and further examine the roadmaps guiding the evolution and\nimplementation of AGI models in the medical sector, summarizing their present\napplications, potentialities, and associated challenges. In addition, we\nhighlight potential future research directions, offering a holistic view on\nupcoming ventures. This comprehensive review aims to offer insights into the\nfuture implications of AGI in medical imaging, healthcare, and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Artificial General Intelligence (AGI) models, including Large\nLanguage Models (LLMs) such as ChatGPT/GPT-4, have achieved unprecedented\nsuccess in a variety of general domain tasks. Yet, when applied directly to\nspecialized domains like medical imaging, which require in-depth expertise,\nthese models face notable challenges arising from the medical field's inherent\ncomplexities and unique characteristics. In this review, we delve into the\npotential applications of AGI models in medical imaging and healthcare, with a\nprimary focus on LLMs, Large Vision Models, and Large Multimodal Models. We\nprovide a thorough overview of the key features and enabling techniques of LLMs\nand AGI, and further examine the roadmaps guiding the evolution and\nimplementation of AGI models in the medical sector, summarizing their present\napplications, potentialities, and associated challenges. In addition, we\nhighlight potential future research directions, offering a holistic view on\nupcoming ventures. This comprehensive review aims to offer insights into the\nfuture implications of AGI in medical imaging, healthcare, and beyond."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Lin Zhao"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Hanqi Jiang"
                    },
                    {
                        "name": "Chao Cao"
                    },
                    {
                        "name": "Shaochen Xu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Haixing Dai"
                    },
                    {
                        "name": "Yixuan Yuan"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Dajiang Zhu"
                    },
                    {
                        "name": "Pingkun Yan"
                    },
                    {
                        "name": "Quanzheng Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Dinggang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Dinggang Shen"
                },
                "author": "Dinggang Shen",
                "arxiv_doi": "10.1109/RBME.2024.3493775",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/RBME.2024.3493775",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2306.05480v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.05480v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06058v2",
                "updated": "2024-11-21T21:39:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    21,
                    39,
                    4,
                    3,
                    326,
                    0
                ],
                "published": "2024-05-09T19:02:53Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    19,
                    2,
                    53,
                    3,
                    130,
                    0
                ],
                "title": "Large Language Models Show Human-like Social Desirability Biases in\n  Survey Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Show Human-like Social Desirability Biases in\n  Survey Responses"
                },
                "summary": "As Large Language Models (LLMs) become widely used to model and simulate\nhuman behavior, understanding their biases becomes critical. We developed an\nexperimental framework using Big Five personality surveys and uncovered a\npreviously undetected social desirability bias in a wide range of LLMs. By\nsystematically varying the number of questions LLMs were exposed to, we\ndemonstrate their ability to infer when they are being evaluated. When\npersonality evaluation is inferred, LLMs skew their scores towards the\ndesirable ends of trait dimensions (i.e., increased extraversion, decreased\nneuroticism, etc). This bias exists in all tested models, including GPT-4/3.5,\nClaude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent\nmodels, with GPT-4's survey responses changing by 1.20 (human) standard\ndeviations and Llama 3's by 0.98 standard deviations-very large effects. This\nbias is robust to randomization of question order and paraphrasing.\nReverse-coding all the questions decreases bias levels but does not eliminate\nthem, suggesting that this effect cannot be attributed to acquiescence bias.\nOur findings reveal an emergent social desirability bias and suggest\nconstraints on profiling LLMs with psychometric tests and on using LLMs as\nproxies for human participants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become widely used to model and simulate\nhuman behavior, understanding their biases becomes critical. We developed an\nexperimental framework using Big Five personality surveys and uncovered a\npreviously undetected social desirability bias in a wide range of LLMs. By\nsystematically varying the number of questions LLMs were exposed to, we\ndemonstrate their ability to infer when they are being evaluated. When\npersonality evaluation is inferred, LLMs skew their scores towards the\ndesirable ends of trait dimensions (i.e., increased extraversion, decreased\nneuroticism, etc). This bias exists in all tested models, including GPT-4/3.5,\nClaude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent\nmodels, with GPT-4's survey responses changing by 1.20 (human) standard\ndeviations and Llama 3's by 0.98 standard deviations-very large effects. This\nbias is robust to randomization of question order and paraphrasing.\nReverse-coding all the questions decreases bias levels but does not eliminate\nthem, suggesting that this effect cannot be attributed to acquiescence bias.\nOur findings reveal an emergent social desirability bias and suggest\nconstraints on profiling LLMs with psychometric tests and on using LLMs as\nproxies for human participants."
                },
                "authors": [
                    {
                        "name": "Aadesh Salecha"
                    },
                    {
                        "name": "Molly E. Ireland"
                    },
                    {
                        "name": "Shashanka Subrahmanya"
                    },
                    {
                        "name": "João Sedoc"
                    },
                    {
                        "name": "Lyle H. Ungar"
                    },
                    {
                        "name": "Johannes C. Eichstaedt"
                    }
                ],
                "author_detail": {
                    "name": "Johannes C. Eichstaedt"
                },
                "author": "Johannes C. Eichstaedt",
                "arxiv_comment": "3 pages, 2 figures, accepted at PNAS Nexus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14594v1",
                "updated": "2024-11-21T21:27:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    21,
                    27,
                    30,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T21:27:30Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    21,
                    27,
                    30,
                    3,
                    326,
                    0
                ],
                "title": "Solving Zero-Shot 3D Visual Grounding as Constraint Satisfaction\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving Zero-Shot 3D Visual Grounding as Constraint Satisfaction\n  Problems"
                },
                "summary": "3D visual grounding (3DVG) aims to locate objects in a 3D scene with natural\nlanguage descriptions. Supervised methods have achieved decent accuracy, but\nhave a closed vocabulary and limited language understanding ability. Zero-shot\nmethods mostly utilize large language models (LLMs) to handle natural language\ndescriptions, yet suffer from slow inference speed. To address these problems,\nin this work, we propose a zero-shot method that reformulates the 3DVG task as\na Constraint Satisfaction Problem (CSP), where the variables and constraints\nrepresent objects and their spatial relations, respectively. This allows a\nglobal reasoning of all relevant objects, producing grounding results of both\nthe target and anchor objects. Moreover, we demonstrate the flexibility of our\nframework by handling negation- and counting-based queries with only minor\nextra coding efforts. Our system, Constraint Satisfaction Visual Grounding\n(CSVG), has been extensively evaluated on the public datasets ScanRefer and\nNr3D datasets using only open-source LLMs. Results show the effectiveness of\nCSVG and superior grounding accuracy over current state-of-the-art zero-shot\n3DVG methods with improvements of $+7.0\\%$ (Acc@0.5 score) and $+11.2\\%$ on the\nScanRefer and Nr3D datasets, respectively. The code of our system is publicly\navailable at https://github.com/sunsleaf/CSVG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D visual grounding (3DVG) aims to locate objects in a 3D scene with natural\nlanguage descriptions. Supervised methods have achieved decent accuracy, but\nhave a closed vocabulary and limited language understanding ability. Zero-shot\nmethods mostly utilize large language models (LLMs) to handle natural language\ndescriptions, yet suffer from slow inference speed. To address these problems,\nin this work, we propose a zero-shot method that reformulates the 3DVG task as\na Constraint Satisfaction Problem (CSP), where the variables and constraints\nrepresent objects and their spatial relations, respectively. This allows a\nglobal reasoning of all relevant objects, producing grounding results of both\nthe target and anchor objects. Moreover, we demonstrate the flexibility of our\nframework by handling negation- and counting-based queries with only minor\nextra coding efforts. Our system, Constraint Satisfaction Visual Grounding\n(CSVG), has been extensively evaluated on the public datasets ScanRefer and\nNr3D datasets using only open-source LLMs. Results show the effectiveness of\nCSVG and superior grounding accuracy over current state-of-the-art zero-shot\n3DVG methods with improvements of $+7.0\\%$ (Acc@0.5 score) and $+11.2\\%$ on the\nScanRefer and Nr3D datasets, respectively. The code of our system is publicly\navailable at https://github.com/sunsleaf/CSVG."
                },
                "authors": [
                    {
                        "name": "Qihao Yuan"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Kailai Li"
                    },
                    {
                        "name": "Rainer Stiefelhagen"
                    }
                ],
                "author_detail": {
                    "name": "Rainer Stiefelhagen"
                },
                "author": "Rainer Stiefelhagen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14592v1",
                "updated": "2024-11-21T21:22:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    21,
                    22,
                    58,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T21:22:58Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    21,
                    22,
                    58,
                    3,
                    326,
                    0
                ],
                "title": "G-RAG: Knowledge Expansion in Material Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-RAG: Knowledge Expansion in Material Science"
                },
                "summary": "In the field of Material Science, effective information retrieval systems are\nessential for facilitating research. Traditional Retrieval-Augmented Generation\n(RAG) approaches in Large Language Models (LLMs) often encounter challenges\nsuch as outdated information, hallucinations, limited interpretability due to\ncontext constraints, and inaccurate retrieval. To address these issues, Graph\nRAG integrates graph databases to enhance the retrieval process. Our proposed\nmethod processes Material Science documents by extracting key entities\n(referred to as MatIDs) from sentences, which are then utilized to query\nexternal Wikipedia knowledge bases (KBs) for additional relevant information.\nWe implement an agent-based parsing technique to achieve a more detailed\nrepresentation of the documents. Our improved version of Graph RAG called G-RAG\nfurther leverages a graph database to capture relationships between these\nentities, improving both retrieval accuracy and contextual understanding. This\nenhanced approach demonstrates significant improvements in performance for\ndomains that require precise information retrieval, such as Material Science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of Material Science, effective information retrieval systems are\nessential for facilitating research. Traditional Retrieval-Augmented Generation\n(RAG) approaches in Large Language Models (LLMs) often encounter challenges\nsuch as outdated information, hallucinations, limited interpretability due to\ncontext constraints, and inaccurate retrieval. To address these issues, Graph\nRAG integrates graph databases to enhance the retrieval process. Our proposed\nmethod processes Material Science documents by extracting key entities\n(referred to as MatIDs) from sentences, which are then utilized to query\nexternal Wikipedia knowledge bases (KBs) for additional relevant information.\nWe implement an agent-based parsing technique to achieve a more detailed\nrepresentation of the documents. Our improved version of Graph RAG called G-RAG\nfurther leverages a graph database to capture relationships between these\nentities, improving both retrieval accuracy and contextual understanding. This\nenhanced approach demonstrates significant improvements in performance for\ndomains that require precise information retrieval, such as Material Science."
                },
                "authors": [
                    {
                        "name": "Radeen Mostafa"
                    },
                    {
                        "name": "Mirza Nihal Baig"
                    },
                    {
                        "name": "Mashaekh Tausif Ehsan"
                    },
                    {
                        "name": "Jakir Hasan"
                    }
                ],
                "author_detail": {
                    "name": "Jakir Hasan"
                },
                "author": "Jakir Hasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15850v2",
                "updated": "2024-11-21T20:54:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    54,
                    35,
                    3,
                    326,
                    0
                ],
                "published": "2024-07-22T17:59:56Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    17,
                    59,
                    56,
                    0,
                    204,
                    0
                ],
                "title": "AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description"
                },
                "summary": "Our objective is to generate Audio Descriptions (ADs) for both movies and TV\nseries in a training-free manner. We use the power of off-the-shelf\nVisual-Language Models (VLMs) and Large Language Models (LLMs), and develop\nvisual and text prompting strategies for this task. Our contributions are\nthree-fold: (i) We demonstrate that a VLM can successfully name and refer to\ncharacters if directly prompted with character information through visual\nindications without requiring any fine-tuning; (ii) A two-stage process is\ndeveloped to generate ADs, with the first stage asking the VLM to\ncomprehensively describe the video, followed by a second stage utilising a LLM\nto summarise dense textual information into one succinct AD sentence; (iii) A\nnew dataset for TV audio description is formulated. Our approach, named\nAutoAD-Zero, demonstrates outstanding performance (even competitive with some\nmodels fine-tuned on ground truth ADs) in AD generation for both movies and TV\nseries, achieving state-of-the-art CRITIC scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our objective is to generate Audio Descriptions (ADs) for both movies and TV\nseries in a training-free manner. We use the power of off-the-shelf\nVisual-Language Models (VLMs) and Large Language Models (LLMs), and develop\nvisual and text prompting strategies for this task. Our contributions are\nthree-fold: (i) We demonstrate that a VLM can successfully name and refer to\ncharacters if directly prompted with character information through visual\nindications without requiring any fine-tuning; (ii) A two-stage process is\ndeveloped to generate ADs, with the first stage asking the VLM to\ncomprehensively describe the video, followed by a second stage utilising a LLM\nto summarise dense textual information into one succinct AD sentence; (iii) A\nnew dataset for TV audio description is formulated. Our approach, named\nAutoAD-Zero, demonstrates outstanding performance (even competitive with some\nmodels fine-tuned on ground truth ADs) in AD generation for both movies and TV\nseries, achieving state-of-the-art CRITIC scores."
                },
                "authors": [
                    {
                        "name": "Junyu Xie"
                    },
                    {
                        "name": "Tengda Han"
                    },
                    {
                        "name": "Max Bain"
                    },
                    {
                        "name": "Arsha Nagrani"
                    },
                    {
                        "name": "Gül Varol"
                    },
                    {
                        "name": "Weidi Xie"
                    },
                    {
                        "name": "Andrew Zisserman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zisserman"
                },
                "author": "Andrew Zisserman",
                "arxiv_comment": "Project Page: https://www.robots.ox.ac.uk/~vgg/research/autoad-zero/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14585v1",
                "updated": "2024-11-21T20:48:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    48,
                    40,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T20:48:40Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    48,
                    40,
                    3,
                    326,
                    0
                ],
                "title": "Efficient Spatio-Temporal Signal Recognition on Edge Devices Using\n  PointLCA-Net",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Spatio-Temporal Signal Recognition on Edge Devices Using\n  PointLCA-Net"
                },
                "summary": "Recent advancements in machine learning, particularly through deep learning\narchitectures like PointNet, have transformed the processing of\nthree-dimensional (3D) point clouds, significantly improving 3D object\nclassification and segmentation tasks. While 3D point clouds provide detailed\nspatial information, spatio-temporal signals introduce a dynamic element that\naccounts for changes over time. However, applying deep learning techniques to\nspatio-temporal signals and deploying them on edge devices presents challenges,\nincluding real-time processing, memory capacity, and power consumption. To\naddress these issues, this paper presents a novel approach that combines\nPointNet's feature extraction with the in-memory computing capabilities and\nenergy efficiency of neuromorphic systems for spatio-temporal signal\nrecognition. The proposed method consists of a two-stage process: in the first\nstage, PointNet extracts features from the spatio-temporal signals, which are\nthen stored in non-volatile memristor crossbar arrays. In the second stage,\nthese features are processed by a single-layer spiking neural encoder-decoder\nthat employs the Locally Competitive Algorithm (LCA) for efficient encoding and\nclassification. This work integrates the strengths of both PointNet and LCA,\nenhancing computational efficiency and energy performance on edge devices.\nPointLCA-Net achieves high recognition accuracy for spatio-temporal data with\nsubstantially lower energy burden during both inference and training than\ncomparable approaches, thus advancing the deployment of advanced neural\narchitectures in energy-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in machine learning, particularly through deep learning\narchitectures like PointNet, have transformed the processing of\nthree-dimensional (3D) point clouds, significantly improving 3D object\nclassification and segmentation tasks. While 3D point clouds provide detailed\nspatial information, spatio-temporal signals introduce a dynamic element that\naccounts for changes over time. However, applying deep learning techniques to\nspatio-temporal signals and deploying them on edge devices presents challenges,\nincluding real-time processing, memory capacity, and power consumption. To\naddress these issues, this paper presents a novel approach that combines\nPointNet's feature extraction with the in-memory computing capabilities and\nenergy efficiency of neuromorphic systems for spatio-temporal signal\nrecognition. The proposed method consists of a two-stage process: in the first\nstage, PointNet extracts features from the spatio-temporal signals, which are\nthen stored in non-volatile memristor crossbar arrays. In the second stage,\nthese features are processed by a single-layer spiking neural encoder-decoder\nthat employs the Locally Competitive Algorithm (LCA) for efficient encoding and\nclassification. This work integrates the strengths of both PointNet and LCA,\nenhancing computational efficiency and energy performance on edge devices.\nPointLCA-Net achieves high recognition accuracy for spatio-temporal data with\nsubstantially lower energy burden during both inference and training than\ncomparable approaches, thus advancing the deployment of advanced neural\narchitectures in energy-constrained environments."
                },
                "authors": [
                    {
                        "name": "Sanaz Mahmoodi Takaghaj"
                    },
                    {
                        "name": "Jack Sampson"
                    }
                ],
                "author_detail": {
                    "name": "Jack Sampson"
                },
                "author": "Jack Sampson",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2411.00140",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14574v1",
                "updated": "2024-11-21T20:41:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    41,
                    55,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T20:41:55Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    41,
                    55,
                    3,
                    326,
                    0
                ],
                "title": "SRSA: A Cost-Efficient Strategy-Router Search Agent for Real-world\n  Human-Machine Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRSA: A Cost-Efficient Strategy-Router Search Agent for Real-world\n  Human-Machine Interactions"
                },
                "summary": "Recently, as Large Language Models (LLMs) have shown impressive emerging\ncapabilities and gained widespread popularity, research on LLM-based search\nagents has proliferated. In real-world situations, users often input contextual\nand highly personalized queries to chatbots, challenging LLMs to capture\ncontext and generate appropriate answers. However, much of the prior research\nhas not focused specifically on authentic human-machine dialogue scenarios. It\nalso ignores the important balance between response quality and computational\ncost by forcing all queries to follow the same agent process. To address these\ngaps, we propose a Strategy-Router Search Agent (SRSA), routing different\nqueries to appropriate search strategies and enabling fine-grained serial\nsearches to obtain high-quality results at a relatively low cost. To evaluate\nour work, we introduce a new dataset, Contextual Query Enhancement Dataset\n(CQED), comprising contextual queries to simulate authentic and daily\ninteractions between humans and chatbots. Using LLM-based automatic evaluation\nmetrics, we assessed SRSA's performance in terms of informativeness,\ncompleteness, novelty, and actionability. To conclude, SRSA provides an\napproach that resolves the issue of simple serial searches leading to\ndegenerate answers for lengthy and contextual queries, effectively and\nefficiently parses complex user queries, and generates more comprehensive and\ninformative responses without fine-tuning an LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, as Large Language Models (LLMs) have shown impressive emerging\ncapabilities and gained widespread popularity, research on LLM-based search\nagents has proliferated. In real-world situations, users often input contextual\nand highly personalized queries to chatbots, challenging LLMs to capture\ncontext and generate appropriate answers. However, much of the prior research\nhas not focused specifically on authentic human-machine dialogue scenarios. It\nalso ignores the important balance between response quality and computational\ncost by forcing all queries to follow the same agent process. To address these\ngaps, we propose a Strategy-Router Search Agent (SRSA), routing different\nqueries to appropriate search strategies and enabling fine-grained serial\nsearches to obtain high-quality results at a relatively low cost. To evaluate\nour work, we introduce a new dataset, Contextual Query Enhancement Dataset\n(CQED), comprising contextual queries to simulate authentic and daily\ninteractions between humans and chatbots. Using LLM-based automatic evaluation\nmetrics, we assessed SRSA's performance in terms of informativeness,\ncompleteness, novelty, and actionability. To conclude, SRSA provides an\napproach that resolves the issue of simple serial searches leading to\ndegenerate answers for lengthy and contextual queries, effectively and\nefficiently parses complex user queries, and generates more comprehensive and\ninformative responses without fine-tuning an LLM."
                },
                "authors": [
                    {
                        "name": "Yaqi Wang"
                    },
                    {
                        "name": "Haipei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Haipei Xu"
                },
                "author": "Haipei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19311v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19311v5",
                "updated": "2024-11-21T20:40:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    40,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-05-29T17:35:55Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    17,
                    35,
                    55,
                    2,
                    150,
                    0
                ],
                "title": "The Effect of Tornadic Supercell Thunderstorms on the Atmospheric Muon\n  Flux",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Effect of Tornadic Supercell Thunderstorms on the Atmospheric Muon\n  Flux"
                },
                "summary": "Tornadoes are severe weather phenomena characterized by a violently rotating\ncolumn of air connecting the ground to a parent storm. Within the United\nStates, hundreds of tornadoes occur every year. Despite this, the dynamics of\ntornado formation and propagation are not particularly well understood, in part\ndue to the challenge of instrumentation: many existing instruments for\nmeasuring atmospheric properties are in-situ detectors, making deployment in or\nnear an active or developing tornado difficult. Here, we combine local\natmospheric and cosmic ray air shower simulation to explore the potential for\nremote measurement of the pressure field within tornado-producing supercell\nthunderstorms by examining directional variations of the atmospheric muon flux.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tornadoes are severe weather phenomena characterized by a violently rotating\ncolumn of air connecting the ground to a parent storm. Within the United\nStates, hundreds of tornadoes occur every year. Despite this, the dynamics of\ntornado formation and propagation are not particularly well understood, in part\ndue to the challenge of instrumentation: many existing instruments for\nmeasuring atmospheric properties are in-situ detectors, making deployment in or\nnear an active or developing tornado difficult. Here, we combine local\natmospheric and cosmic ray air shower simulation to explore the potential for\nremote measurement of the pressure field within tornado-producing supercell\nthunderstorms by examining directional variations of the atmospheric muon flux."
                },
                "authors": [
                    {
                        "name": "William Luszczak"
                    },
                    {
                        "name": "Leigh Orf"
                    }
                ],
                "author_detail": {
                    "name": "Leigh Orf"
                },
                "author": "Leigh Orf",
                "arxiv_comment": "Accepted to PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19311v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19311v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14572v1",
                "updated": "2024-11-21T20:39:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    39,
                    13,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T20:39:13Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    39,
                    13,
                    3,
                    326,
                    0
                ],
                "title": "Towards Knowledge Checking in Retrieval-augmented Generation: A\n  Representation Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Knowledge Checking in Retrieval-augmented Generation: A\n  Representation Perspective"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems have shown promise in enhancing\nthe performance of Large Language Models (LLMs). However, these systems face\nchallenges in effectively integrating external knowledge with the LLM's\ninternal knowledge, often leading to issues with misleading or unhelpful\ninformation. This work aims to provide a systematic study on knowledge checking\nin RAG systems. We conduct a comprehensive analysis of LLM representation\nbehaviors and demonstrate the significance of using representations in\nknowledge checking. Motivated by the findings, we further develop\nrepresentation-based classifiers for knowledge filtering. We show substantial\nimprovements in RAG performance, even when dealing with noisy knowledge\ndatabases. Our study provides new insights into leveraging LLM representations\nfor enhancing the reliability and effectiveness of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems have shown promise in enhancing\nthe performance of Large Language Models (LLMs). However, these systems face\nchallenges in effectively integrating external knowledge with the LLM's\ninternal knowledge, often leading to issues with misleading or unhelpful\ninformation. This work aims to provide a systematic study on knowledge checking\nin RAG systems. We conduct a comprehensive analysis of LLM representation\nbehaviors and demonstrate the significance of using representations in\nknowledge checking. Motivated by the findings, we further develop\nrepresentation-based classifiers for knowledge filtering. We show substantial\nimprovements in RAG performance, even when dealing with noisy knowledge\ndatabases. Our study provides new insights into leveraging LLM representations\nfor enhancing the reliability and effectiveness of RAG systems."
                },
                "authors": [
                    {
                        "name": "Shenglai Zeng"
                    },
                    {
                        "name": "Jiankun Zhang"
                    },
                    {
                        "name": "Bingheng Li"
                    },
                    {
                        "name": "Yuping Lin"
                    },
                    {
                        "name": "Tianqi Zheng"
                    },
                    {
                        "name": "Dante Everaert"
                    },
                    {
                        "name": "Hanqing Lu"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Yue Xing"
                    },
                    {
                        "name": "Monica Xiao Cheng"
                    },
                    {
                        "name": "Jiliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Tang"
                },
                "author": "Jiliang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14571v1",
                "updated": "2024-11-21T20:36:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    36,
                    36,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T20:36:36Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    36,
                    36,
                    3,
                    326,
                    0
                ],
                "title": "Assessment of LLM Responses to End-user Security Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessment of LLM Responses to End-user Security Questions"
                },
                "summary": "Answering end user security questions is challenging. While large language\nmodels (LLMs) like GPT, LLAMA, and Gemini are far from error-free, they have\nshown promise in answering a variety of questions outside of security. We\nstudied LLM performance in the area of end user security by qualitatively\nevaluating 3 popular LLMs on 900 systematically collected end user security\nquestions.\n  While LLMs demonstrate broad generalist ``knowledge'' of end user security\ninformation, there are patterns of errors and limitations across LLMs\nconsisting of stale and inaccurate answers, and indirect or unresponsive\ncommunication styles, all of which impacts the quality of information received.\nBased on these patterns, we suggest directions for model improvement and\nrecommend user strategies for interacting with LLMs when seeking assistance\nwith security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering end user security questions is challenging. While large language\nmodels (LLMs) like GPT, LLAMA, and Gemini are far from error-free, they have\nshown promise in answering a variety of questions outside of security. We\nstudied LLM performance in the area of end user security by qualitatively\nevaluating 3 popular LLMs on 900 systematically collected end user security\nquestions.\n  While LLMs demonstrate broad generalist ``knowledge'' of end user security\ninformation, there are patterns of errors and limitations across LLMs\nconsisting of stale and inaccurate answers, and indirect or unresponsive\ncommunication styles, all of which impacts the quality of information received.\nBased on these patterns, we suggest directions for model improvement and\nrecommend user strategies for interacting with LLMs when seeking assistance\nwith security."
                },
                "authors": [
                    {
                        "name": "Vijay Prakash"
                    },
                    {
                        "name": "Kevin Lee"
                    },
                    {
                        "name": "Arkaprabha Bhattacharya"
                    },
                    {
                        "name": "Danny Yuxing Huang"
                    },
                    {
                        "name": "Jessica Staddon"
                    }
                ],
                "author_detail": {
                    "name": "Jessica Staddon"
                },
                "author": "Jessica Staddon",
                "arxiv_comment": "18 pages, 1 figure, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14569v1",
                "updated": "2024-11-21T20:31:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    31,
                    48,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T20:31:48Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    31,
                    48,
                    3,
                    326,
                    0
                ],
                "title": "Variable Extraction for Model Recovery in Scientific Literature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable Extraction for Model Recovery in Scientific Literature"
                },
                "summary": "The global output of academic publications exceeds 5 million articles per\nyear, making it difficult for humans to keep up with even a tiny fraction of\nscientific output. We need methods to navigate and interpret the artifacts --\ntexts, graphs, charts, code, models, and datasets -- that make up the\nliterature. This paper evaluates various methods for extracting mathematical\nmodel variables from epidemiological studies, such as ``infection rate\n($\\alpha$),'' ``recovery rate ($\\gamma$),'' and ``mortality rate ($\\mu$).''\nVariable extraction appears to be a basic task, but plays a pivotal role in\nrecovering models from scientific literature. Once extracted, we can use these\nvariables for automatic mathematical modeling, simulation, and replication of\npublished results.\n  We introduce a benchmark dataset comprising manually-annotated variable\ndescriptions and variable values extracted from scientific papers. Based on\nthis dataset, we present several baseline methods for variable extraction based\non Large Language Models (LLMs) and rule-based information extraction systems.\nOur analysis shows that LLM-based solutions perform the best. Despite the\nincremental benefits of combining rule-based extraction outputs with LLMs, the\nleap in performance attributed to the transfer-learning and instruction-tuning\ncapabilities of LLMs themselves is far more significant. This investigation\ndemonstrates the potential of LLMs to enhance automatic comprehension of\nscientific artifacts and for automatic model recovery and simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The global output of academic publications exceeds 5 million articles per\nyear, making it difficult for humans to keep up with even a tiny fraction of\nscientific output. We need methods to navigate and interpret the artifacts --\ntexts, graphs, charts, code, models, and datasets -- that make up the\nliterature. This paper evaluates various methods for extracting mathematical\nmodel variables from epidemiological studies, such as ``infection rate\n($\\alpha$),'' ``recovery rate ($\\gamma$),'' and ``mortality rate ($\\mu$).''\nVariable extraction appears to be a basic task, but plays a pivotal role in\nrecovering models from scientific literature. Once extracted, we can use these\nvariables for automatic mathematical modeling, simulation, and replication of\npublished results.\n  We introduce a benchmark dataset comprising manually-annotated variable\ndescriptions and variable values extracted from scientific papers. Based on\nthis dataset, we present several baseline methods for variable extraction based\non Large Language Models (LLMs) and rule-based information extraction systems.\nOur analysis shows that LLM-based solutions perform the best. Despite the\nincremental benefits of combining rule-based extraction outputs with LLMs, the\nleap in performance attributed to the transfer-learning and instruction-tuning\ncapabilities of LLMs themselves is far more significant. This investigation\ndemonstrates the potential of LLMs to enhance automatic comprehension of\nscientific artifacts and for automatic model recovery and simulation."
                },
                "authors": [
                    {
                        "name": "Chunwei Liu"
                    },
                    {
                        "name": "Enrique Noriega-Atala"
                    },
                    {
                        "name": "Adarsh Pyarelal"
                    },
                    {
                        "name": "Clayton T Morrison"
                    },
                    {
                        "name": "Mike Cafarella"
                    }
                ],
                "author_detail": {
                    "name": "Mike Cafarella"
                },
                "author": "Mike Cafarella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14568v1",
                "updated": "2024-11-21T20:31:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    31,
                    45,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T20:31:45Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    31,
                    45,
                    3,
                    326,
                    0
                ],
                "title": "Maximum Solar Energy Tracking Leverage High-DoF Robotics System with\n  Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maximum Solar Energy Tracking Leverage High-DoF Robotics System with\n  Deep Reinforcement Learning"
                },
                "summary": "Solar trajectory monitoring is a pivotal challenge in solar energy systems,\nunderpinning applications such as autonomous energy harvesting and\nenvironmental sensing. A prevalent failure mode in sustained solar tracking\narises when the predictive algorithm erroneously diverges from the solar locus,\nerroneously anchoring to extraneous celestial or terrestrial features. This\nphenomenon is attributable to an inadequate assimilation of solar-specific\nobjectness attributes within the tracking paradigm. To mitigate this deficiency\ninherent in extant methodologies, we introduce an innovative objectness\nregularization framework that compels tracking points to remain confined within\nthe delineated boundaries of the solar entity. By encapsulating solar\nobjectness indicators during the training phase, our approach obviates the\nnecessity for explicit solar mask computation during operational deployment.\nFurthermore, we leverage the high-DoF robot arm to integrate our method to\nimprove its robustness and flexibility in different outdoor environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solar trajectory monitoring is a pivotal challenge in solar energy systems,\nunderpinning applications such as autonomous energy harvesting and\nenvironmental sensing. A prevalent failure mode in sustained solar tracking\narises when the predictive algorithm erroneously diverges from the solar locus,\nerroneously anchoring to extraneous celestial or terrestrial features. This\nphenomenon is attributable to an inadequate assimilation of solar-specific\nobjectness attributes within the tracking paradigm. To mitigate this deficiency\ninherent in extant methodologies, we introduce an innovative objectness\nregularization framework that compels tracking points to remain confined within\nthe delineated boundaries of the solar entity. By encapsulating solar\nobjectness indicators during the training phase, our approach obviates the\nnecessity for explicit solar mask computation during operational deployment.\nFurthermore, we leverage the high-DoF robot arm to integrate our method to\nimprove its robustness and flexibility in different outdoor environments."
                },
                "authors": [
                    {
                        "name": "Anjie Jiang"
                    },
                    {
                        "name": "Kangtong Mo"
                    },
                    {
                        "name": "Satoshi Fujimoto"
                    },
                    {
                        "name": "Michael Taylor"
                    },
                    {
                        "name": "Sanjay Kumar"
                    },
                    {
                        "name": "Chiotis Dimitrios"
                    },
                    {
                        "name": "Emilia Ruiz"
                    }
                ],
                "author_detail": {
                    "name": "Emilia Ruiz"
                },
                "author": "Emilia Ruiz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14565v1",
                "updated": "2024-11-21T20:29:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    29,
                    59,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T20:29:59Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    29,
                    59,
                    3,
                    326,
                    0
                ],
                "title": "Privacy-Preserving Video Anomaly Detection: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Video Anomaly Detection: A Survey"
                },
                "summary": "Video Anomaly Detection (VAD) aims to automatically analyze spatiotemporal\npatterns in surveillance videos collected from open spaces to detect anomalous\nevents that may cause harm without physical contact. However, vision-based\nsurveillance systems such as closed-circuit television often capture personally\nidentifiable information. The lack of transparency and interpretability in\nvideo transmission and usage raises public concerns about privacy and ethics,\nlimiting the real-world application of VAD. Recently, researchers have focused\non privacy concerns in VAD by conducting systematic studies from various\nperspectives including data, features, and systems, making Privacy-Preserving\nVideo Anomaly Detection (P2VAD) a hotspot in the AI community. However, current\nresearch in P2VAD is fragmented, and prior reviews have mostly focused on\nmethods using RGB sequences, overlooking privacy leakage and appearance bias\nconsiderations. To address this gap, this article systematically reviews the\nprogress of P2VAD for the first time, defining its scope and providing an\nintuitive taxonomy. We outline the basic assumptions, learning frameworks, and\noptimization objectives of various approaches, analyzing their strengths,\nweaknesses, and potential correlations. Additionally, we provide open access to\nresearch resources such as benchmark datasets and available code. Finally, we\ndiscuss key challenges and future opportunities from the perspectives of AI\ndevelopment and P2VAD deployment, aiming to guide future work in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Anomaly Detection (VAD) aims to automatically analyze spatiotemporal\npatterns in surveillance videos collected from open spaces to detect anomalous\nevents that may cause harm without physical contact. However, vision-based\nsurveillance systems such as closed-circuit television often capture personally\nidentifiable information. The lack of transparency and interpretability in\nvideo transmission and usage raises public concerns about privacy and ethics,\nlimiting the real-world application of VAD. Recently, researchers have focused\non privacy concerns in VAD by conducting systematic studies from various\nperspectives including data, features, and systems, making Privacy-Preserving\nVideo Anomaly Detection (P2VAD) a hotspot in the AI community. However, current\nresearch in P2VAD is fragmented, and prior reviews have mostly focused on\nmethods using RGB sequences, overlooking privacy leakage and appearance bias\nconsiderations. To address this gap, this article systematically reviews the\nprogress of P2VAD for the first time, defining its scope and providing an\nintuitive taxonomy. We outline the basic assumptions, learning frameworks, and\noptimization objectives of various approaches, analyzing their strengths,\nweaknesses, and potential correlations. Additionally, we provide open access to\nresearch resources such as benchmark datasets and available code. Finally, we\ndiscuss key challenges and future opportunities from the perspectives of AI\ndevelopment and P2VAD deployment, aiming to guide future work in the field."
                },
                "authors": [
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xiaoguang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoguang Zhu"
                },
                "author": "Xiaoguang Zhu",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04282v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04282v2",
                "updated": "2024-11-21T20:29:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    20,
                    29,
                    9,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-06T22:02:30Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    22,
                    2,
                    30,
                    2,
                    311,
                    0
                ],
                "title": "Language Models are Hidden Reasoners: Unlocking Latent Reasoning\n  Capabilities via Self-Rewarding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models are Hidden Reasoners: Unlocking Latent Reasoning\n  Capabilities via Self-Rewarding"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities, but still\nstruggle with complex reasoning tasks requiring multiple steps. While\nprompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at\ninference time, optimizing reasoning capabilities during training remains\nchallenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled\nframework that formulates reasoning as sampling from a latent distribution and\noptimizes it via variational approaches. LaTRO enables LLMs to concurrently\nimprove both their reasoning process and ability to evaluate reasoning quality,\nwithout requiring external feedback or reward models. We validate LaTRO through\nexperiments on GSM8K and ARC-Challenge datasets using multiple model\narchitectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of\n12.5% over base models and 9.6% over supervised fine-tuning across\nPhi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that\npre-trained LLMs possess latent reasoning capabilities that can be unlocked and\nenhanced through our proposed optimization approach in a self-improvement\nmanner. The code of LaTRO is available at\n\\url{https://github.com/SalesforceAIResearch/LaTRO}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities, but still\nstruggle with complex reasoning tasks requiring multiple steps. While\nprompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at\ninference time, optimizing reasoning capabilities during training remains\nchallenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled\nframework that formulates reasoning as sampling from a latent distribution and\noptimizes it via variational approaches. LaTRO enables LLMs to concurrently\nimprove both their reasoning process and ability to evaluate reasoning quality,\nwithout requiring external feedback or reward models. We validate LaTRO through\nexperiments on GSM8K and ARC-Challenge datasets using multiple model\narchitectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of\n12.5% over base models and 9.6% over supervised fine-tuning across\nPhi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that\npre-trained LLMs possess latent reasoning capabilities that can be unlocked and\nenhanced through our proposed optimization approach in a self-improvement\nmanner. The code of LaTRO is available at\n\\url{https://github.com/SalesforceAIResearch/LaTRO}."
                },
                "authors": [
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Ricky Ho"
                    },
                    {
                        "name": "Phil Mui"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04282v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18060v2",
                "updated": "2024-11-21T19:43:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    19,
                    43,
                    0,
                    3,
                    326,
                    0
                ],
                "published": "2024-06-26T04:33:13Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    4,
                    33,
                    13,
                    2,
                    178,
                    0
                ],
                "title": "AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for\n  Memory-Efficient Large Language Models Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for\n  Memory-Efficient Large Language Models Fine-Tuning"
                },
                "summary": "Fine-tuning large language models (LLMs) has achieved remarkable performance\nacross various natural language processing tasks, yet it demands more and more\nmemory as model sizes keep growing. To address this issue, the recently\nproposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs\nusing only forward passes, thereby avoiding the need for a backpropagation\ngraph. However, significant performance drops and a high risk of divergence\nhave limited their widespread adoption. In this paper, we propose the Adaptive\nZeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed\nto improve the performance and convergence of the ZO methods. To enhance\ndimension-dependent ZO estimation accuracy, we introduce a fast-forward,\nlow-parameter tensorized adapter. To tackle the frequently observed divergence\nissue in large-scale ZO fine-tuning tasks, we propose an adaptive query number\nschedule that guarantees convergence. Detailed theoretical analysis and\nextensive experimental results on Roberta-Large and Llama-2-7B models\nsubstantiate the efficacy of our AdaZeta framework in terms of accuracy, memory\nefficiency, and convergence speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) has achieved remarkable performance\nacross various natural language processing tasks, yet it demands more and more\nmemory as model sizes keep growing. To address this issue, the recently\nproposed Memory-efficient Zeroth-order (MeZO) methods attempt to fine-tune LLMs\nusing only forward passes, thereby avoiding the need for a backpropagation\ngraph. However, significant performance drops and a high risk of divergence\nhave limited their widespread adoption. In this paper, we propose the Adaptive\nZeroth-order Tensor-Train Adaption (AdaZeta) framework, specifically designed\nto improve the performance and convergence of the ZO methods. To enhance\ndimension-dependent ZO estimation accuracy, we introduce a fast-forward,\nlow-parameter tensorized adapter. To tackle the frequently observed divergence\nissue in large-scale ZO fine-tuning tasks, we propose an adaptive query number\nschedule that guarantees convergence. Detailed theoretical analysis and\nextensive experimental results on Roberta-Large and Llama-2-7B models\nsubstantiate the efficacy of our AdaZeta framework in terms of accuracy, memory\nefficiency, and convergence speed."
                },
                "authors": [
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Kai Zhen"
                    },
                    {
                        "name": "Ershad Banijamal"
                    },
                    {
                        "name": "Athanasios Mouchtaris"
                    },
                    {
                        "name": "Zheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Zhang"
                },
                "author": "Zheng Zhang",
                "arxiv_comment": "Accepted for publication in EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14432v1",
                "updated": "2024-11-21T18:59:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    59,
                    55,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T18:59:55Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    59,
                    55,
                    3,
                    326,
                    0
                ],
                "title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) demonstrate enhanced capabilities and\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\nreasoning, high-quality long-chain reasoning data and optimized training\npipelines still remain inadequately explored in vision-language tasks. In this\npaper, we present Insight-V, an early effort to 1) scalably produce long and\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\ndata without human labor, we design a two-step pipeline with a progressive\nstrategy to generate sufficiently long and diverse reasoning paths and a\nmulti-granularity assessment method to ensure data quality. We observe that\ndirectly supervising MLLMs with such long and complex reasoning data will not\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\nsystem consisting of a reasoning agent dedicated to performing long-chain\nreasoning and a summary agent trained to judge and summarize reasoning results.\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\nand our stronger base MLLM, we demonstrate significant performance gains across\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\nour multi-agent system, Insight-V can also easily maintain or improve\nperformance on perception-focused multi-modal tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate enhanced capabilities and\nreliability by reasoning more, evolving from Chain-of-Thought prompting to\nproduct-level solutions like OpenAI o1. Despite various efforts to improve LLM\nreasoning, high-quality long-chain reasoning data and optimized training\npipelines still remain inadequately explored in vision-language tasks. In this\npaper, we present Insight-V, an early effort to 1) scalably produce long and\nrobust reasoning data for complex multi-modal tasks, and 2) an effective\ntraining pipeline to enhance the reasoning capabilities of multi-modal large\nlanguage models (MLLMs). Specifically, to create long and structured reasoning\ndata without human labor, we design a two-step pipeline with a progressive\nstrategy to generate sufficiently long and diverse reasoning paths and a\nmulti-granularity assessment method to ensure data quality. We observe that\ndirectly supervising MLLMs with such long and complex reasoning data will not\nyield ideal reasoning ability. To tackle this problem, we design a multi-agent\nsystem consisting of a reasoning agent dedicated to performing long-chain\nreasoning and a summary agent trained to judge and summarize reasoning results.\nWe further incorporate an iterative DPO algorithm to enhance the reasoning\nagent's generation stability and quality. Based on the popular LLaVA-NeXT model\nand our stronger base MLLM, we demonstrate significant performance gains across\nchallenging multi-modal benchmarks requiring visual reasoning. Benefiting from\nour multi-agent system, Insight-V can also easily maintain or improve\nperformance on perception-focused multi-modal tasks."
                },
                "authors": [
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Hai-Long Sun"
                    },
                    {
                        "name": "Jingkang Yang"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ziwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziwei Liu"
                },
                "author": "Ziwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18970v2",
                "updated": "2024-11-21T18:59:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    59,
                    45,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-24T17:59:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    59,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "ConceptDrift: Uncovering Biases through the Lens of Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConceptDrift: Uncovering Biases through the Lens of Foundation Models"
                },
                "summary": "An important goal of ML research is to identify and mitigate unwanted biases\nintrinsic to datasets and already incorporated into pre-trained models.\nPrevious approaches have identified biases using highly curated validation\nsubsets, that require human knowledge to create in the first place. This limits\nthe ability to automate the discovery of unknown biases in new datasets. We\nsolve this by using interpretable vision-language models, combined with a\nfiltration method using LLMs and known concept hierarchies. More exactly, for a\ndataset, we use pre-trained CLIP models that have an associated embedding for\neach class and see how it drifts through learning towards embeddings that\ndisclose hidden biases. We call this approach ConceptDrift and show that it can\nbe scaled to automatically identify biases in datasets like ImageNet without\nhuman prior knowledge. We propose two bias identification evaluation protocols\nto fill the gap in the previous work and show that our method significantly\nimproves over SoTA methods, both using our protocol and classical evaluations.\nAlongside validating the identified biases, we also show that they can be\nleveraged to improve the performance of different methods. Our method is not\nbounded to a single modality, and we empirically validate it both on image\n(Waterbirds, CelebA, ImageNet), and text datasets (CivilComments).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An important goal of ML research is to identify and mitigate unwanted biases\nintrinsic to datasets and already incorporated into pre-trained models.\nPrevious approaches have identified biases using highly curated validation\nsubsets, that require human knowledge to create in the first place. This limits\nthe ability to automate the discovery of unknown biases in new datasets. We\nsolve this by using interpretable vision-language models, combined with a\nfiltration method using LLMs and known concept hierarchies. More exactly, for a\ndataset, we use pre-trained CLIP models that have an associated embedding for\neach class and see how it drifts through learning towards embeddings that\ndisclose hidden biases. We call this approach ConceptDrift and show that it can\nbe scaled to automatically identify biases in datasets like ImageNet without\nhuman prior knowledge. We propose two bias identification evaluation protocols\nto fill the gap in the previous work and show that our method significantly\nimproves over SoTA methods, both using our protocol and classical evaluations.\nAlongside validating the identified biases, we also show that they can be\nleveraged to improve the performance of different methods. Our method is not\nbounded to a single modality, and we empirically validate it both on image\n(Waterbirds, CelebA, ImageNet), and text datasets (CivilComments)."
                },
                "authors": [
                    {
                        "name": "Cristian Daniel Păduraru"
                    },
                    {
                        "name": "Antonio Bărbălau"
                    },
                    {
                        "name": "Radu Filipescu"
                    },
                    {
                        "name": "Andrei Liviu Nicolicioiu"
                    },
                    {
                        "name": "Elena Burceanu"
                    }
                ],
                "author_detail": {
                    "name": "Elena Burceanu"
                },
                "author": "Elena Burceanu",
                "arxiv_comment": "8 pages, 4 figures, 6 tables, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14398v1",
                "updated": "2024-11-21T18:27:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    27,
                    25,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T18:27:25Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    27,
                    25,
                    3,
                    326,
                    0
                ],
                "title": "Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings"
                },
                "summary": "With the recent proliferation of large language models (LLMs), enterprises\nhave been able to rapidly develop proof-of-concepts and prototypes. As a\nresult, there is a growing need to implement robust guardrails that monitor,\nquantize and control an LLM's behavior, ensuring that the use is reliable,\nsafe, accurate and also aligned with the users' expectations. Previous\napproaches for filtering out inappropriate user prompts or system outputs, such\nas LlamaGuard and OpenAI's MOD API, have achieved significant success by\nfine-tuning existing LLMs. However, using fine-tuned LLMs as guardrails\nintroduces increased latency and higher maintenance costs, which may not be\npractical or scalable for cost-efficient deployments. We take a different\napproach, focusing on fine-tuning a lightweight architecture: Sentence-BERT.\nThis method reduces the model size from LlamaGuard's 7 billion parameters to\napproximately 67 million, while maintaining comparable performance on the AEGIS\nsafety benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent proliferation of large language models (LLMs), enterprises\nhave been able to rapidly develop proof-of-concepts and prototypes. As a\nresult, there is a growing need to implement robust guardrails that monitor,\nquantize and control an LLM's behavior, ensuring that the use is reliable,\nsafe, accurate and also aligned with the users' expectations. Previous\napproaches for filtering out inappropriate user prompts or system outputs, such\nas LlamaGuard and OpenAI's MOD API, have achieved significant success by\nfine-tuning existing LLMs. However, using fine-tuned LLMs as guardrails\nintroduces increased latency and higher maintenance costs, which may not be\npractical or scalable for cost-efficient deployments. We take a different\napproach, focusing on fine-tuning a lightweight architecture: Sentence-BERT.\nThis method reduces the model size from LlamaGuard's 7 billion parameters to\napproximately 67 million, while maintaining comparable performance on the AEGIS\nsafety benchmark."
                },
                "authors": [
                    {
                        "name": "Aaron Zheng"
                    },
                    {
                        "name": "Mansi Rana"
                    },
                    {
                        "name": "Andreas Stolcke"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Stolcke"
                },
                "author": "Andreas Stolcke",
                "arxiv_comment": "To appear in Proceedings of COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10079v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10079v3",
                "updated": "2024-11-21T17:58:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    58,
                    55,
                    3,
                    326,
                    0
                ],
                "published": "2024-06-14T14:35:58Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    14,
                    35,
                    58,
                    4,
                    166,
                    0
                ],
                "title": "Localizing Events in Videos with Multimodal Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing Events in Videos with Multimodal Queries"
                },
                "summary": "Localizing events in videos based on semantic queries is a pivotal task in\nvideo understanding, with the growing significance of user-oriented\napplications like video search. Yet, current research predominantly relies on\nnatural language queries (NLQs), overlooking the potential of using multimodal\nqueries (MQs) that integrate images to more flexibly represent semantic queries\n-- especially when it is difficult to express non-verbal or unfamiliar concepts\nin words. To bridge this gap, we introduce ICQ, a new benchmark designed for\nlocalizing events in videos with MQs, alongside an evaluation dataset\nICQ-Highlight. To accommodate and evaluate existing video localization models\nfor this new task, we propose 3 Multimodal Query Adaptation methods and a novel\nSurrogate Fine-tuning on pseudo-MQs strategy. ICQ systematically benchmarks 12\nstate-of-the-art backbone models, spanning from specialized video localization\nmodels to Video LLMs, across diverse application domains. Our experiments\nhighlight the high potential of MQs in real-world applications. We believe this\nbenchmark is a first step toward advancing MQs in video event localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing events in videos based on semantic queries is a pivotal task in\nvideo understanding, with the growing significance of user-oriented\napplications like video search. Yet, current research predominantly relies on\nnatural language queries (NLQs), overlooking the potential of using multimodal\nqueries (MQs) that integrate images to more flexibly represent semantic queries\n-- especially when it is difficult to express non-verbal or unfamiliar concepts\nin words. To bridge this gap, we introduce ICQ, a new benchmark designed for\nlocalizing events in videos with MQs, alongside an evaluation dataset\nICQ-Highlight. To accommodate and evaluate existing video localization models\nfor this new task, we propose 3 Multimodal Query Adaptation methods and a novel\nSurrogate Fine-tuning on pseudo-MQs strategy. ICQ systematically benchmarks 12\nstate-of-the-art backbone models, spanning from specialized video localization\nmodels to Video LLMs, across diverse application domains. Our experiments\nhighlight the high potential of MQs in real-world applications. We believe this\nbenchmark is a first step toward advancing MQs in video event localization."
                },
                "authors": [
                    {
                        "name": "Gengyuan Zhang"
                    },
                    {
                        "name": "Mang Ling Ada Fok"
                    },
                    {
                        "name": "Jialu Ma"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Daniel Cremers"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Jindong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Gu"
                },
                "author": "Jindong Gu",
                "arxiv_comment": "20 pages (including references and appendix); for the project\n  homepage, see https://icq-benchmark.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10079v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10079v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14512v2",
                "updated": "2024-11-21T17:48:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    48,
                    25,
                    3,
                    326,
                    0
                ],
                "published": "2024-08-25T04:32:45Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    4,
                    32,
                    45,
                    6,
                    238,
                    0
                ],
                "title": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings"
                },
                "summary": "Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors."
                },
                "authors": [
                    {
                        "name": "Duo Wang"
                    },
                    {
                        "name": "Yuan Zuo"
                    },
                    {
                        "name": "Fengzhi Li"
                    },
                    {
                        "name": "Junjie Wu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Wu"
                },
                "author": "Junjie Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14343v1",
                "updated": "2024-11-21T17:41:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    41,
                    8,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T17:41:08Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    17,
                    41,
                    8,
                    3,
                    326,
                    0
                ],
                "title": "UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs\n  on Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs\n  on Low-Resource Languages"
                },
                "summary": "Large language models (LLMs) under-perform on low-resource languages due to\nlimited training data. We present a method to efficiently collect text data for\nlow-resource languages from the entire Common Crawl corpus. Our approach,\nUnifiedCrawl, filters and extracts common crawl using minimal compute\nresources, yielding mono-lingual datasets much larger than previously available\nsources. We demonstrate that leveraging this data to fine-tuning multilingual\nLLMs via efficient adapter methods (QLoRA) significantly boosts performance on\nthe low-resource language, while minimizing VRAM usage. Our experiments show\nlarge improvements in language modeling perplexity and an increase in few-shot\nprompting scores. Our work and released source code provide an affordable\napproach to improve LLMs for low-resource languages using consumer hardware.\nOur source code is available here at\nhttps://github.com/bethelmelesse/unifiedcrawl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) under-perform on low-resource languages due to\nlimited training data. We present a method to efficiently collect text data for\nlow-resource languages from the entire Common Crawl corpus. Our approach,\nUnifiedCrawl, filters and extracts common crawl using minimal compute\nresources, yielding mono-lingual datasets much larger than previously available\nsources. We demonstrate that leveraging this data to fine-tuning multilingual\nLLMs via efficient adapter methods (QLoRA) significantly boosts performance on\nthe low-resource language, while minimizing VRAM usage. Our experiments show\nlarge improvements in language modeling perplexity and an increase in few-shot\nprompting scores. Our work and released source code provide an affordable\napproach to improve LLMs for low-resource languages using consumer hardware.\nOur source code is available here at\nhttps://github.com/bethelmelesse/unifiedcrawl."
                },
                "authors": [
                    {
                        "name": "Bethel Melesse Tessema"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Tae-Sun Chung"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Sun Chung"
                },
                "arxiv_affiliation": "Ajou University",
                "author": "Tae-Sun Chung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14303v1",
                "updated": "2024-11-21T16:56:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    56,
                    33,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:56:33Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    56,
                    33,
                    3,
                    326,
                    0
                ],
                "title": "Automated Generation of Code Debugging Exercises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Generation of Code Debugging Exercises"
                },
                "summary": "Debugging is an essential skill when learning to program, yet its instruction\nand emphasis often vary widely across introductory courses. In the era of\ncode-generating large language models (LLMs), the ability for students to\nreason about code and identify errors is increasingly important. However,\nstudents frequently resort to trial-and-error methods to resolve bugs without\nfully understanding the underlying issues. Developing the ability to identify\nand hypothesize the cause of bugs is crucial but can be time-consuming to teach\neffectively through traditional means. This paper introduces BugSpotter, an\ninnovative tool that leverages an LLM to generate buggy code from a problem\ndescription and verify the synthesized bugs via a test suite. Students interact\nwith BugSpotter by designing failing test cases, where the buggy code's output\ndiffers from the expected result as defined by the problem specification. This\nnot only provides opportunities for students to enhance their debugging skills,\nbut also to practice reading and understanding problem specifications. We\ndeployed BugSpotter in a large classroom setting and compared the debugging\nexercises it generated to exercises hand-crafted by an instructor for the same\nproblems. We found that the LLM-generated exercises produced by BugSpotter\nvaried in difficulty and were well-matched to the problem specifications.\nImportantly, the LLM-generated exercises were comparable to those manually\ncreated by instructors with respect to student performance, suggesting that\nBugSpotter could be an effective and efficient aid for learning debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging is an essential skill when learning to program, yet its instruction\nand emphasis often vary widely across introductory courses. In the era of\ncode-generating large language models (LLMs), the ability for students to\nreason about code and identify errors is increasingly important. However,\nstudents frequently resort to trial-and-error methods to resolve bugs without\nfully understanding the underlying issues. Developing the ability to identify\nand hypothesize the cause of bugs is crucial but can be time-consuming to teach\neffectively through traditional means. This paper introduces BugSpotter, an\ninnovative tool that leverages an LLM to generate buggy code from a problem\ndescription and verify the synthesized bugs via a test suite. Students interact\nwith BugSpotter by designing failing test cases, where the buggy code's output\ndiffers from the expected result as defined by the problem specification. This\nnot only provides opportunities for students to enhance their debugging skills,\nbut also to practice reading and understanding problem specifications. We\ndeployed BugSpotter in a large classroom setting and compared the debugging\nexercises it generated to exercises hand-crafted by an instructor for the same\nproblems. We found that the LLM-generated exercises produced by BugSpotter\nvaried in difficulty and were well-matched to the problem specifications.\nImportantly, the LLM-generated exercises were comparable to those manually\ncreated by instructors with respect to student performance, suggesting that\nBugSpotter could be an effective and efficient aid for learning debugging."
                },
                "authors": [
                    {
                        "name": "Victor-Alexandru Pădurean"
                    },
                    {
                        "name": "Paul Denny"
                    },
                    {
                        "name": "Adish Singla"
                    }
                ],
                "author_detail": {
                    "name": "Adish Singla"
                },
                "author": "Adish Singla",
                "arxiv_comment": "Preprint of the SIGCSE'25 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14299v1",
                "updated": "2024-11-21T16:50:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    50,
                    11,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T16:50:11Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    50,
                    11,
                    3,
                    326,
                    0
                ],
                "title": "Auto-SPICE: Leveraging LLMs for Dataset Creation via Automated SPICE\n  Netlist Extraction from Analog Circuit Diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-SPICE: Leveraging LLMs for Dataset Creation via Automated SPICE\n  Netlist Extraction from Analog Circuit Diagrams"
                },
                "summary": "Auto-SPICE is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-SPICE is the first fully automated framework leveraging large language\nmodels (LLMs) to generate Simulation Programs with Integrated Circuit Emphasis\n(SPICE) netlists. It addresses a long-standing challenge in automating netlist\ngeneration for analog circuits within circuit design automation. Automating\nthis workflow could accelerate the creation of finetuned LLMs for analog\ncircuit design and verification. We identify key challenges in this automation\nand evaluate the multi-modal capabilities of state-of-the-art LLMs,\nparticularly GPT-4, to address these issues. We propose a three-step workflow\nto overcome current limitations: labeling analog circuits, prompt tuning, and\nnetlist verification. This approach aims to create an end-to-end SPICE netlist\ngenerator from circuit schematic images, tackling the long-standing hurdle of\naccurate netlist generation. Our framework demonstrates significant performance\nimprovements, tested on approximately 2,100 schematics of varying complexity.\nWe open-source this solution for community-driven development."
                },
                "authors": [
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Vineet Bhat"
                    },
                    {
                        "name": "Yuheng He"
                    },
                    {
                        "name": "Siddharth Garg"
                    },
                    {
                        "name": "Hamed Rahmani"
                    },
                    {
                        "name": "Ramesh Karri"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Karri"
                },
                "author": "Ramesh Karri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13009v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13009v2",
                "updated": "2024-11-21T16:49:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    16,
                    49,
                    51,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-20T03:17:51Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    17,
                    51,
                    2,
                    325,
                    0
                ],
                "title": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts"
                },
                "summary": "As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods."
                },
                "authors": [
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13009v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]